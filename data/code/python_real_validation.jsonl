{"code":"def close(self) -> None:\n        \"\"\"Stop accepting new pipelinig messages and close\n        connection when handlers done processing messages\"\"\"\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()","return_type":"None","function_name":"RequestHandler.close","stripped_code":"def close(self):\n        \"\"\"Stop accepting new pipelinig messages and close\n        connection when handlers done processing messages\"\"\"\n        self._close = True\n        if self._waiter:\n            self._waiter.cancel()"}
{"code":"def try_parse_int(\n        s: str,\n        default: Optional[Any] = None,\n        minimum: Optional[int] = None,\n        maximum: Optional[int] = None) -> Optional[Any]:\n    \"\"\" Try parsing a string into an integer.\n        On failure, return `default`.\n        If the number is less then `minimum` or greater than `maximum`,\n        return `default`.\n        Returns an integer on success.\n    \"\"\"\n    try:\n        n = int(s)\n    except ValueError:\n        return default\n    if (minimum is not None) and (n < minimum):\n        return default\n    elif (maximum is not None) and (n > maximum):\n        return default\n    return n","return_type":"Optional[Any]","function_name":"try_parse_int","stripped_code":"def try_parse_int(\n        s: str,\n        default: Optional[Any] = None,\n        minimum: Optional[int] = None,\n        maximum: Optional[int] = None):\n    \"\"\" Try parsing a string into an integer.\n        On failure, return `default`.\n        If the number is less then `minimum` or greater than `maximum`,\n        return `default`.\n        Returns an integer on success.\n    \"\"\"\n    try:\n        n = int(s)\n    except ValueError:\n        return default\n    if (minimum is not None) and (n < minimum):\n        return default\n    elif (maximum is not None) and (n > maximum):\n        return default\n    return n"}
{"code":"def item_transaction(self, item) -> Transaction:\n        \"\"\"Begin transaction state for item.\n\n        A transaction state is exists to prevent writing out to disk, mainly for performance reasons.\n        All changes to the object are delayed until the transaction state exits.\n\n        This method is thread safe.\n        \"\"\"\n        items = self.__build_transaction_items(item)\n        transaction = Transaction(self, item, items)\n        self.__transactions.append(transaction)\n        return transaction","return_type":"Transaction","function_name":"TransactionManager.item_transaction","stripped_code":"def item_transaction(self, item):\n        \"\"\"Begin transaction state for item.\n\n        A transaction state is exists to prevent writing out to disk, mainly for performance reasons.\n        All changes to the object are delayed until the transaction state exits.\n\n        This method is thread safe.\n        \"\"\"\n        items = self.__build_transaction_items(item)\n        transaction = Transaction(self, item, items)\n        self.__transactions.append(transaction)\n        return transaction"}
{"code":"def next_state_fluent_ordering(self) -> List[str]:\n        '''The list of next state-fluent names in canonical order.\n\n        Returns:\n            List[str]: A list of fluent names.\n        '''\n        key = lambda x: x.name\n        return [cpf.name for cpf in sorted(self.state_cpfs, key=key)]","return_type":"List[str]","function_name":"Domain.next_state_fluent_ordering","stripped_code":"def next_state_fluent_ordering(self):\n        '''The list of next state-fluent names in canonical order.\n\n        Returns:\n            List[str]: A list of fluent names.\n        '''\n        key = lambda x: x.name\n        return [cpf.name for cpf in sorted(self.state_cpfs, key=key)]"}
{"code":"def _render_trajectories(self,\n            trajectories: Tuple[NonFluents, Fluents, Fluents, Fluents, np.array]) -> None:\n        '''Prints the first batch of simulated `trajectories`.\n\n        Args:\n            trajectories: NonFluents, states, actions, interms and rewards.\n        '''\n        if self._verbose:\n            non_fluents, initial_state, states, actions, interms, rewards = trajectories\n            shape = states[0][1].shape\n            batch_size, horizon, = shape[0], shape[1]\n            states = [(s[0], s[1][0]) for s in states]\n            interms = [(f[0], f[1][0]) for f in interms]\n            actions = [(a[0], a[1][0]) for a in actions]\n            rewards = np.reshape(rewards, [batch_size, horizon])[0]\n            self._render_batch(non_fluents, states, actions, interms, rewards)","return_type":"None","function_name":"GenericVisualizer._render_trajectories","stripped_code":"def _render_trajectories(self,\n            trajectories: Tuple[NonFluents, Fluents, Fluents, Fluents, np.array]):\n        '''Prints the first batch of simulated `trajectories`.\n\n        Args:\n            trajectories: NonFluents, states, actions, interms and rewards.\n        '''\n        if self._verbose:\n            non_fluents, initial_state, states, actions, interms, rewards = trajectories\n            shape = states[0][1].shape\n            batch_size, horizon, = shape[0], shape[1]\n            states = [(s[0], s[1][0]) for s in states]\n            interms = [(f[0], f[1][0]) for f in interms]\n            actions = [(a[0], a[1][0]) for a in actions]\n            rewards = np.reshape(rewards, [batch_size, horizon])[0]\n            self._render_batch(non_fluents, states, actions, interms, rewards)"}
{"code":"def _is_dataclass(node: astroid.ClassDef) -> bool:\n    \"\"\"Check if a class definition defines a Python 3.7+ dataclass\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents a dataclass class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    if not node.decorators:\n        return False\n\n    root_locals = node.root().locals\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, astroid.Call):\n            decorator = decorator.func\n        if not isinstance(decorator, (astroid.Name, astroid.Attribute)):\n            continue\n        if isinstance(decorator, astroid.Name):\n            name = decorator.name\n        else:\n            name = decorator.attrname\n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals:\n            return True\n    return False","return_type":"bool","function_name":"_is_dataclass","stripped_code":"def _is_dataclass(node: astroid.ClassDef):\n    \"\"\"Check if a class definition defines a Python 3.7+ dataclass\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents a dataclass class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    if not node.decorators:\n        return False\n\n    root_locals = node.root().locals\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, astroid.Call):\n            decorator = decorator.func\n        if not isinstance(decorator, (astroid.Name, astroid.Attribute)):\n            continue\n        if isinstance(decorator, astroid.Name):\n            name = decorator.name\n        else:\n            name = decorator.attrname\n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals:\n            return True\n    return False"}
{"code":"def parse_stack_refs(stack_references: List[str]) -> List[str]:\n    '''\n    Check if items included in `stack_references` are Senza definition\n    file paths or stack name reference. If Senza definition file path,\n    substitute the definition file path by the stack name in the same\n    position on the list.\n    '''\n    stack_names = []\n    references = list(stack_references)\n    references.reverse()\n    while references:\n        current = references.pop()\n        # current that might be a file\n        file_path = os.path.abspath(current)\n        if os.path.exists(file_path) and os.path.isfile(file_path):\n            try:\n                with open(file_path) as fd:\n                    data = yaml.safe_load(fd)\n                current = data['SenzaInfo']['StackName']\n            except (KeyError, TypeError, YAMLError):\n                raise click.UsageError(\n                    'Invalid senza definition {}'.format(current)\n                )\n        stack_names.append(current)\n    return stack_names","return_type":"List[str]","function_name":"parse_stack_refs","stripped_code":"def parse_stack_refs(stack_references: List[str]):\n    '''\n    Check if items included in `stack_references` are Senza definition\n    file paths or stack name reference. If Senza definition file path,\n    substitute the definition file path by the stack name in the same\n    position on the list.\n    '''\n    stack_names = []\n    references = list(stack_references)\n    references.reverse()\n    while references:\n        current = references.pop()\n        # current that might be a file\n        file_path = os.path.abspath(current)\n        if os.path.exists(file_path) and os.path.isfile(file_path):\n            try:\n                with open(file_path) as fd:\n                    data = yaml.safe_load(fd)\n                current = data['SenzaInfo']['StackName']\n            except (KeyError, TypeError, YAMLError):\n                raise click.UsageError(\n                    'Invalid senza definition {}'.format(current)\n                )\n        stack_names.append(current)\n    return stack_names"}
{"code":"def current() -> 'Process':\n        \"\"\"\n        Returns the instance of the process that is executing at the current moment.\n        \"\"\"\n        curr = greenlet.getcurrent()\n        if not isinstance(curr, Process):\n            raise TypeError(\"Current greenlet does not correspond to a Process instance.\")\n        return cast(Process, greenlet.getcurrent())","return_type":"'Process'","function_name":"Process.current","stripped_code":"def current():\n        \"\"\"\n        Returns the instance of the process that is executing at the current moment.\n        \"\"\"\n        curr = greenlet.getcurrent()\n        if not isinstance(curr, Process):\n            raise TypeError(\"Current greenlet does not correspond to a Process instance.\")\n        return cast(Process, greenlet.getcurrent())"}
{"code":"def write_desc(self) -> None:\n        \"\"\" Writes a description of the model to the exp_dir. \"\"\"\n\n        path = os.path.join(self.exp_dir, \"model_description.txt\")\n        with open(path, \"w\") as desc_f:\n            for key, val in self.__dict__.items():\n                print(\"%s=%s\" % (key, val), file=desc_f)\n\n        import json\n        json_path = os.path.join(self.exp_dir, \"model_description.json\")\n        desc = { } #type: Dict[str, Any]\n        # For use in decoding from a saved model\n        desc[\"topology\"] = {\n            \"batch_x_name\" : self.batch_x.name, #type: ignore\n            \"batch_x_lens_name\" : self.batch_x_lens.name, #type: ignore\n            \"dense_decoded_name\" : self.dense_decoded.name #type: ignore\n        }\n        desc[\"model_type\"] = str(self.__class__)\n        for key, val in self.__dict__.items():\n            if isinstance(val, int):\n                desc[str(key)] = val\n            elif isinstance(val, tf.Tensor):\n                desc[key] = {\n                    \"type\": \"tf.Tensor\",\n                    \"name\": val.name, #type: ignore\n                    \"shape\": str(val.shape), #type: ignore\n                    \"dtype\" : str(val.dtype), #type: ignore\n                    \"value\" : str(val),\n                }\n            elif isinstance(val, tf.SparseTensor): #type: ignore\n                desc[key] = {\n                    \"type\": \"tf.SparseTensor\",\n                    \"value\": str(val), #type: ignore\n                }\n            else:\n                desc[str(key)] = str(val)\n        with open(json_path, \"w\") as json_desc_f:\n            json.dump(desc, json_desc_f, skipkeys=True)","return_type":"None","function_name":"Model.write_desc","stripped_code":"def write_desc(self):\n        \"\"\" Writes a description of the model to the exp_dir. \"\"\"\n\n        path = os.path.join(self.exp_dir, \"model_description.txt\")\n        with open(path, \"w\") as desc_f:\n            for key, val in self.__dict__.items():\n                print(\"%s=%s\" % (key, val), file=desc_f)\n\n        import json\n        json_path = os.path.join(self.exp_dir, \"model_description.json\")\n        desc = { } #type: Dict[str, Any]\n        # For use in decoding from a saved model\n        desc[\"topology\"] = {\n            \"batch_x_name\" : self.batch_x.name, #type: ignore\n            \"batch_x_lens_name\" : self.batch_x_lens.name, #type: ignore\n            \"dense_decoded_name\" : self.dense_decoded.name #type: ignore\n        }\n        desc[\"model_type\"] = str(self.__class__)\n        for key, val in self.__dict__.items():\n            if isinstance(val, int):\n                desc[str(key)] = val\n            elif isinstance(val, tf.Tensor):\n                desc[key] = {\n                    \"type\": \"tf.Tensor\",\n                    \"name\": val.name, #type: ignore\n                    \"shape\": str(val.shape), #type: ignore\n                    \"dtype\" : str(val.dtype), #type: ignore\n                    \"value\" : str(val),\n                }\n            elif isinstance(val, tf.SparseTensor): #type: ignore\n                desc[key] = {\n                    \"type\": \"tf.SparseTensor\",\n                    \"value\": str(val), #type: ignore\n                }\n            else:\n                desc[str(key)] = str(val)\n        with open(json_path, \"w\") as json_desc_f:\n            json.dump(desc, json_desc_f, skipkeys=True)"}
{"code":"def require_auth(request: Request, exceptions: bool=True) -> User:\n    \"\"\"\n    Returns authenticated User.\n    :param request: HttpRequest\n    :param exceptions: Raise (NotAuthenticated) exception. Default is True.\n    :return: User\n    \"\"\"\n    if not request.user or not request.user.is_authenticated:\n        if exceptions:\n            raise NotAuthenticated()\n        return None\n    return request.user","return_type":"User","function_name":"require_auth","stripped_code":"def require_auth(request: Request, exceptions: bool=True):\n    \"\"\"\n    Returns authenticated User.\n    :param request: HttpRequest\n    :param exceptions: Raise (NotAuthenticated) exception. Default is True.\n    :return: User\n    \"\"\"\n    if not request.user or not request.user.is_authenticated:\n        if exceptions:\n            raise NotAuthenticated()\n        return None\n    return request.user"}
{"code":"def flush(self) -> None:\n        \"\"\"\n        To act as a file.\n        \"\"\"\n        self.underlying_stream.flush()\n        self.file.flush()\n        os.fsync(self.file.fileno())","return_type":"None","function_name":"TeeContextManager.flush","stripped_code":"def flush(self):\n        \"\"\"\n        To act as a file.\n        \"\"\"\n        self.underlying_stream.flush()\n        self.file.flush()\n        os.fsync(self.file.fileno())"}
{"code":"def memory_write(self, offset: int, data: List[Union[int, BitVec]]) -> None:\n        \"\"\"Writes data to memory starting at offset.\n\n        :param offset:\n        :param data:\n        \"\"\"\n        self.mem_extend(offset, len(data))\n        self.memory[offset : offset + len(data)] = data","return_type":"None","function_name":"MachineState.memory_write","stripped_code":"def memory_write(self, offset: int, data: List[Union[int, BitVec]]):\n        \"\"\"Writes data to memory starting at offset.\n\n        :param offset:\n        :param data:\n        \"\"\"\n        self.mem_extend(offset, len(data))\n        self.memory[offset : offset + len(data)] = data"}
{"code":"def insert(self, index: int, item: object) -> None:\n        \"\"\"\n        The Abstract class `MutableSequence` leverages this insert method to\n        perform the `BlueprintGroup.append` operation.\n\n        :param index: Index to use for removing a new Blueprint item\n        :param item: New `Blueprint` object.\n        :return: None\n        \"\"\"\n        self._blueprints.insert(index, item)","return_type":"None","function_name":"BlueprintGroup.insert","stripped_code":"def insert(self, index: int, item: object):\n        \"\"\"\n        The Abstract class `MutableSequence` leverages this insert method to\n        perform the `BlueprintGroup.append` operation.\n\n        :param index: Index to use for removing a new Blueprint item\n        :param item: New `Blueprint` object.\n        :return: None\n        \"\"\"\n        self._blueprints.insert(index, item)"}
{"code":"def grab_earliest(self, timeout: float=None) -> typing.List[DataAndMetadata.DataAndMetadata]:\n        \"\"\"Grab the earliest data from the buffer, blocking until one is available.\"\"\"\n        timeout = timeout if timeout is not None else 10.0\n        with self.__buffer_lock:\n            if len(self.__buffer) == 0:\n                done_event = threading.Event()\n                self.__done_events.append(done_event)\n                self.__buffer_lock.release()\n                done = done_event.wait(timeout)\n                self.__buffer_lock.acquire()\n                if not done:\n                    raise Exception(\"Could not grab latest.\")\n            return self.__buffer.pop(0)","return_type":"typing.List[DataAndMetadata.DataAndMetadata]","function_name":"DataChannelBuffer.grab_earliest","stripped_code":"def grab_earliest(self, timeout: float=None):\n        \"\"\"Grab the earliest data from the buffer, blocking until one is available.\"\"\"\n        timeout = timeout if timeout is not None else 10.0\n        with self.__buffer_lock:\n            if len(self.__buffer) == 0:\n                done_event = threading.Event()\n                self.__done_events.append(done_event)\n                self.__buffer_lock.release()\n                done = done_event.wait(timeout)\n                self.__buffer_lock.acquire()\n                if not done:\n                    raise Exception(\"Could not grab latest.\")\n            return self.__buffer.pop(0)"}
{"code":"def forward(self,  # pylint: disable=arguments-differ\n                inputs: torch.Tensor,\n                mask: torch.LongTensor) -> torch.Tensor:\n        \"\"\"\n        Parameters\n        ----------\n        inputs : ``torch.Tensor``, required.\n            A Tensor of shape ``(batch_size, sequence_length, hidden_size)``.\n        mask : ``torch.LongTensor``, required.\n            A binary mask of shape ``(batch_size, sequence_length)`` representing the\n            non-padded elements in each sequence in the batch.\n\n        Returns\n        -------\n        A ``torch.Tensor`` of shape (num_layers, batch_size, sequence_length, hidden_size),\n        where the num_layers dimension represents the LSTM output from that layer.\n        \"\"\"\n        batch_size, total_sequence_length = mask.size()\n        stacked_sequence_output, final_states, restoration_indices = \\\n            self.sort_and_run_forward(self._lstm_forward, inputs, mask)\n\n        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n        if num_valid < batch_size:\n            zeros = stacked_sequence_output.new_zeros(num_layers,\n                                                      batch_size - num_valid,\n                                                      returned_timesteps,\n                                                      encoder_dim)\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n\n            # The states also need to have invalid rows added back.\n            new_states = []\n            for state in final_states:\n                state_dim = state.size(-1)\n                zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)\n                new_states.append(torch.cat([state, zeros], 1))\n            final_states = new_states\n\n        # It's possible to need to pass sequences which are padded to longer than the\n        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n        # the sequences mean that the returned tensor won't include these dimensions, because\n        # the RNN did not need to process them. We add them back on in the form of zeros here.\n        sequence_length_difference = total_sequence_length - returned_timesteps\n        if sequence_length_difference > 0:\n            zeros = stacked_sequence_output.new_zeros(num_layers,\n                                                      batch_size,\n                                                      sequence_length_difference,\n                                                      stacked_sequence_output[0].size(-1))\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n\n        self._update_states(final_states, restoration_indices)\n\n        # Restore the original indices and return the sequence.\n        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n        return stacked_sequence_output.index_select(1, restoration_indices)","return_type":"torch.Tensor","function_name":"ElmoLstm.forward","stripped_code":"def forward(self,  # pylint: disable=arguments-differ\n                inputs: torch.Tensor,\n                mask: torch.LongTensor):\n        \"\"\"\n        Parameters\n        ----------\n        inputs : ``torch.Tensor``, required.\n            A Tensor of shape ``(batch_size, sequence_length, hidden_size)``.\n        mask : ``torch.LongTensor``, required.\n            A binary mask of shape ``(batch_size, sequence_length)`` representing the\n            non-padded elements in each sequence in the batch.\n\n        Returns\n        -------\n        A ``torch.Tensor`` of shape (num_layers, batch_size, sequence_length, hidden_size),\n        where the num_layers dimension represents the LSTM output from that layer.\n        \"\"\"\n        batch_size, total_sequence_length = mask.size()\n        stacked_sequence_output, final_states, restoration_indices = \\\n            self.sort_and_run_forward(self._lstm_forward, inputs, mask)\n\n        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()\n        # Add back invalid rows which were removed in the call to sort_and_run_forward.\n        if num_valid < batch_size:\n            zeros = stacked_sequence_output.new_zeros(num_layers,\n                                                      batch_size - num_valid,\n                                                      returned_timesteps,\n                                                      encoder_dim)\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)\n\n            # The states also need to have invalid rows added back.\n            new_states = []\n            for state in final_states:\n                state_dim = state.size(-1)\n                zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)\n                new_states.append(torch.cat([state, zeros], 1))\n            final_states = new_states\n\n        # It's possible to need to pass sequences which are padded to longer than the\n        # max length of the sequence to a Seq2StackEncoder. However, packing and unpacking\n        # the sequences mean that the returned tensor won't include these dimensions, because\n        # the RNN did not need to process them. We add them back on in the form of zeros here.\n        sequence_length_difference = total_sequence_length - returned_timesteps\n        if sequence_length_difference > 0:\n            zeros = stacked_sequence_output.new_zeros(num_layers,\n                                                      batch_size,\n                                                      sequence_length_difference,\n                                                      stacked_sequence_output[0].size(-1))\n            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)\n\n        self._update_states(final_states, restoration_indices)\n\n        # Restore the original indices and return the sequence.\n        # Has shape (num_layers, batch_size, sequence_length, hidden_size)\n        return stacked_sequence_output.index_select(1, restoration_indices)"}
{"code":"def translate_pname(self, pname: PrefName, mid: ModuleId) -> QualName:\n        \"\"\"Translate a prefixed name to a qualified name.\n        Args:\n            pname: Name with an optional prefix.\n            mid: Identifier of the module in which `pname` appears.\n        Raises:\n            ModuleNotRegistered: If `mid` is not registered in the data model.\n            UnknownPrefix: If the prefix specified in `pname` is not declared.\n        \"\"\"\n        loc, nid = self.resolve_pname(pname, mid)\n        return (loc, self.namespace(nid))","return_type":"QualName","function_name":"SchemaData.translate_pname","stripped_code":"def translate_pname(self, pname: PrefName, mid: ModuleId):\n        \"\"\"Translate a prefixed name to a qualified name.\n        Args:\n            pname: Name with an optional prefix.\n            mid: Identifier of the module in which `pname` appears.\n        Raises:\n            ModuleNotRegistered: If `mid` is not registered in the data model.\n            UnknownPrefix: If the prefix specified in `pname` is not declared.\n        \"\"\"\n        loc, nid = self.resolve_pname(pname, mid)\n        return (loc, self.namespace(nid))"}
{"code":"def _unbox_scalar(\n            self,\n            value: Union[Period, Timestamp, Timedelta, NaTType],\n    ) -> int:\n        \"\"\"\n        Unbox the integer value of a scalar `value`.\n\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\n        10000000000\n        \"\"\"\n        raise AbstractMethodError(self)","return_type":"int","function_name":"AttributesMixin._unbox_scalar","stripped_code":"def _unbox_scalar(\n            self,\n            value: Union[Period, Timestamp, Timedelta, NaTType],\n    ):\n        \"\"\"\n        Unbox the integer value of a scalar `value`.\n\n        Parameters\n        ----------\n        value : Union[Period, Timestamp, Timedelta]\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP\n        10000000000\n        \"\"\"\n        raise AbstractMethodError(self)"}
{"code":"def make_blank_pdf(filename: str, paper: str = \"A4\") -> None:\n    \"\"\"\n    NOT USED.\n    Makes a blank single-page PDF, using ImageMagick's ``convert``.\n    \"\"\"\n    # https://unix.stackexchange.com/questions/277892/how-do-i-create-a-blank-pdf-from-the-command-line  # noqa\n    require(CONVERT, HELP_MISSING_IMAGEMAGICK)\n    run([CONVERT, \"xc:none\", \"-page\", paper, filename])","return_type":"None","function_name":"make_blank_pdf","stripped_code":"def make_blank_pdf(filename: str, paper: str = \"A4\"):\n    \"\"\"\n    NOT USED.\n    Makes a blank single-page PDF, using ImageMagick's ``convert``.\n    \"\"\"\n    # https://unix.stackexchange.com/questions/277892/how-do-i-create-a-blank-pdf-from-the-command-line  # noqa\n    require(CONVERT, HELP_MISSING_IMAGEMAGICK)\n    run([CONVERT, \"xc:none\", \"-page\", paper, filename])"}
{"code":"def print_setting(self) -> str:\n        \"\"\"\n        Presents the QSE settings as a string.\n\n        :return: The formatted settings of the QSE instance\n        \"\"\"\n        ret = \"\\n\"\n        ret += \"==================== Setting of {} ============================\\n\".format(self.configuration['name'])\n        ret += \"{}\".format(self.setting)\n        ret += \"===============================================================\\n\"\n        ret += \"{}\".format(self._var_form.setting)\n        ret += \"===============================================================\\n\"\n        return ret","return_type":"str","function_name":"QSE.print_setting","stripped_code":"def print_setting(self):\n        \"\"\"\n        Presents the QSE settings as a string.\n\n        :return: The formatted settings of the QSE instance\n        \"\"\"\n        ret = \"\\n\"\n        ret += \"==================== Setting of {} ============================\\n\".format(self.configuration['name'])\n        ret += \"{}\".format(self.setting)\n        ret += \"===============================================================\\n\"\n        ret += \"{}\".format(self._var_form.setting)\n        ret += \"===============================================================\\n\"\n        return ret"}
{"code":"def ping(self, data: bytes = b\"\") -> None:\n        \"\"\"Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``ping_interval`` argument to\n        `websocket_connect` instead of sending pings manually.\n\n        .. versionadded:: 5.1\n\n        \"\"\"\n        data = utf8(data)\n        if self.protocol is None:\n            raise WebSocketClosedError()\n        self.protocol.write_ping(data)","return_type":"None","function_name":"WebSocketClientConnection.ping","stripped_code":"def ping(self, data: bytes = b\"\"):\n        \"\"\"Send ping frame to the remote end.\n\n        The data argument allows a small amount of data (up to 125\n        bytes) to be sent as a part of the ping message. Note that not\n        all websocket implementations expose this data to\n        applications.\n\n        Consider using the ``ping_interval`` argument to\n        `websocket_connect` instead of sending pings manually.\n\n        .. versionadded:: 5.1\n\n        \"\"\"\n        data = utf8(data)\n        if self.protocol is None:\n            raise WebSocketClosedError()\n        self.protocol.write_ping(data)"}
{"code":"def to_json_(self) -> str:\n        \"\"\"Convert the main dataframe to json\n\n        :return: json data\n        :rtype: str\n\n        :example: ``ds.to_json_()``\n        \"\"\"\n        try:\n            renderer = pytablewriter.JsonTableWriter\n            data = self._build_export(renderer)\n            return data\n        except Exception as e:\n            self.err(e, \"Can not convert data to json\")","return_type":"str","function_name":"Export.to_json_","stripped_code":"def to_json_(self):\n        \"\"\"Convert the main dataframe to json\n\n        :return: json data\n        :rtype: str\n\n        :example: ``ds.to_json_()``\n        \"\"\"\n        try:\n            renderer = pytablewriter.JsonTableWriter\n            data = self._build_export(renderer)\n            return data\n        except Exception as e:\n            self.err(e, \"Can not convert data to json\")"}
{"code":"def from_JASCO(filepath, name=None, parent=None, verbose=True) -> Data:\n    \"\"\"Create a data object from JASCO UV-Vis spectrometers.\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to .txt file.\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Name to give to the created data object. If None, filename is used.\n        Default is None.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    data\n        New data object(s).\n    \"\"\"\n    # parse filepath\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    if not \".txt\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".txt\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create data\n    kwargs = {\"name\": name, \"kind\": \"JASCO\", \"source\": filestr}\n    if parent is None:\n        data = Data(**kwargs)\n    else:\n        data = parent.create_data(**kwargs)\n    # array\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    arr = np.genfromtxt(f, skip_header=18).T\n    f.close()\n\n    # chew through all scans\n    data.create_variable(name=\"energy\", values=arr[0], units=\"nm\")\n    data.create_channel(name=\"signal\", values=arr[1])\n    data.transform(\"energy\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  range: {0} to {1} (nm)\".format(data.energy[0], data.energy[-1]))\n        print(\"  size: {0}\".format(data.size))\n    return data","return_type":"Data","function_name":"from_JASCO","stripped_code":"def from_JASCO(filepath, name=None, parent=None, verbose=True):\n    \"\"\"Create a data object from JASCO UV-Vis spectrometers.\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to .txt file.\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Name to give to the created data object. If None, filename is used.\n        Default is None.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    data\n        New data object(s).\n    \"\"\"\n    # parse filepath\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    if not \".txt\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".txt\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create data\n    kwargs = {\"name\": name, \"kind\": \"JASCO\", \"source\": filestr}\n    if parent is None:\n        data = Data(**kwargs)\n    else:\n        data = parent.create_data(**kwargs)\n    # array\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    arr = np.genfromtxt(f, skip_header=18).T\n    f.close()\n\n    # chew through all scans\n    data.create_variable(name=\"energy\", values=arr[0], units=\"nm\")\n    data.create_channel(name=\"signal\", values=arr[1])\n    data.transform(\"energy\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  range: {0} to {1} (nm)\".format(data.energy[0], data.energy[-1]))\n        print(\"  size: {0}\".format(data.size))\n    return data"}
{"code":"def search_by_attribute_set(\n        url: str,\n        profile: Tuple[str],\n        limit: Optional[int] = 100,\n        namespace_filter: Optional[str]=None) -> Dict:\n    \"\"\"\n    Given a list of phenotypes, returns a ranked list of individuals\n    individuals can be filtered by namespace, eg MONDO, MGI, HGNC\n    :returns Dict with the structure: {\n        'unresolved' : [...]\n        'query_IRIs' : [...]\n        'results': {...}\n    }\n    :raises JSONDecodeError: If the response body does not contain valid json.\n    \"\"\"\n    owlsim_url = url + 'searchByAttributeSet'\n\n    params = {\n        'a': profile,\n        'limit': limit,\n        'target': namespace_filter\n    }\n    return requests.get(owlsim_url, params=params, timeout=TIMEOUT).json()","return_type":"Dict","function_name":"search_by_attribute_set","stripped_code":"def search_by_attribute_set(\n        url: str,\n        profile: Tuple[str],\n        limit: Optional[int] = 100,\n        namespace_filter: Optional[str]=None):\n    \"\"\"\n    Given a list of phenotypes, returns a ranked list of individuals\n    individuals can be filtered by namespace, eg MONDO, MGI, HGNC\n    :returns Dict with the structure: {\n        'unresolved' : [...]\n        'query_IRIs' : [...]\n        'results': {...}\n    }\n    :raises JSONDecodeError: If the response body does not contain valid json.\n    \"\"\"\n    owlsim_url = url + 'searchByAttributeSet'\n\n    params = {\n        'a': profile,\n        'limit': limit,\n        'target': namespace_filter\n    }\n    return requests.get(owlsim_url, params=params, timeout=TIMEOUT).json()"}
{"code":"def build(args) -> None:\n    \"\"\"Build using CMake\"\"\"\n    venv_exe = shutil.which('virtualenv')\n    pyexe = shutil.which(args.pyexe)\n    if not venv_exe:\n        logging.warn(\"virtualenv wasn't found in path, it's recommended to install virtualenv to manage python environments\")\n    if not pyexe:\n        logging.warn(\"Python executable %s not found in path\", args.pyexe)\n    if args.cmake_options:\n        cmake = CMake(args.cmake_options)\n    else:\n        cmake = CMake()\n    cmake()\n    create_virtualenv(venv_exe, pyexe, args.venv)","return_type":"None","function_name":"build","stripped_code":"def build(args):\n    \"\"\"Build using CMake\"\"\"\n    venv_exe = shutil.which('virtualenv')\n    pyexe = shutil.which(args.pyexe)\n    if not venv_exe:\n        logging.warn(\"virtualenv wasn't found in path, it's recommended to install virtualenv to manage python environments\")\n    if not pyexe:\n        logging.warn(\"Python executable %s not found in path\", args.pyexe)\n    if args.cmake_options:\n        cmake = CMake(args.cmake_options)\n    else:\n        cmake = CMake()\n    cmake()\n    create_virtualenv(venv_exe, pyexe, args.venv)"}
{"code":"def _handle_system_status_event(self, event: SystemStatusEvent) -> None:\n        \"\"\"\n        DISARMED -> ARMED_AWAY -> EXIT_DELAY_START -> EXIT_DELAY_END\n         (trip): -> ALARM -> OUTPUT_ON -> ALARM_RESTORE\n            (disarm): -> DISARMED -> OUTPUT_OFF\n         (disarm): -> DISARMED\n         (disarm before EXIT_DELAY_END): -> DISARMED -> EXIT_DELAY_END\n\n        TODO(NW): Check ALARM_RESTORE state transition to move back into ARMED_AWAY state\n        \"\"\"\n        if event.type == SystemStatusEvent.EventType.UNSEALED:\n            return self._update_zone(event.zone, True)\n        elif event.type == SystemStatusEvent.EventType.SEALED:\n            return self._update_zone(event.zone, False)\n        elif event.type == SystemStatusEvent.EventType.ALARM:\n            return self._update_arming_state(ArmingState.TRIGGERED)\n        elif event.type == SystemStatusEvent.EventType.ALARM_RESTORE:\n            if self.arming_state != ArmingState.DISARMED:\n                return self._update_arming_state(ArmingState.ARMED)\n        elif event.type == SystemStatusEvent.EventType.ENTRY_DELAY_START:\n            return self._update_arming_state(ArmingState.ENTRY_DELAY)\n        elif event.type == SystemStatusEvent.EventType.ENTRY_DELAY_END:\n            pass\n        elif event.type == SystemStatusEvent.EventType.EXIT_DELAY_START:\n            return self._update_arming_state(ArmingState.EXIT_DELAY)\n        elif event.type == SystemStatusEvent.EventType.EXIT_DELAY_END:\n            # Exit delay finished - if we were in the process of arming update\n            # state to armed\n            if self.arming_state == ArmingState.EXIT_DELAY:\n                return self._update_arming_state(ArmingState.ARMED)\n        elif event.type in Alarm.ARM_EVENTS:\n            return self._update_arming_state(ArmingState.ARMING)\n        elif event.type == SystemStatusEvent.EventType.DISARMED:\n            return self._update_arming_state(ArmingState.DISARMED)\n        elif event.type == SystemStatusEvent.EventType.ARMING_DELAYED:\n            pass","return_type":"None","function_name":"Alarm._handle_system_status_event","stripped_code":"def _handle_system_status_event(self, event: SystemStatusEvent):\n        \"\"\"\n        DISARMED -> ARMED_AWAY -> EXIT_DELAY_START -> EXIT_DELAY_END\n         (trip): -> ALARM -> OUTPUT_ON -> ALARM_RESTORE\n            (disarm): -> DISARMED -> OUTPUT_OFF\n         (disarm): -> DISARMED\n         (disarm before EXIT_DELAY_END): -> DISARMED -> EXIT_DELAY_END\n\n        TODO(NW): Check ALARM_RESTORE state transition to move back into ARMED_AWAY state\n        \"\"\"\n        if event.type == SystemStatusEvent.EventType.UNSEALED:\n            return self._update_zone(event.zone, True)\n        elif event.type == SystemStatusEvent.EventType.SEALED:\n            return self._update_zone(event.zone, False)\n        elif event.type == SystemStatusEvent.EventType.ALARM:\n            return self._update_arming_state(ArmingState.TRIGGERED)\n        elif event.type == SystemStatusEvent.EventType.ALARM_RESTORE:\n            if self.arming_state != ArmingState.DISARMED:\n                return self._update_arming_state(ArmingState.ARMED)\n        elif event.type == SystemStatusEvent.EventType.ENTRY_DELAY_START:\n            return self._update_arming_state(ArmingState.ENTRY_DELAY)\n        elif event.type == SystemStatusEvent.EventType.ENTRY_DELAY_END:\n            pass\n        elif event.type == SystemStatusEvent.EventType.EXIT_DELAY_START:\n            return self._update_arming_state(ArmingState.EXIT_DELAY)\n        elif event.type == SystemStatusEvent.EventType.EXIT_DELAY_END:\n            # Exit delay finished - if we were in the process of arming update\n            # state to armed\n            if self.arming_state == ArmingState.EXIT_DELAY:\n                return self._update_arming_state(ArmingState.ARMED)\n        elif event.type in Alarm.ARM_EVENTS:\n            return self._update_arming_state(ArmingState.ARMING)\n        elif event.type == SystemStatusEvent.EventType.DISARMED:\n            return self._update_arming_state(ArmingState.DISARMED)\n        elif event.type == SystemStatusEvent.EventType.ARMING_DELAYED:\n            pass"}
{"code":"def rewrite_relationships(oldobj: object,\n                          newobj: object,\n                          objmap: Dict[object, object],\n                          debug: bool = False,\n                          skip_table_names: List[str] = None) -> None:\n    \"\"\"\n    A utility function only.\n    Used in copying objects between SQLAlchemy sessions.\n\n    Both ``oldobj`` and ``newobj`` are SQLAlchemy instances. The instance\n    ``newobj`` is already a copy of ``oldobj`` but we wish to rewrite its\n    relationships, according to the map ``objmap``, which maps old to new\n    objects.\n\n    For example:\n\n    - Suppose a source session has a Customer record and a Sale record\n      containing ``sale.customer_id``, a foreign key to Customer.\n\n    - We may have corresponding Python SQLAlchemy ORM objects\n      ``customer_1_src`` and ``sale_1_src``.\n\n    - We copy them into a destination database, where their Python ORM objects\n      are ``customer_1_dest`` and ``sale_1_dest``.\n\n    - In the process we set up an object map looking like:\n\n      .. code-block:: none\n\n        Old session     New session\n        -------------------------------\n        customer_1_src  customer_1_dest\n        sale_1_src      sale_1_dest\n\n    - Now, we wish to make ``sale_1_dest`` have a relationship to\n      ``customer_1_dest``, in the same way that ``sale_1_src`` has a\n      relationship to ``customer_1_src``. This function will modify\n      ``sale_1_dest`` accordingly, given this object map. It will observe that\n      ``sale_1_src`` (here ``oldobj``) has a relationship to\n      ``customer_1_src``; it will note that ``objmap`` maps ``customer_1_src``\n      to ``customer_1_dest``; it will create the relationship from\n      ``sale_1_dest`` (here ``newobj``) to ``customer_1_dest``.\n\n    Args:\n        oldobj: SQLAlchemy ORM object to read from\n\n        newobj: SQLAlchemy ORM object to write to\n\n        objmap: dictionary mapping \"source\" objects to their corresponding\n            \"destination\" object.\n\n        debug: be verbose\n\n        skip_table_names: if a related table's name is in this (optional) list,\n            that relationship is skipped\n    \"\"\"\n    skip_table_names = skip_table_names or []  # type: List[str]\n    insp = inspect(oldobj)  # type: InstanceState\n    # insp.mapper.relationships is of type\n    # sqlalchemy.utils._collections.ImmutableProperties, which is basically\n    # a sort of AttrDict.\n    for attrname_rel in insp.mapper.relationships.items():  # type: Tuple[str, RelationshipProperty]  # noqa\n        attrname = attrname_rel[0]\n        rel_prop = attrname_rel[1]\n        if rel_prop.viewonly:\n            if debug:\n                log.debug(\"Skipping viewonly relationship\")\n            continue  # don't attempt to write viewonly relationships  # noqa\n        related_class = rel_prop.mapper.class_\n        related_table_name = related_class.__tablename__  # type: str\n        if related_table_name in skip_table_names:\n            if debug:\n                log.debug(\"Skipping relationship for related table {!r}\",\n                          related_table_name)\n            continue\n        # The relationship is an abstract object (so getting the\n        # relationship from the old object and from the new, with e.g.\n        # newrel = newinsp.mapper.relationships[oldrel.key],\n        # yield the same object. All we need from it is the key name.\n        #       rel_key = rel.key  # type: str\n        # ... but also available from the mapper as attrname, above\n        related_old = getattr(oldobj, attrname)\n        if rel_prop.uselist:\n            related_new = [objmap[r] for r in related_old]\n        elif related_old is not None:\n            related_new = objmap[related_old]\n        else:\n            related_new = None\n        if debug:\n            log.debug(\"rewrite_relationships: relationship {} -> {}\",\n                      attrname, related_new)\n        setattr(newobj, attrname, related_new)","return_type":"None","function_name":"rewrite_relationships","stripped_code":"def rewrite_relationships(oldobj: object,\n                          newobj: object,\n                          objmap: Dict[object, object],\n                          debug: bool = False,\n                          skip_table_names: List[str] = None):\n    \"\"\"\n    A utility function only.\n    Used in copying objects between SQLAlchemy sessions.\n\n    Both ``oldobj`` and ``newobj`` are SQLAlchemy instances. The instance\n    ``newobj`` is already a copy of ``oldobj`` but we wish to rewrite its\n    relationships, according to the map ``objmap``, which maps old to new\n    objects.\n\n    For example:\n\n    - Suppose a source session has a Customer record and a Sale record\n      containing ``sale.customer_id``, a foreign key to Customer.\n\n    - We may have corresponding Python SQLAlchemy ORM objects\n      ``customer_1_src`` and ``sale_1_src``.\n\n    - We copy them into a destination database, where their Python ORM objects\n      are ``customer_1_dest`` and ``sale_1_dest``.\n\n    - In the process we set up an object map looking like:\n\n      .. code-block:: none\n\n        Old session     New session\n        -------------------------------\n        customer_1_src  customer_1_dest\n        sale_1_src      sale_1_dest\n\n    - Now, we wish to make ``sale_1_dest`` have a relationship to\n      ``customer_1_dest``, in the same way that ``sale_1_src`` has a\n      relationship to ``customer_1_src``. This function will modify\n      ``sale_1_dest`` accordingly, given this object map. It will observe that\n      ``sale_1_src`` (here ``oldobj``) has a relationship to\n      ``customer_1_src``; it will note that ``objmap`` maps ``customer_1_src``\n      to ``customer_1_dest``; it will create the relationship from\n      ``sale_1_dest`` (here ``newobj``) to ``customer_1_dest``.\n\n    Args:\n        oldobj: SQLAlchemy ORM object to read from\n\n        newobj: SQLAlchemy ORM object to write to\n\n        objmap: dictionary mapping \"source\" objects to their corresponding\n            \"destination\" object.\n\n        debug: be verbose\n\n        skip_table_names: if a related table's name is in this (optional) list,\n            that relationship is skipped\n    \"\"\"\n    skip_table_names = skip_table_names or []  # type: List[str]\n    insp = inspect(oldobj)  # type: InstanceState\n    # insp.mapper.relationships is of type\n    # sqlalchemy.utils._collections.ImmutableProperties, which is basically\n    # a sort of AttrDict.\n    for attrname_rel in insp.mapper.relationships.items():  # type: Tuple[str, RelationshipProperty]  # noqa\n        attrname = attrname_rel[0]\n        rel_prop = attrname_rel[1]\n        if rel_prop.viewonly:\n            if debug:\n                log.debug(\"Skipping viewonly relationship\")\n            continue  # don't attempt to write viewonly relationships  # noqa\n        related_class = rel_prop.mapper.class_\n        related_table_name = related_class.__tablename__  # type: str\n        if related_table_name in skip_table_names:\n            if debug:\n                log.debug(\"Skipping relationship for related table {!r}\",\n                          related_table_name)\n            continue\n        # The relationship is an abstract object (so getting the\n        # relationship from the old object and from the new, with e.g.\n        # newrel = newinsp.mapper.relationships[oldrel.key],\n        # yield the same object. All we need from it is the key name.\n        #       rel_key = rel.key  # type: str\n        # ... but also available from the mapper as attrname, above\n        related_old = getattr(oldobj, attrname)\n        if rel_prop.uselist:\n            related_new = [objmap[r] for r in related_old]\n        elif related_old is not None:\n            related_new = objmap[related_old]\n        else:\n            related_new = None\n        if debug:\n            log.debug(\"rewrite_relationships: relationship {} -> {}\",\n                      attrname, related_new)\n        setattr(newobj, attrname, related_new)"}
{"code":"def extend_schema(\n    schema: GraphQLSchema,\n    document_ast: DocumentNode,\n    assume_valid=False,\n    assume_valid_sdl=False,\n) -> GraphQLSchema:\n    \"\"\"Extend the schema with extensions from a given document.\n\n    Produces a new schema given an existing schema and a document which may contain\n    GraphQL type extensions and definitions. The original schema will remain unaltered.\n\n    Because a schema represents a graph of references, a schema cannot be extended\n    without effectively making an entire copy. We do not know until it's too late if\n    subgraphs remain unchanged.\n\n    This algorithm copies the provided schema, applying extensions while producing the\n    copy. The original schema remains unaltered.\n\n    When extending a schema with a known valid extension, it might be safe to assume the\n    schema is valid. Set `assume_valid` to true to assume the produced schema is valid.\n    Set `assume_valid_sdl` to True to assume it is already a valid SDL document.\n    \"\"\"\n    assert_schema(schema)\n\n    if not isinstance(document_ast, DocumentNode):\n        \"Must provide valid Document AST\"\n\n    if not (assume_valid or assume_valid_sdl):\n        from ..validation.validate import assert_valid_sdl_extension\n\n        assert_valid_sdl_extension(document_ast, schema)\n\n    # Collect the type definitions and extensions found in the document.\n    type_defs: List[TypeDefinitionNode] = []\n    type_exts_map: Dict[str, Any] = defaultdict(list)\n\n    # New directives and types are separate because a directives and types can have the\n    # same name. For example, a type named \"skip\".\n    directive_defs: List[DirectiveDefinitionNode] = []\n\n    schema_def: Optional[SchemaDefinitionNode] = None\n    # Schema extensions are collected which may add additional operation types.\n    schema_exts: List[SchemaExtensionNode] = []\n\n    for def_ in document_ast.definitions:\n        if isinstance(def_, SchemaDefinitionNode):\n            schema_def = def_\n        elif isinstance(def_, SchemaExtensionNode):\n            schema_exts.append(def_)\n        elif isinstance(def_, TypeDefinitionNode):\n            type_defs.append(def_)\n        elif isinstance(def_, TypeExtensionNode):\n            extended_type_name = def_.name.value\n            type_exts_map[extended_type_name].append(def_)\n        elif isinstance(def_, DirectiveDefinitionNode):\n            directive_defs.append(def_)\n\n    # If this document contains no new types, extensions, or directives then return the\n    # same unmodified GraphQLSchema instance.\n    if (\n        not type_exts_map\n        and not type_defs\n        and not directive_defs\n        and not schema_exts\n        and not schema_def\n    ):\n        return schema\n\n    # Below are functions used for producing this schema that have closed over this\n    # scope and have access to the schema, cache, and newly defined types.\n\n    # noinspection PyTypeChecker,PyUnresolvedReferences\n    def replace_type(type_: GraphQLType) -> GraphQLType:\n        if is_list_type(type_):\n            return GraphQLList(replace_type(type_.of_type))  # type: ignore\n        if is_non_null_type(type_):\n            return GraphQLNonNull(replace_type(type_.of_type))  # type: ignore\n        return replace_named_type(type_)  # type: ignore\n\n    def replace_named_type(type_: GraphQLNamedType) -> GraphQLNamedType:\n        return type_map[type_.name]\n\n    def get_maybe_type_by_name(type_name: Optional[str]) -> Optional[GraphQLNamedType]:\n        return type_map[type_name] if type_name else None\n\n    def get_merged_directives() -> List[GraphQLDirective]:\n        if not schema.directives:\n            raise TypeError(\"schema must have default directives\")\n\n        return list(\n            chain(\n                map(extend_directive, schema.directives),\n                map(ast_builder.build_directive, directive_defs),\n            )\n        )\n\n    def extend_named_type(type_: GraphQLNamedType) -> GraphQLNamedType:\n        if is_introspection_type(type_) or is_specified_scalar_type(type_):\n            # Builtin types are not extended.\n            return type_\n        if is_scalar_type(type_):\n            type_ = cast(GraphQLScalarType, type_)\n            return extend_scalar_type(type_)\n        if is_object_type(type_):\n            type_ = cast(GraphQLObjectType, type_)\n            return extend_object_type(type_)\n        if is_interface_type(type_):\n            type_ = cast(GraphQLInterfaceType, type_)\n            return extend_interface_type(type_)\n        if is_union_type(type_):\n            type_ = cast(GraphQLUnionType, type_)\n            return extend_union_type(type_)\n        if is_enum_type(type_):\n            type_ = cast(GraphQLEnumType, type_)\n            return extend_enum_type(type_)\n        if is_input_object_type(type_):\n            type_ = cast(GraphQLInputObjectType, type_)\n            return extend_input_object_type(type_)\n\n        # Not reachable. All possible types have been considered.\n        raise TypeError(f\"Unexpected type: '{inspect(type_)}'.\")  # pragma: no cover\n\n    def extend_directive(directive: GraphQLDirective) -> GraphQLDirective:\n        kwargs = directive.to_kwargs()\n        return GraphQLDirective(  # type: ignore\n            **{\n                **kwargs,\n                \"args\": {name: extend_arg(arg) for name, arg in kwargs[\"args\"].items()},\n            }\n        )\n\n    def extend_input_object_type(\n        type_: GraphQLInputObjectType\n    ) -> GraphQLInputObjectType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLInputObjectType(\n            **{\n                **kwargs,\n                \"fields\": lambda: {\n                    **{\n                        name: GraphQLInputField(  # type: ignore\n                            **{**field.to_kwargs(), \"type_\": replace_type(field.type)}\n                        )\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        field.name.value: ast_builder.build_input_field(field)\n                        for field in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_enum_type(type_: GraphQLEnumType) -> GraphQLEnumType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        value_nodes = chain.from_iterable(node.values or [] for node in extensions)\n\n        return GraphQLEnumType(\n            **{\n                **kwargs,\n                \"values\": {\n                    **kwargs[\"values\"],\n                    **{\n                        value.name.value: ast_builder.build_enum_value(value)\n                        for value in value_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_scalar_type(type_: GraphQLScalarType) -> GraphQLScalarType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n\n        return GraphQLScalarType(\n            **{\n                **kwargs,\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_object_type(type_: GraphQLObjectType) -> GraphQLObjectType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        interface_nodes = chain.from_iterable(\n            node.interfaces or [] for node in extensions\n        )\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLObjectType(\n            **{\n                **kwargs,\n                \"interfaces\": lambda: [\n                    replace_named_type(interface) for interface in kwargs[\"interfaces\"]\n                ]\n                # Note: While this could make early assertions to get the correctly\n                # typed values, that would throw immediately while type system\n                # validation with validate_schema will produce more actionable results.\n                + [ast_builder.get_named_type(node) for node in interface_nodes],\n                \"fields\": lambda: {\n                    **{\n                        name: extend_field(field)\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        node.name.value: ast_builder.build_field(node)\n                        for node in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_interface_type(type_: GraphQLInterfaceType) -> GraphQLInterfaceType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLInterfaceType(\n            **{\n                **kwargs,\n                \"fields\": lambda: {\n                    **{\n                        name: extend_field(field)\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        node.name.value: ast_builder.build_field(node)\n                        for node in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_union_type(type_: GraphQLUnionType) -> GraphQLUnionType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        type_nodes = chain.from_iterable(node.types or [] for node in extensions)\n\n        return GraphQLUnionType(\n            **{\n                **kwargs,\n                \"types\": lambda: [\n                    replace_named_type(member_type) for member_type in kwargs[\"types\"]\n                ]\n                # Note: While this could make early assertions to get the correctly\n                # typed values, that would throw immediately while type system\n                # validation with validate_schema will produce more actionable results.\n                + [ast_builder.get_named_type(node) for node in type_nodes],\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_field(field: GraphQLField) -> GraphQLField:\n        return GraphQLField(  # type: ignore\n            **{\n                **field.to_kwargs(),\n                \"type_\": replace_type(field.type),\n                \"args\": {name: extend_arg(arg) for name, arg in field.args.items()},\n            }\n        )\n\n    def extend_arg(arg: GraphQLArgument) -> GraphQLArgument:\n        return GraphQLArgument(  # type: ignore\n            **{**arg.to_kwargs(), \"type_\": replace_type(arg.type)}\n        )\n\n    # noinspection PyShadowingNames\n    def resolve_type(type_name: str) -> GraphQLNamedType:\n        type_ = type_map.get(type_name)\n        if not type_:\n            raise TypeError(f\"Unknown type: '{type_name}'.\")\n        return type_\n\n    ast_builder = ASTDefinitionBuilder(\n        assume_valid=assume_valid, resolve_type=resolve_type\n    )\n\n    type_map = {node.name.value: ast_builder.build_type(node) for node in type_defs}\n    for existing_type_name, existing_type in schema.type_map.items():\n        type_map[existing_type_name] = extend_named_type(existing_type)\n\n    # Get the extended root operation types.\n    operation_types: Dict[OperationType, Optional[str]] = {\n        OperationType.QUERY: schema.query_type.name if schema.query_type else None,\n        OperationType.MUTATION: schema.mutation_type.name\n        if schema.mutation_type\n        else None,\n        OperationType.SUBSCRIPTION: schema.subscription_type.name\n        if schema.subscription_type\n        else None,\n    }\n\n    if schema_def:\n        for operation_type in schema_def.operation_types:\n            operation = operation_type.operation\n            operation_types[operation] = operation_type.type.name.value\n\n    # Then, incorporate schema definition and all schema extensions.\n    for schema_ext in schema_exts:\n        if schema_ext.operation_types:\n            for operation_type in schema_ext.operation_types:\n                operation = operation_type.operation\n                operation_types[operation] = operation_type.type.name.value\n\n    # Then produce and return a Schema with these types.\n    return GraphQLSchema(  # type: ignore\n        # Note: While this could make early assertions to get the correctly\n        # typed values, that would throw immediately while type system\n        # validation with validateSchema() will produce more actionable results.\n        query=get_maybe_type_by_name(operation_types[OperationType.QUERY]),\n        mutation=get_maybe_type_by_name(operation_types[OperationType.MUTATION]),\n        subscription=get_maybe_type_by_name(\n            operation_types[OperationType.SUBSCRIPTION]\n        ),\n        types=list(type_map.values()),\n        directives=get_merged_directives(),\n        ast_node=schema_def or schema.ast_node,\n        extension_ast_nodes=(\n            schema.extension_ast_nodes or cast(Tuple[SchemaExtensionNode], ())\n        )\n        + tuple(schema_exts),\n    )","return_type":"GraphQLSchema","function_name":"extend_schema","stripped_code":"def extend_schema(\n    schema: GraphQLSchema,\n    document_ast: DocumentNode,\n    assume_valid=False,\n    assume_valid_sdl=False,\n):\n    \"\"\"Extend the schema with extensions from a given document.\n\n    Produces a new schema given an existing schema and a document which may contain\n    GraphQL type extensions and definitions. The original schema will remain unaltered.\n\n    Because a schema represents a graph of references, a schema cannot be extended\n    without effectively making an entire copy. We do not know until it's too late if\n    subgraphs remain unchanged.\n\n    This algorithm copies the provided schema, applying extensions while producing the\n    copy. The original schema remains unaltered.\n\n    When extending a schema with a known valid extension, it might be safe to assume the\n    schema is valid. Set `assume_valid` to true to assume the produced schema is valid.\n    Set `assume_valid_sdl` to True to assume it is already a valid SDL document.\n    \"\"\"\n    assert_schema(schema)\n\n    if not isinstance(document_ast, DocumentNode):\n        \"Must provide valid Document AST\"\n\n    if not (assume_valid or assume_valid_sdl):\n        from ..validation.validate import assert_valid_sdl_extension\n\n        assert_valid_sdl_extension(document_ast, schema)\n\n    # Collect the type definitions and extensions found in the document.\n    type_defs: List[TypeDefinitionNode] = []\n    type_exts_map: Dict[str, Any] = defaultdict(list)\n\n    # New directives and types are separate because a directives and types can have the\n    # same name. For example, a type named \"skip\".\n    directive_defs: List[DirectiveDefinitionNode] = []\n\n    schema_def: Optional[SchemaDefinitionNode] = None\n    # Schema extensions are collected which may add additional operation types.\n    schema_exts: List[SchemaExtensionNode] = []\n\n    for def_ in document_ast.definitions:\n        if isinstance(def_, SchemaDefinitionNode):\n            schema_def = def_\n        elif isinstance(def_, SchemaExtensionNode):\n            schema_exts.append(def_)\n        elif isinstance(def_, TypeDefinitionNode):\n            type_defs.append(def_)\n        elif isinstance(def_, TypeExtensionNode):\n            extended_type_name = def_.name.value\n            type_exts_map[extended_type_name].append(def_)\n        elif isinstance(def_, DirectiveDefinitionNode):\n            directive_defs.append(def_)\n\n    # If this document contains no new types, extensions, or directives then return the\n    # same unmodified GraphQLSchema instance.\n    if (\n        not type_exts_map\n        and not type_defs\n        and not directive_defs\n        and not schema_exts\n        and not schema_def\n    ):\n        return schema\n\n    # Below are functions used for producing this schema that have closed over this\n    # scope and have access to the schema, cache, and newly defined types.\n\n    # noinspection PyTypeChecker,PyUnresolvedReferences\n    def replace_type(type_: GraphQLType) -> GraphQLType:\n        if is_list_type(type_):\n            return GraphQLList(replace_type(type_.of_type))  # type: ignore\n        if is_non_null_type(type_):\n            return GraphQLNonNull(replace_type(type_.of_type))  # type: ignore\n        return replace_named_type(type_)  # type: ignore\n\n    def replace_named_type(type_: GraphQLNamedType) -> GraphQLNamedType:\n        return type_map[type_.name]\n\n    def get_maybe_type_by_name(type_name: Optional[str]) -> Optional[GraphQLNamedType]:\n        return type_map[type_name] if type_name else None\n\n    def get_merged_directives() -> List[GraphQLDirective]:\n        if not schema.directives:\n            raise TypeError(\"schema must have default directives\")\n\n        return list(\n            chain(\n                map(extend_directive, schema.directives),\n                map(ast_builder.build_directive, directive_defs),\n            )\n        )\n\n    def extend_named_type(type_: GraphQLNamedType) -> GraphQLNamedType:\n        if is_introspection_type(type_) or is_specified_scalar_type(type_):\n            # Builtin types are not extended.\n            return type_\n        if is_scalar_type(type_):\n            type_ = cast(GraphQLScalarType, type_)\n            return extend_scalar_type(type_)\n        if is_object_type(type_):\n            type_ = cast(GraphQLObjectType, type_)\n            return extend_object_type(type_)\n        if is_interface_type(type_):\n            type_ = cast(GraphQLInterfaceType, type_)\n            return extend_interface_type(type_)\n        if is_union_type(type_):\n            type_ = cast(GraphQLUnionType, type_)\n            return extend_union_type(type_)\n        if is_enum_type(type_):\n            type_ = cast(GraphQLEnumType, type_)\n            return extend_enum_type(type_)\n        if is_input_object_type(type_):\n            type_ = cast(GraphQLInputObjectType, type_)\n            return extend_input_object_type(type_)\n\n        # Not reachable. All possible types have been considered.\n        raise TypeError(f\"Unexpected type: '{inspect(type_)}'.\")  # pragma: no cover\n\n    def extend_directive(directive: GraphQLDirective) -> GraphQLDirective:\n        kwargs = directive.to_kwargs()\n        return GraphQLDirective(  # type: ignore\n            **{\n                **kwargs,\n                \"args\": {name: extend_arg(arg) for name, arg in kwargs[\"args\"].items()},\n            }\n        )\n\n    def extend_input_object_type(\n        type_: GraphQLInputObjectType\n    ) -> GraphQLInputObjectType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLInputObjectType(\n            **{\n                **kwargs,\n                \"fields\": lambda: {\n                    **{\n                        name: GraphQLInputField(  # type: ignore\n                            **{**field.to_kwargs(), \"type_\": replace_type(field.type)}\n                        )\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        field.name.value: ast_builder.build_input_field(field)\n                        for field in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_enum_type(type_: GraphQLEnumType) -> GraphQLEnumType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        value_nodes = chain.from_iterable(node.values or [] for node in extensions)\n\n        return GraphQLEnumType(\n            **{\n                **kwargs,\n                \"values\": {\n                    **kwargs[\"values\"],\n                    **{\n                        value.name.value: ast_builder.build_enum_value(value)\n                        for value in value_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_scalar_type(type_: GraphQLScalarType) -> GraphQLScalarType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n\n        return GraphQLScalarType(\n            **{\n                **kwargs,\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_object_type(type_: GraphQLObjectType) -> GraphQLObjectType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        interface_nodes = chain.from_iterable(\n            node.interfaces or [] for node in extensions\n        )\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLObjectType(\n            **{\n                **kwargs,\n                \"interfaces\": lambda: [\n                    replace_named_type(interface) for interface in kwargs[\"interfaces\"]\n                ]\n                # Note: While this could make early assertions to get the correctly\n                # typed values, that would throw immediately while type system\n                # validation with validate_schema will produce more actionable results.\n                + [ast_builder.get_named_type(node) for node in interface_nodes],\n                \"fields\": lambda: {\n                    **{\n                        name: extend_field(field)\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        node.name.value: ast_builder.build_field(node)\n                        for node in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_interface_type(type_: GraphQLInterfaceType) -> GraphQLInterfaceType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        field_nodes = chain.from_iterable(node.fields or [] for node in extensions)\n\n        return GraphQLInterfaceType(\n            **{\n                **kwargs,\n                \"fields\": lambda: {\n                    **{\n                        name: extend_field(field)\n                        for name, field in kwargs[\"fields\"].items()\n                    },\n                    **{\n                        node.name.value: ast_builder.build_field(node)\n                        for node in field_nodes\n                    },\n                },\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_union_type(type_: GraphQLUnionType) -> GraphQLUnionType:\n        kwargs = type_.to_kwargs()\n        extensions = type_exts_map.get(kwargs[\"name\"], [])\n        type_nodes = chain.from_iterable(node.types or [] for node in extensions)\n\n        return GraphQLUnionType(\n            **{\n                **kwargs,\n                \"types\": lambda: [\n                    replace_named_type(member_type) for member_type in kwargs[\"types\"]\n                ]\n                # Note: While this could make early assertions to get the correctly\n                # typed values, that would throw immediately while type system\n                # validation with validate_schema will produce more actionable results.\n                + [ast_builder.get_named_type(node) for node in type_nodes],\n                \"extension_ast_nodes\": kwargs[\"extension_ast_nodes\"]\n                + tuple(extensions),\n            }\n        )\n\n    def extend_field(field: GraphQLField) -> GraphQLField:\n        return GraphQLField(  # type: ignore\n            **{\n                **field.to_kwargs(),\n                \"type_\": replace_type(field.type),\n                \"args\": {name: extend_arg(arg) for name, arg in field.args.items()},\n            }\n        )\n\n    def extend_arg(arg: GraphQLArgument) -> GraphQLArgument:\n        return GraphQLArgument(  # type: ignore\n            **{**arg.to_kwargs(), \"type_\": replace_type(arg.type)}\n        )\n\n    # noinspection PyShadowingNames\n    def resolve_type(type_name: str) -> GraphQLNamedType:\n        type_ = type_map.get(type_name)\n        if not type_:\n            raise TypeError(f\"Unknown type: '{type_name}'.\")\n        return type_\n\n    ast_builder = ASTDefinitionBuilder(\n        assume_valid=assume_valid, resolve_type=resolve_type\n    )\n\n    type_map = {node.name.value: ast_builder.build_type(node) for node in type_defs}\n    for existing_type_name, existing_type in schema.type_map.items():\n        type_map[existing_type_name] = extend_named_type(existing_type)\n\n    # Get the extended root operation types.\n    operation_types: Dict[OperationType, Optional[str]] = {\n        OperationType.QUERY: schema.query_type.name if schema.query_type else None,\n        OperationType.MUTATION: schema.mutation_type.name\n        if schema.mutation_type\n        else None,\n        OperationType.SUBSCRIPTION: schema.subscription_type.name\n        if schema.subscription_type\n        else None,\n    }\n\n    if schema_def:\n        for operation_type in schema_def.operation_types:\n            operation = operation_type.operation\n            operation_types[operation] = operation_type.type.name.value\n\n    # Then, incorporate schema definition and all schema extensions.\n    for schema_ext in schema_exts:\n        if schema_ext.operation_types:\n            for operation_type in schema_ext.operation_types:\n                operation = operation_type.operation\n                operation_types[operation] = operation_type.type.name.value\n\n    # Then produce and return a Schema with these types.\n    return GraphQLSchema(  # type: ignore\n        # Note: While this could make early assertions to get the correctly\n        # typed values, that would throw immediately while type system\n        # validation with validateSchema() will produce more actionable results.\n        query=get_maybe_type_by_name(operation_types[OperationType.QUERY]),\n        mutation=get_maybe_type_by_name(operation_types[OperationType.MUTATION]),\n        subscription=get_maybe_type_by_name(\n            operation_types[OperationType.SUBSCRIPTION]\n        ),\n        types=list(type_map.values()),\n        directives=get_merged_directives(),\n        ast_node=schema_def or schema.ast_node,\n        extension_ast_nodes=(\n            schema.extension_ast_nodes or cast(Tuple[SchemaExtensionNode], ())\n        )\n        + tuple(schema_exts),\n    )"}
{"code":"def anglesep(lon0: float, lat0: float,\n             lon1: float, lat1: float, deg: bool = True) -> float:\n    \"\"\"\n    Parameters\n    ----------\n\n    lon0 : float\n        longitude of first point\n    lat0 : float\n        latitude of first point\n    lon1 : float\n        longitude of second point\n    lat1 : float\n        latitude of second point\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    sep_rad : float or numpy.ndarray of float\n        angular separation\n\n    For reference, this is from astropy astropy/coordinates/angle_utilities.py\n    Angular separation between two points on a sphere.\n    \"\"\"\n    if angular_separation is None:\n        raise ImportError('angledist requires AstroPy. Try angledis_meeus')\n\n    if deg:\n        lon0 = radians(lon0)\n        lat0 = radians(lat0)\n        lon1 = radians(lon1)\n        lat1 = radians(lat1)\n\n    sep_rad = angular_separation(lon0, lat0, lon1, lat1)\n\n    if deg:\n        return degrees(sep_rad)\n    else:\n        return sep_rad","return_type":"float","function_name":"anglesep","stripped_code":"def anglesep(lon0: float, lat0: float,\n             lon1: float, lat1: float, deg: bool = True):\n    \"\"\"\n    Parameters\n    ----------\n\n    lon0 : float\n        longitude of first point\n    lat0 : float\n        latitude of first point\n    lon1 : float\n        longitude of second point\n    lat1 : float\n        latitude of second point\n    deg : bool, optional\n          degrees input/output  (False: radians in/out)\n\n    Returns\n    -------\n\n    sep_rad : float or numpy.ndarray of float\n        angular separation\n\n    For reference, this is from astropy astropy/coordinates/angle_utilities.py\n    Angular separation between two points on a sphere.\n    \"\"\"\n    if angular_separation is None:\n        raise ImportError('angledist requires AstroPy. Try angledis_meeus')\n\n    if deg:\n        lon0 = radians(lon0)\n        lat0 = radians(lat0)\n        lon1 = radians(lon1)\n        lat1 = radians(lat1)\n\n    sep_rad = angular_separation(lon0, lat0, lon1, lat1)\n\n    if deg:\n        return degrees(sep_rad)\n    else:\n        return sep_rad"}
{"code":"def produce_scansion(self, stresses: list, syllables_wspaces: List[str],\n                         offset_map: Dict[int, int]) -> str:\n        \"\"\"\n        Create a scansion string that has stressed and unstressed syllable positions in locations\n        that correspond with the original texts syllable vowels.\n\n        :param stresses list of syllable positions\n        :param syllables_wspaces list of syllables with spaces escaped for punctuation or elision\n        :param offset_map dictionary of syllable positions, and an offset amount which is the\n        number of spaces to skip in the original line before inserting the accent.\n        \"\"\"\n        scansion = list(\" \" * len(string_utils.flatten(syllables_wspaces)))\n        unstresses = string_utils.get_unstresses(stresses, len(syllables_wspaces))\n        try:\n            for idx in unstresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.UNSTRESSED\n            for idx in stresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.STRESSED\n        except Exception as e:\n            LOG.error(\"problem with syllables; check syllabification {}, {}\".format(\n                syllables_wspaces, e))\n        return \"\".join(scansion)","return_type":"str","function_name":"VerseScanner.produce_scansion","stripped_code":"def produce_scansion(self, stresses: list, syllables_wspaces: List[str],\n                         offset_map: Dict[int, int]):\n        \"\"\"\n        Create a scansion string that has stressed and unstressed syllable positions in locations\n        that correspond with the original texts syllable vowels.\n\n        :param stresses list of syllable positions\n        :param syllables_wspaces list of syllables with spaces escaped for punctuation or elision\n        :param offset_map dictionary of syllable positions, and an offset amount which is the\n        number of spaces to skip in the original line before inserting the accent.\n        \"\"\"\n        scansion = list(\" \" * len(string_utils.flatten(syllables_wspaces)))\n        unstresses = string_utils.get_unstresses(stresses, len(syllables_wspaces))\n        try:\n            for idx in unstresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.UNSTRESSED\n            for idx in stresses:\n                location = offset_map.get(idx)\n                if location is not None:\n                    scansion[location] = self.constants.STRESSED\n        except Exception as e:\n            LOG.error(\"problem with syllables; check syllabification {}, {}\".format(\n                syllables_wspaces, e))\n        return \"\".join(scansion)"}
{"code":"def parse_line(self, line: str) -> None:\n        \"\"\"Updates the dictionary with a single header line.\n\n        >>> h = HTTPHeaders()\n        >>> h.parse_line(\"Content-Type: text/html\")\n        >>> h.get('content-type')\n        'text/html'\n        \"\"\"\n        if line[0].isspace():\n            # continuation of a multi-line header\n            if self._last_key is None:\n                raise HTTPInputError(\"first header line cannot start with whitespace\")\n            new_part = \" \" + line.lstrip()\n            self._as_list[self._last_key][-1] += new_part\n            self._dict[self._last_key] += new_part\n        else:\n            try:\n                name, value = line.split(\":\", 1)\n            except ValueError:\n                raise HTTPInputError(\"no colon in header line\")\n            self.add(name, value.strip())","return_type":"None","function_name":"HTTPHeaders.parse_line","stripped_code":"def parse_line(self, line: str):\n        \"\"\"Updates the dictionary with a single header line.\n\n        >>> h = HTTPHeaders()\n        >>> h.parse_line(\"Content-Type: text/html\")\n        >>> h.get('content-type')\n        'text/html'\n        \"\"\"\n        if line[0].isspace():\n            # continuation of a multi-line header\n            if self._last_key is None:\n                raise HTTPInputError(\"first header line cannot start with whitespace\")\n            new_part = \" \" + line.lstrip()\n            self._as_list[self._last_key][-1] += new_part\n            self._dict[self._last_key] += new_part\n        else:\n            try:\n                name, value = line.split(\":\", 1)\n            except ValueError:\n                raise HTTPInputError(\"no colon in header line\")\n            self.add(name, value.strip())"}
{"code":"def html_visit_inheritance_diagram(\n    self: NodeVisitor, node: inheritance_diagram\n) -> None:\n    \"\"\"\n    Builds HTML output from an :py:class:`~uqbar.sphinx.inheritance.inheritance_diagram` node.\n    \"\"\"\n    inheritance_graph = node[\"graph\"]\n    urls = build_urls(self, node)\n    graphviz_graph = inheritance_graph.build_graph(urls)\n    dot_code = format(graphviz_graph, \"graphviz\")\n    # TODO: We can perform unflattening here\n    aspect_ratio = inheritance_graph.aspect_ratio\n    if aspect_ratio:\n        aspect_ratio = math.ceil(math.sqrt(aspect_ratio[1] / aspect_ratio[0]))\n    if aspect_ratio > 1:\n        process = subprocess.Popen(\n            [\"unflatten\", \"-l\", str(aspect_ratio), \"-c\", str(aspect_ratio), \"-f\"],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = process.communicate(dot_code.encode())\n        dot_code = stdout.decode()\n    render_dot_html(self, node, dot_code, {}, \"inheritance\", \"inheritance\")\n    raise SkipNode","return_type":"None","function_name":"html_visit_inheritance_diagram","stripped_code":"def html_visit_inheritance_diagram(\n    self: NodeVisitor, node: inheritance_diagram\n):\n    \"\"\"\n    Builds HTML output from an :py:class:`~uqbar.sphinx.inheritance.inheritance_diagram` node.\n    \"\"\"\n    inheritance_graph = node[\"graph\"]\n    urls = build_urls(self, node)\n    graphviz_graph = inheritance_graph.build_graph(urls)\n    dot_code = format(graphviz_graph, \"graphviz\")\n    # TODO: We can perform unflattening here\n    aspect_ratio = inheritance_graph.aspect_ratio\n    if aspect_ratio:\n        aspect_ratio = math.ceil(math.sqrt(aspect_ratio[1] / aspect_ratio[0]))\n    if aspect_ratio > 1:\n        process = subprocess.Popen(\n            [\"unflatten\", \"-l\", str(aspect_ratio), \"-c\", str(aspect_ratio), \"-f\"],\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = process.communicate(dot_code.encode())\n        dot_code = stdout.decode()\n    render_dot_html(self, node, dot_code, {}, \"inheritance\", \"inheritance\")\n    raise SkipNode"}
{"code":"def _validate(self, inst: \"InstanceNode\", scope: ValidationScope,\n                  ctype: ContentType) -> None:\n        \"\"\"Extend the superclass method.\"\"\"\n        if scope.value & ValidationScope.syntax.value:   # schema\n            self._check_schema_pattern(inst, ctype)\n        for m in inst:\n            inst._member(m).validate(scope, ctype)\n        super()._validate(inst, scope, ctype)","return_type":"None","function_name":"InternalNode._validate","stripped_code":"def _validate(self, inst: \"InstanceNode\", scope: ValidationScope,\n                  ctype: ContentType):\n        \"\"\"Extend the superclass method.\"\"\"\n        if scope.value & ValidationScope.syntax.value:   # schema\n            self._check_schema_pattern(inst, ctype)\n        for m in inst:\n            inst._member(m).validate(scope, ctype)\n        super()._validate(inst, scope, ctype)"}
{"code":"def handleRestartRequest(self, req: Request) -> None:\n        \"\"\"\n        Handles transaction of type POOL_RESTART\n        Can schedule or cancel restart to a newer\n        version at specified time\n\n        :param req:\n        \"\"\"\n        txn = req.operation\n        if txn[TXN_TYPE] != POOL_RESTART:\n            return\n\n        action = txn[ACTION]\n        if action == START:\n            when = dateutil.parser.parse(txn[DATETIME]) \\\n                if DATETIME in txn.keys() and txn[DATETIME] not in [\"0\", \"\", None] \\\n                else None\n            fail_timeout = txn.get(TIMEOUT, self.defaultActionTimeout)\n            self.requestRestart(when, fail_timeout)\n            return\n\n        if action == CANCEL:\n            if self.scheduledAction:\n                self._cancelScheduledRestart()\n                logger.info(\"Node '{}' cancels restart\".format(\n                    self.nodeName))\n            return\n\n        logger.error(\n            \"Got {} transaction with unsupported action {}\".format(\n                POOL_RESTART, action))","return_type":"None","function_name":"Restarter.handleRestartRequest","stripped_code":"def handleRestartRequest(self, req: Request):\n        \"\"\"\n        Handles transaction of type POOL_RESTART\n        Can schedule or cancel restart to a newer\n        version at specified time\n\n        :param req:\n        \"\"\"\n        txn = req.operation\n        if txn[TXN_TYPE] != POOL_RESTART:\n            return\n\n        action = txn[ACTION]\n        if action == START:\n            when = dateutil.parser.parse(txn[DATETIME]) \\\n                if DATETIME in txn.keys() and txn[DATETIME] not in [\"0\", \"\", None] \\\n                else None\n            fail_timeout = txn.get(TIMEOUT, self.defaultActionTimeout)\n            self.requestRestart(when, fail_timeout)\n            return\n\n        if action == CANCEL:\n            if self.scheduledAction:\n                self._cancelScheduledRestart()\n                logger.info(\"Node '{}' cancels restart\".format(\n                    self.nodeName))\n            return\n\n        logger.error(\n            \"Got {} transaction with unsupported action {}\".format(\n                POOL_RESTART, action))"}
{"code":"def sanitize(x: Any) -> Any:  # pylint: disable=invalid-name,too-many-return-statements\n    \"\"\"\n    Sanitize turns PyTorch and Numpy types into basic Python types so they\n    can be serialized into JSON.\n    \"\"\"\n    if isinstance(x, (str, float, int, bool)):\n        # x is already serializable\n        return x\n    elif isinstance(x, torch.Tensor):\n        # tensor needs to be converted to a list (and moved to cpu if necessary)\n        return x.cpu().tolist()\n    elif isinstance(x, numpy.ndarray):\n        # array needs to be converted to a list\n        return x.tolist()\n    elif isinstance(x, numpy.number):  # pylint: disable=no-member\n        # NumPy numbers need to be converted to Python numbers\n        return x.item()\n    elif isinstance(x, dict):\n        # Dicts need their values sanitized\n        return {key: sanitize(value) for key, value in x.items()}\n    elif isinstance(x, (spacy.tokens.Token, allennlp.data.Token)):\n        # Tokens get sanitized to just their text.\n        return x.text\n    elif isinstance(x, (list, tuple)):\n        # Lists and Tuples need their values sanitized\n        return [sanitize(x_i) for x_i in x]\n    elif x is None:\n        return \"None\"\n    elif hasattr(x, 'to_json'):\n        return x.to_json()\n    else:\n        raise ValueError(f\"Cannot sanitize {x} of type {type(x)}. \"\n                         \"If this is your own custom class, add a `to_json(self)` method \"\n                         \"that returns a JSON-like object.\")","return_type":"Any","function_name":"sanitize","stripped_code":"def sanitize(x: Any):  # pylint: disable=invalid-name,too-many-return-statements\n    \"\"\"\n    Sanitize turns PyTorch and Numpy types into basic Python types so they\n    can be serialized into JSON.\n    \"\"\"\n    if isinstance(x, (str, float, int, bool)):\n        # x is already serializable\n        return x\n    elif isinstance(x, torch.Tensor):\n        # tensor needs to be converted to a list (and moved to cpu if necessary)\n        return x.cpu().tolist()\n    elif isinstance(x, numpy.ndarray):\n        # array needs to be converted to a list\n        return x.tolist()\n    elif isinstance(x, numpy.number):  # pylint: disable=no-member\n        # NumPy numbers need to be converted to Python numbers\n        return x.item()\n    elif isinstance(x, dict):\n        # Dicts need their values sanitized\n        return {key: sanitize(value) for key, value in x.items()}\n    elif isinstance(x, (spacy.tokens.Token, allennlp.data.Token)):\n        # Tokens get sanitized to just their text.\n        return x.text\n    elif isinstance(x, (list, tuple)):\n        # Lists and Tuples need their values sanitized\n        return [sanitize(x_i) for x_i in x]\n    elif x is None:\n        return \"None\"\n    elif hasattr(x, 'to_json'):\n        return x.to_json()\n    else:\n        raise ValueError(f\"Cannot sanitize {x} of type {type(x)}. \"\n                         \"If this is your own custom class, add a `to_json(self)` method \"\n                         \"that returns a JSON-like object.\")"}
{"code":"def entropy_bits_nrange(\n        minimum: Union[int, float], maximum: Union[int, float]\n) -> float:\n    \"\"\"Calculate the number of entropy bits in a range of numbers.\"\"\"\n    # Shannon:\n    # d = fabs(maximum - minimum)\n    # ent = -(1/d) * log(1/d, 2) * d\n    # Aprox form: log10(digits) * log2(10)\n    if not isinstance(minimum, (int, float)):\n        raise TypeError('minimum can only be int or float')\n    if not isinstance(maximum, (int, float)):\n        raise TypeError('maximum can only be int or float')\n    if minimum < 0:\n        raise ValueError('minimum should be greater than 0')\n    if maximum < 0:\n        raise ValueError('maximum should be greater than 0')\n\n    dif = fabs(maximum - minimum)\n    if dif == 0:\n        return 0.0\n\n    ent = log10(dif) * 3.321928\n    return ent","return_type":"float","function_name":"entropy_bits_nrange","stripped_code":"def entropy_bits_nrange(\n        minimum: Union[int, float], maximum: Union[int, float]\n):\n    \"\"\"Calculate the number of entropy bits in a range of numbers.\"\"\"\n    # Shannon:\n    # d = fabs(maximum - minimum)\n    # ent = -(1/d) * log(1/d, 2) * d\n    # Aprox form: log10(digits) * log2(10)\n    if not isinstance(minimum, (int, float)):\n        raise TypeError('minimum can only be int or float')\n    if not isinstance(maximum, (int, float)):\n        raise TypeError('maximum can only be int or float')\n    if minimum < 0:\n        raise ValueError('minimum should be greater than 0')\n    if maximum < 0:\n        raise ValueError('maximum should be greater than 0')\n\n    dif = fabs(maximum - minimum)\n    if dif == 0:\n        return 0.0\n\n    ent = log10(dif) * 3.321928\n    return ent"}
{"code":"def find_deck_spawns(provider: Provider, prod: bool=True) -> Iterable[str]:\n    '''find deck spawn transactions via Provider,\n    it requires that Deck spawn P2TH were imported in local node or\n    that remote API knows about P2TH address.'''\n\n    pa_params = param_query(provider.network)\n\n    if isinstance(provider, RpcNode):\n\n        if prod:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PAPROD\"))\n        else:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PATEST\"))\n\n    if isinstance(provider, Cryptoid) or isinstance(provider, Explorer):\n\n        if prod:\n            decks = (i for i in provider.listtransactions(pa_params.P2TH_addr))\n        else:\n            decks = (i for i in provider.listtransactions(pa_params.test_P2TH_addr))\n\n    return decks","return_type":"Iterable[str]","function_name":"find_deck_spawns","stripped_code":"def find_deck_spawns(provider: Provider, prod: bool=True):\n    '''find deck spawn transactions via Provider,\n    it requires that Deck spawn P2TH were imported in local node or\n    that remote API knows about P2TH address.'''\n\n    pa_params = param_query(provider.network)\n\n    if isinstance(provider, RpcNode):\n\n        if prod:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PAPROD\"))\n        else:\n            decks = (i[\"txid\"] for i in provider.listtransactions(\"PATEST\"))\n\n    if isinstance(provider, Cryptoid) or isinstance(provider, Explorer):\n\n        if prod:\n            decks = (i for i in provider.listtransactions(pa_params.P2TH_addr))\n        else:\n            decks = (i for i in provider.listtransactions(pa_params.test_P2TH_addr))\n\n    return decks"}
{"code":"def synset_signatures(ss: \"wn.Synset\", hyperhypo=True, adapted=False,\n                      remove_stopwords=True, to_lemmatize=True, remove_numbers=True,\n                      lowercase=True, original_lesk=False, from_cache=True) -> set:\n    \"\"\"\n    Takes a Synset and returns its signature words.\n\n    :param ss: An instance of wn.Synset.\n    :return: A set of signature strings\n    \"\"\"\n    if from_cache:\n        return synset_signatures_from_cache(ss, hyperhypo, adapted, original_lesk)\n\n    # Collects the signatures from WordNet.\n    signature = []\n\n    # Adds the definition, example sentences and lemma_names.\n    signature += word_tokenize(ss.definition())\n\n    # If the original lesk signature is requested, skip the other signatures.\n    if original_lesk:\n        return set(signature)\n\n    # Adds the examples and lemma names.\n    signature += chain(*[word_tokenize(eg) for eg in ss.examples()])\n    signature += ss.lemma_names()\n\n    # Includes lemma_names of hyper-/hyponyms.\n    if hyperhypo:\n        hyperhyponyms = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())\n        signature += set(chain(*[i.lemma_names() for i in hyperhyponyms]))\n\n    # Includes signatures from related senses as in Adapted Lesk.\n    if adapted:\n        # Includes lemma_names from holonyms, meronyms and similar_tos\n        related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() + \\\n                             ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() + \\\n                             ss.similar_tos())\n        signature += set(chain(*[i.lemma_names() for i in related_senses]))\n\n    # Lowercase.\n    signature = set(s.lower() for s in signature) if lowercase else signature\n\n    # Removes stopwords.\n    signature = set(signature).difference(EN_STOPWORDS) if remove_stopwords else signature\n\n    # Lemmatized context is preferred over stemmed context.\n    if to_lemmatize:\n        signature = [lemmatize(s) if lowercase else lemmatize(s) # Lowercasing checks here.\n                     for s in signature\n                     # We only throw away if both remove_numbers and s is a digit are true.\n                     if not (remove_numbers and s.isdigit())]\n\n    # Keep only the unique bag-of-words\n    return set(signature)","return_type":"set","function_name":"synset_signatures","stripped_code":"def synset_signatures(ss: \"wn.Synset\", hyperhypo=True, adapted=False,\n                      remove_stopwords=True, to_lemmatize=True, remove_numbers=True,\n                      lowercase=True, original_lesk=False, from_cache=True):\n    \"\"\"\n    Takes a Synset and returns its signature words.\n\n    :param ss: An instance of wn.Synset.\n    :return: A set of signature strings\n    \"\"\"\n    if from_cache:\n        return synset_signatures_from_cache(ss, hyperhypo, adapted, original_lesk)\n\n    # Collects the signatures from WordNet.\n    signature = []\n\n    # Adds the definition, example sentences and lemma_names.\n    signature += word_tokenize(ss.definition())\n\n    # If the original lesk signature is requested, skip the other signatures.\n    if original_lesk:\n        return set(signature)\n\n    # Adds the examples and lemma names.\n    signature += chain(*[word_tokenize(eg) for eg in ss.examples()])\n    signature += ss.lemma_names()\n\n    # Includes lemma_names of hyper-/hyponyms.\n    if hyperhypo:\n        hyperhyponyms = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())\n        signature += set(chain(*[i.lemma_names() for i in hyperhyponyms]))\n\n    # Includes signatures from related senses as in Adapted Lesk.\n    if adapted:\n        # Includes lemma_names from holonyms, meronyms and similar_tos\n        related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() + \\\n                             ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() + \\\n                             ss.similar_tos())\n        signature += set(chain(*[i.lemma_names() for i in related_senses]))\n\n    # Lowercase.\n    signature = set(s.lower() for s in signature) if lowercase else signature\n\n    # Removes stopwords.\n    signature = set(signature).difference(EN_STOPWORDS) if remove_stopwords else signature\n\n    # Lemmatized context is preferred over stemmed context.\n    if to_lemmatize:\n        signature = [lemmatize(s) if lowercase else lemmatize(s) # Lowercasing checks here.\n                     for s in signature\n                     # We only throw away if both remove_numbers and s is a digit are true.\n                     if not (remove_numbers and s.isdigit())]\n\n    # Keep only the unique bag-of-words\n    return set(signature)"}
{"code":"def add_untagged_ok(self, text: MaybeBytes,\n                        code: Optional[ResponseCode] = None) -> None:\n        \"\"\"Add an untagged ``OK`` response.\n\n        See Also:\n            :meth:`.add_untagged`, :class:`ResponseOk`\n\n        Args:\n            text: The response text.\n            code: Optional response code.\n\n        \"\"\"\n        response = ResponseOk(b'*', text, code)\n        self.add_untagged(response)","return_type":"None","function_name":"Response.add_untagged_ok","stripped_code":"def add_untagged_ok(self, text: MaybeBytes,\n                        code: Optional[ResponseCode] = None):\n        \"\"\"Add an untagged ``OK`` response.\n\n        See Also:\n            :meth:`.add_untagged`, :class:`ResponseOk`\n\n        Args:\n            text: The response text.\n            code: Optional response code.\n\n        \"\"\"\n        response = ResponseOk(b'*', text, code)\n        self.add_untagged(response)"}
{"code":"def get_holding_accounts(self) -> List[Account]:\n        \"\"\" Returns the (cached) list of holding accounts \"\"\"\n        if not self.__holding_accounts:\n            self.__holding_accounts = self.__get_holding_accounts_query().all()\n\n        return self.__holding_accounts","return_type":"List[Account]","function_name":"SecurityAggregate.get_holding_accounts","stripped_code":"def get_holding_accounts(self):\n        \"\"\" Returns the (cached) list of holding accounts \"\"\"\n        if not self.__holding_accounts:\n            self.__holding_accounts = self.__get_holding_accounts_query().all()\n\n        return self.__holding_accounts"}
{"code":"def add_app_template_filter(self, func: Callable, name: Optional[str]=None) -> None:\n        \"\"\"Add an application wide template filter.\n\n        This is designed to be used on the blueprint directly, and\n        has the same arguments as\n        :meth:`~quart.Quart.add_template_filter`. An example usage,\n\n        .. code-block:: python\n\n            def filter():\n                ...\n\n            blueprint = Blueprint(__name__)\n            blueprint.add_app_template_filter(filter)\n        \"\"\"\n        self.record_once(lambda state: state.register_template_filter(func, name))","return_type":"None","function_name":"Blueprint.add_app_template_filter","stripped_code":"def add_app_template_filter(self, func: Callable, name: Optional[str]=None):\n        \"\"\"Add an application wide template filter.\n\n        This is designed to be used on the blueprint directly, and\n        has the same arguments as\n        :meth:`~quart.Quart.add_template_filter`. An example usage,\n\n        .. code-block:: python\n\n            def filter():\n                ...\n\n            blueprint = Blueprint(__name__)\n            blueprint.add_app_template_filter(filter)\n        \"\"\"\n        self.record_once(lambda state: state.register_template_filter(func, name))"}
{"code":"def _encode_header(key: str, pdict: Dict[str, str]) -> str:\n    \"\"\"Inverse of _parse_header.\n\n    >>> _encode_header('permessage-deflate',\n    ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})\n    'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'\n    \"\"\"\n    if not pdict:\n        return key\n    out = [key]\n    # Sort the parameters just to make it easy to test.\n    for k, v in sorted(pdict.items()):\n        if v is None:\n            out.append(k)\n        else:\n            # TODO: quote if necessary.\n            out.append(\"%s=%s\" % (k, v))\n    return \"; \".join(out)","return_type":"str","function_name":"_encode_header","stripped_code":"def _encode_header(key: str, pdict: Dict[str, str]):\n    \"\"\"Inverse of _parse_header.\n\n    >>> _encode_header('permessage-deflate',\n    ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})\n    'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'\n    \"\"\"\n    if not pdict:\n        return key\n    out = [key]\n    # Sort the parameters just to make it easy to test.\n    for k, v in sorted(pdict.items()):\n        if v is None:\n            out.append(k)\n        else:\n            # TODO: quote if necessary.\n            out.append(\"%s=%s\" % (k, v))\n    return \"; \".join(out)"}
{"code":"def channel_is_closed(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ) -> bool:\n        \"\"\" Returns true if the channel is in a closed state, false otherwise. \"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        return channel_state == ChannelState.CLOSED","return_type":"bool","function_name":"TokenNetwork.channel_is_closed","stripped_code":"def channel_is_closed(\n            self,\n            participant1: Address,\n            participant2: Address,\n            block_identifier: BlockSpecification,\n            channel_identifier: ChannelID,\n    ):\n        \"\"\" Returns true if the channel is in a closed state, false otherwise. \"\"\"\n        try:\n            channel_state = self._get_channel_state(\n                participant1=participant1,\n                participant2=participant2,\n                block_identifier=block_identifier,\n                channel_identifier=channel_identifier,\n            )\n        except RaidenRecoverableError:\n            return False\n        return channel_state == ChannelState.CLOSED"}
{"code":"def _load_dict_hierarchical(self, db_key: str) -> dict:\n        \"\"\"Load a dictionary stored hierarchically at db_key.\"\"\"\n        db_keys = self._db.keys(pattern=db_key + '*')\n        my_dict = {}\n        for _db_key in db_keys:\n            if self._db.type(_db_key) == 'list':\n                db_values = self._db.lrange(_db_key, 0, -1)\n                for i, value in enumerate(db_values):\n                    try:\n                        db_values[i] = ast.literal_eval(value)\n                    except SyntaxError:\n                        pass\n                    except ValueError:\n                        pass\n            else:  # self._db.type == 'hash'\n                db_values = self._db.hgetall(_db_key)\n                for _key, _value in db_values.items():\n                    try:\n                        db_values[_key] = ast.literal_eval(_value)\n                    except SyntaxError:\n                        pass\n                    except ValueError:\n                        pass\n            my_dict = self._build_dict(my_dict, _db_key.split(':'),\n                                       db_values)\n        return my_dict[db_key]","return_type":"dict","function_name":"ConfigDb._load_dict_hierarchical","stripped_code":"def _load_dict_hierarchical(self, db_key: str):\n        \"\"\"Load a dictionary stored hierarchically at db_key.\"\"\"\n        db_keys = self._db.keys(pattern=db_key + '*')\n        my_dict = {}\n        for _db_key in db_keys:\n            if self._db.type(_db_key) == 'list':\n                db_values = self._db.lrange(_db_key, 0, -1)\n                for i, value in enumerate(db_values):\n                    try:\n                        db_values[i] = ast.literal_eval(value)\n                    except SyntaxError:\n                        pass\n                    except ValueError:\n                        pass\n            else:  # self._db.type == 'hash'\n                db_values = self._db.hgetall(_db_key)\n                for _key, _value in db_values.items():\n                    try:\n                        db_values[_key] = ast.literal_eval(_value)\n                    except SyntaxError:\n                        pass\n                    except ValueError:\n                        pass\n            my_dict = self._build_dict(my_dict, _db_key.split(':'),\n                                       db_values)\n        return my_dict[db_key]"}
{"code":"def heightmap_lerp_hm(\n    hm1: np.ndarray, hm2: np.ndarray, hm3: np.ndarray, coef: float\n) -> None:\n    \"\"\"Perform linear interpolation between two heightmaps storing the result\n    in ``hm3``.\n\n    This is the same as doing ``hm3[:] = hm1[:] + (hm2[:] - hm1[:]) * coef``\n\n    Args:\n        hm1 (numpy.ndarray): The first heightmap.\n        hm2 (numpy.ndarray): The second heightmap to add to the first.\n        hm3 (numpy.ndarray): A destination heightmap to store the result.\n        coef (float): The linear interpolation coefficient.\n    \"\"\"\n    lib.TCOD_heightmap_lerp_hm(\n        _heightmap_cdata(hm1),\n        _heightmap_cdata(hm2),\n        _heightmap_cdata(hm3),\n        coef,\n    )","return_type":"None","function_name":"heightmap_lerp_hm","stripped_code":"def heightmap_lerp_hm(\n    hm1: np.ndarray, hm2: np.ndarray, hm3: np.ndarray, coef: float\n):\n    \"\"\"Perform linear interpolation between two heightmaps storing the result\n    in ``hm3``.\n\n    This is the same as doing ``hm3[:] = hm1[:] + (hm2[:] - hm1[:]) * coef``\n\n    Args:\n        hm1 (numpy.ndarray): The first heightmap.\n        hm2 (numpy.ndarray): The second heightmap to add to the first.\n        hm3 (numpy.ndarray): A destination heightmap to store the result.\n        coef (float): The linear interpolation coefficient.\n    \"\"\"\n    lib.TCOD_heightmap_lerp_hm(\n        _heightmap_cdata(hm1),\n        _heightmap_cdata(hm2),\n        _heightmap_cdata(hm3),\n        coef,\n    )"}
{"code":"def reload(self) -> None:\n        \"\"\"Reload Entity from the repository\"\"\"\n        if not self.state_.is_persisted or self.state_.is_changed:\n            raise InvalidStateError(f'`{self.__class__.__name__}` object is in invalid state')\n\n        # Retrieve the entity's ID by the configured Identifier field\n        identifier = getattr(self, self.meta_.id_field.field_name)\n        logger.debug(f'Lookup `{self.__class__.__name__}` object with '\n                     f'identifier {self.meta_.id_field}')\n\n        # Fetch the entity data from db by its identifier\n        db_value = self.get(identifier)\n\n        # Update own data from fetched entity data\n        # This allows us to ``dog.reload()`` instead of ``dog = dog.reload()``\n        self._update_data(db_value.to_dict())","return_type":"None","function_name":"Entity.reload","stripped_code":"def reload(self):\n        \"\"\"Reload Entity from the repository\"\"\"\n        if not self.state_.is_persisted or self.state_.is_changed:\n            raise InvalidStateError(f'`{self.__class__.__name__}` object is in invalid state')\n\n        # Retrieve the entity's ID by the configured Identifier field\n        identifier = getattr(self, self.meta_.id_field.field_name)\n        logger.debug(f'Lookup `{self.__class__.__name__}` object with '\n                     f'identifier {self.meta_.id_field}')\n\n        # Fetch the entity data from db by its identifier\n        db_value = self.get(identifier)\n\n        # Update own data from fetched entity data\n        # This allows us to ``dog.reload()`` instead of ``dog = dog.reload()``\n        self._update_data(db_value.to_dict())"}
{"code":"def _get_path_for_op_id(self, id: str) -> Optional[str]:\n        \"\"\"Searches the spec for a path matching the operation id.\n\n        Args:\n            id: operation id\n\n        Returns:\n            path to the endpoint, or None if not found\n        \"\"\"\n        for path_key, path_value in self._get_spec()['paths'].items():\n            for method in self.METHODS:\n                if method in path_value:\n                    if self.OPERATION_ID_KEY in path_value[method]:\n                        if path_value[method][self.OPERATION_ID_KEY] == id:\n                            return path_key\n        return None","return_type":"Optional[str]","function_name":"Preston._get_path_for_op_id","stripped_code":"def _get_path_for_op_id(self, id: str):\n        \"\"\"Searches the spec for a path matching the operation id.\n\n        Args:\n            id: operation id\n\n        Returns:\n            path to the endpoint, or None if not found\n        \"\"\"\n        for path_key, path_value in self._get_spec()['paths'].items():\n            for method in self.METHODS:\n                if method in path_value:\n                    if self.OPERATION_ID_KEY in path_value[method]:\n                        if path_value[method][self.OPERATION_ID_KEY] == id:\n                            return path_key\n        return None"}
{"code":"def sugartex_preprocess(source: str) -> str:\n    \"\"\"\n    Preprocess text for SugarTeX Pandoc filter.\n    Replaces '\u02ce' with `$` (except `\\\u02ce`), replaces `\\\u02ce` with `\u02ce`\n    \"\"\"\n    rep = {r'\\\u02ce': '\u02ce', '\u02ce': '$'}\n    return re.sub(r'\\\\\u02ce|\u02ce', lambda m: rep[m.group(0)], source)","return_type":"str","function_name":"sugartex_preprocess","stripped_code":"def sugartex_preprocess(source: str):\n    \"\"\"\n    Preprocess text for SugarTeX Pandoc filter.\n    Replaces '\u02ce' with `$` (except `\\\u02ce`), replaces `\\\u02ce` with `\u02ce`\n    \"\"\"\n    rep = {r'\\\u02ce': '\u02ce', '\u02ce': '$'}\n    return re.sub(r'\\\\\u02ce|\u02ce', lambda m: rep[m.group(0)], source)"}
{"code":"def snapshot_data_item(self, data_item: DataItem) -> DataItem:\n        \"\"\"Snapshot a data item. Similar to copy but with a data snapshot.\n\n        .. versionadded:: 1.0\n\n        Scriptable: No\n        \"\"\"\n        data_item = data_item._data_item.snapshot()\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)","return_type":"DataItem","function_name":"Library.snapshot_data_item","stripped_code":"def snapshot_data_item(self, data_item: DataItem):\n        \"\"\"Snapshot a data item. Similar to copy but with a data snapshot.\n\n        .. versionadded:: 1.0\n\n        Scriptable: No\n        \"\"\"\n        data_item = data_item._data_item.snapshot()\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)"}
{"code":"def validate_args(self, qubits: Sequence[Qid]) -> None:\n        \"\"\"Checks if this gate can be applied to the given qubits.\n\n        By default checks if input is of type Qid and qubit count.\n        Child classes can override.\n\n        Args:\n            qubits: The collection of qubits to potentially apply the gate to.\n\n        Throws:\n            ValueError: The gate can't be applied to the qubits.\n        \"\"\"\n        if len(qubits) == 0:\n            raise ValueError(\n                \"Applied a gate to an empty set of qubits. Gate: {}\".format(\n                    repr(self)))\n\n        if len(qubits) != self.num_qubits():\n            raise ValueError(\n                'Wrong number of qubits for <{!r}>. '\n                'Expected {} qubits but got <{!r}>.'.format(\n                    self,\n                    self.num_qubits(),\n                    qubits))\n\n        if any([not isinstance(qubit, Qid)\n                for qubit in qubits]):\n            raise ValueError(\n                    'Gate was called with type different than Qid.')","return_type":"None","function_name":"Gate.validate_args","stripped_code":"def validate_args(self, qubits: Sequence[Qid]):\n        \"\"\"Checks if this gate can be applied to the given qubits.\n\n        By default checks if input is of type Qid and qubit count.\n        Child classes can override.\n\n        Args:\n            qubits: The collection of qubits to potentially apply the gate to.\n\n        Throws:\n            ValueError: The gate can't be applied to the qubits.\n        \"\"\"\n        if len(qubits) == 0:\n            raise ValueError(\n                \"Applied a gate to an empty set of qubits. Gate: {}\".format(\n                    repr(self)))\n\n        if len(qubits) != self.num_qubits():\n            raise ValueError(\n                'Wrong number of qubits for <{!r}>. '\n                'Expected {} qubits but got <{!r}>.'.format(\n                    self,\n                    self.num_qubits(),\n                    qubits))\n\n        if any([not isinstance(qubit, Qid)\n                for qubit in qubits]):\n            raise ValueError(\n                    'Gate was called with type different than Qid.')"}
{"code":"def read_until(data: bytes, *, return_tail: bool = True, from_=None) -> bytes:\n    \"\"\"\n    read until some bytes appear\n    \"\"\"\n    return (yield (Traps._read_until, data, return_tail, from_))","return_type":"bytes","function_name":"read_until","stripped_code":"def read_until(data: bytes, *, return_tail: bool = True, from_=None):\n    \"\"\"\n    read until some bytes appear\n    \"\"\"\n    return (yield (Traps._read_until, data, return_tail, from_))"}
{"code":"def masked_flip(padded_sequence: torch.Tensor,\n                sequence_lengths: List[int]) -> torch.Tensor:\n    \"\"\"\n        Flips a padded tensor along the time dimension without affecting masked entries.\n\n        Parameters\n        ----------\n        padded_sequence : ``torch.Tensor``\n            The tensor to flip along the time dimension.\n            Assumed to be of dimensions (batch size, num timesteps, ...)\n        sequence_lengths : ``torch.Tensor``\n            A list containing the lengths of each unpadded sequence in the batch.\n\n        Returns\n        -------\n        A ``torch.Tensor`` of the same shape as padded_sequence.\n        \"\"\"\n    assert padded_sequence.size(0) == len(sequence_lengths), \\\n        f'sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}'\n    num_timesteps = padded_sequence.size(1)\n    flipped_padded_sequence = torch.flip(padded_sequence, [1])\n    sequences = [flipped_padded_sequence[i, num_timesteps - length:] for i, length in enumerate(sequence_lengths)]\n    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)","return_type":"torch.Tensor","function_name":"masked_flip","stripped_code":"def masked_flip(padded_sequence: torch.Tensor,\n                sequence_lengths: List[int]):\n    \"\"\"\n        Flips a padded tensor along the time dimension without affecting masked entries.\n\n        Parameters\n        ----------\n        padded_sequence : ``torch.Tensor``\n            The tensor to flip along the time dimension.\n            Assumed to be of dimensions (batch size, num timesteps, ...)\n        sequence_lengths : ``torch.Tensor``\n            A list containing the lengths of each unpadded sequence in the batch.\n\n        Returns\n        -------\n        A ``torch.Tensor`` of the same shape as padded_sequence.\n        \"\"\"\n    assert padded_sequence.size(0) == len(sequence_lengths), \\\n        f'sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}'\n    num_timesteps = padded_sequence.size(1)\n    flipped_padded_sequence = torch.flip(padded_sequence, [1])\n    sequences = [flipped_padded_sequence[i, num_timesteps - length:] for i, length in enumerate(sequence_lengths)]\n    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)"}
{"code":"def strids2ids(tokens: Iterable[str]) -> List[int]:\n    \"\"\"\n    Returns sequence of integer ids given a sequence of string ids.\n\n    :param tokens: List of integer tokens.\n    :return: List of word ids.\n    \"\"\"\n    return list(map(int, tokens))","return_type":"List[int]","function_name":"strids2ids","stripped_code":"def strids2ids(tokens: Iterable[str]):\n    \"\"\"\n    Returns sequence of integer ids given a sequence of string ids.\n\n    :param tokens: List of integer tokens.\n    :return: List of word ids.\n    \"\"\"\n    return list(map(int, tokens))"}
{"code":"def association_generator(self, file, skipheader=False, outfile=None) -> Dict:\n        \"\"\"\n        Returns a generator that yields successive associations from file\n\n        Yields\n        ------\n        association\n        \"\"\"\n        file = self._ensure_file(file)\n        for line in file:\n            parsed_result = self.parse_line(line)\n            self.report.report_parsed_result(parsed_result, outfile, self.config.filtered_evidence_file, self.config.filter_out_evidence)\n            for association in parsed_result.associations:\n                # yield association if we don't care if it's a header or if it's definitely a real gaf line\n                if not skipheader or \"header\" not in association:\n                    yield association\n\n        logging.info(self.report.short_summary())\n        file.close()","return_type":"Dict","function_name":"AssocParser.association_generator","stripped_code":"def association_generator(self, file, skipheader=False, outfile=None):\n        \"\"\"\n        Returns a generator that yields successive associations from file\n\n        Yields\n        ------\n        association\n        \"\"\"\n        file = self._ensure_file(file)\n        for line in file:\n            parsed_result = self.parse_line(line)\n            self.report.report_parsed_result(parsed_result, outfile, self.config.filtered_evidence_file, self.config.filter_out_evidence)\n            for association in parsed_result.associations:\n                # yield association if we don't care if it's a header or if it's definitely a real gaf line\n                if not skipheader or \"header\" not in association:\n                    yield association\n\n        logging.info(self.report.short_summary())\n        file.close()"}
{"code":"def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)","return_type":"None","function_name":"_assert_recur_is_tail","stripped_code":"def _assert_recur_is_tail(node: Node):  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)"}
{"code":"def add_and_rename_file(self, filename: str, new_filename: str) -> None:\n        \"\"\"\n        Copies the specified file into the working directory of this\n        sandbox and renames it to new_filename.\n        \"\"\"\n        dest = os.path.join(\n            self.name + ':' + SANDBOX_WORKING_DIR_NAME,\n            new_filename)\n        subprocess.check_call(['docker', 'cp', filename, dest])\n        self._chown_files([new_filename])","return_type":"None","function_name":"AutograderSandbox.add_and_rename_file","stripped_code":"def add_and_rename_file(self, filename: str, new_filename: str):\n        \"\"\"\n        Copies the specified file into the working directory of this\n        sandbox and renames it to new_filename.\n        \"\"\"\n        dest = os.path.join(\n            self.name + ':' + SANDBOX_WORKING_DIR_NAME,\n            new_filename)\n        subprocess.check_call(['docker', 'cp', filename, dest])\n        self._chown_files([new_filename])"}
{"code":"def nm_width(center, width, units=\"wn\") -> float:\n    \"\"\"Given a center and width, in energy units, get back a width in nm.\n\n    Parameters\n    ----------\n    center : number\n        Center (in energy units).\n    width : number\n        Width (in energy units).\n    units : string (optional)\n        Input units. Default is wn.\n\n    Returns\n    -------\n    number\n        Width in nm.\n    \"\"\"\n    red = wt_units.converter(center - width / 2., units, \"nm\")\n    blue = wt_units.converter(center + width / 2., units, \"nm\")\n    return red - blue","return_type":"float","function_name":"nm_width","stripped_code":"def nm_width(center, width, units=\"wn\"):\n    \"\"\"Given a center and width, in energy units, get back a width in nm.\n\n    Parameters\n    ----------\n    center : number\n        Center (in energy units).\n    width : number\n        Width (in energy units).\n    units : string (optional)\n        Input units. Default is wn.\n\n    Returns\n    -------\n    number\n        Width in nm.\n    \"\"\"\n    red = wt_units.converter(center - width / 2., units, \"nm\")\n    blue = wt_units.converter(center + width / 2., units, \"nm\")\n    return red - blue"}
{"code":"def get_neighbor_names(self, node_name: str, order: int = 1) -> list:\n        \"\"\"Get the names of all neighbors of a node, and the node itself.\n\n        :param node_name: Node whose neighbor names are requested.\n        :return: A list of names of all neighbors of a node, and the node itself.\n        \"\"\"\n        logger.info(\"In get_neighbor_names()\")\n        node = self.graph.vs.find(name=node_name)\n        neighbors = self.graph.neighborhood(node, order=order)\n        names = self.graph.vs[neighbors][\"name\"]\n        names.append(node_name)\n        return list(names)","return_type":"list","function_name":"NeighborhoodNetwork.get_neighbor_names","stripped_code":"def get_neighbor_names(self, node_name: str, order: int = 1):\n        \"\"\"Get the names of all neighbors of a node, and the node itself.\n\n        :param node_name: Node whose neighbor names are requested.\n        :return: A list of names of all neighbors of a node, and the node itself.\n        \"\"\"\n        logger.info(\"In get_neighbor_names()\")\n        node = self.graph.vs.find(name=node_name)\n        neighbors = self.graph.neighborhood(node, order=order)\n        names = self.graph.vs[neighbors][\"name\"]\n        names.append(node_name)\n        return list(names)"}
{"code":"def distribute(self,\n                   volume: float,\n                   source: Well,\n                   dest: List[Well],\n                   *args, **kwargs) -> 'InstrumentContext':\n        \"\"\"\n        Move a volume of liquid from one source to multiple destinations.\n\n        :param volume: The amount of volume to distribute to each destination\n                       well.\n        :param source: A single well from where liquid will be aspirated.\n        :param dest: List of Wells where liquid will be dispensed to.\n        :param kwargs: See :py:meth:`transfer`.\n        :returns: This instance\n        \"\"\"\n        self._log.debug(\"Distributing {} from {} to {}\"\n                        .format(volume, source, dest))\n        kwargs['mode'] = 'distribute'\n        kwargs['disposal_volume'] = kwargs.get('disposal_vol', self.min_volume)\n        return self.transfer(volume, source, dest, **kwargs)","return_type":"'InstrumentContext'","function_name":"InstrumentContext.distribute","stripped_code":"def distribute(self,\n                   volume: float,\n                   source: Well,\n                   dest: List[Well],\n                   *args, **kwargs):\n        \"\"\"\n        Move a volume of liquid from one source to multiple destinations.\n\n        :param volume: The amount of volume to distribute to each destination\n                       well.\n        :param source: A single well from where liquid will be aspirated.\n        :param dest: List of Wells where liquid will be dispensed to.\n        :param kwargs: See :py:meth:`transfer`.\n        :returns: This instance\n        \"\"\"\n        self._log.debug(\"Distributing {} from {} to {}\"\n                        .format(volume, source, dest))\n        kwargs['mode'] = 'distribute'\n        kwargs['disposal_volume'] = kwargs.get('disposal_vol', self.min_volume)\n        return self.transfer(volume, source, dest, **kwargs)"}
{"code":"def qft_circuit(qubits: Qubits) -> Circuit:\n    \"\"\"Returns the Quantum Fourier Transform circuit\"\"\"\n    # Kudos: Adapted from Rigetti Grove, grove/qft/fourier.py\n\n    N = len(qubits)\n    circ = Circuit()\n    for n0 in range(N):\n        q0 = qubits[n0]\n        circ += H(q0)\n        for n1 in range(n0+1, N):\n            q1 = qubits[n1]\n            angle = pi / 2 ** (n1-n0)\n            circ += CPHASE(angle, q1, q0)\n    circ.extend(reversal_circuit(qubits))\n    return circ","return_type":"Circuit","function_name":"qft_circuit","stripped_code":"def qft_circuit(qubits: Qubits):\n    \"\"\"Returns the Quantum Fourier Transform circuit\"\"\"\n    # Kudos: Adapted from Rigetti Grove, grove/qft/fourier.py\n\n    N = len(qubits)\n    circ = Circuit()\n    for n0 in range(N):\n        q0 = qubits[n0]\n        circ += H(q0)\n        for n1 in range(n0+1, N):\n            q1 = qubits[n1]\n            angle = pi / 2 ** (n1-n0)\n            circ += CPHASE(angle, q1, q0)\n    circ.extend(reversal_circuit(qubits))\n    return circ"}
{"code":"def peek_text(self, text: str) -> bool:\n        \"\"\"Same as readText but doesn't consume the stream.\"\"\"\n        start = self._stream.index\n        stop = start + len(text)\n        if stop > self._stream.eos_index:\n            return False\n        return self._stream[self._stream.index:stop] == text","return_type":"bool","function_name":"BasicParser.peek_text","stripped_code":"def peek_text(self, text: str):\n        \"\"\"Same as readText but doesn't consume the stream.\"\"\"\n        start = self._stream.index\n        stop = start + len(text)\n        if stop > self._stream.eos_index:\n            return False\n        return self._stream[self._stream.index:stop] == text"}
{"code":"def get_combined_dim(combination: str, tensor_dims: List[int]) -> int:\n    \"\"\"\n    For use with :func:`combine_tensors`.  This function computes the resultant dimension when\n    calling ``combine_tensors(combination, tensors)``, when the tensor dimension is known.  This is\n    necessary for knowing the sizes of weight matrices when building models that use\n    ``combine_tensors``.\n\n    Parameters\n    ----------\n    combination : ``str``\n        A comma-separated list of combination pieces, like ``\"1,2,1*2\"``, specified identically to\n        ``combination`` in :func:`combine_tensors`.\n    tensor_dims : ``List[int]``\n        A list of tensor dimensions, where each dimension is from the `last axis` of the tensors\n        that will be input to :func:`combine_tensors`.\n    \"\"\"\n    if len(tensor_dims) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    return sum([_get_combination_dim(piece, tensor_dims) for piece in combination.split(',')])","return_type":"int","function_name":"get_combined_dim","stripped_code":"def get_combined_dim(combination: str, tensor_dims: List[int]):\n    \"\"\"\n    For use with :func:`combine_tensors`.  This function computes the resultant dimension when\n    calling ``combine_tensors(combination, tensors)``, when the tensor dimension is known.  This is\n    necessary for knowing the sizes of weight matrices when building models that use\n    ``combine_tensors``.\n\n    Parameters\n    ----------\n    combination : ``str``\n        A comma-separated list of combination pieces, like ``\"1,2,1*2\"``, specified identically to\n        ``combination`` in :func:`combine_tensors`.\n    tensor_dims : ``List[int]``\n        A list of tensor dimensions, where each dimension is from the `last axis` of the tensors\n        that will be input to :func:`combine_tensors`.\n    \"\"\"\n    if len(tensor_dims) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    return sum([_get_combination_dim(piece, tensor_dims) for piece in combination.split(',')])"}
{"code":"def validate_manifest_against_schema(manifest: Dict[str, Any]) -> None:\n    \"\"\"\n    Load and validate manifest against schema\n    located at MANIFEST_SCHEMA_PATH.\n    \"\"\"\n    schema_data = _load_schema_data()\n    try:\n        validate(manifest, schema_data)\n    except jsonValidationError as e:\n        raise ValidationError(\n            f\"Manifest invalid for schema version {schema_data['version']}. \"\n            f\"Reason: {e.message}\"\n        )","return_type":"None","function_name":"validate_manifest_against_schema","stripped_code":"def validate_manifest_against_schema(manifest: Dict[str, Any]):\n    \"\"\"\n    Load and validate manifest against schema\n    located at MANIFEST_SCHEMA_PATH.\n    \"\"\"\n    schema_data = _load_schema_data()\n    try:\n        validate(manifest, schema_data)\n    except jsonValidationError as e:\n        raise ValidationError(\n            f\"Manifest invalid for schema version {schema_data['version']}. \"\n            f\"Reason: {e.message}\"\n        )"}
{"code":"def get_content_type(\n            self,\n            bucket: str,\n            key: str\n    ) -> str:\n        \"\"\"\n        Retrieves the content-type for a given object in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which content-type is being retrieved.\n        :return: the content-type\n        \"\"\"\n        response = self.get_all_metadata(bucket, key)\n        # hilariously, the ETag is quoted.  Unclear why.\n        return response['ContentType']","return_type":"str","function_name":"S3BlobStore.get_content_type","stripped_code":"def get_content_type(\n            self,\n            bucket: str,\n            key: str\n    ):\n        \"\"\"\n        Retrieves the content-type for a given object in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which content-type is being retrieved.\n        :return: the content-type\n        \"\"\"\n        response = self.get_all_metadata(bucket, key)\n        # hilariously, the ETag is quoted.  Unclear why.\n        return response['ContentType']"}
{"code":"def get_sub_commands(parser: argparse.ArgumentParser) -> List[str]:\n    \"\"\"Get a list of sub-commands for an ArgumentParser\"\"\"\n    sub_cmds = []\n\n    # Check if this is parser has sub-commands\n    if parser is not None and parser._subparsers is not None:\n\n        # Find the _SubParsersAction for the sub-commands of this parser\n        for action in parser._subparsers._actions:\n            if isinstance(action, argparse._SubParsersAction):\n                for sub_cmd, sub_cmd_parser in action.choices.items():\n                    sub_cmds.append(sub_cmd)\n\n                    # Look for nested sub-commands\n                    for nested_sub_cmd in get_sub_commands(sub_cmd_parser):\n                        sub_cmds.append('{} {}'.format(sub_cmd, nested_sub_cmd))\n\n                break\n\n    sub_cmds.sort()\n    return sub_cmds","return_type":"List[str]","function_name":"get_sub_commands","stripped_code":"def get_sub_commands(parser: argparse.ArgumentParser):\n    \"\"\"Get a list of sub-commands for an ArgumentParser\"\"\"\n    sub_cmds = []\n\n    # Check if this is parser has sub-commands\n    if parser is not None and parser._subparsers is not None:\n\n        # Find the _SubParsersAction for the sub-commands of this parser\n        for action in parser._subparsers._actions:\n            if isinstance(action, argparse._SubParsersAction):\n                for sub_cmd, sub_cmd_parser in action.choices.items():\n                    sub_cmds.append(sub_cmd)\n\n                    # Look for nested sub-commands\n                    for nested_sub_cmd in get_sub_commands(sub_cmd_parser):\n                        sub_cmds.append('{} {}'.format(sub_cmd, nested_sub_cmd))\n\n                break\n\n    sub_cmds.sort()\n    return sub_cmds"}
{"code":"def decode_file_args(self, argv: List[str]) -> List[str]:\n        \"\"\"\n        Preprocess a configuration file.  The location of the configuration file is stored in the parser so that the\n        FileOrURI action can add relative locations.\n        :param argv: raw options list\n        :return: options list with '--conf' references replaced with file contents\n        \"\"\"\n        for i in range(0, len(argv) - 1):\n            # TODO: take prefix into account\n            if argv[i] == '--conf':\n                del argv[i]\n                conf_file = argv[i]\n                del (argv[i])\n                with open(conf_file) as config_file:\n                    conf_args = shlex.split(config_file.read())\n                    # We take advantage of a poential bug in the parser where you can say \"foo -u 1 -u 2\" and get\n                    # 2 as a result\n                    argv = self.fix_rel_paths(conf_args, conf_file) + argv\n                return self.decode_file_args(argv)\n        return argv","return_type":"List[str]","function_name":"FileAwareParser.decode_file_args","stripped_code":"def decode_file_args(self, argv: List[str]):\n        \"\"\"\n        Preprocess a configuration file.  The location of the configuration file is stored in the parser so that the\n        FileOrURI action can add relative locations.\n        :param argv: raw options list\n        :return: options list with '--conf' references replaced with file contents\n        \"\"\"\n        for i in range(0, len(argv) - 1):\n            # TODO: take prefix into account\n            if argv[i] == '--conf':\n                del argv[i]\n                conf_file = argv[i]\n                del (argv[i])\n                with open(conf_file) as config_file:\n                    conf_args = shlex.split(config_file.read())\n                    # We take advantage of a poential bug in the parser where you can say \"foo -u 1 -u 2\" and get\n                    # 2 as a result\n                    argv = self.fix_rel_paths(conf_args, conf_file) + argv\n                return self.decode_file_args(argv)\n        return argv"}
{"code":"def annealing_cos(start:Number, end:Number, pct:float)->Number:\n    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n    cos_out = np.cos(np.pi * pct) + 1\n    return end + (start-end)/2 * cos_out","return_type":"Number","function_name":"annealing_cos","stripped_code":"def annealing_cos(start:Number, end:Number, pct:float):\n    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n    cos_out = np.cos(np.pi * pct) + 1\n    return end + (start-end)/2 * cos_out"}
{"code":"def filter_whitespace(mode: str, text: str) -> str:\n    \"\"\"Transform whitespace in ``text`` according to ``mode``.\n\n    Available modes are:\n\n    * ``all``: Return all whitespace unmodified.\n    * ``single``: Collapse consecutive whitespace with a single whitespace\n      character, preserving newlines.\n    * ``oneline``: Collapse all runs of whitespace into a single space\n      character, removing all newlines in the process.\n\n    .. versionadded:: 4.3\n    \"\"\"\n    if mode == \"all\":\n        return text\n    elif mode == \"single\":\n        text = re.sub(r\"([\\t ]+)\", \" \", text)\n        text = re.sub(r\"(\\s*\\n\\s*)\", \"\\n\", text)\n        return text\n    elif mode == \"oneline\":\n        return re.sub(r\"(\\s+)\", \" \", text)\n    else:\n        raise Exception(\"invalid whitespace mode %s\" % mode)","return_type":"str","function_name":"filter_whitespace","stripped_code":"def filter_whitespace(mode: str, text: str):\n    \"\"\"Transform whitespace in ``text`` according to ``mode``.\n\n    Available modes are:\n\n    * ``all``: Return all whitespace unmodified.\n    * ``single``: Collapse consecutive whitespace with a single whitespace\n      character, preserving newlines.\n    * ``oneline``: Collapse all runs of whitespace into a single space\n      character, removing all newlines in the process.\n\n    .. versionadded:: 4.3\n    \"\"\"\n    if mode == \"all\":\n        return text\n    elif mode == \"single\":\n        text = re.sub(r\"([\\t ]+)\", \" \", text)\n        text = re.sub(r\"(\\s*\\n\\s*)\", \"\\n\", text)\n        return text\n    elif mode == \"oneline\":\n        return re.sub(r\"(\\s+)\", \" \", text)\n    else:\n        raise Exception(\"invalid whitespace mode %s\" % mode)"}
{"code":"def same_unit(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]) -> \"Units\":\n        \"\"\" Usage:\n        'self.units.same_tech(UnitTypeId.COMMANDCENTER)'\n        returns CommandCenter and CommandCenterFlying,\n        'self.units.same_tech(UnitTypeId.ORBITALCOMMAND)'\n        returns OrbitalCommand and OrbitalCommandFlying\n        This also works with a set/list/dict parameter, e.g. 'self.units.same_tech({UnitTypeId.COMMANDCENTER, UnitTypeId.SUPPLYDEPOT})'\n        Untested: This should return the equivalents for WarpPrism, Observer, Overseer, SupplyDepot and others\n        \"\"\"\n        if isinstance(other, UnitTypeId):\n            other = {other}\n        unit_alias_types = set(other)\n        for unitType in other:\n            unit_alias = self.game_data.units[unitType.value].unit_alias\n            if unit_alias:\n                unit_alias_types.add(unit_alias)\n        return self.filter(\n            lambda unit: unit.type_id in unit_alias_types\n            or unit._type_data.unit_alias is not None\n            and unit._type_data.unit_alias in unit_alias_types\n        )","return_type":"\"Units\"","function_name":"Units.same_unit","stripped_code":"def same_unit(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]):\n        \"\"\" Usage:\n        'self.units.same_tech(UnitTypeId.COMMANDCENTER)'\n        returns CommandCenter and CommandCenterFlying,\n        'self.units.same_tech(UnitTypeId.ORBITALCOMMAND)'\n        returns OrbitalCommand and OrbitalCommandFlying\n        This also works with a set/list/dict parameter, e.g. 'self.units.same_tech({UnitTypeId.COMMANDCENTER, UnitTypeId.SUPPLYDEPOT})'\n        Untested: This should return the equivalents for WarpPrism, Observer, Overseer, SupplyDepot and others\n        \"\"\"\n        if isinstance(other, UnitTypeId):\n            other = {other}\n        unit_alias_types = set(other)\n        for unitType in other:\n            unit_alias = self.game_data.units[unitType.value].unit_alias\n            if unit_alias:\n                unit_alias_types.add(unit_alias)\n        return self.filter(\n            lambda unit: unit.type_id in unit_alias_types\n            or unit._type_data.unit_alias is not None\n            and unit._type_data.unit_alias in unit_alias_types\n        )"}
{"code":"def disable_dao_fork(chain_class: Type[BaseChain]) -> Type[BaseChain]:\n    \"\"\"\n    Set the ``support_dao_fork`` flag to ``False`` on the\n    :class:`~eth.vm.forks.homestead.HomesteadVM`.  Requires that presence of\n    the :class:`~eth.vm.forks.homestead.HomesteadVM`  in the\n    ``vm_configuration``\n    \"\"\"\n    homstead_vms_found = any(\n        _is_homestead(vm_class) for _, vm_class in chain_class.vm_configuration\n    )\n    if not homstead_vms_found:\n        raise ValidationError(\"No HomesteadVM found in vm_configuration.\")\n\n    vm_configuration = _set_vm_dao_support_false(chain_class.vm_configuration)\n    return chain_class.configure(vm_configuration=vm_configuration)","return_type":"Type[BaseChain]","function_name":"disable_dao_fork","stripped_code":"def disable_dao_fork(chain_class: Type[BaseChain]):\n    \"\"\"\n    Set the ``support_dao_fork`` flag to ``False`` on the\n    :class:`~eth.vm.forks.homestead.HomesteadVM`.  Requires that presence of\n    the :class:`~eth.vm.forks.homestead.HomesteadVM`  in the\n    ``vm_configuration``\n    \"\"\"\n    homstead_vms_found = any(\n        _is_homestead(vm_class) for _, vm_class in chain_class.vm_configuration\n    )\n    if not homstead_vms_found:\n        raise ValidationError(\"No HomesteadVM found in vm_configuration.\")\n\n    vm_configuration = _set_vm_dao_support_false(chain_class.vm_configuration)\n    return chain_class.configure(vm_configuration=vm_configuration)"}
{"code":"def parse_logical_form(self,\n                           logical_form: str,\n                           remove_var_function: bool = True) -> Expression:\n        \"\"\"\n        Takes a logical form as a string, maps its tokens using the mapping and returns a parsed expression.\n\n        Parameters\n        ----------\n        logical_form : ``str``\n            Logical form to parse\n        remove_var_function : ``bool`` (optional)\n            ``var`` is a special function that some languages use within lambda functions to\n            indicate the usage of a variable. If your language uses it, and you do not want to\n            include it in the parsed expression, set this flag. You may want to do this if you are\n            generating an action sequence from this parsed expression, because it is easier to let\n            the decoder not produce this function due to the way constrained decoding is currently\n            implemented.\n        \"\"\"\n        if not logical_form.startswith(\"(\"):\n            logical_form = f\"({logical_form})\"\n        if remove_var_function:\n            # Replace \"(x)\" with \"x\"\n            logical_form = re.sub(r'\\(([x-z])\\)', r'\\1', logical_form)\n            # Replace \"(var x)\" with \"(x)\"\n            logical_form = re.sub(r'\\(var ([x-z])\\)', r'(\\1)', logical_form)\n        parsed_lisp = semparse_util.lisp_to_nested_expression(logical_form)\n        translated_string = self._process_nested_expression(parsed_lisp)\n        type_signature = self.local_type_signatures.copy()\n        type_signature.update(self.global_type_signatures)\n        return self._logic_parser.parse(translated_string, signature=type_signature)","return_type":"Expression","function_name":"World.parse_logical_form","stripped_code":"def parse_logical_form(self,\n                           logical_form: str,\n                           remove_var_function: bool = True):\n        \"\"\"\n        Takes a logical form as a string, maps its tokens using the mapping and returns a parsed expression.\n\n        Parameters\n        ----------\n        logical_form : ``str``\n            Logical form to parse\n        remove_var_function : ``bool`` (optional)\n            ``var`` is a special function that some languages use within lambda functions to\n            indicate the usage of a variable. If your language uses it, and you do not want to\n            include it in the parsed expression, set this flag. You may want to do this if you are\n            generating an action sequence from this parsed expression, because it is easier to let\n            the decoder not produce this function due to the way constrained decoding is currently\n            implemented.\n        \"\"\"\n        if not logical_form.startswith(\"(\"):\n            logical_form = f\"({logical_form})\"\n        if remove_var_function:\n            # Replace \"(x)\" with \"x\"\n            logical_form = re.sub(r'\\(([x-z])\\)', r'\\1', logical_form)\n            # Replace \"(var x)\" with \"(x)\"\n            logical_form = re.sub(r'\\(var ([x-z])\\)', r'(\\1)', logical_form)\n        parsed_lisp = semparse_util.lisp_to_nested_expression(logical_form)\n        translated_string = self._process_nested_expression(parsed_lisp)\n        type_signature = self.local_type_signatures.copy()\n        type_signature.update(self.global_type_signatures)\n        return self._logic_parser.parse(translated_string, signature=type_signature)"}
{"code":"def flatMap(f: Callable, xs: Iterable) -> List:\n    \"\"\" Map a function onto an iterable and flatten the result. \"\"\"\n    return flatten(lmap(f, xs))","return_type":"List","function_name":"flatMap","stripped_code":"def flatMap(f: Callable, xs: Iterable):\n    \"\"\" Map a function onto an iterable and flatten the result. \"\"\"\n    return flatten(lmap(f, xs))"}
{"code":"def get_overlaps(self) -> \"IntervalList\":\n        \"\"\"\n        Returns an :class:`IntervalList` containing intervals representing\n        periods of overlap between intervals in this one.\n        \"\"\"\n        overlaps = IntervalList()\n        for i in range(len(self.intervals)):\n            for j in range(i + 1, len(self.intervals)):\n                first = self.intervals[i]\n                second = self.intervals[j]\n                ol = first.intersection(second)\n                if ol is not None:\n                    overlaps.add(ol)\n        return overlaps","return_type":"\"IntervalList\"","function_name":"IntervalList.get_overlaps","stripped_code":"def get_overlaps(self):\n        \"\"\"\n        Returns an :class:`IntervalList` containing intervals representing\n        periods of overlap between intervals in this one.\n        \"\"\"\n        overlaps = IntervalList()\n        for i in range(len(self.intervals)):\n            for j in range(i + 1, len(self.intervals)):\n                first = self.intervals[i]\n                second = self.intervals[j]\n                ol = first.intersection(second)\n                if ol is not None:\n                    overlaps.add(ol)\n        return overlaps"}
{"code":"def map_is_in_fov(m: tcod.map.Map, x: int, y: int) -> bool:\n    \"\"\"Return True if the cell at x,y is lit by the last field-of-view\n    algorithm.\n\n    .. note::\n        This function is slow.\n    .. deprecated:: 4.5\n        Use :any:`tcod.map.Map.fov` to check this property.\n    \"\"\"\n    return bool(lib.TCOD_map_is_in_fov(m.map_c, x, y))","return_type":"bool","function_name":"map_is_in_fov","stripped_code":"def map_is_in_fov(m: tcod.map.Map, x: int, y: int):\n    \"\"\"Return True if the cell at x,y is lit by the last field-of-view\n    algorithm.\n\n    .. note::\n        This function is slow.\n    .. deprecated:: 4.5\n        Use :any:`tcod.map.Map.fov` to check this property.\n    \"\"\"\n    return bool(lib.TCOD_map_is_in_fov(m.map_c, x, y))"}
{"code":"def add_transcription(self, gene: Gene, rna: Union[Rna, MicroRna]) -> str:\n        \"\"\"Add a transcription relation from a gene to an RNA or miRNA node.\n\n        :param gene: A gene node\n        :param rna: An RNA or microRNA node\n        \"\"\"\n        return self.add_unqualified_edge(gene, rna, TRANSCRIBED_TO)","return_type":"str","function_name":"BELGraph.add_transcription","stripped_code":"def add_transcription(self, gene: Gene, rna: Union[Rna, MicroRna]):\n        \"\"\"Add a transcription relation from a gene to an RNA or miRNA node.\n\n        :param gene: A gene node\n        :param rna: An RNA or microRNA node\n        \"\"\"\n        return self.add_unqualified_edge(gene, rna, TRANSCRIBED_TO)"}
{"code":"def _add_onchain_locksroot_to_snapshot(\n        raiden: RaidenService,\n        storage: SQLiteStorage,\n        snapshot_record: StateChangeRecord,\n) -> str:\n    \"\"\"\n    Add `onchain_locksroot` to each NettingChannelEndState\n    \"\"\"\n    snapshot = json.loads(snapshot_record.data)\n\n    for payment_network in snapshot.get('identifiers_to_paymentnetworks', dict()).values():\n        for token_network in payment_network.get('tokennetworks', list()):\n            channelidentifiers_to_channels = token_network.get(\n                'channelidentifiers_to_channels',\n                dict(),\n            )\n            for channel in channelidentifiers_to_channels.values():\n                our_locksroot, partner_locksroot = _get_onchain_locksroots(\n                    raiden=raiden,\n                    storage=storage,\n                    token_network=token_network,\n                    channel=channel,\n                )\n                channel['our_state']['onchain_locksroot'] = serialize_bytes(our_locksroot)\n                channel['partner_state']['onchain_locksroot'] = serialize_bytes(partner_locksroot)\n\n    return json.dumps(snapshot, indent=4), snapshot_record.identifier","return_type":"str","function_name":"_add_onchain_locksroot_to_snapshot","stripped_code":"def _add_onchain_locksroot_to_snapshot(\n        raiden: RaidenService,\n        storage: SQLiteStorage,\n        snapshot_record: StateChangeRecord,\n):\n    \"\"\"\n    Add `onchain_locksroot` to each NettingChannelEndState\n    \"\"\"\n    snapshot = json.loads(snapshot_record.data)\n\n    for payment_network in snapshot.get('identifiers_to_paymentnetworks', dict()).values():\n        for token_network in payment_network.get('tokennetworks', list()):\n            channelidentifiers_to_channels = token_network.get(\n                'channelidentifiers_to_channels',\n                dict(),\n            )\n            for channel in channelidentifiers_to_channels.values():\n                our_locksroot, partner_locksroot = _get_onchain_locksroots(\n                    raiden=raiden,\n                    storage=storage,\n                    token_network=token_network,\n                    channel=channel,\n                )\n                channel['our_state']['onchain_locksroot'] = serialize_bytes(our_locksroot)\n                channel['partner_state']['onchain_locksroot'] = serialize_bytes(partner_locksroot)\n\n    return json.dumps(snapshot, indent=4), snapshot_record.identifier"}
{"code":"def _describe_bitmask(\n    bits: int, table: Dict[Any, str], default: str = \"0\"\n) -> str:\n    \"\"\"Returns a bitmask in human readable form.\n\n    This is a private function, used internally.\n\n    Args:\n        bits (int): The bitmask to be represented.\n        table (Dict[Any,str]): A reverse lookup table.\n        default (Any): A default return value when bits is 0.\n\n    Returns: str: A printable version of the bits variable.\n    \"\"\"\n    result = []\n    for bit, name in table.items():\n        if bit & bits:\n            result.append(name)\n    if not result:\n        return default\n    return \"|\".join(result)","return_type":"str","function_name":"_describe_bitmask","stripped_code":"def _describe_bitmask(\n    bits: int, table: Dict[Any, str], default: str = \"0\"\n):\n    \"\"\"Returns a bitmask in human readable form.\n\n    This is a private function, used internally.\n\n    Args:\n        bits (int): The bitmask to be represented.\n        table (Dict[Any,str]): A reverse lookup table.\n        default (Any): A default return value when bits is 0.\n\n    Returns: str: A printable version of the bits variable.\n    \"\"\"\n    result = []\n    for bit, name in table.items():\n        if bit & bits:\n            result.append(name)\n    if not result:\n        return default\n    return \"|\".join(result)"}
{"code":"def _add_eap_args(eap_args: Dict[str, str]) -> List[str]:\n    \"\"\" Add configuration options suitable for an nmcli con add command\n    for WPA-EAP configuration. These options are mostly in the\n    802-1x group.\n\n    The eap_args dict should be a flat structure of arguments. They\n    must contain at least 'eapType', specifying the EAP type to use\n    (the qualified_name() of one of the members of EAP_TYPES) and the\n    required arguments for that EAP type.\n    \"\"\"\n    args = ['wifi-sec.key-mgmt', 'wpa-eap']\n    eap_type = EAP_TYPES.by_qualified_name(eap_args['eapType'])\n    type_args = eap_type.args()\n    args += ['802-1x.eap', eap_type.outer.value.name]\n    if eap_type.inner:\n        args += ['802-1x.phase2-autheap', eap_type.inner.value.name]\n    for ta in type_args:\n        if ta['name'] in eap_args:\n            if ta['type'] == 'file':\n                # Keyfiles must be prepended with file:// so nm-cli\n                # knows that we\u2019re not giving it DER-encoded blobs\n                _make_host_symlink_if_necessary()\n                path = _rewrite_key_path_to_host_path(eap_args[ta['name']])\n                val = 'file://' + path\n            else:\n                val = eap_args[ta['name']]\n            args += ['802-1x.' + ta['nmName'], val]\n    return args","return_type":"List[str]","function_name":"_add_eap_args","stripped_code":"def _add_eap_args(eap_args: Dict[str, str]):\n    \"\"\" Add configuration options suitable for an nmcli con add command\n    for WPA-EAP configuration. These options are mostly in the\n    802-1x group.\n\n    The eap_args dict should be a flat structure of arguments. They\n    must contain at least 'eapType', specifying the EAP type to use\n    (the qualified_name() of one of the members of EAP_TYPES) and the\n    required arguments for that EAP type.\n    \"\"\"\n    args = ['wifi-sec.key-mgmt', 'wpa-eap']\n    eap_type = EAP_TYPES.by_qualified_name(eap_args['eapType'])\n    type_args = eap_type.args()\n    args += ['802-1x.eap', eap_type.outer.value.name]\n    if eap_type.inner:\n        args += ['802-1x.phase2-autheap', eap_type.inner.value.name]\n    for ta in type_args:\n        if ta['name'] in eap_args:\n            if ta['type'] == 'file':\n                # Keyfiles must be prepended with file:// so nm-cli\n                # knows that we\u2019re not giving it DER-encoded blobs\n                _make_host_symlink_if_necessary()\n                path = _rewrite_key_path_to_host_path(eap_args[ta['name']])\n                val = 'file://' + path\n            else:\n                val = eap_args[ta['name']]\n            args += ['802-1x.' + ta['nmName'], val]\n    return args"}
{"code":"def encode(self) -> str:\n        \"\"\"\n        Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str\n        \"\"\"\n        payload = {}\n        payload.update(self.registered_claims)\n        payload.update(self.payload)\n        return encode(self.secret, payload, self.alg, self.header)","return_type":"str","function_name":"Jwt.encode","stripped_code":"def encode(self):\n        \"\"\"\n        Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str\n        \"\"\"\n        payload = {}\n        payload.update(self.registered_claims)\n        payload.update(self.payload)\n        return encode(self.secret, payload, self.alg, self.header)"}
{"code":"def get_func_signature(self, hsh: bytes) -> Optional[str]:\n        \"\"\"Returns the signature of the normal function with the selector ``hsh``,\n        or ``None`` if no such function exists.\n\n        This function returns ``None`` for any selector that will be dispatched to a fallback function.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        return self._function_signatures_by_selector.get(hsh)","return_type":"Optional[str]","function_name":"SolidityMetadata.get_func_signature","stripped_code":"def get_func_signature(self, hsh: bytes):\n        \"\"\"Returns the signature of the normal function with the selector ``hsh``,\n        or ``None`` if no such function exists.\n\n        This function returns ``None`` for any selector that will be dispatched to a fallback function.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        return self._function_signatures_by_selector.get(hsh)"}
{"code":"def login_or_register(\n        client: GMatrixClient,\n        signer: Signer,\n        prev_user_id: str = None,\n        prev_access_token: str = None,\n) -> User:\n    \"\"\"Login to a Raiden matrix server with password and displayname proof-of-keys\n\n    - Username is in the format: 0x<eth_address>(.<suffix>)?, where the suffix is not required,\n    but a deterministic (per-account) random 8-hex string to prevent DoS by other users registering\n    our address\n    - Password is the signature of the server hostname, verified by the server to prevent account\n    creation spam\n    - Displayname currently is the signature of the whole user_id (including homeserver), to be\n    verified by other peers. May include in the future other metadata such as protocol version\n\n    Params:\n        client: GMatrixClient instance configured with desired homeserver\n        signer: raiden.utils.signer.Signer instance for signing password and displayname\n        prev_user_id: (optional) previous persisted client.user_id. Must match signer's account\n        prev_access_token: (optional) previous persistend client.access_token for prev_user_id\n    Returns:\n        Own matrix_client.User\n    \"\"\"\n    server_url = client.api.base_url\n    server_name = urlparse(server_url).netloc\n\n    base_username = to_normalized_address(signer.address)\n    _match_user = re.match(\n        f'^@{re.escape(base_username)}.*:{re.escape(server_name)}$',\n        prev_user_id or '',\n    )\n    if _match_user:  # same user as before\n        log.debug('Trying previous user login', user_id=prev_user_id)\n        client.set_access_token(user_id=prev_user_id, token=prev_access_token)\n\n        try:\n            client.api.get_devices()\n        except MatrixRequestError as ex:\n            log.debug(\n                'Couldn\\'t use previous login credentials, discarding',\n                prev_user_id=prev_user_id,\n                _exception=ex,\n            )\n        else:\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # initial_sync\n            client.set_sync_limit(prev_sync_limit)\n            log.debug('Success. Valid previous credentials', user_id=prev_user_id)\n            return client.get_user(client.user_id)\n    elif prev_user_id:\n        log.debug(\n            'Different server or account, discarding',\n            prev_user_id=prev_user_id,\n            current_address=base_username,\n            current_server=server_name,\n        )\n\n    # password is signed server address\n    password = encode_hex(signer.sign(server_name.encode()))\n    rand = None\n    # try login and register on first 5 possible accounts\n    for i in range(JOIN_RETRIES):\n        username = base_username\n        if i:\n            if not rand:\n                rand = Random()  # deterministic, random secret for username suffixes\n                # initialize rand for seed (which requires a signature) only if/when needed\n                rand.seed(int.from_bytes(signer.sign(b'seed')[-32:], 'big'))\n            username = f'{username}.{rand.randint(0, 0xffffffff):08x}'\n\n        try:\n            client.login(username, password, sync=False)\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # when logging, do initial_sync with limit=0\n            client.set_sync_limit(prev_sync_limit)\n            log.debug(\n                'Login',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            break\n        except MatrixRequestError as ex:\n            if ex.code != 403:\n                raise\n            log.debug(\n                'Could not login. Trying register',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            try:\n                client.register_with_password(username, password)\n                log.debug(\n                    'Register',\n                    homeserver=server_name,\n                    server_url=server_url,\n                    username=username,\n                )\n                break\n            except MatrixRequestError as ex:\n                if ex.code != 400:\n                    raise\n                log.debug('Username taken. Continuing')\n                continue\n    else:\n        raise ValueError('Could not register or login!')\n\n    name = encode_hex(signer.sign(client.user_id.encode()))\n    user = client.get_user(client.user_id)\n    user.set_display_name(name)\n    return user","return_type":"User","function_name":"login_or_register","stripped_code":"def login_or_register(\n        client: GMatrixClient,\n        signer: Signer,\n        prev_user_id: str = None,\n        prev_access_token: str = None,\n):\n    \"\"\"Login to a Raiden matrix server with password and displayname proof-of-keys\n\n    - Username is in the format: 0x<eth_address>(.<suffix>)?, where the suffix is not required,\n    but a deterministic (per-account) random 8-hex string to prevent DoS by other users registering\n    our address\n    - Password is the signature of the server hostname, verified by the server to prevent account\n    creation spam\n    - Displayname currently is the signature of the whole user_id (including homeserver), to be\n    verified by other peers. May include in the future other metadata such as protocol version\n\n    Params:\n        client: GMatrixClient instance configured with desired homeserver\n        signer: raiden.utils.signer.Signer instance for signing password and displayname\n        prev_user_id: (optional) previous persisted client.user_id. Must match signer's account\n        prev_access_token: (optional) previous persistend client.access_token for prev_user_id\n    Returns:\n        Own matrix_client.User\n    \"\"\"\n    server_url = client.api.base_url\n    server_name = urlparse(server_url).netloc\n\n    base_username = to_normalized_address(signer.address)\n    _match_user = re.match(\n        f'^@{re.escape(base_username)}.*:{re.escape(server_name)}$',\n        prev_user_id or '',\n    )\n    if _match_user:  # same user as before\n        log.debug('Trying previous user login', user_id=prev_user_id)\n        client.set_access_token(user_id=prev_user_id, token=prev_access_token)\n\n        try:\n            client.api.get_devices()\n        except MatrixRequestError as ex:\n            log.debug(\n                'Couldn\\'t use previous login credentials, discarding',\n                prev_user_id=prev_user_id,\n                _exception=ex,\n            )\n        else:\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # initial_sync\n            client.set_sync_limit(prev_sync_limit)\n            log.debug('Success. Valid previous credentials', user_id=prev_user_id)\n            return client.get_user(client.user_id)\n    elif prev_user_id:\n        log.debug(\n            'Different server or account, discarding',\n            prev_user_id=prev_user_id,\n            current_address=base_username,\n            current_server=server_name,\n        )\n\n    # password is signed server address\n    password = encode_hex(signer.sign(server_name.encode()))\n    rand = None\n    # try login and register on first 5 possible accounts\n    for i in range(JOIN_RETRIES):\n        username = base_username\n        if i:\n            if not rand:\n                rand = Random()  # deterministic, random secret for username suffixes\n                # initialize rand for seed (which requires a signature) only if/when needed\n                rand.seed(int.from_bytes(signer.sign(b'seed')[-32:], 'big'))\n            username = f'{username}.{rand.randint(0, 0xffffffff):08x}'\n\n        try:\n            client.login(username, password, sync=False)\n            prev_sync_limit = client.set_sync_limit(0)\n            client._sync()  # when logging, do initial_sync with limit=0\n            client.set_sync_limit(prev_sync_limit)\n            log.debug(\n                'Login',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            break\n        except MatrixRequestError as ex:\n            if ex.code != 403:\n                raise\n            log.debug(\n                'Could not login. Trying register',\n                homeserver=server_name,\n                server_url=server_url,\n                username=username,\n            )\n            try:\n                client.register_with_password(username, password)\n                log.debug(\n                    'Register',\n                    homeserver=server_name,\n                    server_url=server_url,\n                    username=username,\n                )\n                break\n            except MatrixRequestError as ex:\n                if ex.code != 400:\n                    raise\n                log.debug('Username taken. Continuing')\n                continue\n    else:\n        raise ValueError('Could not register or login!')\n\n    name = encode_hex(signer.sign(client.user_id.encode()))\n    user = client.get_user(client.user_id)\n    user.set_display_name(name)\n    return user"}
{"code":"def close(self, code: int = None, reason: str = None) -> None:\n        \"\"\"Closes the websocket connection.\n\n        ``code`` and ``reason`` are documented under\n        `WebSocketHandler.close`.\n\n        .. versionadded:: 3.2\n\n        .. versionchanged:: 4.0\n\n           Added the ``code`` and ``reason`` arguments.\n        \"\"\"\n        if self.protocol is not None:\n            self.protocol.close(code, reason)\n            self.protocol = None","return_type":"None","function_name":"WebSocketClientConnection.close","stripped_code":"def close(self, code: int = None, reason: str = None):\n        \"\"\"Closes the websocket connection.\n\n        ``code`` and ``reason`` are documented under\n        `WebSocketHandler.close`.\n\n        .. versionadded:: 3.2\n\n        .. versionchanged:: 4.0\n\n           Added the ``code`` and ``reason`` arguments.\n        \"\"\"\n        if self.protocol is not None:\n            self.protocol.close(code, reason)\n            self.protocol = None"}
{"code":"def resolve(self, other: Type) -> Type:\n        \"\"\"See ``PlaceholderType.resolve``\"\"\"\n        if not isinstance(other, NltkComplexType):\n            return None\n        resolved_second = NUMBER_TYPE.resolve(other.second)\n        if not resolved_second:\n            return None\n        return CountType(other.first)","return_type":"Type","function_name":"CountType.resolve","stripped_code":"def resolve(self, other: Type):\n        \"\"\"See ``PlaceholderType.resolve``\"\"\"\n        if not isinstance(other, NltkComplexType):\n            return None\n        resolved_second = NUMBER_TYPE.resolve(other.second)\n        if not resolved_second:\n            return None\n        return CountType(other.first)"}
{"code":"def byte_op(computation: BaseComputation) -> None:\n    \"\"\"\n    Bitwise And\n    \"\"\"\n    position, value = computation.stack_pop(num_items=2, type_hint=constants.UINT256)\n\n    if position >= 32:\n        result = 0\n    else:\n        result = (value // pow(256, 31 - position)) % 256\n\n    computation.stack_push(result)","return_type":"None","function_name":"byte_op","stripped_code":"def byte_op(computation: BaseComputation):\n    \"\"\"\n    Bitwise And\n    \"\"\"\n    position, value = computation.stack_pop(num_items=2, type_hint=constants.UINT256)\n\n    if position >= 32:\n        result = 0\n    else:\n        result = (value // pow(256, 31 - position)) % 256\n\n    computation.stack_push(result)"}
{"code":"def env_int(name: str, required: bool=False, default: Union[Type[empty], int]=empty) -> int:\n    \"\"\"Pulls an environment variable out of the environment and casts it to an\n    integer. If the name is not present in the environment and no default is\n    specified then a ``ValueError`` will be raised. Similarly, if the\n    environment value is not castable to an integer, a ``ValueError`` will be\n    raised.\n\n    :param name: The name of the environment variable be pulled\n    :type name: str\n\n    :param required: Whether the environment variable is required. If ``True``\n    and the variable is not present, a ``KeyError`` is raised.\n    :type required: bool\n\n    :param default: The value to return if the environment variable is not\n    present. (Providing a default alongside setting ``required=True`` will raise\n    a ``ValueError``)\n    :type default: bool\n    \"\"\"\n    value = get_env_value(name, required=required, default=default)\n    if value is empty:\n        raise ValueError(\n            \"`env_int` requires either a default value to be specified, or for \"\n            \"the variable to be present in the environment\"\n        )\n    return int(value)","return_type":"int","function_name":"env_int","stripped_code":"def env_int(name: str, required: bool=False, default: Union[Type[empty], int]=empty):\n    \"\"\"Pulls an environment variable out of the environment and casts it to an\n    integer. If the name is not present in the environment and no default is\n    specified then a ``ValueError`` will be raised. Similarly, if the\n    environment value is not castable to an integer, a ``ValueError`` will be\n    raised.\n\n    :param name: The name of the environment variable be pulled\n    :type name: str\n\n    :param required: Whether the environment variable is required. If ``True``\n    and the variable is not present, a ``KeyError`` is raised.\n    :type required: bool\n\n    :param default: The value to return if the environment variable is not\n    present. (Providing a default alongside setting ``required=True`` will raise\n    a ``ValueError``)\n    :type default: bool\n    \"\"\"\n    value = get_env_value(name, required=required, default=default)\n    if value is empty:\n        raise ValueError(\n            \"`env_int` requires either a default value to be specified, or for \"\n            \"the variable to be present in the environment\"\n        )\n    return int(value)"}
{"code":"def from_raw(self, raw: RawScalar) -> Optional[bytes]:\n        \"\"\"Override superclass method.\"\"\"\n        try:\n            return base64.b64decode(raw, validate=True)\n        except TypeError:\n            return None","return_type":"Optional[bytes]","function_name":"BinaryType.from_raw","stripped_code":"def from_raw(self, raw: RawScalar):\n        \"\"\"Override superclass method.\"\"\"\n        try:\n            return base64.b64decode(raw, validate=True)\n        except TypeError:\n            return None"}
{"code":"def _save(self) -> None:\n        \"\"\"Save output to a directory.\"\"\"\n        self._log.info(\"Saving results to '%s'\" % self.folder)\n        path: str = self.folder + \"/\"\n        for job in self.results:\n            if job['domain'] in self.saved:\n                continue\n            job['start_time'] = str_datetime(job['start_time'])\n            job['end_time'] = str_datetime(job['end_time'])\n            jid: int = random.randint(100000, 999999)\n            filename: str = \"%s_%s_%d_job.json\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            handle.write(json.dumps(job, indent=4))\n            handle.close()\n\n            filename = \"%s_%s_%d_emails.txt\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            for email in job['results']['emails']:\n                handle.write(email + \"\\n\")\n            handle.close()\n            self.saved.append(job['domain'])","return_type":"None","function_name":"Frisbee._save","stripped_code":"def _save(self):\n        \"\"\"Save output to a directory.\"\"\"\n        self._log.info(\"Saving results to '%s'\" % self.folder)\n        path: str = self.folder + \"/\"\n        for job in self.results:\n            if job['domain'] in self.saved:\n                continue\n            job['start_time'] = str_datetime(job['start_time'])\n            job['end_time'] = str_datetime(job['end_time'])\n            jid: int = random.randint(100000, 999999)\n            filename: str = \"%s_%s_%d_job.json\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            handle.write(json.dumps(job, indent=4))\n            handle.close()\n\n            filename = \"%s_%s_%d_emails.txt\" % (self.project, job['domain'], jid)\n            handle = open(path + filename, 'w')\n            for email in job['results']['emails']:\n                handle.write(email + \"\\n\")\n            handle.close()\n            self.saved.append(job['domain'])"}
{"code":"def deleted_keys(self) -> Iterable[bytes]:\n        \"\"\"\n        List all the keys that have been deleted.\n        \"\"\"\n        for key, value in self._changes.items():\n            if value is DELETED:\n                yield key","return_type":"Iterable[bytes]","function_name":"DBDiff.deleted_keys","stripped_code":"def deleted_keys(self):\n        \"\"\"\n        List all the keys that have been deleted.\n        \"\"\"\n        for key, value in self._changes.items():\n            if value is DELETED:\n                yield key"}
{"code":"def _unique_class_name(namespace: Dict[str, Any], uuid: uuid.UUID) -> str:\n    '''Generate unique to namespace name for a class using uuid.\n\n    **Parameters**\n\n    :``namespace``: the namespace to verify uniqueness against\n    :``uuid``:      the \"unique\" portion of the name\n\n    **Return Value(s)**\n\n    A unique string (in namespace) using uuid.\n\n    '''\n\n    count = 0\n\n    name = original_name = 'f_' + uuid.hex\n    while name in namespace:\n        count += 1\n        name = original_name + '_' + str(count)\n\n    return name","return_type":"str","function_name":"_unique_class_name","stripped_code":"def _unique_class_name(namespace: Dict[str, Any], uuid: uuid.UUID):\n    '''Generate unique to namespace name for a class using uuid.\n\n    **Parameters**\n\n    :``namespace``: the namespace to verify uniqueness against\n    :``uuid``:      the \"unique\" portion of the name\n\n    **Return Value(s)**\n\n    A unique string (in namespace) using uuid.\n\n    '''\n\n    count = 0\n\n    name = original_name = 'f_' + uuid.hex\n    while name in namespace:\n        count += 1\n        name = original_name + '_' + str(count)\n\n    return name"}
{"code":"def _attend(self,\n                queries: mx.sym.Symbol,\n                keys: mx.sym.Symbol,\n                values: mx.sym.Symbol,\n                lengths: Optional[mx.sym.Symbol] = None,\n                bias: Optional[mx.sym.Symbol] = None) -> mx.sym.Symbol:\n        \"\"\"\n        Returns context vectors of multi-head dot attention.\n\n        :param queries: Query tensor. Shape: (batch_size, query_max_length, depth).\n        :param keys: Keys. Shape: (batch_size, memory_max_length, depth).\n        :param values: Values. Shape: (batch_size, memory_max_length, depth).\n        :param lengths: Optional lengths of keys. Shape: (batch_size,).\n        :param bias: Optional 3d bias.\n        :return: Context vectors. Shape: (batch_size, query_max_length, output_depth).\n        \"\"\"\n        # scale by sqrt(depth_per_head)\n        queries = queries * (self.depth_per_head ** -0.5)\n\n        # (batch*heads, length, depth/heads)\n        queries = split_heads(queries, self.depth_per_head, self.heads)\n        keys = split_heads(keys, self.depth_per_head, self.heads)\n        values = split_heads(values, self.depth_per_head, self.heads)\n        lengths = broadcast_to_heads(lengths, self.heads, ndim=1, fold_heads=True) if lengths is not None else lengths\n\n        # (batch*heads, query_max_length, depth_per_head)\n        contexts = dot_attention(queries, keys, values,\n                                 lengths=lengths, dropout=self.dropout, bias=bias, prefix=self.prefix)\n\n        # (batch, query_max_length, depth)\n        contexts = combine_heads(contexts, self.depth_per_head, self.heads)\n\n        # contexts: (batch, query_max_length, output_depth)\n        contexts = mx.sym.FullyConnected(data=contexts,\n                                         weight=self.w_h2o,\n                                         no_bias=True,\n                                         num_hidden=self.depth_out,\n                                         flatten=False)\n\n        return contexts","return_type":"mx.sym.Symbol","function_name":"MultiHeadAttentionBase._attend","stripped_code":"def _attend(self,\n                queries: mx.sym.Symbol,\n                keys: mx.sym.Symbol,\n                values: mx.sym.Symbol,\n                lengths: Optional[mx.sym.Symbol] = None,\n                bias: Optional[mx.sym.Symbol] = None):\n        \"\"\"\n        Returns context vectors of multi-head dot attention.\n\n        :param queries: Query tensor. Shape: (batch_size, query_max_length, depth).\n        :param keys: Keys. Shape: (batch_size, memory_max_length, depth).\n        :param values: Values. Shape: (batch_size, memory_max_length, depth).\n        :param lengths: Optional lengths of keys. Shape: (batch_size,).\n        :param bias: Optional 3d bias.\n        :return: Context vectors. Shape: (batch_size, query_max_length, output_depth).\n        \"\"\"\n        # scale by sqrt(depth_per_head)\n        queries = queries * (self.depth_per_head ** -0.5)\n\n        # (batch*heads, length, depth/heads)\n        queries = split_heads(queries, self.depth_per_head, self.heads)\n        keys = split_heads(keys, self.depth_per_head, self.heads)\n        values = split_heads(values, self.depth_per_head, self.heads)\n        lengths = broadcast_to_heads(lengths, self.heads, ndim=1, fold_heads=True) if lengths is not None else lengths\n\n        # (batch*heads, query_max_length, depth_per_head)\n        contexts = dot_attention(queries, keys, values,\n                                 lengths=lengths, dropout=self.dropout, bias=bias, prefix=self.prefix)\n\n        # (batch, query_max_length, depth)\n        contexts = combine_heads(contexts, self.depth_per_head, self.heads)\n\n        # contexts: (batch, query_max_length, output_depth)\n        contexts = mx.sym.FullyConnected(data=contexts,\n                                         weight=self.w_h2o,\n                                         no_bias=True,\n                                         num_hidden=self.depth_out,\n                                         flatten=False)\n\n        return contexts"}
{"code":"def first_sense(ambiguous_word: str, pos: str = None) -> \"wn.Synset\":\n    \"\"\"\n    Returns the first sense.\n\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: The first Synset in the wn.synsets(word) list.\n    \"\"\"\n\n    if pos is None:\n        return wn.synsets(ambiguous_word)[0]\n    else:\n        return wn.synsets(ambiguous_word, pos)[0]","return_type":"\"wn.Synset\"","function_name":"first_sense","stripped_code":"def first_sense(ambiguous_word: str, pos: str = None):\n    \"\"\"\n    Returns the first sense.\n\n    :param ambiguous_word: String, a single word.\n    :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.\n    :return: The first Synset in the wn.synsets(word) list.\n    \"\"\"\n\n    if pos is None:\n        return wn.synsets(ambiguous_word)[0]\n    else:\n        return wn.synsets(ambiguous_word, pos)[0]"}
{"code":"def parse_deckspawn_metainfo(protobuf: bytes, version: int) -> dict:\n    '''Decode deck_spawn tx op_return protobuf message and validate it,\n       Raise error if deck_spawn metainfo incomplete or version mistmatch.'''\n\n    deck = DeckSpawnProto()\n    deck.ParseFromString(protobuf)\n\n    error = {\"error\": \"Deck ({deck}) metainfo incomplete, deck must have a name.\".format(deck=deck.name)}\n\n    if deck.name == \"\":\n        raise InvalidDeckMetainfo(error)\n\n    if deck.version != version:\n        raise InvalidDeckVersion({\"error\", \"Deck version mismatch.\"})\n\n    return {\n        \"version\": deck.version,\n        \"name\": deck.name,\n        \"issue_mode\": deck.issue_mode,\n        \"number_of_decimals\": deck.number_of_decimals,\n        \"asset_specific_data\": deck.asset_specific_data\n        }","return_type":"dict","function_name":"parse_deckspawn_metainfo","stripped_code":"def parse_deckspawn_metainfo(protobuf: bytes, version: int):\n    '''Decode deck_spawn tx op_return protobuf message and validate it,\n       Raise error if deck_spawn metainfo incomplete or version mistmatch.'''\n\n    deck = DeckSpawnProto()\n    deck.ParseFromString(protobuf)\n\n    error = {\"error\": \"Deck ({deck}) metainfo incomplete, deck must have a name.\".format(deck=deck.name)}\n\n    if deck.name == \"\":\n        raise InvalidDeckMetainfo(error)\n\n    if deck.version != version:\n        raise InvalidDeckVersion({\"error\", \"Deck version mismatch.\"})\n\n    return {\n        \"version\": deck.version,\n        \"name\": deck.name,\n        \"issue_mode\": deck.issue_mode,\n        \"number_of_decimals\": deck.number_of_decimals,\n        \"asset_specific_data\": deck.asset_specific_data\n        }"}
{"code":"def _upload(self, items: Iterable[Tuple[str, str]]) -> None:\n        \"\"\"\n        Upload a collection of paths to S3.\n\n        :param items: An iterable of pairs containing the local path of the\n                      file to upload, and the remote path to upload it to. The\n                      prefix will be appended to each remote path.\n        \"\"\"\n        for src, key in items:\n            logger.info(f'Uploading {src} to {key}')\n            mimetype, _ = mimetypes.guess_type(src)\n            if mimetype is None:\n                logger.warning(f'Could not guess MIME type for {src}')\n                mimetype = 'application/octet-stream'\n\n            logger.debug(f'Deduced MIME type: {mimetype}')\n            self._bucket.upload_file(src, key, ExtraArgs={\n                    'ContentType': mimetype\n                })","return_type":"None","function_name":"Syncer._upload","stripped_code":"def _upload(self, items: Iterable[Tuple[str, str]]):\n        \"\"\"\n        Upload a collection of paths to S3.\n\n        :param items: An iterable of pairs containing the local path of the\n                      file to upload, and the remote path to upload it to. The\n                      prefix will be appended to each remote path.\n        \"\"\"\n        for src, key in items:\n            logger.info(f'Uploading {src} to {key}')\n            mimetype, _ = mimetypes.guess_type(src)\n            if mimetype is None:\n                logger.warning(f'Could not guess MIME type for {src}')\n                mimetype = 'application/octet-stream'\n\n            logger.debug(f'Deduced MIME type: {mimetype}')\n            self._bucket.upload_file(src, key, ExtraArgs={\n                    'ContentType': mimetype\n                })"}
{"code":"def card_bundle_parser(bundle: CardBundle, debug=False) -> Iterator:\n    '''this function wraps all the card transfer parsing'''\n\n    try:\n        # first vout of the bundle must pay to deck.p2th\n        validate_card_transfer_p2th(bundle.deck, bundle.vouts[0])\n\n        # second vout must be OP_RETURN with card_metainfo\n        card_metainfo = parse_card_transfer_metainfo(\n                            read_tx_opreturn(bundle.vouts[1]),\n                            bundle.deck.version\n                            )\n\n    # if any of this exceptions is raised, return None\n    except (InvalidCardTransferP2TH,\n            CardVersionMismatch,\n            CardNumberOfDecimalsMismatch,\n            RecieverAmountMismatch,\n            DecodeError,\n            TypeError,\n            InvalidNulldataOutput) as e:\n\n        if debug:\n            print(e)  # re-do as logging later on\n        return\n        yield\n\n    # check for decimals\n    if not card_metainfo[\"number_of_decimals\"] == bundle.deck.number_of_decimals:\n        raise CardNumberOfDecimalsMismatch(\n                {\"error\": \"Number of decimals does not match.\"}\n                )\n\n    # deduce the individual cards in the bundle\n    cards = card_postprocess(card_metainfo, bundle.vouts)\n\n    #  drop the vouts property\n    del bundle.__dict__['vouts']\n\n    for c in cards:\n        d = {**c, **bundle.__dict__}\n\n        try:\n            yield CardTransfer(**d)\n\n        # issuing cards to issuing address is forbidden,\n        # this will except the error\n        except InvalidCardIssue as e:\n\n            if debug:\n                print(e)","return_type":"Iterator","function_name":"card_bundle_parser","stripped_code":"def card_bundle_parser(bundle: CardBundle, debug=False):\n    '''this function wraps all the card transfer parsing'''\n\n    try:\n        # first vout of the bundle must pay to deck.p2th\n        validate_card_transfer_p2th(bundle.deck, bundle.vouts[0])\n\n        # second vout must be OP_RETURN with card_metainfo\n        card_metainfo = parse_card_transfer_metainfo(\n                            read_tx_opreturn(bundle.vouts[1]),\n                            bundle.deck.version\n                            )\n\n    # if any of this exceptions is raised, return None\n    except (InvalidCardTransferP2TH,\n            CardVersionMismatch,\n            CardNumberOfDecimalsMismatch,\n            RecieverAmountMismatch,\n            DecodeError,\n            TypeError,\n            InvalidNulldataOutput) as e:\n\n        if debug:\n            print(e)  # re-do as logging later on\n        return\n        yield\n\n    # check for decimals\n    if not card_metainfo[\"number_of_decimals\"] == bundle.deck.number_of_decimals:\n        raise CardNumberOfDecimalsMismatch(\n                {\"error\": \"Number of decimals does not match.\"}\n                )\n\n    # deduce the individual cards in the bundle\n    cards = card_postprocess(card_metainfo, bundle.vouts)\n\n    #  drop the vouts property\n    del bundle.__dict__['vouts']\n\n    for c in cards:\n        d = {**c, **bundle.__dict__}\n\n        try:\n            yield CardTransfer(**d)\n\n        # issuing cards to issuing address is forbidden,\n        # this will except the error\n        except InvalidCardIssue as e:\n\n            if debug:\n                print(e)"}
{"code":"def as_int(self, val: Tuple[str]) -> int:\n        \"\"\"Transform a \"bits\" value to an integer.\"\"\"\n        res = 0\n        try:\n            for b in val:\n                res += 1 << self.bit[b]\n        except KeyError:\n            return None\n        return res","return_type":"int","function_name":"BitsType.as_int","stripped_code":"def as_int(self, val: Tuple[str]):\n        \"\"\"Transform a \"bits\" value to an integer.\"\"\"\n        res = 0\n        try:\n            for b in val:\n                res += 1 << self.bit[b]\n        except KeyError:\n            return None\n        return res"}
{"code":"def trim_wav_pydub(in_path: Path, out_path: Path,\n                start_time: int, end_time: int) -> None:\n    \"\"\" Crops the wav file. \"\"\"\n\n    logger.info(\n        \"Using pydub/ffmpeg to create {} from {}\".format(out_path, in_path) +\n        \" using a start_time of {} and an end_time of {}\".format(start_time,\n                                                                 end_time))\n\n    if out_path.is_file():\n        return\n\n    # TODO add logging here\n    #print(\"in_fn: {}\".format(in_fn))\n    #print(\"out_fn: {}\".format(out_fn))\n    in_ext = in_path.suffix[1:]\n    out_ext = out_path.suffix[1:]\n    audio = AudioSegment.from_file(str(in_path), in_ext)\n    trimmed = audio[start_time:end_time]\n    # pydub evidently doesn't actually use the parameters when outputting wavs,\n    # since it doesn't use FFMPEG to deal with outputtting WAVs. This is a bit\n    # of a leaky abstraction. No warning is given, so normalization to 16Khz\n    # mono wavs has to happen later. Leaving the parameters here in case it\n    # changes\n    trimmed.export(str(out_path), format=out_ext,\n                   parameters=[\"-ac\", \"1\", \"-ar\", \"16000\"])","return_type":"None","function_name":"trim_wav_pydub","stripped_code":"def trim_wav_pydub(in_path: Path, out_path: Path,\n                start_time: int, end_time: int):\n    \"\"\" Crops the wav file. \"\"\"\n\n    logger.info(\n        \"Using pydub/ffmpeg to create {} from {}\".format(out_path, in_path) +\n        \" using a start_time of {} and an end_time of {}\".format(start_time,\n                                                                 end_time))\n\n    if out_path.is_file():\n        return\n\n    # TODO add logging here\n    #print(\"in_fn: {}\".format(in_fn))\n    #print(\"out_fn: {}\".format(out_fn))\n    in_ext = in_path.suffix[1:]\n    out_ext = out_path.suffix[1:]\n    audio = AudioSegment.from_file(str(in_path), in_ext)\n    trimmed = audio[start_time:end_time]\n    # pydub evidently doesn't actually use the parameters when outputting wavs,\n    # since it doesn't use FFMPEG to deal with outputtting WAVs. This is a bit\n    # of a leaky abstraction. No warning is given, so normalization to 16Khz\n    # mono wavs has to happen later. Leaving the parameters here in case it\n    # changes\n    trimmed.export(str(out_path), format=out_ext,\n                   parameters=[\"-ac\", \"1\", \"-ar\", \"16000\"])"}
{"code":"def _shell_to_ini(shell_file_contents: List[str]) -> List[str]:\n    \"\"\"\n    Converts a shell file, which just contains comments and \"export *\" statements into an ini file.\n    :param shell_file_contents: the contents of the shell file\n    :return: lines of an equivalent ini file\n    \"\"\"\n    line_number = 0\n    while line_number < len(shell_file_contents):\n        line = shell_file_contents[line_number].strip()\n        if \"=\" not in line:\n            del shell_file_contents[line_number]\n        else:\n            if line.strip().startswith(_EXPORT_COMMAND):\n                shell_file_contents[line_number] = line.replace(_EXPORT_COMMAND, \"\").strip()\n            line_number += 1\n    return shell_file_contents","return_type":"List[str]","function_name":"_shell_to_ini","stripped_code":"def _shell_to_ini(shell_file_contents: List[str]):\n    \"\"\"\n    Converts a shell file, which just contains comments and \"export *\" statements into an ini file.\n    :param shell_file_contents: the contents of the shell file\n    :return: lines of an equivalent ini file\n    \"\"\"\n    line_number = 0\n    while line_number < len(shell_file_contents):\n        line = shell_file_contents[line_number].strip()\n        if \"=\" not in line:\n            del shell_file_contents[line_number]\n        else:\n            if line.strip().startswith(_EXPORT_COMMAND):\n                shell_file_contents[line_number] = line.replace(_EXPORT_COMMAND, \"\").strip()\n            line_number += 1\n    return shell_file_contents"}
{"code":"def distribution_files(self) -> Iterator[str]:\n        \"\"\"Find distribution packages.\"\"\"\n        # This is verbatim from flake8\n        if self.distribution.packages:\n            package_dirs = self.distribution.package_dir or {}\n            for package in self.distribution.packages:\n                pkg_dir = package\n                if package in package_dirs:\n                    pkg_dir = package_dirs[package]\n                elif '' in package_dirs:\n                    pkg_dir = package_dirs[''] + os.path.sep + pkg_dir\n                yield pkg_dir.replace('.', os.path.sep)\n\n        if self.distribution.py_modules:\n            for filename in self.distribution.py_modules:\n                yield \"%s.py\" % filename\n        # Don't miss the setup.py file itself\n        yield \"setup.py\"","return_type":"Iterator[str]","function_name":"ISortCommand.distribution_files","stripped_code":"def distribution_files(self):\n        \"\"\"Find distribution packages.\"\"\"\n        # This is verbatim from flake8\n        if self.distribution.packages:\n            package_dirs = self.distribution.package_dir or {}\n            for package in self.distribution.packages:\n                pkg_dir = package\n                if package in package_dirs:\n                    pkg_dir = package_dirs[package]\n                elif '' in package_dirs:\n                    pkg_dir = package_dirs[''] + os.path.sep + pkg_dir\n                yield pkg_dir.replace('.', os.path.sep)\n\n        if self.distribution.py_modules:\n            for filename in self.distribution.py_modules:\n                yield \"%s.py\" % filename\n        # Don't miss the setup.py file itself\n        yield \"setup.py\""}
{"code":"def require_debian_packages(packages: List[str]) -> None:\n    \"\"\"\n    Ensure specific packages are installed under Debian.\n\n    Args:\n        packages: list of packages\n\n    Raises:\n        ValueError: if any are missing\n\n    \"\"\"\n    present = are_debian_packages_installed(packages)\n    missing_packages = [k for k, v in present.items() if not v]\n    if missing_packages:\n        missing_packages.sort()\n        msg = (\n            \"Debian packages are missing, as follows. Suggest:\\n\\n\"\n            \"sudo apt install {}\".format(\" \".join(missing_packages))\n        )\n        log.critical(msg)\n        raise ValueError(msg)","return_type":"None","function_name":"require_debian_packages","stripped_code":"def require_debian_packages(packages: List[str]):\n    \"\"\"\n    Ensure specific packages are installed under Debian.\n\n    Args:\n        packages: list of packages\n\n    Raises:\n        ValueError: if any are missing\n\n    \"\"\"\n    present = are_debian_packages_installed(packages)\n    missing_packages = [k for k, v in present.items() if not v]\n    if missing_packages:\n        missing_packages.sort()\n        msg = (\n            \"Debian packages are missing, as follows. Suggest:\\n\\n\"\n            \"sudo apt install {}\".format(\" \".join(missing_packages))\n        )\n        log.critical(msg)\n        raise ValueError(msg)"}
{"code":"def add(self, component: Union[Component, Sequence[Component]]) -> None:\n        \"\"\"Add a widget to the grid in the next available cell.\n\n        Searches over columns then rows for available cells.\n\n        Parameters\n        ----------\n        components : bowtie._Component\n            A Bowtie widget instance.\n\n        \"\"\"\n        try:\n            self[Span(*self._available_cell())] = component\n        except NoUnusedCellsError:\n            span = list(self._spans.keys())[-1]\n            self._spans[span] += component","return_type":"None","function_name":"View.add","stripped_code":"def add(self, component: Union[Component, Sequence[Component]]):\n        \"\"\"Add a widget to the grid in the next available cell.\n\n        Searches over columns then rows for available cells.\n\n        Parameters\n        ----------\n        components : bowtie._Component\n            A Bowtie widget instance.\n\n        \"\"\"\n        try:\n            self[Span(*self._available_cell())] = component\n        except NoUnusedCellsError:\n            span = list(self._spans.keys())[-1]\n            self._spans[span] += component"}
{"code":"def make_room_alias(chain_id: ChainID, *suffixes: str) -> str:\n    \"\"\"Given a chain_id and any number of suffixes (global room names, pair of addresses),\n    compose and return the canonical room name for raiden network\n\n    network name from raiden_contracts.constants.ID_TO_NETWORKNAME is used for name, if available,\n    else numeric id\n    Params:\n        chain_id: numeric blockchain id for that room, as raiden rooms are per-chain specific\n        *suffixes: one or more suffixes for the name\n    Returns:\n        Qualified full room name. e.g.:\n            make_room_alias(3, 'discovery') == 'raiden_ropsten_discovery'\n    \"\"\"\n    network_name = ID_TO_NETWORKNAME.get(chain_id, str(chain_id))\n    return ROOM_NAME_SEPARATOR.join([ROOM_NAME_PREFIX, network_name, *suffixes])","return_type":"str","function_name":"make_room_alias","stripped_code":"def make_room_alias(chain_id: ChainID, *suffixes: str):\n    \"\"\"Given a chain_id and any number of suffixes (global room names, pair of addresses),\n    compose and return the canonical room name for raiden network\n\n    network name from raiden_contracts.constants.ID_TO_NETWORKNAME is used for name, if available,\n    else numeric id\n    Params:\n        chain_id: numeric blockchain id for that room, as raiden rooms are per-chain specific\n        *suffixes: one or more suffixes for the name\n    Returns:\n        Qualified full room name. e.g.:\n            make_room_alias(3, 'discovery') == 'raiden_ropsten_discovery'\n    \"\"\"\n    network_name = ID_TO_NETWORKNAME.get(chain_id, str(chain_id))\n    return ROOM_NAME_SEPARATOR.join([ROOM_NAME_PREFIX, network_name, *suffixes])"}
{"code":"def asdensity(self) -> 'Density':\n        \"\"\"Convert a pure state to a density matrix\"\"\"\n        matrix = bk.outer(self.tensor, bk.conj(self.tensor))\n        return Density(matrix, self.qubits, self._memory)","return_type":"'Density'","function_name":"State.asdensity","stripped_code":"def asdensity(self):\n        \"\"\"Convert a pure state to a density matrix\"\"\"\n        matrix = bk.outer(self.tensor, bk.conj(self.tensor))\n        return Density(matrix, self.qubits, self._memory)"}
{"code":"def is_any_type_set(sett: Set[Type]) -> bool:\n    \"\"\"\n    Helper method to check if a set of types is the {AnyObject} singleton\n\n    :param sett:\n    :return:\n    \"\"\"\n    return len(sett) == 1 and is_any_type(min(sett))","return_type":"bool","function_name":"is_any_type_set","stripped_code":"def is_any_type_set(sett: Set[Type]):\n    \"\"\"\n    Helper method to check if a set of types is the {AnyObject} singleton\n\n    :param sett:\n    :return:\n    \"\"\"\n    return len(sett) == 1 and is_any_type(min(sett))"}
{"code":"def interact_model(config: Union[str, Path, dict]) -> None:\n    \"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input('{}::'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {'exit', 'stop', 'quit', 'q'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred = zip(*pred)\n\n        print('>>', *pred)","return_type":"None","function_name":"interact_model","stripped_code":"def interact_model(config: Union[str, Path, dict]):\n    \"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input('{}::'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {'exit', 'stop', 'quit', 'q'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred = zip(*pred)\n\n        print('>>', *pred)"}
{"code":"def GET_save_conditionvalues(self) -> None:\n        \"\"\"Save the |StateSequence| and |LogSequence| object values of the\n        current |HydPy| instance for the current simulation endpoint.\"\"\"\n        state.conditions[self._id] = state.conditions.get(self._id, {})\n        state.conditions[self._id][state.idx2] = state.hp.conditions","return_type":"None","function_name":"HydPyServer.GET_save_conditionvalues","stripped_code":"def GET_save_conditionvalues(self):\n        \"\"\"Save the |StateSequence| and |LogSequence| object values of the\n        current |HydPy| instance for the current simulation endpoint.\"\"\"\n        state.conditions[self._id] = state.conditions.get(self._id, {})\n        state.conditions[self._id][state.idx2] = state.hp.conditions"}
{"code":"def compare_overlaps_greedy(context: list, synsets_signatures: dict) -> \"wn.Synset\":\n    \"\"\"\n    Calculate overlaps between the context sentence and the synset_signatures\n    and returns the synset with the highest overlap.\n\n    Note: Greedy algorithm only keeps the best sense,\n    see https://en.wikipedia.org/wiki/Greedy_algorithm\n\n    Only used by original_lesk(). Keeping greedy algorithm for documentary sake,\n    because original_lesks is greedy.\n\n    :param context: List of strings, tokenized sentence or document.\n    :param synsets_signatures: dict of Synsets and the set of their corresponding signatures.\n    :return: The Synset with the highest number of overlaps with its signatures.\n    \"\"\"\n\n    max_overlaps = 0; lesk_sense = None\n    for ss in synsets_signatures:\n        overlaps = set(synsets_signatures[ss]).intersection(context)\n        if len(overlaps) > max_overlaps:\n            lesk_sense = ss\n            max_overlaps = len(overlaps)\n\n    return lesk_sense","return_type":"\"wn.Synset\"","function_name":"compare_overlaps_greedy","stripped_code":"def compare_overlaps_greedy(context: list, synsets_signatures: dict):\n    \"\"\"\n    Calculate overlaps between the context sentence and the synset_signatures\n    and returns the synset with the highest overlap.\n\n    Note: Greedy algorithm only keeps the best sense,\n    see https://en.wikipedia.org/wiki/Greedy_algorithm\n\n    Only used by original_lesk(). Keeping greedy algorithm for documentary sake,\n    because original_lesks is greedy.\n\n    :param context: List of strings, tokenized sentence or document.\n    :param synsets_signatures: dict of Synsets and the set of their corresponding signatures.\n    :return: The Synset with the highest number of overlaps with its signatures.\n    \"\"\"\n\n    max_overlaps = 0; lesk_sense = None\n    for ss in synsets_signatures:\n        overlaps = set(synsets_signatures[ss]).intersection(context)\n        if len(overlaps) > max_overlaps:\n            lesk_sense = ss\n            max_overlaps = len(overlaps)\n\n    return lesk_sense"}
{"code":"def reset(self) -> None:\n        \"\"\"\n        Reset the timers.\n        \"\"\"\n        self._overallstart = get_now_utc_pendulum()\n        self._starttimes.clear()\n        self._totaldurations.clear()\n        self._count.clear()\n        self._stack.clear()","return_type":"None","function_name":"MultiTimer.reset","stripped_code":"def reset(self):\n        \"\"\"\n        Reset the timers.\n        \"\"\"\n        self._overallstart = get_now_utc_pendulum()\n        self._starttimes.clear()\n        self._totaldurations.clear()\n        self._count.clear()\n        self._stack.clear()"}
{"code":"def visit_Attribute(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s representation as attribute access.\"\"\"\n        return '.'.join((self.visit(node.value), node.attr))","return_type":"str","function_name":"SourceGenerator.visit_Attribute","stripped_code":"def visit_Attribute(self, node: AST, dfltChaining: bool = True):\n        \"\"\"Return `node`s representation as attribute access.\"\"\"\n        return '.'.join((self.visit(node.value), node.attr))"}
{"code":"def add_prefix(self, ncname: str) -> None:\n        \"\"\" Look up ncname and add it to the prefix map if necessary\n\n        @param ncname: name to add\n        \"\"\"\n        if ncname not in self.prefixmap:\n            uri = cu.expand_uri(ncname + ':', self.curi_maps)\n            if uri and '://' in uri:\n                self.prefixmap[ncname] = uri\n            else:\n                print(f\"Unrecognized prefix: {ncname}\", file=sys.stderr)\n                self.prefixmap[ncname] = f\"http://example.org/unknown/{ncname}/\"","return_type":"None","function_name":"ContextGenerator.add_prefix","stripped_code":"def add_prefix(self, ncname: str):\n        \"\"\" Look up ncname and add it to the prefix map if necessary\n\n        @param ncname: name to add\n        \"\"\"\n        if ncname not in self.prefixmap:\n            uri = cu.expand_uri(ncname + ':', self.curi_maps)\n            if uri and '://' in uri:\n                self.prefixmap[ncname] = uri\n            else:\n                print(f\"Unrecognized prefix: {ncname}\", file=sys.stderr)\n                self.prefixmap[ncname] = f\"http://example.org/unknown/{ncname}/\""}
{"code":"def update_brand(self) -> None:\n        \"\"\"Update brand group of parameters.\"\"\"\n        self.update(path=URL_GET + GROUP.format(group=BRAND))","return_type":"None","function_name":"Brand.update_brand","stripped_code":"def update_brand(self):\n        \"\"\"Update brand group of parameters.\"\"\"\n        self.update(path=URL_GET + GROUP.format(group=BRAND))"}
{"code":"def json_get_default(json: JsonValue, path: str,\n                     default: Any, expected_type: Any = ANY) -> Any:\n    \"\"\"Get a JSON value by path, optionally checking its type.\n\n    This works exactly like json_get(), but instead of raising\n    ValueError or IndexError when a path part is not found, return\n    the provided default value:\n\n    >>> json_get_default({}, \"/foo\", \"I am a default value\")\n    'I am a default value'\n\n    TypeErrors will be raised as in json_get() if an expected_type\n    is provided:\n\n    >>> json_get_default({\"foo\": \"bar\"}, \"/foo\", 123, int)\n    Traceback (most recent call last):\n        ...\n    TypeError: wrong JSON type int != str\n    \"\"\"\n    try:\n        return json_get(json, path, expected_type)\n    except (ValueError, IndexError):\n        return default","return_type":"Any","function_name":"json_get_default","stripped_code":"def json_get_default(json: JsonValue, path: str,\n                     default: Any, expected_type: Any = ANY):\n    \"\"\"Get a JSON value by path, optionally checking its type.\n\n    This works exactly like json_get(), but instead of raising\n    ValueError or IndexError when a path part is not found, return\n    the provided default value:\n\n    >>> json_get_default({}, \"/foo\", \"I am a default value\")\n    'I am a default value'\n\n    TypeErrors will be raised as in json_get() if an expected_type\n    is provided:\n\n    >>> json_get_default({\"foo\": \"bar\"}, \"/foo\", 123, int)\n    Traceback (most recent call last):\n        ...\n    TypeError: wrong JSON type int != str\n    \"\"\"\n    try:\n        return json_get(json, path, expected_type)\n    except (ValueError, IndexError):\n        return default"}
{"code":"def set_bn_eval(m:nn.Module)->None:\n    \"Set bn layers in eval mode for all recursive children of `m`.\"\n    for l in m.children():\n        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n            l.eval()\n        set_bn_eval(l)","return_type":"None","function_name":"set_bn_eval","stripped_code":"def set_bn_eval(m:nn.Module):\n    \"Set bn layers in eval mode for all recursive children of `m`.\"\n    for l in m.children():\n        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n            l.eval()\n        set_bn_eval(l)"}
{"code":"def nickmask(prefix: str, kwargs: Dict[str, Any]) -> None:\n    \"\"\" store nick, user, host in kwargs if prefix is correct format \"\"\"\n    if \"!\" in prefix and \"@\" in prefix:\n        # From a user\n        kwargs[\"nick\"], remainder = prefix.split(\"!\", 1)\n        kwargs[\"user\"], kwargs[\"host\"] = remainder.split(\"@\", 1)\n    else:\n        # From a server, probably the host\n        kwargs[\"host\"] = prefix","return_type":"None","function_name":"nickmask","stripped_code":"def nickmask(prefix: str, kwargs: Dict[str, Any]):\n    \"\"\" store nick, user, host in kwargs if prefix is correct format \"\"\"\n    if \"!\" in prefix and \"@\" in prefix:\n        # From a user\n        kwargs[\"nick\"], remainder = prefix.split(\"!\", 1)\n        kwargs[\"user\"], kwargs[\"host\"] = remainder.split(\"@\", 1)\n    else:\n        # From a server, probably the host\n        kwargs[\"host\"] = prefix"}
{"code":"def parse_dge(\n        dge_path: str,\n        entrez_id_header: str,\n        log2_fold_change_header: str,\n        adj_p_header: str,\n        entrez_delimiter: str,\n        base_mean_header: Optional[str] = None\n) -> List[Gene]:\n    \"\"\"Parse a differential expression file.\n\n    :param dge_path: Path to the file.\n    :param entrez_id_header: Header for the Entrez identifier column\n    :param log2_fold_change_header: Header for the log2 fold change column\n    :param adj_p_header: Header for the adjusted p-value column\n    :param entrez_delimiter: Delimiter between Entrez ids.\n    :param base_mean_header: Header for the base mean column.\n    :return: A list of genes.\n    \"\"\"\n    if dge_path.endswith('.xlsx'):\n        return parsers.parse_excel(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n        )\n\n    if dge_path.endswith('.csv'):\n        return parsers.parse_csv(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n        )\n\n    if dge_path.endswith('.tsv'):\n        return parsers.parse_csv(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n            sep=\"\\t\"\n        )\n\n    raise ValueError(f'Unsupported extension: {dge_path}')","return_type":"List[Gene]","function_name":"parse_dge","stripped_code":"def parse_dge(\n        dge_path: str,\n        entrez_id_header: str,\n        log2_fold_change_header: str,\n        adj_p_header: str,\n        entrez_delimiter: str,\n        base_mean_header: Optional[str] = None\n):\n    \"\"\"Parse a differential expression file.\n\n    :param dge_path: Path to the file.\n    :param entrez_id_header: Header for the Entrez identifier column\n    :param log2_fold_change_header: Header for the log2 fold change column\n    :param adj_p_header: Header for the adjusted p-value column\n    :param entrez_delimiter: Delimiter between Entrez ids.\n    :param base_mean_header: Header for the base mean column.\n    :return: A list of genes.\n    \"\"\"\n    if dge_path.endswith('.xlsx'):\n        return parsers.parse_excel(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n        )\n\n    if dge_path.endswith('.csv'):\n        return parsers.parse_csv(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n        )\n\n    if dge_path.endswith('.tsv'):\n        return parsers.parse_csv(\n            dge_path,\n            entrez_id_header=entrez_id_header,\n            log_fold_change_header=log2_fold_change_header,\n            adjusted_p_value_header=adj_p_header,\n            entrez_delimiter=entrez_delimiter,\n            base_mean_header=base_mean_header,\n            sep=\"\\t\"\n        )\n\n    raise ValueError(f'Unsupported extension: {dge_path}')"}
{"code":"def string_to_general_float(s: str) -> float:\n    \"\"\"\n    Convert a string to corresponding single or double precision scientific number.\n\n    :param s: a string could be '0.1', '1e-5', '1.0D-5', or any other validated number\n    :return: a float or raise an error\n\n    .. doctest::\n\n        >>> string_to_general_float('1.0D-5')\n        1e-05\n        >>> string_to_general_float('1Dx')\n        Traceback (most recent call last):\n            ...\n        ValueError: The string '1Dx' does not corresponds to a double precision number!\n        >>> string_to_general_float('.8d234')\n        8e+233\n        >>> string_to_general_float('0.1')\n        0.1\n    \"\"\"\n    if 'D' in s.upper():  # Possible double precision number\n        try:\n            return string_to_double_precision_float(s)\n        except ValueError:\n            raise ValueError(\n                \"The string '{0}' does not corresponds to a double precision number!\".format(s))\n    else:\n        return float(s)","return_type":"float","function_name":"string_to_general_float","stripped_code":"def string_to_general_float(s: str):\n    \"\"\"\n    Convert a string to corresponding single or double precision scientific number.\n\n    :param s: a string could be '0.1', '1e-5', '1.0D-5', or any other validated number\n    :return: a float or raise an error\n\n    .. doctest::\n\n        >>> string_to_general_float('1.0D-5')\n        1e-05\n        >>> string_to_general_float('1Dx')\n        Traceback (most recent call last):\n            ...\n        ValueError: The string '1Dx' does not corresponds to a double precision number!\n        >>> string_to_general_float('.8d234')\n        8e+233\n        >>> string_to_general_float('0.1')\n        0.1\n    \"\"\"\n    if 'D' in s.upper():  # Possible double precision number\n        try:\n            return string_to_double_precision_float(s)\n        except ValueError:\n            raise ValueError(\n                \"The string '{0}' does not corresponds to a double precision number!\".format(s))\n    else:\n        return float(s)"}
{"code":"def ackermann_naive(m: int, n: int) -> int:\n    \"\"\"Ackermann number.\n    \"\"\"\n    if m == 0:\n        return n + 1\n    elif n == 0:\n        return ackermann(m - 1, 1)\n    else:\n        return ackermann(m - 1, ackermann(m, n - 1))","return_type":"int","function_name":"ackermann_naive","stripped_code":"def ackermann_naive(m: int, n: int):\n    \"\"\"Ackermann number.\n    \"\"\"\n    if m == 0:\n        return n + 1\n    elif n == 0:\n        return ackermann(m - 1, 1)\n    else:\n        return ackermann(m - 1, ackermann(m, n - 1))"}
{"code":"def matches_hostname(cls, certificate: cryptography.x509.Certificate, hostname: str) -> None:\n        \"\"\"Verify that the certificate was issued for the given hostname.\n\n        Raises:\n            CertificateError: If the certificate was not issued for the supplied hostname.\n        \"\"\"\n        # Extract the names from the certificate to create the properly-formatted dictionary\n        certificate_names = {\n            'subject': (tuple([('commonName', name) for name in cls.get_common_names(certificate.subject)]),),\n            'subjectAltName': tuple([('DNS', name) for name in cls.get_dns_subject_alternative_names(certificate)]),\n        }\n        # CertificateError is raised on failure\n        ssl.match_hostname(certificate_names, hostname)","return_type":"None","function_name":"CertificateUtils.matches_hostname","stripped_code":"def matches_hostname(cls, certificate: cryptography.x509.Certificate, hostname: str):\n        \"\"\"Verify that the certificate was issued for the given hostname.\n\n        Raises:\n            CertificateError: If the certificate was not issued for the supplied hostname.\n        \"\"\"\n        # Extract the names from the certificate to create the properly-formatted dictionary\n        certificate_names = {\n            'subject': (tuple([('commonName', name) for name in cls.get_common_names(certificate.subject)]),),\n            'subjectAltName': tuple([('DNS', name) for name in cls.get_dns_subject_alternative_names(certificate)]),\n        }\n        # CertificateError is raised on failure\n        ssl.match_hostname(certificate_names, hostname)"}
{"code":"def webify(v: Any, preserve_newlines: bool = True) -> str:\n    \"\"\"\n    Converts a value into an HTML-safe ``str`` (formerly, in Python 2:\n    ``unicode``).\n\n    Converts value ``v`` to a string; escapes it to be safe in HTML\n    format (escaping ampersands, replacing newlines with ``<br>``, etc.).\n    Returns ``\"\"`` for blank input.\n    \"\"\"\n    nl = \"<br>\" if preserve_newlines else \" \"\n    if v is None:\n        return \"\"\n    if not isinstance(v, str):\n        v = str(v)\n    # noinspection PyDeprecation\n    return cgi.escape(v).replace(\"\\n\", nl).replace(\"\\\\n\", nl)","return_type":"str","function_name":"webify","stripped_code":"def webify(v: Any, preserve_newlines: bool = True):\n    \"\"\"\n    Converts a value into an HTML-safe ``str`` (formerly, in Python 2:\n    ``unicode``).\n\n    Converts value ``v`` to a string; escapes it to be safe in HTML\n    format (escaping ampersands, replacing newlines with ``<br>``, etc.).\n    Returns ``\"\"`` for blank input.\n    \"\"\"\n    nl = \"<br>\" if preserve_newlines else \" \"\n    if v is None:\n        return \"\"\n    if not isinstance(v, str):\n        v = str(v)\n    # noinspection PyDeprecation\n    return cgi.escape(v).replace(\"\\n\", nl).replace(\"\\\\n\", nl)"}
{"code":"def read_cfg(path) -> dict:\n    \"\"\"\n    :param path:  example: \"/.rwmeta/developer_settings.json\"\n    :return: dict\n    \"\"\"\n    ret = None\n    full_path = __build_path(path)\n    if os.path.isfile(full_path):\n        with open(full_path, 'r') as myfile:\n            ret = json.loads(myfile.read())\n    return ret","return_type":"dict","function_name":"read_cfg","stripped_code":"def read_cfg(path):\n    \"\"\"\n    :param path:  example: \"/.rwmeta/developer_settings.json\"\n    :return: dict\n    \"\"\"\n    ret = None\n    full_path = __build_path(path)\n    if os.path.isfile(full_path):\n        with open(full_path, 'r') as myfile:\n            ret = json.loads(myfile.read())\n    return ret"}
{"code":"def process_macros(self, content: str) -> str:\n        '''Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros\n        '''\n\n        def _sub(macro):\n            name = macro.group('body')\n            params = self.get_options(macro.group('options'))\n\n            return self.options['macros'].get(name, '').format_map(params)\n\n        return self.pattern.sub(_sub, content)","return_type":"str","function_name":"Preprocessor.process_macros","stripped_code":"def process_macros(self, content: str):\n        '''Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros\n        '''\n\n        def _sub(macro):\n            name = macro.group('body')\n            params = self.get_options(macro.group('options'))\n\n            return self.options['macros'].get(name, '').format_map(params)\n\n        return self.pattern.sub(_sub, content)"}
{"code":"def _get_scaled_score(\n            simple_score: float,\n            categorical_score: float,\n            category_weight: Optional[float] = .5) -> float:\n        \"\"\"\n        Scaled score is the weighted average of the simple score and\n        categorical score\n        \"\"\"\n        return np.average(\n            [simple_score, categorical_score], weights=[1, category_weight]\n        )","return_type":"float","function_name":"AnnotationScorer._get_scaled_score","stripped_code":"def _get_scaled_score(\n            simple_score: float,\n            categorical_score: float,\n            category_weight: Optional[float] = .5):\n        \"\"\"\n        Scaled score is the weighted average of the simple score and\n        categorical score\n        \"\"\"\n        return np.average(\n            [simple_score, categorical_score], weights=[1, category_weight]\n        )"}
{"code":"def _pattern(trie: dict) -> str:\n    \"\"\"Convert a trie to a regex pattern.\"\"\"\n    if '' in trie:\n        if len(trie) == 1:\n            return ''\n        optional = True\n        del trie['']\n    else:\n        optional = False\n\n    subpattern_to_chars = _defaultdict(list)\n\n    for char, sub_trie in trie.items():\n        subpattern = _pattern(sub_trie)\n        subpattern_to_chars[subpattern].append(char)\n\n    alts = []\n    for subpattern, chars in subpattern_to_chars.items():\n        if len(chars) == 1:\n            alts.append(chars[0] + subpattern)\n        else:\n            chars.sort(reverse=True)\n            alts.append('[' + ''.join(chars) + ']' + subpattern)\n\n    if len(alts) == 1:\n        result = alts[0]\n        if optional:\n            if len(result) == 1:\n                result += '?+'\n            else:  # more than one character in alts[0]\n                result = '(?:' + result + ')?+'\n    else:\n        alts.sort(reverse=True)\n        result = '(?>' + '|'.join(alts) + ')'\n        if optional:\n            result += '?+'\n    return result","return_type":"str","function_name":"_pattern","stripped_code":"def _pattern(trie: dict):\n    \"\"\"Convert a trie to a regex pattern.\"\"\"\n    if '' in trie:\n        if len(trie) == 1:\n            return ''\n        optional = True\n        del trie['']\n    else:\n        optional = False\n\n    subpattern_to_chars = _defaultdict(list)\n\n    for char, sub_trie in trie.items():\n        subpattern = _pattern(sub_trie)\n        subpattern_to_chars[subpattern].append(char)\n\n    alts = []\n    for subpattern, chars in subpattern_to_chars.items():\n        if len(chars) == 1:\n            alts.append(chars[0] + subpattern)\n        else:\n            chars.sort(reverse=True)\n            alts.append('[' + ''.join(chars) + ']' + subpattern)\n\n    if len(alts) == 1:\n        result = alts[0]\n        if optional:\n            if len(result) == 1:\n                result += '?+'\n            else:  # more than one character in alts[0]\n                result = '(?:' + result + ')?+'\n    else:\n        alts.sort(reverse=True)\n        result = '(?>' + '|'.join(alts) + ')'\n        if optional:\n            result += '?+'\n    return result"}
{"code":"def from_yaml(cls, data: str, force_snake_case=True, force_cast: bool=False, restrict: bool=True) -> T:\n        \"\"\"From yaml string to instance\n\n        :param data: Yaml string\n        :param force_snake_case: Keys are transformed to snake case in order to compliant PEP8 if True\n        :param force_cast: Cast forcibly if True\n        :param restrict: Prohibit extra parameters if True\n        :return: Instance\n\n        Usage:\n\n            >>> from owlmixin.samples import Human\n            >>> human: Human = Human.from_yaml('''\n            ... id: 1\n            ... name: Tom\n            ... favorites:\n            ...   - name: Apple\n            ...     names_by_lang:\n            ...       en: Apple\n            ...       de: Apfel\n            ...   - name: Orange\n            ... ''')\n            >>> human.id\n            1\n            >>> human.name\n            'Tom'\n            >>> human.favorites[0].names_by_lang.get()[\"de\"]\n            'Apfel'\n        \"\"\"\n        return cls.from_dict(util.load_yaml(data),\n                             force_snake_case=force_snake_case,\n                             force_cast=force_cast,\n                             restrict=restrict)","return_type":"T","function_name":"OwlMixin.from_yaml","stripped_code":"def from_yaml(cls, data: str, force_snake_case=True, force_cast: bool=False, restrict: bool=True):\n        \"\"\"From yaml string to instance\n\n        :param data: Yaml string\n        :param force_snake_case: Keys are transformed to snake case in order to compliant PEP8 if True\n        :param force_cast: Cast forcibly if True\n        :param restrict: Prohibit extra parameters if True\n        :return: Instance\n\n        Usage:\n\n            >>> from owlmixin.samples import Human\n            >>> human: Human = Human.from_yaml('''\n            ... id: 1\n            ... name: Tom\n            ... favorites:\n            ...   - name: Apple\n            ...     names_by_lang:\n            ...       en: Apple\n            ...       de: Apfel\n            ...   - name: Orange\n            ... ''')\n            >>> human.id\n            1\n            >>> human.name\n            'Tom'\n            >>> human.favorites[0].names_by_lang.get()[\"de\"]\n            'Apfel'\n        \"\"\"\n        return cls.from_dict(util.load_yaml(data),\n                             force_snake_case=force_snake_case,\n                             force_cast=force_cast,\n                             restrict=restrict)"}
{"code":"def users_lookupByEmail(self, *, email: str, **kwargs) -> SlackResponse:\n        \"\"\"Find a user with an email address.\n\n        Args:\n            email (str): An email address belonging to a user in the workspace.\n                e.g. 'spengler@ghostbusters.example.com'\n        \"\"\"\n        kwargs.update({\"email\": email})\n        return self.api_call(\"users.lookupByEmail\", http_verb=\"GET\", params=kwargs)","return_type":"SlackResponse","function_name":"WebClient.users_lookupByEmail","stripped_code":"def users_lookupByEmail(self, *, email: str, **kwargs):\n        \"\"\"Find a user with an email address.\n\n        Args:\n            email (str): An email address belonging to a user in the workspace.\n                e.g. 'spengler@ghostbusters.example.com'\n        \"\"\"\n        kwargs.update({\"email\": email})\n        return self.api_call(\"users.lookupByEmail\", http_verb=\"GET\", params=kwargs)"}
{"code":"def gitlab(\n    task: Task,\n    url: str,\n    token: str,\n    repository: str,\n    filename: str,\n    content: str = \"\",\n    action: str = \"create\",\n    dry_run: bool = False,\n    branch: str = \"master\",\n    destination: str = \"\",\n    ref: str = \"master\",\n    commit_message: str = \"\",\n) -> Result:\n    \"\"\"\n    Exposes some of the Gitlab API functionality for operations on files\n    in a Gitlab repository.\n\n    Example:\n\n        nornir.run(files.gitlab,\n                   action=\"create\",\n                   url=\"https://gitlab.localhost.com\",\n                   token=\"ABCD1234\",\n                   repository=\"test\",\n                   filename=\"config\",\n                   ref=\"master\")\n\n    Arguments:\n        dry_run: Whether to apply changes or not\n        url: Gitlab instance URL\n        token: Personal access token\n        repository: source/destination repository\n        filename: source/destination file name\n        content: content to write\n        action: ``create``, ``update``, ``get``\n        branch: destination branch\n        destination: local destination filename (only used in get action)\n        ref: branch, commit hash or tag (only used in get action)\n        commit_message: commit message\n\n    Returns:\n        Result object with the following attributes set:\n            * changed (``bool``):\n            * diff (``str``): unified diff\n\n    \"\"\"\n    dry_run = task.is_dry_run(dry_run)\n\n    session = requests.session()\n    session.headers.update({\"PRIVATE-TOKEN\": token})\n\n    if commit_message == \"\":\n        commit_message = \"File created with nornir\"\n\n    pid = _get_repository(session, url, repository)\n\n    if action == \"create\":\n        diff = _create(\n            task, session, url, pid, filename, content, branch, commit_message, dry_run\n        )\n    elif action == \"update\":\n        diff = _update(\n            task, session, url, pid, filename, content, branch, commit_message, dry_run\n        )\n    elif action == \"get\":\n        diff = _get(task, session, url, pid, filename, destination, ref, dry_run)\n    return Result(host=task.host, diff=diff, changed=bool(diff))","return_type":"Result","function_name":"gitlab","stripped_code":"def gitlab(\n    task: Task,\n    url: str,\n    token: str,\n    repository: str,\n    filename: str,\n    content: str = \"\",\n    action: str = \"create\",\n    dry_run: bool = False,\n    branch: str = \"master\",\n    destination: str = \"\",\n    ref: str = \"master\",\n    commit_message: str = \"\",\n):\n    \"\"\"\n    Exposes some of the Gitlab API functionality for operations on files\n    in a Gitlab repository.\n\n    Example:\n\n        nornir.run(files.gitlab,\n                   action=\"create\",\n                   url=\"https://gitlab.localhost.com\",\n                   token=\"ABCD1234\",\n                   repository=\"test\",\n                   filename=\"config\",\n                   ref=\"master\")\n\n    Arguments:\n        dry_run: Whether to apply changes or not\n        url: Gitlab instance URL\n        token: Personal access token\n        repository: source/destination repository\n        filename: source/destination file name\n        content: content to write\n        action: ``create``, ``update``, ``get``\n        branch: destination branch\n        destination: local destination filename (only used in get action)\n        ref: branch, commit hash or tag (only used in get action)\n        commit_message: commit message\n\n    Returns:\n        Result object with the following attributes set:\n            * changed (``bool``):\n            * diff (``str``): unified diff\n\n    \"\"\"\n    dry_run = task.is_dry_run(dry_run)\n\n    session = requests.session()\n    session.headers.update({\"PRIVATE-TOKEN\": token})\n\n    if commit_message == \"\":\n        commit_message = \"File created with nornir\"\n\n    pid = _get_repository(session, url, repository)\n\n    if action == \"create\":\n        diff = _create(\n            task, session, url, pid, filename, content, branch, commit_message, dry_run\n        )\n    elif action == \"update\":\n        diff = _update(\n            task, session, url, pid, filename, content, branch, commit_message, dry_run\n        )\n    elif action == \"get\":\n        diff = _get(task, session, url, pid, filename, destination, ref, dry_run)\n    return Result(host=task.host, diff=diff, changed=bool(diff))"}
{"code":"def at_block_number(block_number: BlockNumber, chain: MiningChain) -> MiningChain:\n    \"\"\"\n    Rewind the chain back to the given block number.  Calls to things like\n    ``get_canonical_head`` will still return the canonical head of the chain,\n    however, you can use ``mine_block`` to mine fork chains.\n    \"\"\"\n    if not isinstance(chain, MiningChain):\n        raise ValidationError(\"`at_block_number` may only be used with 'MiningChain\")\n    at_block = chain.get_canonical_block_by_number(block_number)\n\n    db = chain.chaindb.db\n    chain_at_block = type(chain)(db, chain.create_header_from_parent(at_block.header))\n    return chain_at_block","return_type":"MiningChain","function_name":"at_block_number","stripped_code":"def at_block_number(block_number: BlockNumber, chain: MiningChain):\n    \"\"\"\n    Rewind the chain back to the given block number.  Calls to things like\n    ``get_canonical_head`` will still return the canonical head of the chain,\n    however, you can use ``mine_block`` to mine fork chains.\n    \"\"\"\n    if not isinstance(chain, MiningChain):\n        raise ValidationError(\"`at_block_number` may only be used with 'MiningChain\")\n    at_block = chain.get_canonical_block_by_number(block_number)\n\n    db = chain.chaindb.db\n    chain_at_block = type(chain)(db, chain.create_header_from_parent(at_block.header))\n    return chain_at_block"}
{"code":"def put(\n        self, item: _T, timeout: Union[float, datetime.timedelta] = None\n    ) -> \"Future[None]\":\n        \"\"\"Put an item into the queue, perhaps waiting until there is room.\n\n        Returns a Future, which raises `tornado.util.TimeoutError` after a\n        timeout.\n\n        ``timeout`` may be a number denoting a time (on the same\n        scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n        `datetime.timedelta` object for a deadline relative to the\n        current time.\n        \"\"\"\n        future = Future()  # type: Future[None]\n        try:\n            self.put_nowait(item)\n        except QueueFull:\n            self._putters.append((item, future))\n            _set_timeout(future, timeout)\n        else:\n            future.set_result(None)\n        return future","return_type":"\"Future[None]\"","function_name":"Queue.put","stripped_code":"def put(\n        self, item: _T, timeout: Union[float, datetime.timedelta] = None\n    ):\n        \"\"\"Put an item into the queue, perhaps waiting until there is room.\n\n        Returns a Future, which raises `tornado.util.TimeoutError` after a\n        timeout.\n\n        ``timeout`` may be a number denoting a time (on the same\n        scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n        `datetime.timedelta` object for a deadline relative to the\n        current time.\n        \"\"\"\n        future = Future()  # type: Future[None]\n        try:\n            self.put_nowait(item)\n        except QueueFull:\n            self._putters.append((item, future))\n            _set_timeout(future, timeout)\n        else:\n            future.set_result(None)\n        return future"}
{"code":"def get_batch_size(batch: Union[Dict, torch.Tensor]) -> int:\n    \"\"\"\n    Returns the size of the batch dimension. Assumes a well-formed batch,\n    returns 0 otherwise.\n    \"\"\"\n    if isinstance(batch, torch.Tensor):\n        return batch.size(0) # type: ignore\n    elif isinstance(batch, Dict):\n        return get_batch_size(next(iter(batch.values())))\n    else:\n        return 0","return_type":"int","function_name":"get_batch_size","stripped_code":"def get_batch_size(batch: Union[Dict, torch.Tensor]):\n    \"\"\"\n    Returns the size of the batch dimension. Assumes a well-formed batch,\n    returns 0 otherwise.\n    \"\"\"\n    if isinstance(batch, torch.Tensor):\n        return batch.size(0) # type: ignore\n    elif isinstance(batch, Dict):\n        return get_batch_size(next(iter(batch.values())))\n    else:\n        return 0"}
{"code":"def ask_int(question: str, default: int = None) -> int:\n    \"\"\"Asks for a number in a question\"\"\"\n    default_q = \" [default: {0}]: \".format(\n        default) if default is not None else \"\"\n    answer = input(\"{0} [{1}]: \".format(question, default_q))\n\n    if not answer:\n        if default is None:\n            print(\"No default set, try again.\")\n            return ask_int(question, default)\n        return default\n\n    if any(x not in \"1234567890\" for x in answer):\n        print(\"Please enter only numbers (0-9).\")\n        return ask_int(question, default)\n\n    return int(answer)","return_type":"int","function_name":"ask_int","stripped_code":"def ask_int(question: str, default: int = None):\n    \"\"\"Asks for a number in a question\"\"\"\n    default_q = \" [default: {0}]: \".format(\n        default) if default is not None else \"\"\n    answer = input(\"{0} [{1}]: \".format(question, default_q))\n\n    if not answer:\n        if default is None:\n            print(\"No default set, try again.\")\n            return ask_int(question, default)\n        return default\n\n    if any(x not in \"1234567890\" for x in answer):\n        print(\"Please enter only numbers (0-9).\")\n        return ask_int(question, default)\n\n    return int(answer)"}
{"code":"def _potential_cross_whole_w(moment_index: int,\n                             op: ops.Operation,\n                             tolerance: float,\n                             state: _OptimizerState) -> None:\n    \"\"\"Grabs or cancels a held W gate against an existing W gate.\n\n    [Where W(a) is shorthand for PhasedX(phase_exponent=a).]\n\n    Uses the following identity:\n        \u2500\u2500\u2500W(a)\u2500\u2500\u2500W(b)\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500X\u2500\u2500\u2500Z^a\u2500\u2500\u2500Z^-b\u2500\u2500\u2500X\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^b\u2500\u2500\u2500X\u2500\u2500\u2500X\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^b\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^2(b-a)\u2500\u2500\u2500\n    \"\"\"\n    state.deletions.append((moment_index, op))\n\n    _, phase_exponent = cast(Tuple[float, float],\n                             _try_get_known_phased_pauli(op))\n    q = op.qubits[0]\n    a = state.held_w_phases.get(q)\n    b = phase_exponent\n\n    if a is None:\n        # Collect the gate.\n        state.held_w_phases[q] = b\n    else:\n        # Cancel the gate.\n        state.held_w_phases[q] = None\n        t = 2*(b - a)\n        if not decompositions.is_negligible_turn(t / 2, tolerance):\n            leftover_phase = ops.Z(q)**t\n            state.inline_intos.append((moment_index, leftover_phase))","return_type":"None","function_name":"_potential_cross_whole_w","stripped_code":"def _potential_cross_whole_w(moment_index: int,\n                             op: ops.Operation,\n                             tolerance: float,\n                             state: _OptimizerState):\n    \"\"\"Grabs or cancels a held W gate against an existing W gate.\n\n    [Where W(a) is shorthand for PhasedX(phase_exponent=a).]\n\n    Uses the following identity:\n        \u2500\u2500\u2500W(a)\u2500\u2500\u2500W(b)\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500X\u2500\u2500\u2500Z^a\u2500\u2500\u2500Z^-b\u2500\u2500\u2500X\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^b\u2500\u2500\u2500X\u2500\u2500\u2500X\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^-a\u2500\u2500\u2500Z^b\u2500\u2500\u2500Z^b\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^2(b-a)\u2500\u2500\u2500\n    \"\"\"\n    state.deletions.append((moment_index, op))\n\n    _, phase_exponent = cast(Tuple[float, float],\n                             _try_get_known_phased_pauli(op))\n    q = op.qubits[0]\n    a = state.held_w_phases.get(q)\n    b = phase_exponent\n\n    if a is None:\n        # Collect the gate.\n        state.held_w_phases[q] = b\n    else:\n        # Cancel the gate.\n        state.held_w_phases[q] = None\n        t = 2*(b - a)\n        if not decompositions.is_negligible_turn(t / 2, tolerance):\n            leftover_phase = ops.Z(q)**t\n            state.inline_intos.append((moment_index, leftover_phase))"}
{"code":"def process_data(input_file: str,\n                 output_file: str,\n                 max_path_length: int,\n                 max_num_logical_forms: int,\n                 ignore_agenda: bool,\n                 write_sequences: bool) -> None:\n    \"\"\"\n    Reads an NLVR dataset and returns a JSON representation containing sentences, labels, correct and\n    incorrect logical forms. The output will contain at most `max_num_logical_forms` logical forms\n    each in both correct and incorrect lists. The output format is:\n        ``[{\"id\": str, \"label\": str, \"sentence\": str, \"correct\": List[str], \"incorrect\": List[str]}]``\n    \"\"\"\n    processed_data: JsonDict = []\n    # We can instantiate the ``ActionSpaceWalker`` with any world because the action space is the\n    # same for all the ``NlvrWorlds``. It is just the execution that differs.\n    serialized_walker_path = f\"serialized_action_space_walker_pl={max_path_length}.pkl\"\n    if os.path.isfile(serialized_walker_path):\n        print(\"Reading walker from serialized file\", file=sys.stderr)\n        walker = pickle.load(open(serialized_walker_path, \"rb\"))\n    else:\n        walker = ActionSpaceWalker(NlvrWorld({}), max_path_length=max_path_length)\n        pickle.dump(walker, open(serialized_walker_path, \"wb\"))\n    for line in open(input_file):\n        instance_id, sentence, structured_reps, label_strings = read_json_line(line)\n        worlds = [NlvrWorld(structured_rep) for structured_rep in structured_reps]\n        labels = [label_string == \"true\" for label_string in label_strings]\n        correct_logical_forms = []\n        incorrect_logical_forms = []\n        if ignore_agenda:\n            # Get 1000 shortest logical forms.\n            logical_forms = walker.get_all_logical_forms(max_num_logical_forms=1000)\n        else:\n            # TODO (pradeep): Assuming all worlds give the same agenda.\n            sentence_agenda = worlds[0].get_agenda_for_sentence(sentence, add_paths_to_agenda=False)\n            logical_forms = walker.get_logical_forms_with_agenda(sentence_agenda,\n                                                                 max_num_logical_forms * 10)\n        for logical_form in logical_forms:\n            if all([world.execute(logical_form) == label for world, label in zip(worlds, labels)]):\n                if len(correct_logical_forms) <= max_num_logical_forms:\n                    correct_logical_forms.append(logical_form)\n            else:\n                if len(incorrect_logical_forms) <= max_num_logical_forms:\n                    incorrect_logical_forms.append(logical_form)\n            if len(correct_logical_forms) >= max_num_logical_forms \\\n               and len(incorrect_logical_forms) >= max_num_logical_forms:\n                break\n        if write_sequences:\n            parsed_correct_forms = [worlds[0].parse_logical_form(logical_form) for logical_form in\n                                    correct_logical_forms]\n            correct_sequences = [worlds[0].get_action_sequence(parsed_form) for parsed_form in\n                                 parsed_correct_forms]\n            parsed_incorrect_forms = [worlds[0].parse_logical_form(logical_form) for logical_form in\n                                      incorrect_logical_forms]\n            incorrect_sequences = [worlds[0].get_action_sequence(parsed_form) for parsed_form in\n                                   parsed_incorrect_forms]\n            processed_data.append({\"id\": instance_id,\n                                   \"sentence\": sentence,\n                                   \"correct_sequences\": correct_sequences,\n                                   \"incorrect_sequences\": incorrect_sequences,\n                                   \"worlds\": structured_reps,\n                                   \"labels\": label_strings})\n        else:\n            processed_data.append({\"id\": instance_id,\n                                   \"sentence\": sentence,\n                                   \"correct_logical_forms\": correct_logical_forms,\n                                   \"incorrect_logical_forms\": incorrect_logical_forms,\n                                   \"worlds\": structured_reps,\n                                   \"labels\": label_strings})\n    with open(output_file, \"w\") as outfile:\n        for instance_processed_data in processed_data:\n            json.dump(instance_processed_data, outfile)\n            outfile.write('\\n')\n        outfile.close()","return_type":"None","function_name":"process_data","stripped_code":"def process_data(input_file: str,\n                 output_file: str,\n                 max_path_length: int,\n                 max_num_logical_forms: int,\n                 ignore_agenda: bool,\n                 write_sequences: bool):\n    \"\"\"\n    Reads an NLVR dataset and returns a JSON representation containing sentences, labels, correct and\n    incorrect logical forms. The output will contain at most `max_num_logical_forms` logical forms\n    each in both correct and incorrect lists. The output format is:\n        ``[{\"id\": str, \"label\": str, \"sentence\": str, \"correct\": List[str], \"incorrect\": List[str]}]``\n    \"\"\"\n    processed_data: JsonDict = []\n    # We can instantiate the ``ActionSpaceWalker`` with any world because the action space is the\n    # same for all the ``NlvrWorlds``. It is just the execution that differs.\n    serialized_walker_path = f\"serialized_action_space_walker_pl={max_path_length}.pkl\"\n    if os.path.isfile(serialized_walker_path):\n        print(\"Reading walker from serialized file\", file=sys.stderr)\n        walker = pickle.load(open(serialized_walker_path, \"rb\"))\n    else:\n        walker = ActionSpaceWalker(NlvrWorld({}), max_path_length=max_path_length)\n        pickle.dump(walker, open(serialized_walker_path, \"wb\"))\n    for line in open(input_file):\n        instance_id, sentence, structured_reps, label_strings = read_json_line(line)\n        worlds = [NlvrWorld(structured_rep) for structured_rep in structured_reps]\n        labels = [label_string == \"true\" for label_string in label_strings]\n        correct_logical_forms = []\n        incorrect_logical_forms = []\n        if ignore_agenda:\n            # Get 1000 shortest logical forms.\n            logical_forms = walker.get_all_logical_forms(max_num_logical_forms=1000)\n        else:\n            # TODO (pradeep): Assuming all worlds give the same agenda.\n            sentence_agenda = worlds[0].get_agenda_for_sentence(sentence, add_paths_to_agenda=False)\n            logical_forms = walker.get_logical_forms_with_agenda(sentence_agenda,\n                                                                 max_num_logical_forms * 10)\n        for logical_form in logical_forms:\n            if all([world.execute(logical_form) == label for world, label in zip(worlds, labels)]):\n                if len(correct_logical_forms) <= max_num_logical_forms:\n                    correct_logical_forms.append(logical_form)\n            else:\n                if len(incorrect_logical_forms) <= max_num_logical_forms:\n                    incorrect_logical_forms.append(logical_form)\n            if len(correct_logical_forms) >= max_num_logical_forms \\\n               and len(incorrect_logical_forms) >= max_num_logical_forms:\n                break\n        if write_sequences:\n            parsed_correct_forms = [worlds[0].parse_logical_form(logical_form) for logical_form in\n                                    correct_logical_forms]\n            correct_sequences = [worlds[0].get_action_sequence(parsed_form) for parsed_form in\n                                 parsed_correct_forms]\n            parsed_incorrect_forms = [worlds[0].parse_logical_form(logical_form) for logical_form in\n                                      incorrect_logical_forms]\n            incorrect_sequences = [worlds[0].get_action_sequence(parsed_form) for parsed_form in\n                                   parsed_incorrect_forms]\n            processed_data.append({\"id\": instance_id,\n                                   \"sentence\": sentence,\n                                   \"correct_sequences\": correct_sequences,\n                                   \"incorrect_sequences\": incorrect_sequences,\n                                   \"worlds\": structured_reps,\n                                   \"labels\": label_strings})\n        else:\n            processed_data.append({\"id\": instance_id,\n                                   \"sentence\": sentence,\n                                   \"correct_logical_forms\": correct_logical_forms,\n                                   \"incorrect_logical_forms\": incorrect_logical_forms,\n                                   \"worlds\": structured_reps,\n                                   \"labels\": label_strings})\n    with open(output_file, \"w\") as outfile:\n        for instance_processed_data in processed_data:\n            json.dump(instance_processed_data, outfile)\n            outfile.write('\\n')\n        outfile.close()"}
{"code":"def register(self, callback_id: str, handler: Any, name: str = \"*\") -> None:\n        \"\"\"\n        Register a new handler for a specific :class:`slack.actions.Action` `callback_id`.\n        Optional routing based on the action name too.\n\n        The name argument is useful for actions of type `interactive_message` to provide\n        a different handler for each individual action.\n\n        Args:\n            callback_id: Callback_id the handler is interested in\n            handler: Callback\n            name: Name of the action (optional).\n        \"\"\"\n        LOG.info(\"Registering %s, %s to %s\", callback_id, name, handler)\n        if name not in self._routes[callback_id]:\n            self._routes[callback_id][name] = []\n\n        self._routes[callback_id][name].append(handler)","return_type":"None","function_name":"Router.register","stripped_code":"def register(self, callback_id: str, handler: Any, name: str = \"*\"):\n        \"\"\"\n        Register a new handler for a specific :class:`slack.actions.Action` `callback_id`.\n        Optional routing based on the action name too.\n\n        The name argument is useful for actions of type `interactive_message` to provide\n        a different handler for each individual action.\n\n        Args:\n            callback_id: Callback_id the handler is interested in\n            handler: Callback\n            name: Name of the action (optional).\n        \"\"\"\n        LOG.info(\"Registering %s, %s to %s\", callback_id, name, handler)\n        if name not in self._routes[callback_id]:\n            self._routes[callback_id][name] = []\n\n        self._routes[callback_id][name].append(handler)"}
{"code":"def permute(self, permutations: List[mx.nd.NDArray]) -> 'ParallelDataSet':\n        \"\"\"\n        Permutes the data within each bucket. The permutation is received as an argument,\n        allowing the data to be unpermuted (i.e., restored) later on.\n\n        :param permutations: For each bucket, a permutation of the data within that bucket.\n        :return: A new, permuted ParallelDataSet.\n        \"\"\"\n        assert len(self) == len(permutations)\n        source = []\n        target = []\n        label = []\n        for buck_idx in range(len(self)):\n            num_samples = self.source[buck_idx].shape[0]\n            if num_samples:  # not empty bucket\n                permutation = permutations[buck_idx]\n                if isinstance(self.source[buck_idx], np.ndarray):\n                    source.append(self.source[buck_idx].take(np.int64(permutation.asnumpy())))\n                else:\n                    source.append(self.source[buck_idx].take(permutation))\n                target.append(self.target[buck_idx].take(permutation))\n                label.append(self.label[buck_idx].take(permutation))\n            else:\n                source.append(self.source[buck_idx])\n                target.append(self.target[buck_idx])\n                label.append(self.label[buck_idx])\n\n        return ParallelDataSet(source, target, label)","return_type":"'ParallelDataSet'","function_name":"ParallelDataSet.permute","stripped_code":"def permute(self, permutations: List[mx.nd.NDArray]):\n        \"\"\"\n        Permutes the data within each bucket. The permutation is received as an argument,\n        allowing the data to be unpermuted (i.e., restored) later on.\n\n        :param permutations: For each bucket, a permutation of the data within that bucket.\n        :return: A new, permuted ParallelDataSet.\n        \"\"\"\n        assert len(self) == len(permutations)\n        source = []\n        target = []\n        label = []\n        for buck_idx in range(len(self)):\n            num_samples = self.source[buck_idx].shape[0]\n            if num_samples:  # not empty bucket\n                permutation = permutations[buck_idx]\n                if isinstance(self.source[buck_idx], np.ndarray):\n                    source.append(self.source[buck_idx].take(np.int64(permutation.asnumpy())))\n                else:\n                    source.append(self.source[buck_idx].take(permutation))\n                target.append(self.target[buck_idx].take(permutation))\n                label.append(self.label[buck_idx].take(permutation))\n            else:\n                source.append(self.source[buck_idx])\n                target.append(self.target[buck_idx])\n                label.append(self.label[buck_idx])\n\n        return ParallelDataSet(source, target, label)"}
{"code":"def sys_set_renderer(renderer: int) -> None:\n    \"\"\"Change the current rendering mode to renderer.\n\n    .. deprecated:: 2.0\n       RENDERER_GLSL and RENDERER_OPENGL are not currently available.\n    \"\"\"\n    lib.TCOD_sys_set_renderer(renderer)\n    if tcod.console._root_console is not None:\n        tcod.console.Console._get_root()","return_type":"None","function_name":"sys_set_renderer","stripped_code":"def sys_set_renderer(renderer: int):\n    \"\"\"Change the current rendering mode to renderer.\n\n    .. deprecated:: 2.0\n       RENDERER_GLSL and RENDERER_OPENGL are not currently available.\n    \"\"\"\n    lib.TCOD_sys_set_renderer(renderer)\n    if tcod.console._root_console is not None:\n        tcod.console.Console._get_root()"}
{"code":"def persist_trie_data_dict(self, trie_data_dict: Dict[Hash32, bytes]) -> None:\n        \"\"\"\n        Store raw trie data to db from a dict\n        \"\"\"\n        with self.db.atomic_batch() as db:\n            for key, value in trie_data_dict.items():\n                db[key] = value","return_type":"None","function_name":"ChainDB.persist_trie_data_dict","stripped_code":"def persist_trie_data_dict(self, trie_data_dict: Dict[Hash32, bytes]):\n        \"\"\"\n        Store raw trie data to db from a dict\n        \"\"\"\n        with self.db.atomic_batch() as db:\n            for key, value in trie_data_dict.items():\n                db[key] = value"}
{"code":"def is_ipfs_uri(value: str) -> bool:\n    \"\"\"\n    Return a bool indicating whether or not the value is a valid IPFS URI.\n    \"\"\"\n    parse_result = parse.urlparse(value)\n    if parse_result.scheme != \"ipfs\":\n        return False\n    if not parse_result.netloc and not parse_result.path:\n        return False\n\n    return True","return_type":"bool","function_name":"is_ipfs_uri","stripped_code":"def is_ipfs_uri(value: str):\n    \"\"\"\n    Return a bool indicating whether or not the value is a valid IPFS URI.\n    \"\"\"\n    parse_result = parse.urlparse(value)\n    if parse_result.scheme != \"ipfs\":\n        return False\n    if not parse_result.netloc and not parse_result.path:\n        return False\n\n    return True"}
{"code":"def purge_archives(base_dir: str, retain_latest: bool = False) -> None:\n        \"\"\"\n        Erase all (or nearly all) cache archives.\n\n        :param base_dir: archive base directory\n        :param retain_latest: retain latest archive if present, purge all others\n        \"\"\"\n\n        LOGGER.debug('purge_archives >>> base_dir: %s, retain_latest: %s', base_dir, retain_latest)\n\n        if isdir(base_dir):\n            timestamps = sorted([int(t) for t in listdir(base_dir) if t.isdigit()])\n            if retain_latest and timestamps:\n                timestamps.pop()\n            for timestamp in timestamps:\n                timestamp_dir = join(base_dir, str(timestamp))\n                rmtree(timestamp_dir)\n                LOGGER.info('Purged archive cache directory %s', timestamp_dir)\n\n        LOGGER.debug('purge_archives <<<')","return_type":"None","function_name":"ArchivableCaches.purge_archives","stripped_code":"def purge_archives(base_dir: str, retain_latest: bool = False):\n        \"\"\"\n        Erase all (or nearly all) cache archives.\n\n        :param base_dir: archive base directory\n        :param retain_latest: retain latest archive if present, purge all others\n        \"\"\"\n\n        LOGGER.debug('purge_archives >>> base_dir: %s, retain_latest: %s', base_dir, retain_latest)\n\n        if isdir(base_dir):\n            timestamps = sorted([int(t) for t in listdir(base_dir) if t.isdigit()])\n            if retain_latest and timestamps:\n                timestamps.pop()\n            for timestamp in timestamps:\n                timestamp_dir = join(base_dir, str(timestamp))\n                rmtree(timestamp_dir)\n                LOGGER.info('Purged archive cache directory %s', timestamp_dir)\n\n        LOGGER.debug('purge_archives <<<')"}
{"code":"def is_transaction_effect_satisfied(\n        chain_state: ChainState,\n        transaction: ContractSendEvent,\n        state_change: StateChange,\n) -> bool:\n    \"\"\" True if the side-effect of `transaction` is satisfied by\n    `state_change`.\n\n    This predicate is used to clear the transaction queue. This should only be\n    done once the expected side effect of a transaction is achieved. This\n    doesn't necessarily mean that the transaction sent by *this* node was\n    mined, but only that *some* transaction which achieves the same side-effect\n    was successfully executed and mined. This distinction is important for\n    restarts and to reduce the number of state changes.\n\n    On restarts: The state of the on-chain channel could have changed while the\n    node was offline. Once the node learns about the change (e.g. the channel\n    was settled), new transactions can be dispatched by Raiden as a side effect for the\n    on-chain *event* (e.g. do the batch unlock with the latest merkle tree),\n    but the dispatched transaction could have been completed by another agent (e.g.\n    the partner node). For these cases, the transaction from a different\n    address which achieves the same side-effect is sufficient, otherwise\n    unnecessary transactions would be sent by the node.\n\n    NOTE: The above is not important for transactions sent as a side-effect for\n    a new *block*. On restart the node first synchronizes its state by querying\n    for new events, only after the off-chain state is up-to-date, a Block state\n    change is dispatched. At this point some transactions are not required\n    anymore and therefore are not dispatched.\n\n    On the number of state changes: Accepting a transaction from another\n    address removes the need for clearing state changes, e.g. when our\n    node's close transaction fails but its partner's close transaction\n    succeeds.\n    \"\"\"\n    # These transactions are not made atomic through the WAL. They are sent\n    # exclusively through the external APIs.\n    #\n    #  - ContractReceiveChannelNew\n    #  - ContractReceiveChannelNewBalance\n    #  - ContractReceiveNewPaymentNetwork\n    #  - ContractReceiveNewTokenNetwork\n    #  - ContractReceiveRouteNew\n    #\n    # Note: Deposits and Withdraws must consider a transaction with a higher\n    # value as sufficient, because the values are monotonically increasing and\n    # the transaction with a lower value will never be executed.\n\n    # Transactions are used to change the on-chain state of a channel. It\n    # doesn't matter if the sender of the transaction is the local node or\n    # another node authorized to perform the operation. So, for the following\n    # transactions, as long as the side-effects are the same, the local\n    # transaction can be removed from the queue.\n    #\n    # - An update transfer can be done by a trusted third party (i.e. monitoring service)\n    # - A close transaction can be sent by our partner\n    # - A settle transaction can be sent by anyone\n    # - A secret reveal can be done by anyone\n\n    # - A lower nonce is not a valid replacement, since that is an older balance\n    #   proof\n    # - A larger raiden state change nonce is impossible.\n    #   That would require the partner node to produce an invalid balance proof,\n    #   and this node to accept the invalid balance proof and sign it\n    is_valid_update_transfer = (\n        isinstance(state_change, ContractReceiveUpdateTransfer) and\n        isinstance(transaction, ContractSendChannelUpdateTransfer) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier and\n        state_change.nonce == transaction.balance_proof.nonce\n    )\n    if is_valid_update_transfer:\n        return True\n\n    # The balance proof data cannot be verified, the local close could have\n    # lost a race against a remote close, and the balance proof data would be\n    # the one provided by this node's partner\n    is_valid_close = (\n        isinstance(state_change, ContractReceiveChannelClosed) and\n        isinstance(transaction, ContractSendChannelClose) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_close:\n        return True\n\n    is_valid_settle = (\n        isinstance(state_change, ContractReceiveChannelSettled) and\n        isinstance(transaction, ContractSendChannelSettle) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_settle:\n        return True\n\n    is_valid_secret_reveal = (\n        isinstance(state_change, ContractReceiveSecretReveal) and\n        isinstance(transaction, ContractSendSecretReveal) and\n        state_change.secret == transaction.secret\n    )\n    if is_valid_secret_reveal:\n        return True\n\n    is_batch_unlock = (\n        isinstance(state_change, ContractReceiveChannelBatchUnlock) and\n        isinstance(transaction, ContractSendChannelBatchUnlock)\n    )\n    if is_batch_unlock:\n        assert isinstance(state_change, ContractReceiveChannelBatchUnlock), MYPY_ANNOTATION\n        assert isinstance(transaction, ContractSendChannelBatchUnlock), MYPY_ANNOTATION\n\n        our_address = chain_state.our_address\n\n        # Don't assume that because we sent the transaction, we are a\n        # participant\n        partner_address = None\n        if state_change.participant == our_address:\n            partner_address = state_change.partner\n        elif state_change.partner == our_address:\n            partner_address = state_change.participant\n\n        # Use the second address as the partner address, but check that a\n        # channel exists for our_address and partner_address\n        if partner_address:\n            channel_state = views.get_channelstate_by_token_network_and_partner(\n                chain_state,\n                TokenNetworkID(state_change.token_network_identifier),\n                partner_address,\n            )\n            # If the channel was cleared, that means that both\n            # sides of the channel were successfully unlocked.\n            # In this case, we clear the batch unlock\n            # transaction from the queue only in case there\n            # were no more locked funds to unlock.\n            if channel_state is None:\n                return True\n\n    return False","return_type":"bool","function_name":"is_transaction_effect_satisfied","stripped_code":"def is_transaction_effect_satisfied(\n        chain_state: ChainState,\n        transaction: ContractSendEvent,\n        state_change: StateChange,\n):\n    \"\"\" True if the side-effect of `transaction` is satisfied by\n    `state_change`.\n\n    This predicate is used to clear the transaction queue. This should only be\n    done once the expected side effect of a transaction is achieved. This\n    doesn't necessarily mean that the transaction sent by *this* node was\n    mined, but only that *some* transaction which achieves the same side-effect\n    was successfully executed and mined. This distinction is important for\n    restarts and to reduce the number of state changes.\n\n    On restarts: The state of the on-chain channel could have changed while the\n    node was offline. Once the node learns about the change (e.g. the channel\n    was settled), new transactions can be dispatched by Raiden as a side effect for the\n    on-chain *event* (e.g. do the batch unlock with the latest merkle tree),\n    but the dispatched transaction could have been completed by another agent (e.g.\n    the partner node). For these cases, the transaction from a different\n    address which achieves the same side-effect is sufficient, otherwise\n    unnecessary transactions would be sent by the node.\n\n    NOTE: The above is not important for transactions sent as a side-effect for\n    a new *block*. On restart the node first synchronizes its state by querying\n    for new events, only after the off-chain state is up-to-date, a Block state\n    change is dispatched. At this point some transactions are not required\n    anymore and therefore are not dispatched.\n\n    On the number of state changes: Accepting a transaction from another\n    address removes the need for clearing state changes, e.g. when our\n    node's close transaction fails but its partner's close transaction\n    succeeds.\n    \"\"\"\n    # These transactions are not made atomic through the WAL. They are sent\n    # exclusively through the external APIs.\n    #\n    #  - ContractReceiveChannelNew\n    #  - ContractReceiveChannelNewBalance\n    #  - ContractReceiveNewPaymentNetwork\n    #  - ContractReceiveNewTokenNetwork\n    #  - ContractReceiveRouteNew\n    #\n    # Note: Deposits and Withdraws must consider a transaction with a higher\n    # value as sufficient, because the values are monotonically increasing and\n    # the transaction with a lower value will never be executed.\n\n    # Transactions are used to change the on-chain state of a channel. It\n    # doesn't matter if the sender of the transaction is the local node or\n    # another node authorized to perform the operation. So, for the following\n    # transactions, as long as the side-effects are the same, the local\n    # transaction can be removed from the queue.\n    #\n    # - An update transfer can be done by a trusted third party (i.e. monitoring service)\n    # - A close transaction can be sent by our partner\n    # - A settle transaction can be sent by anyone\n    # - A secret reveal can be done by anyone\n\n    # - A lower nonce is not a valid replacement, since that is an older balance\n    #   proof\n    # - A larger raiden state change nonce is impossible.\n    #   That would require the partner node to produce an invalid balance proof,\n    #   and this node to accept the invalid balance proof and sign it\n    is_valid_update_transfer = (\n        isinstance(state_change, ContractReceiveUpdateTransfer) and\n        isinstance(transaction, ContractSendChannelUpdateTransfer) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier and\n        state_change.nonce == transaction.balance_proof.nonce\n    )\n    if is_valid_update_transfer:\n        return True\n\n    # The balance proof data cannot be verified, the local close could have\n    # lost a race against a remote close, and the balance proof data would be\n    # the one provided by this node's partner\n    is_valid_close = (\n        isinstance(state_change, ContractReceiveChannelClosed) and\n        isinstance(transaction, ContractSendChannelClose) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_close:\n        return True\n\n    is_valid_settle = (\n        isinstance(state_change, ContractReceiveChannelSettled) and\n        isinstance(transaction, ContractSendChannelSettle) and\n        state_change.token_network_identifier == transaction.token_network_identifier and\n        state_change.channel_identifier == transaction.channel_identifier\n    )\n    if is_valid_settle:\n        return True\n\n    is_valid_secret_reveal = (\n        isinstance(state_change, ContractReceiveSecretReveal) and\n        isinstance(transaction, ContractSendSecretReveal) and\n        state_change.secret == transaction.secret\n    )\n    if is_valid_secret_reveal:\n        return True\n\n    is_batch_unlock = (\n        isinstance(state_change, ContractReceiveChannelBatchUnlock) and\n        isinstance(transaction, ContractSendChannelBatchUnlock)\n    )\n    if is_batch_unlock:\n        assert isinstance(state_change, ContractReceiveChannelBatchUnlock), MYPY_ANNOTATION\n        assert isinstance(transaction, ContractSendChannelBatchUnlock), MYPY_ANNOTATION\n\n        our_address = chain_state.our_address\n\n        # Don't assume that because we sent the transaction, we are a\n        # participant\n        partner_address = None\n        if state_change.participant == our_address:\n            partner_address = state_change.partner\n        elif state_change.partner == our_address:\n            partner_address = state_change.participant\n\n        # Use the second address as the partner address, but check that a\n        # channel exists for our_address and partner_address\n        if partner_address:\n            channel_state = views.get_channelstate_by_token_network_and_partner(\n                chain_state,\n                TokenNetworkID(state_change.token_network_identifier),\n                partner_address,\n            )\n            # If the channel was cleared, that means that both\n            # sides of the channel were successfully unlocked.\n            # In this case, we clear the batch unlock\n            # transaction from the queue only in case there\n            # were no more locked funds to unlock.\n            if channel_state is None:\n                return True\n\n    return False"}
{"code":"def multiply(multiplicand: list, multiplier: list) -> list:\n    \"\"\"\n    :type A: List[List[int]]\n    :type B: List[List[int]]\n    :rtype: List[List[int]]\n    \"\"\"\n    multiplicand_row, multiplicand_col = len(\n        multiplicand), len(multiplicand[0])\n    multiplier_row, multiplier_col = len(multiplier), len(multiplier[0])\n    if(multiplicand_col != multiplier_row):\n        raise Exception(\n            \"Multiplicand matrix not compatible with Multiplier matrix.\")\n    # create a result matrix\n    result = [[0] * multiplier_col for i in range(multiplicand_row)]\n    for i in range(multiplicand_row):\n        for j in range(multiplier_col):\n            for k in range(len(multiplier)):\n                result[i][j] += multiplicand[i][k] * multiplier[k][j]\n    return result","return_type":"list","function_name":"multiply","stripped_code":"def multiply(multiplicand: list, multiplier: list):\n    \"\"\"\n    :type A: List[List[int]]\n    :type B: List[List[int]]\n    :rtype: List[List[int]]\n    \"\"\"\n    multiplicand_row, multiplicand_col = len(\n        multiplicand), len(multiplicand[0])\n    multiplier_row, multiplier_col = len(multiplier), len(multiplier[0])\n    if(multiplicand_col != multiplier_row):\n        raise Exception(\n            \"Multiplicand matrix not compatible with Multiplier matrix.\")\n    # create a result matrix\n    result = [[0] * multiplier_col for i in range(multiplicand_row)]\n    for i in range(multiplicand_row):\n        for j in range(multiplier_col):\n            for k in range(len(multiplier)):\n                result[i][j] += multiplicand[i][k] * multiplier[k][j]\n    return result"}
{"code":"def element_to_objects(payload: Dict) -> List:\n    \"\"\"\n    Transform an Element to a list of entities recursively.\n    \"\"\"\n    entities = []\n    cls = MAPPINGS.get(payload.get('type'))\n    if not cls:\n        return []\n\n    transformed = transform_attributes(payload, cls)\n    entity = cls(**transformed)\n\n    if hasattr(entity, \"post_receive\"):\n        entity.post_receive()\n\n    entities.append(entity)\n\n    return entities","return_type":"List","function_name":"element_to_objects","stripped_code":"def element_to_objects(payload: Dict):\n    \"\"\"\n    Transform an Element to a list of entities recursively.\n    \"\"\"\n    entities = []\n    cls = MAPPINGS.get(payload.get('type'))\n    if not cls:\n        return []\n\n    transformed = transform_attributes(payload, cls)\n    entity = cls(**transformed)\n\n    if hasattr(entity, \"post_receive\"):\n        entity.post_receive()\n\n    entities.append(entity)\n\n    return entities"}
{"code":"def _listload(l: Loader, value, type_) -> List:\n    \"\"\"\n    This loads into something like List[int]\n    \"\"\"\n    t = type_.__args__[0]\n    try:\n        return [l.load(v, t, annotation=Annotation(AnnotationType.INDEX, i)) for i, v in enumerate(value)]\n    except TypeError as e:\n        if isinstance(e, TypedloadException):\n            raise\n        raise TypedloadTypeError(str(e), value=value, type_=type_)","return_type":"List","function_name":"_listload","stripped_code":"def _listload(l: Loader, value, type_):\n    \"\"\"\n    This loads into something like List[int]\n    \"\"\"\n    t = type_.__args__[0]\n    try:\n        return [l.load(v, t, annotation=Annotation(AnnotationType.INDEX, i)) for i, v in enumerate(value)]\n    except TypeError as e:\n        if isinstance(e, TypedloadException):\n            raise\n        raise TypedloadTypeError(str(e), value=value, type_=type_)"}
{"code":"def exist(self, table: str, libref: str = \"\") -> bool:\n        \"\"\"\n        Does the SAS data set currently exist\n\n        :param table: the name of the SAS Data Set\n        :param libref: the libref for the Data Set, defaults to WORK, or USER if assigned\n        :return: Boolean True it the Data Set exists and False if it does not\n        :rtype: bool\n        \"\"\"\n        return self._io.exist(table, libref)","return_type":"bool","function_name":"SASsession.exist","stripped_code":"def exist(self, table: str, libref: str = \"\"):\n        \"\"\"\n        Does the SAS data set currently exist\n\n        :param table: the name of the SAS Data Set\n        :param libref: the libref for the Data Set, defaults to WORK, or USER if assigned\n        :return: Boolean True it the Data Set exists and False if it does not\n        :rtype: bool\n        \"\"\"\n        return self._io.exist(table, libref)"}
{"code":"def removeRemoteByName(self, name: str) -> int:\n        \"\"\"\n        Remove the remote by name.\n\n        :param name: the name of the remote to remove\n        :raises: RemoteNotFound\n        \"\"\"\n        remote = self.getRemote(name)\n        rid = remote.uid\n        self.removeRemote(remote)\n        return rid","return_type":"int","function_name":"NetworkInterface.removeRemoteByName","stripped_code":"def removeRemoteByName(self, name: str):\n        \"\"\"\n        Remove the remote by name.\n\n        :param name: the name of the remote to remove\n        :raises: RemoteNotFound\n        \"\"\"\n        remote = self.getRemote(name)\n        rid = remote.uid\n        self.removeRemote(remote)\n        return rid"}
{"code":"def raw(self) -> str:\n        \"\"\"\n        Return raw string format from the instance\n\n        :return:\n        \"\"\"\n        doc = \"\"\"Version: {0}\nType: Transaction\nCurrency: {1}\n\"\"\".format(self.version,\n           self.currency)\n\n        if self.version >= 3:\n            doc += \"Blockstamp: {0}\\n\".format(self.blockstamp)\n\n        doc += \"Locktime: {0}\\n\".format(self.locktime)\n\n        doc += \"Issuers:\\n\"\n        for p in self.issuers:\n            doc += \"{0}\\n\".format(p)\n\n        doc += \"Inputs:\\n\"\n        for i in self.inputs:\n            doc += \"{0}\\n\".format(i.inline(self.version))\n\n        doc += \"Unlocks:\\n\"\n        for u in self.unlocks:\n            doc += \"{0}\\n\".format(u.inline())\n\n        doc += \"Outputs:\\n\"\n        for o in self.outputs:\n            doc += \"{0}\\n\".format(o.inline())\n\n        doc += \"Comment: \"\n        doc += \"{0}\\n\".format(self.comment)\n\n        return doc","return_type":"str","function_name":"Transaction.raw","stripped_code":"def raw(self):\n        \"\"\"\n        Return raw string format from the instance\n\n        :return:\n        \"\"\"\n        doc = \"\"\"Version: {0}\nType: Transaction\nCurrency: {1}\n\"\"\".format(self.version,\n           self.currency)\n\n        if self.version >= 3:\n            doc += \"Blockstamp: {0}\\n\".format(self.blockstamp)\n\n        doc += \"Locktime: {0}\\n\".format(self.locktime)\n\n        doc += \"Issuers:\\n\"\n        for p in self.issuers:\n            doc += \"{0}\\n\".format(p)\n\n        doc += \"Inputs:\\n\"\n        for i in self.inputs:\n            doc += \"{0}\\n\".format(i.inline(self.version))\n\n        doc += \"Unlocks:\\n\"\n        for u in self.unlocks:\n            doc += \"{0}\\n\".format(u.inline())\n\n        doc += \"Outputs:\\n\"\n        for o in self.outputs:\n            doc += \"{0}\\n\".format(o.inline())\n\n        doc += \"Comment: \"\n        doc += \"{0}\\n\".format(self.comment)\n\n        return doc"}
{"code":"def get_call_signature(fn: FunctionType,\n                       args: ArgsType,\n                       kwargs: KwargsType,\n                       debug_cache: bool = False) -> str:\n    \"\"\"\n    Takes a function and its args/kwargs, and produces a string description\n    of the function call (the call signature) suitable for use indirectly as a\n    cache key. The string is a JSON representation. See ``make_cache_key`` for\n    a more suitable actual cache key.\n    \"\"\"\n    # Note that the function won't have the __self__ argument (as in\n    # fn.__self__), at this point, even if it's a member function.\n    try:\n        call_sig = json_encode((fn.__qualname__, args, kwargs))\n    except TypeError:\n        log.critical(\n            \"\\nTo decorate using @django_cache_function without specifying \"\n            \"cache_key, the decorated function's owning class and its \"\n            \"parameters must be JSON-serializable (see jsonfunc.py, \"\n            \"django_cache_fn.py).\\n\")\n        raise\n    if debug_cache:\n        log.debug(\"Making call signature {!r}\", call_sig)\n    return call_sig","return_type":"str","function_name":"get_call_signature","stripped_code":"def get_call_signature(fn: FunctionType,\n                       args: ArgsType,\n                       kwargs: KwargsType,\n                       debug_cache: bool = False):\n    \"\"\"\n    Takes a function and its args/kwargs, and produces a string description\n    of the function call (the call signature) suitable for use indirectly as a\n    cache key. The string is a JSON representation. See ``make_cache_key`` for\n    a more suitable actual cache key.\n    \"\"\"\n    # Note that the function won't have the __self__ argument (as in\n    # fn.__self__), at this point, even if it's a member function.\n    try:\n        call_sig = json_encode((fn.__qualname__, args, kwargs))\n    except TypeError:\n        log.critical(\n            \"\\nTo decorate using @django_cache_function without specifying \"\n            \"cache_key, the decorated function's owning class and its \"\n            \"parameters must be JSON-serializable (see jsonfunc.py, \"\n            \"django_cache_fn.py).\\n\")\n        raise\n    if debug_cache:\n        log.debug(\"Making call signature {!r}\", call_sig)\n    return call_sig"}
{"code":"def merge_pr(\n    pr_num: int,\n    base_ref: str,\n    target_ref: str,\n    commit_title: str,\n    body: str,\n    pr_repo_desc: str,\n    original_head: str,\n    remote: str,\n    merge_method: str,\n    github_user: str,\n    password: str,\n) -> None:\n    \"\"\"Merge a pull request.\"\"\"\n    git_log = git[\n        \"log\",\n        \"{remote}/{target_ref}..{base_ref}\".format(\n            remote=remote, target_ref=target_ref, base_ref=base_ref\n        ),\n    ]\n\n    commit_authors = git_log[\"--pretty=format:%an <%ae>\"]().splitlines()\n    author_count = collections.Counter(commit_authors)\n    distinct_authors = [author for author, _ in author_count.most_common()]\n    commits = git_log[\"--pretty=format:%h [%an] %s\"]().splitlines()\n\n    merge_message_pieces = []\n    if body:\n        merge_message_pieces.append(\"\\n\".join(textwrap.wrap(body)))\n    merge_message_pieces.extend(map(\"Author: {}\".format, distinct_authors))\n\n    # The string \"Closes #{pull_request_number:d}\" is required for GitHub to\n    # correctly close the PR\n    merge_message_pieces.append(\n        (\n            \"\\nCloses #{pr_num:d} from {pr_repo_desc} and squashes the \"\n            \"following commits:\\n\"\n        ).format(pr_num=pr_num, pr_repo_desc=pr_repo_desc)\n    )\n    merge_message_pieces += commits\n\n    commit_message = \"\\n\".join(merge_message_pieces)\n    # PUT /repos/:owner/:repo/pulls/:number/merge\n    resp = requests.put(\n        \"{GITHUB_API_BASE}/pulls/{pr_num:d}/merge\".format(\n            GITHUB_API_BASE=GITHUB_API_BASE, pr_num=pr_num\n        ),\n        json=dict(\n            commit_title=commit_title,\n            commit_message=commit_message,\n            merge_method=merge_method,\n        ),\n        auth=(github_user, password),\n    )\n    resp.raise_for_status()\n    if resp.status_code == 200:\n        resp_json = resp.json()\n        merged = resp_json[\"merged\"]\n        assert merged is True, merged\n        click.echo(\n            \"Pull request #{pr_num:d} successfully merged.\".format(\n                pr_num=pr_num\n            )\n        )","return_type":"None","function_name":"merge_pr","stripped_code":"def merge_pr(\n    pr_num: int,\n    base_ref: str,\n    target_ref: str,\n    commit_title: str,\n    body: str,\n    pr_repo_desc: str,\n    original_head: str,\n    remote: str,\n    merge_method: str,\n    github_user: str,\n    password: str,\n):\n    \"\"\"Merge a pull request.\"\"\"\n    git_log = git[\n        \"log\",\n        \"{remote}/{target_ref}..{base_ref}\".format(\n            remote=remote, target_ref=target_ref, base_ref=base_ref\n        ),\n    ]\n\n    commit_authors = git_log[\"--pretty=format:%an <%ae>\"]().splitlines()\n    author_count = collections.Counter(commit_authors)\n    distinct_authors = [author for author, _ in author_count.most_common()]\n    commits = git_log[\"--pretty=format:%h [%an] %s\"]().splitlines()\n\n    merge_message_pieces = []\n    if body:\n        merge_message_pieces.append(\"\\n\".join(textwrap.wrap(body)))\n    merge_message_pieces.extend(map(\"Author: {}\".format, distinct_authors))\n\n    # The string \"Closes #{pull_request_number:d}\" is required for GitHub to\n    # correctly close the PR\n    merge_message_pieces.append(\n        (\n            \"\\nCloses #{pr_num:d} from {pr_repo_desc} and squashes the \"\n            \"following commits:\\n\"\n        ).format(pr_num=pr_num, pr_repo_desc=pr_repo_desc)\n    )\n    merge_message_pieces += commits\n\n    commit_message = \"\\n\".join(merge_message_pieces)\n    # PUT /repos/:owner/:repo/pulls/:number/merge\n    resp = requests.put(\n        \"{GITHUB_API_BASE}/pulls/{pr_num:d}/merge\".format(\n            GITHUB_API_BASE=GITHUB_API_BASE, pr_num=pr_num\n        ),\n        json=dict(\n            commit_title=commit_title,\n            commit_message=commit_message,\n            merge_method=merge_method,\n        ),\n        auth=(github_user, password),\n    )\n    resp.raise_for_status()\n    if resp.status_code == 200:\n        resp_json = resp.json()\n        merged = resp_json[\"merged\"]\n        assert merged is True, merged\n        click.echo(\n            \"Pull request #{pr_num:d} successfully merged.\".format(\n                pr_num=pr_num\n            )\n        )"}
{"code":"def nodeSatisfiesStringFacet(cntxt: Context, n: Node, nc: ShExJ.NodeConstraint, _c: DebugContext) -> bool:\n    \"\"\" `5.4.5 XML Schema String Facet Constraints <ttp://shex.io/shex-semantics/#xs-string>`_\n\n     String facet constraints apply to the lexical form of the RDF Literals and IRIs and blank node\n     identifiers (see note below regarding access to blank node identifiers).\n    \"\"\"\n\n    # Let lex =\n    #\n    #  * if the value n is an RDF Literal, the lexical form of the literal (see[rdf11-concepts] section 3.3 Literals).\n    #  * if the value n is an IRI, the IRI string (see[rdf11-concepts] section 3.2 IRIs).\n    #  * if the value n is a blank node, the blank node identifier (see[rdf11-concepts] section 3.4 Blank Nodes).\n    if nc.length is not None or nc.minlength is not None or nc.maxlength is not None \\\n            or nc.pattern is not None:\n        lex = str(n)\n        #  Let len = the number of unicode codepoints in lex\n        # For a node n and constraint value v, nodeSatisfies(n, v):\n        #\n        #  * for \"length\" constraints, v = len,\n        #  * for \"minlength\" constraints, v >= len,\n        #  * for \"maxlength\" constraints, v <= len,\n        #  * for \"pattern\" constraints, v is unescaped into a valid XPath 3.1 regular expression[xpath-functions-31]\n        #    re and invoking fn:matches(lex, re) returns fn:true. If the flags parameter is present, it is passed\n        #    as a third argument to fn:matches. The pattern may have XPath 3.1 regular expression escape sequences\n        #    per the modified production [10] in section 5.6.1.1 as well as numeric escape sequences of the\n        #    form 'u' HEX HEX HEX HEX or 'U' HEX HEX HEX HEX HEX HEX HEX HEX. Unescaping replaces numeric escape\n        #    sequences with the corresponding unicode codepoint\n\n        # TODO: Figure out whether we need to connect this to the lxml exslt functions\n        # TODO: Map flags if not\n        if (nc.length is None or len(lex) == nc.length) and \\\n           (nc.minlength is None or len(lex) >= nc.minlength) and \\\n           (nc.maxlength is None or len(lex) <= nc.maxlength) and \\\n           (nc.pattern is None or pattern_match(nc.pattern, nc.flags, lex)):\n            return True\n        elif nc.length is not None and len(lex) != nc.length:\n            cntxt.fail_reason = f\"String length mismatch - expected: {nc.length} actual: {len(lex)}\"\n        elif nc.minlength is not None and len(lex) < nc.minlength:\n            cntxt.fail_reason = f\"String length violation - minimum: {nc.minlength} actual: {len(lex)}\"\n        elif nc.maxlength is not None and len(lex) > nc.maxlength:\n            cntxt.fail_reason = f\"String length violation - maximum: {nc.maxlength} actual: {len(lex)}\"\n        elif nc.pattern is not None and not pattern_match(nc.pattern, nc.flags, lex):\n            cntxt.fail_reason = f\"Pattern match failure - pattern: {nc.pattern} flags:{nc.flags}\" \\\n                                             f\" string: {lex}\"\n        else:\n            cntxt.fail_reason = \"Programming error - flame the programmer\"\n        return False\n\n\n    else:\n        return True","return_type":"bool","function_name":"nodeSatisfiesStringFacet","stripped_code":"def nodeSatisfiesStringFacet(cntxt: Context, n: Node, nc: ShExJ.NodeConstraint, _c: DebugContext):\n    \"\"\" `5.4.5 XML Schema String Facet Constraints <ttp://shex.io/shex-semantics/#xs-string>`_\n\n     String facet constraints apply to the lexical form of the RDF Literals and IRIs and blank node\n     identifiers (see note below regarding access to blank node identifiers).\n    \"\"\"\n\n    # Let lex =\n    #\n    #  * if the value n is an RDF Literal, the lexical form of the literal (see[rdf11-concepts] section 3.3 Literals).\n    #  * if the value n is an IRI, the IRI string (see[rdf11-concepts] section 3.2 IRIs).\n    #  * if the value n is a blank node, the blank node identifier (see[rdf11-concepts] section 3.4 Blank Nodes).\n    if nc.length is not None or nc.minlength is not None or nc.maxlength is not None \\\n            or nc.pattern is not None:\n        lex = str(n)\n        #  Let len = the number of unicode codepoints in lex\n        # For a node n and constraint value v, nodeSatisfies(n, v):\n        #\n        #  * for \"length\" constraints, v = len,\n        #  * for \"minlength\" constraints, v >= len,\n        #  * for \"maxlength\" constraints, v <= len,\n        #  * for \"pattern\" constraints, v is unescaped into a valid XPath 3.1 regular expression[xpath-functions-31]\n        #    re and invoking fn:matches(lex, re) returns fn:true. If the flags parameter is present, it is passed\n        #    as a third argument to fn:matches. The pattern may have XPath 3.1 regular expression escape sequences\n        #    per the modified production [10] in section 5.6.1.1 as well as numeric escape sequences of the\n        #    form 'u' HEX HEX HEX HEX or 'U' HEX HEX HEX HEX HEX HEX HEX HEX. Unescaping replaces numeric escape\n        #    sequences with the corresponding unicode codepoint\n\n        # TODO: Figure out whether we need to connect this to the lxml exslt functions\n        # TODO: Map flags if not\n        if (nc.length is None or len(lex) == nc.length) and \\\n           (nc.minlength is None or len(lex) >= nc.minlength) and \\\n           (nc.maxlength is None or len(lex) <= nc.maxlength) and \\\n           (nc.pattern is None or pattern_match(nc.pattern, nc.flags, lex)):\n            return True\n        elif nc.length is not None and len(lex) != nc.length:\n            cntxt.fail_reason = f\"String length mismatch - expected: {nc.length} actual: {len(lex)}\"\n        elif nc.minlength is not None and len(lex) < nc.minlength:\n            cntxt.fail_reason = f\"String length violation - minimum: {nc.minlength} actual: {len(lex)}\"\n        elif nc.maxlength is not None and len(lex) > nc.maxlength:\n            cntxt.fail_reason = f\"String length violation - maximum: {nc.maxlength} actual: {len(lex)}\"\n        elif nc.pattern is not None and not pattern_match(nc.pattern, nc.flags, lex):\n            cntxt.fail_reason = f\"Pattern match failure - pattern: {nc.pattern} flags:{nc.flags}\" \\\n                                             f\" string: {lex}\"\n        else:\n            cntxt.fail_reason = \"Programming error - flame the programmer\"\n        return False\n\n\n    else:\n        return True"}
{"code":"def get_base_url(html: str) -> str:\n    \"\"\"\n    Search for login url from VK login page\n    \"\"\"\n    forms = BeautifulSoup(html, 'html.parser').find_all('form')\n    if not forms:\n        raise VVKBaseUrlException('Form for login not found')\n    elif len(forms) > 1:\n        raise VVKBaseUrlException('More than one login form found')\n    login_url = forms[0].get('action')\n    if not login_url:\n        raise VVKBaseUrlException('No action tag in form')\n    return login_url","return_type":"str","function_name":"get_base_url","stripped_code":"def get_base_url(html: str):\n    \"\"\"\n    Search for login url from VK login page\n    \"\"\"\n    forms = BeautifulSoup(html, 'html.parser').find_all('form')\n    if not forms:\n        raise VVKBaseUrlException('Form for login not found')\n    elif len(forms) > 1:\n        raise VVKBaseUrlException('More than one login form found')\n    login_url = forms[0].get('action')\n    if not login_url:\n        raise VVKBaseUrlException('No action tag in form')\n    return login_url"}
{"code":"def get_nodes_by_function(graph: BELGraph, func: Strings) -> Set[BaseEntity]:\n    \"\"\"Get all nodes with the given function(s).\"\"\"\n    return get_nodes(graph, function_inclusion_filter_builder(func))","return_type":"Set[BaseEntity]","function_name":"get_nodes_by_function","stripped_code":"def get_nodes_by_function(graph: BELGraph, func: Strings):\n    \"\"\"Get all nodes with the given function(s).\"\"\"\n    return get_nodes(graph, function_inclusion_filter_builder(func))"}
{"code":"def lan(self, move: Move) -> str:\n        \"\"\"\n        Gets the long algebraic notation of the given move in the context of\n        the current position.\n        \"\"\"\n        return self._algebraic(move, long=True)","return_type":"str","function_name":"Board.lan","stripped_code":"def lan(self, move: Move):\n        \"\"\"\n        Gets the long algebraic notation of the given move in the context of\n        the current position.\n        \"\"\"\n        return self._algebraic(move, long=True)"}
{"code":"def log(self, sequence, infoarray) -> None:\n        \"\"\"Log the given |IOSequence| object either for reading or writing\n        data.\n\n        The optional `array` argument allows for passing alternative data\n        in an |InfoArray| object replacing the series of the |IOSequence|\n        object, which is useful for writing modified (e.g. spatially\n        averaged) time series.\n\n        Logged time series data is available via attribute access:\n\n        >>> from hydpy.core.netcdftools import NetCDFVariableBase\n        >>> from hydpy import make_abc_testable\n        >>> NCVar = make_abc_testable(NetCDFVariableBase)\n        >>> ncvar = NCVar('flux_nkor', isolate=True, timeaxis=1)\n        >>> from hydpy.core.examples import prepare_io_example_1\n        >>> nodes, elements = prepare_io_example_1()\n        >>> nkor = elements.element1.model.sequences.fluxes.nkor\n        >>> ncvar.log(nkor, nkor.series)\n        >>> 'element1' in dir(ncvar)\n        True\n        >>> ncvar.element1.sequence is nkor\n        True\n        >>> 'element2' in dir(ncvar)\n        False\n        >>> ncvar.element2\n        Traceback (most recent call last):\n        ...\n        AttributeError: The NetCDFVariable object `flux_nkor` does \\\nneither handle time series data under the (sub)device name `element2` \\\nnor does it define a member named `element2`.\n        \"\"\"\n        descr_device = sequence.descr_device\n        self.sequences[descr_device] = sequence\n        self.arrays[descr_device] = infoarray","return_type":"None","function_name":"NetCDFVariableBase.log","stripped_code":"def log(self, sequence, infoarray):\n        \"\"\"Log the given |IOSequence| object either for reading or writing\n        data.\n\n        The optional `array` argument allows for passing alternative data\n        in an |InfoArray| object replacing the series of the |IOSequence|\n        object, which is useful for writing modified (e.g. spatially\n        averaged) time series.\n\n        Logged time series data is available via attribute access:\n\n        >>> from hydpy.core.netcdftools import NetCDFVariableBase\n        >>> from hydpy import make_abc_testable\n        >>> NCVar = make_abc_testable(NetCDFVariableBase)\n        >>> ncvar = NCVar('flux_nkor', isolate=True, timeaxis=1)\n        >>> from hydpy.core.examples import prepare_io_example_1\n        >>> nodes, elements = prepare_io_example_1()\n        >>> nkor = elements.element1.model.sequences.fluxes.nkor\n        >>> ncvar.log(nkor, nkor.series)\n        >>> 'element1' in dir(ncvar)\n        True\n        >>> ncvar.element1.sequence is nkor\n        True\n        >>> 'element2' in dir(ncvar)\n        False\n        >>> ncvar.element2\n        Traceback (most recent call last):\n        ...\n        AttributeError: The NetCDFVariable object `flux_nkor` does \\\nneither handle time series data under the (sub)device name `element2` \\\nnor does it define a member named `element2`.\n        \"\"\"\n        descr_device = sequence.descr_device\n        self.sequences[descr_device] = sequence\n        self.arrays[descr_device] = infoarray"}
{"code":"def index_is_empty(self) -> bool:\n        \"\"\"\n        :return: True if index is empty (no staged changes)\n        :rtype: bool\n        \"\"\"\n        index_empty: bool = len(self.repo.index.diff(self.repo.head.commit)) == 0\n        LOGGER.debug('index is empty: %s', index_empty)\n        return index_empty","return_type":"bool","function_name":"Repo.index_is_empty","stripped_code":"def index_is_empty(self):\n        \"\"\"\n        :return: True if index is empty (no staged changes)\n        :rtype: bool\n        \"\"\"\n        index_empty: bool = len(self.repo.index.diff(self.repo.head.commit)) == 0\n        LOGGER.debug('index is empty: %s', index_empty)\n        return index_empty"}
{"code":"def exhaustive_label_check( self,\n                                ontology:pd.DataFrame,\n                                label_predicate='rdfs:label',\n                                diff:bool=True, ) -> Tuple[list]:\n        ''' All entities with conflicting labels gets a full diff\n\n            Args:\n                ontology: pandas DataFrame created from an ontology where the colnames are predicates\n                    and if classes exist it is also thrown into a the colnames.\n                label_predicate: usually in qname form and is the colname of the DataFrame for the label\n                diff: complete exhaustive diff if between curie matches... will take FOREVER if there are a lot -> n^2\n            Returns:\n                inside: entities that are inside of InterLex\n                outside: entities NOT in InterLex\n                diff (optional): List[List[dict]]... so complicated but usefull diff between matches only '''\n\n        inside, outside = [], []\n        header = ['Index'] + list(ontology.columns)\n        for row in ontology.itertuples():\n\n            row = {header[i]:val for i, val in enumerate(row)}\n            label_obj = row[label_predicate]\n            if isinstance(label_obj, list):\n                if len(label_obj) != 1:\n                    exit('Need to have only 1 label in the cell from the onotology.')\n                else:\n                    label_obj = label_obj[0]\n            entity_label = self.local_degrade(label_obj)\n            ilx_rows = self.label2rows.get(entity_label)\n            if ilx_rows:\n                inside.append({\n                    'external_ontology_row': row,\n                    'ilx_rows': ilx_rows,\n                })\n            else:\n                outside.append(row)\n\n        if diff:\n            diff = self.__exhaustive_diff(inside)\n            return inside, outside, diff\n        return inside, outside","return_type":"Tuple[list]","function_name":"InterLexIngestion.exhaustive_label_check","stripped_code":"def exhaustive_label_check( self,\n                                ontology:pd.DataFrame,\n                                label_predicate='rdfs:label',\n                                diff:bool=True, ):\n        ''' All entities with conflicting labels gets a full diff\n\n            Args:\n                ontology: pandas DataFrame created from an ontology where the colnames are predicates\n                    and if classes exist it is also thrown into a the colnames.\n                label_predicate: usually in qname form and is the colname of the DataFrame for the label\n                diff: complete exhaustive diff if between curie matches... will take FOREVER if there are a lot -> n^2\n            Returns:\n                inside: entities that are inside of InterLex\n                outside: entities NOT in InterLex\n                diff (optional): List[List[dict]]... so complicated but usefull diff between matches only '''\n\n        inside, outside = [], []\n        header = ['Index'] + list(ontology.columns)\n        for row in ontology.itertuples():\n\n            row = {header[i]:val for i, val in enumerate(row)}\n            label_obj = row[label_predicate]\n            if isinstance(label_obj, list):\n                if len(label_obj) != 1:\n                    exit('Need to have only 1 label in the cell from the onotology.')\n                else:\n                    label_obj = label_obj[0]\n            entity_label = self.local_degrade(label_obj)\n            ilx_rows = self.label2rows.get(entity_label)\n            if ilx_rows:\n                inside.append({\n                    'external_ontology_row': row,\n                    'ilx_rows': ilx_rows,\n                })\n            else:\n                outside.append(row)\n\n        if diff:\n            diff = self.__exhaustive_diff(inside)\n            return inside, outside, diff\n        return inside, outside"}
{"code":"def get_current_version_by_config_file() -> str:\n    \"\"\"\n    Get current version from the version variable defined in the configuration\n\n    :return: A string with the current version number\n    :raises ImproperConfigurationError: if version variable cannot be parsed\n    \"\"\"\n    debug('get_current_version_by_config_file')\n    filename, variable = config.get('semantic_release',\n                                    'version_variable').split(':')\n    variable = variable.strip()\n    debug(filename, variable)\n    with open(filename, 'r') as fd:\n        parts = re.search(\n            r'^{0}\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]'.format(variable),\n            fd.read(),\n            re.MULTILINE\n        )\n        if not parts:\n            raise ImproperConfigurationError\n        debug(parts)\n        return parts.group(1)","return_type":"str","function_name":"get_current_version_by_config_file","stripped_code":"def get_current_version_by_config_file():\n    \"\"\"\n    Get current version from the version variable defined in the configuration\n\n    :return: A string with the current version number\n    :raises ImproperConfigurationError: if version variable cannot be parsed\n    \"\"\"\n    debug('get_current_version_by_config_file')\n    filename, variable = config.get('semantic_release',\n                                    'version_variable').split(':')\n    variable = variable.strip()\n    debug(filename, variable)\n    with open(filename, 'r') as fd:\n        parts = re.search(\n            r'^{0}\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]'.format(variable),\n            fd.read(),\n            re.MULTILINE\n        )\n        if not parts:\n            raise ImproperConfigurationError\n        debug(parts)\n        return parts.group(1)"}
{"code":"def IntegerSlice(input_vertex: vertex_constructor_param_types, dimension: int, index: int, label: Optional[str]=None) -> Vertex:\n    \"\"\"\n    Takes the slice along a given dimension and index of a vertex\n    \n    :param input_vertex: the input vertex\n    :param dimension: the dimension to extract along\n    :param index: the index of extraction\n    \"\"\"\n    return Integer(context.jvm_view().IntegerSliceVertex, label, cast_to_integer_vertex(input_vertex), cast_to_integer(dimension), cast_to_integer(index))","return_type":"Vertex","function_name":"IntegerSlice","stripped_code":"def IntegerSlice(input_vertex: vertex_constructor_param_types, dimension: int, index: int, label: Optional[str]=None):\n    \"\"\"\n    Takes the slice along a given dimension and index of a vertex\n    \n    :param input_vertex: the input vertex\n    :param dimension: the dimension to extract along\n    :param index: the index of extraction\n    \"\"\"\n    return Integer(context.jvm_view().IntegerSliceVertex, label, cast_to_integer_vertex(input_vertex), cast_to_integer(dimension), cast_to_integer(index))"}
{"code":"def delete(self, user: str) -> None:\n        \"\"\"Remove user.\"\"\"\n        data = {\n            'action': 'remove',\n            'user': user\n        }\n\n        self._request('post', URL, data=data)","return_type":"None","function_name":"Users.delete","stripped_code":"def delete(self, user: str):\n        \"\"\"Remove user.\"\"\"\n        data = {\n            'action': 'remove',\n            'user': user\n        }\n\n        self._request('post', URL, data=data)"}
{"code":"def perform_batch_reply(\n            self,\n            *,\n            callback: Callable[..., str]=None,\n            target_handles: Dict[str, str]=None,\n            lookback_limit: int=20,\n            per_service_lookback_limit: Dict[str, int]=None,\n    ) -> IterationRecord:\n        \"\"\"\n        Performs batch reply on target accounts.\n        Looks up the recent messages of the target user,\n        applies the callback,\n        and replies with\n        what the callback generates.\n\n        :param callback: a callback taking a message id,\n            message contents,\n            and optional extra keys,\n            and returning a message string.\n        :param targets: a dictionary of service names to target handles\n            (currently only one per service).\n        :param lookback_limit: a lookback limit of how many messages to consider (optional).\n        :param per_service_lookback: and a dictionary of service names to per-service\n            lookback limits.\n            takes preference over lookback_limit (optional).\n        :returns: new record of iteration\n        :raises BotSkeletonException: raises BotSkeletonException if batch reply fails or cannot be\n            performed\n        \"\"\"\n        if callback is None:\n            raise BotSkeletonException(\"Callback must be provided.\"\"\")\n\n        if target_handles is None:\n            raise BotSkeletonException(\"Targets must be provided.\"\"\")\n\n        if lookback_limit > self.lookback_limit:\n            raise BotSkeletonException(\n                f\"Lookback_limit cannot exceed {self.lookback_limit}, \" +\n                f\"but it was {lookback_limit}\"\n            )\n\n        # use per-service lookback dict for convenience in a moment.\n        # if necessary, use lookback_limit to fill it out.\n        lookback_dict = per_service_lookback_limit\n        if (lookback_dict is None):\n            lookback_dict = {}\n\n        record = IterationRecord(extra_keys=self.extra_keys)\n        for key, output in self.outputs.items():\n            if key not in lookback_dict:\n                lookback_dict[key] = lookback_limit\n\n            if target_handles.get(key, None) is None:\n                self.log.info(f\"No target for output {key}, skipping this output.\")\n\n            elif not output.get(\"active\", False):\n                self.log.info(f\"Output {key} is inactive. Not calling batch reply.\")\n\n            elif output[\"active\"]:\n                self.log.info(f\"Output {key} is active, calling batch reply on it.\")\n                entry: Any = output[\"obj\"]\n                output_result = entry.perform_batch_reply(callback=callback,\n                                                          target_handle=target_handles[key],\n                                                          lookback_limit=lookback_dict[key],\n                                                          )\n                record.output_records[key] = output_result\n\n        self.history.append(record)\n        self.update_history()\n\n        return record","return_type":"IterationRecord","function_name":"BotSkeleton.perform_batch_reply","stripped_code":"def perform_batch_reply(\n            self,\n            *,\n            callback: Callable[..., str]=None,\n            target_handles: Dict[str, str]=None,\n            lookback_limit: int=20,\n            per_service_lookback_limit: Dict[str, int]=None,\n    ):\n        \"\"\"\n        Performs batch reply on target accounts.\n        Looks up the recent messages of the target user,\n        applies the callback,\n        and replies with\n        what the callback generates.\n\n        :param callback: a callback taking a message id,\n            message contents,\n            and optional extra keys,\n            and returning a message string.\n        :param targets: a dictionary of service names to target handles\n            (currently only one per service).\n        :param lookback_limit: a lookback limit of how many messages to consider (optional).\n        :param per_service_lookback: and a dictionary of service names to per-service\n            lookback limits.\n            takes preference over lookback_limit (optional).\n        :returns: new record of iteration\n        :raises BotSkeletonException: raises BotSkeletonException if batch reply fails or cannot be\n            performed\n        \"\"\"\n        if callback is None:\n            raise BotSkeletonException(\"Callback must be provided.\"\"\")\n\n        if target_handles is None:\n            raise BotSkeletonException(\"Targets must be provided.\"\"\")\n\n        if lookback_limit > self.lookback_limit:\n            raise BotSkeletonException(\n                f\"Lookback_limit cannot exceed {self.lookback_limit}, \" +\n                f\"but it was {lookback_limit}\"\n            )\n\n        # use per-service lookback dict for convenience in a moment.\n        # if necessary, use lookback_limit to fill it out.\n        lookback_dict = per_service_lookback_limit\n        if (lookback_dict is None):\n            lookback_dict = {}\n\n        record = IterationRecord(extra_keys=self.extra_keys)\n        for key, output in self.outputs.items():\n            if key not in lookback_dict:\n                lookback_dict[key] = lookback_limit\n\n            if target_handles.get(key, None) is None:\n                self.log.info(f\"No target for output {key}, skipping this output.\")\n\n            elif not output.get(\"active\", False):\n                self.log.info(f\"Output {key} is inactive. Not calling batch reply.\")\n\n            elif output[\"active\"]:\n                self.log.info(f\"Output {key} is active, calling batch reply on it.\")\n                entry: Any = output[\"obj\"]\n                output_result = entry.perform_batch_reply(callback=callback,\n                                                          target_handle=target_handles[key],\n                                                          lookback_limit=lookback_dict[key],\n                                                          )\n                record.output_records[key] = output_result\n\n        self.history.append(record)\n        self.update_history()\n\n        return record"}
{"code":"def _design_matrix(\n        model: pd.DataFrame,\n        batch_key: str,\n        batch_levels: Collection[str],\n) -> pd.DataFrame:\n    \"\"\"\n    Computes a simple design matrix.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    batch_key\n        Name of the batch column\n    batch_levels\n        Levels of the batch annotation\n\n    Returns\n    --------\n    The design matrix for the regression problem\n    \"\"\"\n    import patsy\n\n    design = patsy.dmatrix(\n        \"~ 0 + C(Q('{}'), levels=batch_levels)\".format(batch_key),\n        model,\n        return_type=\"dataframe\",\n    )\n    model = model.drop([batch_key], axis=1)\n    numerical_covariates = model.select_dtypes('number').columns.values\n\n    logg.info(\"Found {} batches\\n\".format(design.shape[1]))\n    other_cols = [c for c in model.columns.values if c not in numerical_covariates]\n\n    if other_cols:\n        col_repr = \" + \".join(\"Q('{}')\".format(x) for x in other_cols)\n        factor_matrix = patsy.dmatrix(\"~ 0 + {}\".format(col_repr),\n                                      model[other_cols],\n                                      return_type=\"dataframe\")\n\n        design = pd.concat((design, factor_matrix), axis=1)\n        logg.info(\"Found {} categorical variables:\".format(len(other_cols)))\n        logg.info(\"\\t\" + \", \".join(other_cols) + '\\n')\n\n    if numerical_covariates is not None:\n        logg.info(\"Found {} numerical variables:\".format(len(numerical_covariates)))\n        logg.info(\"\\t\" + \", \".join(numerical_covariates) + '\\n')\n\n        for nC in numerical_covariates:\n            design[nC] = model[nC]\n\n    return design","return_type":"pd.DataFrame","function_name":"_design_matrix","stripped_code":"def _design_matrix(\n        model: pd.DataFrame,\n        batch_key: str,\n        batch_levels: Collection[str],\n):\n    \"\"\"\n    Computes a simple design matrix.\n\n    Parameters\n    --------\n    model\n        Contains the batch annotation\n    batch_key\n        Name of the batch column\n    batch_levels\n        Levels of the batch annotation\n\n    Returns\n    --------\n    The design matrix for the regression problem\n    \"\"\"\n    import patsy\n\n    design = patsy.dmatrix(\n        \"~ 0 + C(Q('{}'), levels=batch_levels)\".format(batch_key),\n        model,\n        return_type=\"dataframe\",\n    )\n    model = model.drop([batch_key], axis=1)\n    numerical_covariates = model.select_dtypes('number').columns.values\n\n    logg.info(\"Found {} batches\\n\".format(design.shape[1]))\n    other_cols = [c for c in model.columns.values if c not in numerical_covariates]\n\n    if other_cols:\n        col_repr = \" + \".join(\"Q('{}')\".format(x) for x in other_cols)\n        factor_matrix = patsy.dmatrix(\"~ 0 + {}\".format(col_repr),\n                                      model[other_cols],\n                                      return_type=\"dataframe\")\n\n        design = pd.concat((design, factor_matrix), axis=1)\n        logg.info(\"Found {} categorical variables:\".format(len(other_cols)))\n        logg.info(\"\\t\" + \", \".join(other_cols) + '\\n')\n\n    if numerical_covariates is not None:\n        logg.info(\"Found {} numerical variables:\".format(len(numerical_covariates)))\n        logg.info(\"\\t\" + \", \".join(numerical_covariates) + '\\n')\n\n        for nC in numerical_covariates:\n            design[nC] = model[nC]\n\n    return design"}
{"code":"def format(self, password: str = '') -> str:\n        \"\"\"Format command along with any arguments, ready to be sent.\"\"\"\n        return MARKER_START + \\\n            self.name + \\\n            self.action + \\\n            self.args + \\\n            password + \\\n            MARKER_END","return_type":"str","function_name":"Command.format","stripped_code":"def format(self, password: str = ''):\n        \"\"\"Format command along with any arguments, ready to be sent.\"\"\"\n        return MARKER_START + \\\n            self.name + \\\n            self.action + \\\n            self.args + \\\n            password + \\\n            MARKER_END"}
{"code":"def find_types_removed_from_unions(\n    old_schema: GraphQLSchema, new_schema: GraphQLSchema\n) -> List[BreakingChange]:\n    \"\"\"Find types removed from unions.\n\n    Given two schemas, returns a list containing descriptions of any breaking changes\n    in the new_schema related to removing types from a union type.\n    \"\"\"\n    old_type_map = old_schema.type_map\n    new_type_map = new_schema.type_map\n\n    types_removed_from_union = []\n    for old_type_name, old_type in old_type_map.items():\n        new_type = new_type_map.get(old_type_name)\n        if not (is_union_type(old_type) and is_union_type(new_type)):\n            continue\n        old_type = cast(GraphQLUnionType, old_type)\n        new_type = cast(GraphQLUnionType, new_type)\n        type_names_in_new_union = {type_.name for type_ in new_type.types}\n        for type_ in old_type.types:\n            type_name = type_.name\n            if type_name not in type_names_in_new_union:\n                types_removed_from_union.append(\n                    BreakingChange(\n                        BreakingChangeType.TYPE_REMOVED_FROM_UNION,\n                        f\"{type_name} was removed from union type {old_type_name}.\",\n                    )\n                )\n    return types_removed_from_union","return_type":"List[BreakingChange]","function_name":"find_types_removed_from_unions","stripped_code":"def find_types_removed_from_unions(\n    old_schema: GraphQLSchema, new_schema: GraphQLSchema\n):\n    \"\"\"Find types removed from unions.\n\n    Given two schemas, returns a list containing descriptions of any breaking changes\n    in the new_schema related to removing types from a union type.\n    \"\"\"\n    old_type_map = old_schema.type_map\n    new_type_map = new_schema.type_map\n\n    types_removed_from_union = []\n    for old_type_name, old_type in old_type_map.items():\n        new_type = new_type_map.get(old_type_name)\n        if not (is_union_type(old_type) and is_union_type(new_type)):\n            continue\n        old_type = cast(GraphQLUnionType, old_type)\n        new_type = cast(GraphQLUnionType, new_type)\n        type_names_in_new_union = {type_.name for type_ in new_type.types}\n        for type_ in old_type.types:\n            type_name = type_.name\n            if type_name not in type_names_in_new_union:\n                types_removed_from_union.append(\n                    BreakingChange(\n                        BreakingChangeType.TYPE_REMOVED_FROM_UNION,\n                        f\"{type_name} was removed from union type {old_type_name}.\",\n                    )\n                )\n    return types_removed_from_union"}
{"code":"def process_bind_param(self, value: Optional[List[str]],\n                           dialect: Dialect) -> str:\n        \"\"\"Convert things on the way from Python to the database.\"\"\"\n        retval = self._strlist_to_dbstr(value)\n        return retval","return_type":"str","function_name":"StringListType.process_bind_param","stripped_code":"def process_bind_param(self, value: Optional[List[str]],\n                           dialect: Dialect):\n        \"\"\"Convert things on the way from Python to the database.\"\"\"\n        retval = self._strlist_to_dbstr(value)\n        return retval"}
{"code":"def dataset_path_iterator(file_path: str) -> Iterator[str]:\n        \"\"\"\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        \"\"\"\n        logger.info(\"Reading CONLL sentences from dataset files at: %s\", file_path)\n        for root, _, files in list(os.walk(file_path)):\n            for data_file in files:\n                # These are a relic of the dataset pre-processing. Every\n                # file will be duplicated - one file called filename.gold_skel\n                # and one generated from the preprocessing called filename.gold_conll.\n                if not data_file.endswith(\"gold_conll\"):\n                    continue\n\n                yield os.path.join(root, data_file)","return_type":"Iterator[str]","function_name":"Ontonotes.dataset_path_iterator","stripped_code":"def dataset_path_iterator(file_path: str):\n        \"\"\"\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        \"\"\"\n        logger.info(\"Reading CONLL sentences from dataset files at: %s\", file_path)\n        for root, _, files in list(os.walk(file_path)):\n            for data_file in files:\n                # These are a relic of the dataset pre-processing. Every\n                # file will be duplicated - one file called filename.gold_skel\n                # and one generated from the preprocessing called filename.gold_conll.\n                if not data_file.endswith(\"gold_conll\"):\n                    continue\n\n                yield os.path.join(root, data_file)"}
{"code":"def get_files(path:PathOrStr, extensions:Collection[str]=None, recurse:bool=False,\n              include:Optional[Collection[str]]=None)->FilePathList:\n    \"Return list of files in `path` that have a suffix in `extensions`; optionally `recurse`.\"\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path)):\n            # skip hidden dirs\n            if include is not None and i==0:  d[:] = [o for o in d if o in include]\n            else:                             d[:] = [o for o in d if not o.startswith('.')]\n            res += _get_files(path, p, f, extensions)\n        return res\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        return _get_files(path, path, f, extensions)","return_type":"FilePathList","function_name":"get_files","stripped_code":"def get_files(path:PathOrStr, extensions:Collection[str]=None, recurse:bool=False,\n              include:Optional[Collection[str]]=None):\n    \"Return list of files in `path` that have a suffix in `extensions`; optionally `recurse`.\"\n    if recurse:\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path)):\n            # skip hidden dirs\n            if include is not None and i==0:  d[:] = [o for o in d if o in include]\n            else:                             d[:] = [o for o in d if not o.startswith('.')]\n            res += _get_files(path, p, f, extensions)\n        return res\n    else:\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        return _get_files(path, path, f, extensions)"}
{"code":"def setNamedItem(self, item: Attr) -> None:\n        \"\"\"Set ``Attr`` object in this collection.\"\"\"\n        from wdom.web_node import WdomElement\n        if not isinstance(item, Attr):\n            raise TypeError('item must be an instance of Attr')\n        if isinstance(self._owner, WdomElement):\n            self._owner.js_exec('setAttribute', item.name,  # type: ignore\n                                item.value)\n        self._dict[item.name] = item\n        item._owner = self._owner","return_type":"None","function_name":"NamedNodeMap.setNamedItem","stripped_code":"def setNamedItem(self, item: Attr):\n        \"\"\"Set ``Attr`` object in this collection.\"\"\"\n        from wdom.web_node import WdomElement\n        if not isinstance(item, Attr):\n            raise TypeError('item must be an instance of Attr')\n        if isinstance(self._owner, WdomElement):\n            self._owner.js_exec('setAttribute', item.name,  # type: ignore\n                                item.value)\n        self._dict[item.name] = item\n        item._owner = self._owner"}
{"code":"def murmur3_x86_32(data: Union[bytes, bytearray], seed: int = 0) -> int:\n    \"\"\"\n    Pure 32-bit Python implementation of MurmurHash3; see\n    http://stackoverflow.com/questions/13305290/is-there-a-pure-python-implementation-of-murmurhash.\n     \n    Args:\n        data: data to hash \n        seed: seed\n\n    Returns:\n        integer hash\n\n    \"\"\"  # noqa\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed\n    rounded_end = (length & 0xfffffffc)  # round down to 4 byte block\n    for i in range(0, rounded_end, 4):\n        # little endian load order\n        # RNC: removed ord() calls\n        k1 = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | \\\n             ((data[i + 2] & 0xff) << 16) | (data[i + 3] << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1, 15)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1, 13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    k1 = 0\n\n    val = length & 0x03\n    if val == 3:\n        k1 = (data[rounded_end + 2] & 0xff) << 16\n    # fallthrough\n    if val in (2, 3):\n        k1 |= (data[rounded_end + 1] & 0xff) << 8\n    # fallthrough\n    if val in (1, 2, 3):\n        k1 |= data[rounded_end] & 0xff\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1, 15)\n        k1 *= c2\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n\n    # fmix(h1)\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n    h1 *= 0x85ebca6b\n    h1 ^= ((h1 & 0xffffffff) >> 13)\n    h1 *= 0xc2b2ae35\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n\n    return h1 & 0xffffffff","return_type":"int","function_name":"murmur3_x86_32","stripped_code":"def murmur3_x86_32(data: Union[bytes, bytearray], seed: int = 0):\n    \"\"\"\n    Pure 32-bit Python implementation of MurmurHash3; see\n    http://stackoverflow.com/questions/13305290/is-there-a-pure-python-implementation-of-murmurhash.\n     \n    Args:\n        data: data to hash \n        seed: seed\n\n    Returns:\n        integer hash\n\n    \"\"\"  # noqa\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed\n    rounded_end = (length & 0xfffffffc)  # round down to 4 byte block\n    for i in range(0, rounded_end, 4):\n        # little endian load order\n        # RNC: removed ord() calls\n        k1 = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | \\\n             ((data[i + 2] & 0xff) << 16) | (data[i + 3] << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1, 15)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1, 13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    k1 = 0\n\n    val = length & 0x03\n    if val == 3:\n        k1 = (data[rounded_end + 2] & 0xff) << 16\n    # fallthrough\n    if val in (2, 3):\n        k1 |= (data[rounded_end + 1] & 0xff) << 8\n    # fallthrough\n    if val in (1, 2, 3):\n        k1 |= data[rounded_end] & 0xff\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1, 15)\n        k1 *= c2\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n\n    # fmix(h1)\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n    h1 *= 0x85ebca6b\n    h1 ^= ((h1 & 0xffffffff) >> 13)\n    h1 *= 0xc2b2ae35\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n\n    return h1 & 0xffffffff"}
{"code":"def load_configuration(self) -> None:\n        \"\"\"\n        Read the configuration from a configuration file\n        \"\"\"\n        config_file = self.default_config_file\n        if self.config_file:\n            config_file = self.config_file\n        self.config = ConfigParser()\n        self.config.read(config_file)","return_type":"None","function_name":"BandersnatchConfig.load_configuration","stripped_code":"def load_configuration(self):\n        \"\"\"\n        Read the configuration from a configuration file\n        \"\"\"\n        config_file = self.default_config_file\n        if self.config_file:\n            config_file = self.config_file\n        self.config = ConfigParser()\n        self.config.read(config_file)"}
{"code":"def needs_settling(self, e: AccountEntry) -> bool:\n        \"\"\"\n        Returns True if all of following conditions are True:\n        a) entry type is settlement\n        b) entry has been recorded to this account\n        c) invoice to be settled has been set\n        d) entry has not been settled (=child set empty)\n        :param e: AccountEntry (settlement)\n        :return: bool\n        \"\"\"\n        return e.type and e.type.is_settlement and e.account.id == self.id and e.settled_invoice and AccountEntry.objects.filter(parent=e).count() == 0","return_type":"bool","function_name":"Account.needs_settling","stripped_code":"def needs_settling(self, e: AccountEntry):\n        \"\"\"\n        Returns True if all of following conditions are True:\n        a) entry type is settlement\n        b) entry has been recorded to this account\n        c) invoice to be settled has been set\n        d) entry has not been settled (=child set empty)\n        :param e: AccountEntry (settlement)\n        :return: bool\n        \"\"\"\n        return e.type and e.type.is_settlement and e.account.id == self.id and e.settled_invoice and AccountEntry.objects.filter(parent=e).count() == 0"}
{"code":"def get_client_str(username: str, ip_address: str, user_agent: str, path_info: str) -> str:\n    \"\"\"\n    Get a readable string that can be used in e.g. logging to distinguish client requests.\n\n    Example log format would be ``{username: \"example\", ip_address: \"127.0.0.1\", path_info: \"/example/\"}``\n    \"\"\"\n\n    client_dict = dict()\n\n    if settings.AXES_VERBOSE:\n        # Verbose mode logs every attribute that is available\n        client_dict['username'] = username\n        client_dict['ip_address'] = ip_address\n        client_dict['user_agent'] = user_agent\n    else:\n        # Other modes initialize the attributes that are used for the actual lockouts\n        client_dict = get_client_parameters(username, ip_address, user_agent)\n\n    # Path info is always included as last component in the client string for traceability purposes\n    if path_info and isinstance(path_info, (tuple, list)):\n        path_info = path_info[0]\n    client_dict['path_info'] = path_info\n\n    # Template the internal dictionary representation into a readable and concatenated key: \"value\" format\n    template = ', '.join(\n        f'{key}: \"{value}\"'\n        for key, value\n        in client_dict.items()\n    )\n\n    # Wrap the internal dict into a single {key: \"value\"} bracing in the output\n    # which requires double braces when done with the Python string templating system\n    # i.e. {{key: \"value\"}} becomes {key: \"value\"} when run through a .format() call\n    template = '{{' + template + '}}'\n\n    return template.format(client_dict)","return_type":"str","function_name":"get_client_str","stripped_code":"def get_client_str(username: str, ip_address: str, user_agent: str, path_info: str):\n    \"\"\"\n    Get a readable string that can be used in e.g. logging to distinguish client requests.\n\n    Example log format would be ``{username: \"example\", ip_address: \"127.0.0.1\", path_info: \"/example/\"}``\n    \"\"\"\n\n    client_dict = dict()\n\n    if settings.AXES_VERBOSE:\n        # Verbose mode logs every attribute that is available\n        client_dict['username'] = username\n        client_dict['ip_address'] = ip_address\n        client_dict['user_agent'] = user_agent\n    else:\n        # Other modes initialize the attributes that are used for the actual lockouts\n        client_dict = get_client_parameters(username, ip_address, user_agent)\n\n    # Path info is always included as last component in the client string for traceability purposes\n    if path_info and isinstance(path_info, (tuple, list)):\n        path_info = path_info[0]\n    client_dict['path_info'] = path_info\n\n    # Template the internal dictionary representation into a readable and concatenated key: \"value\" format\n    template = ', '.join(\n        f'{key}: \"{value}\"'\n        for key, value\n        in client_dict.items()\n    )\n\n    # Wrap the internal dict into a single {key: \"value\"} bracing in the output\n    # which requires double braces when done with the Python string templating system\n    # i.e. {{key: \"value\"}} becomes {key: \"value\"} when run through a .format() call\n    template = '{{' + template + '}}'\n\n    return template.format(client_dict)"}
{"code":"def astensor(array: TensorLike) -> BKTensor:\n    \"\"\"Converts a numpy array to the backend's tensor object\n    \"\"\"\n    array = np.asarray(array, dtype=CTYPE)\n    return array","return_type":"BKTensor","function_name":"astensor","stripped_code":"def astensor(array: TensorLike):\n    \"\"\"Converts a numpy array to the backend's tensor object\n    \"\"\"\n    array = np.asarray(array, dtype=CTYPE)\n    return array"}
{"code":"def merge_single_qubit_gates_into_phased_x_z(\n        circuit: circuits.Circuit,\n        atol: float = 1e-8) -> None:\n    \"\"\"Canonicalizes runs of single-qubit rotations in a circuit.\n\n    Specifically, any run of non-parameterized circuits will be replaced by an\n    optional PhasedX operation followed by an optional Z operation.\n\n    Args:\n        circuit: The circuit to rewrite. This value is mutated in-place.\n        atol: Absolute tolerance to angle error. Larger values allow more\n            negligible gates to be dropped, smaller values increase accuracy.\n    \"\"\"\n\n    def synth(qubit: ops.Qid, matrix: np.ndarray) -> List[ops.Operation]:\n        out_gates = decompositions.single_qubit_matrix_to_phased_x_z(\n            matrix, atol)\n        return [gate(qubit) for gate in out_gates]\n\n    MergeSingleQubitGates(synthesizer=synth).optimize_circuit(circuit)","return_type":"None","function_name":"merge_single_qubit_gates_into_phased_x_z","stripped_code":"def merge_single_qubit_gates_into_phased_x_z(\n        circuit: circuits.Circuit,\n        atol: float = 1e-8):\n    \"\"\"Canonicalizes runs of single-qubit rotations in a circuit.\n\n    Specifically, any run of non-parameterized circuits will be replaced by an\n    optional PhasedX operation followed by an optional Z operation.\n\n    Args:\n        circuit: The circuit to rewrite. This value is mutated in-place.\n        atol: Absolute tolerance to angle error. Larger values allow more\n            negligible gates to be dropped, smaller values increase accuracy.\n    \"\"\"\n\n    def synth(qubit: ops.Qid, matrix: np.ndarray) -> List[ops.Operation]:\n        out_gates = decompositions.single_qubit_matrix_to_phased_x_z(\n            matrix, atol)\n        return [gate(qubit) for gate in out_gates]\n\n    MergeSingleQubitGates(synthesizer=synth).optimize_circuit(circuit)"}
{"code":"def do_parse(infilename: str, outfilename: str, verbose: bool) -> bool:\n    \"\"\"\n    Parse the jsg in infilename and save the results in outfilename\n    :param infilename: file containing jsg\n    :param outfilename: target python file\n    :param verbose: verbose output flag\n    :return: true if success\n    \"\"\"\n    python = parse(FileStream(infilename, encoding=\"utf-8\"), infilename)\n    if python is not None:\n        with open(outfilename, 'w') as outfile:\n            outfile.write(python)\n        if verbose:\n            print(\"Output written to {}\".format(outfilename))\n        return True\n    return False","return_type":"bool","function_name":"do_parse","stripped_code":"def do_parse(infilename: str, outfilename: str, verbose: bool):\n    \"\"\"\n    Parse the jsg in infilename and save the results in outfilename\n    :param infilename: file containing jsg\n    :param outfilename: target python file\n    :param verbose: verbose output flag\n    :return: true if success\n    \"\"\"\n    python = parse(FileStream(infilename, encoding=\"utf-8\"), infilename)\n    if python is not None:\n        with open(outfilename, 'w') as outfile:\n            outfile.write(python)\n        if verbose:\n            print(\"Output written to {}\".format(outfilename))\n        return True\n    return False"}
{"code":"def netmiko_file_transfer(\n    task: Task, source_file: str, dest_file: str, **kwargs: Any\n) -> Result:\n    \"\"\"\n    Execute Netmiko file_transfer method\n\n    Arguments:\n        source_file: Source file.\n        dest_file: Destination file.\n        kwargs: Additional arguments to pass to file_transfer\n\n    Returns:\n        Result object with the following attributes set:\n          * result (``bool``): file exists and MD5 is valid\n          * changed (``bool``): the destination file was changed\n\n    \"\"\"\n    net_connect = task.host.get_connection(\"netmiko\", task.nornir.config)\n    kwargs.setdefault(\"direction\", \"put\")\n    scp_result = file_transfer(\n        net_connect, source_file=source_file, dest_file=dest_file, **kwargs\n    )\n    if kwargs.get(\"disable_md5\") is True:\n        file_valid = scp_result[\"file_exists\"]\n    else:\n        file_valid = scp_result[\"file_exists\"] and scp_result[\"file_verified\"]\n    return Result(\n        host=task.host, result=file_valid, changed=scp_result[\"file_transferred\"]\n    )","return_type":"Result","function_name":"netmiko_file_transfer","stripped_code":"def netmiko_file_transfer(\n    task: Task, source_file: str, dest_file: str, **kwargs: Any\n):\n    \"\"\"\n    Execute Netmiko file_transfer method\n\n    Arguments:\n        source_file: Source file.\n        dest_file: Destination file.\n        kwargs: Additional arguments to pass to file_transfer\n\n    Returns:\n        Result object with the following attributes set:\n          * result (``bool``): file exists and MD5 is valid\n          * changed (``bool``): the destination file was changed\n\n    \"\"\"\n    net_connect = task.host.get_connection(\"netmiko\", task.nornir.config)\n    kwargs.setdefault(\"direction\", \"put\")\n    scp_result = file_transfer(\n        net_connect, source_file=source_file, dest_file=dest_file, **kwargs\n    )\n    if kwargs.get(\"disable_md5\") is True:\n        file_valid = scp_result[\"file_exists\"]\n    else:\n        file_valid = scp_result[\"file_exists\"] and scp_result[\"file_verified\"]\n    return Result(\n        host=task.host, result=file_valid, changed=scp_result[\"file_transferred\"]\n    )"}
{"code":"def handle_namespace_url(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``DEFINE NAMESPACE X AS URL \"Y\"``.\n\n        :raises: RedefinedNamespaceError\n        :raises: pybel.resources.exc.ResourceError\n        \"\"\"\n        namespace = tokens['name']\n        self.raise_for_redefined_namespace(line, position, namespace)\n\n        url = tokens['url']\n        self.namespace_url_dict[namespace] = url\n\n        if self.skip_validation:\n            return tokens\n\n        namespace_result = self.manager.get_or_create_namespace(url)\n\n        if isinstance(namespace_result, dict):\n            self.namespace_to_term[namespace] = namespace_result\n            self.uncachable_namespaces.add(url)\n        else:\n            self.namespace_to_term[namespace] = self.manager.get_namespace_encoding(url)\n\n        return tokens","return_type":"ParseResults","function_name":"MetadataParser.handle_namespace_url","stripped_code":"def handle_namespace_url(self, line: str, position: int, tokens: ParseResults):\n        \"\"\"Handle statements like ``DEFINE NAMESPACE X AS URL \"Y\"``.\n\n        :raises: RedefinedNamespaceError\n        :raises: pybel.resources.exc.ResourceError\n        \"\"\"\n        namespace = tokens['name']\n        self.raise_for_redefined_namespace(line, position, namespace)\n\n        url = tokens['url']\n        self.namespace_url_dict[namespace] = url\n\n        if self.skip_validation:\n            return tokens\n\n        namespace_result = self.manager.get_or_create_namespace(url)\n\n        if isinstance(namespace_result, dict):\n            self.namespace_to_term[namespace] = namespace_result\n            self.uncachable_namespaces.add(url)\n        else:\n            self.namespace_to_term[namespace] = self.manager.get_namespace_encoding(url)\n\n        return tokens"}
{"code":"def load_conditions(self) -> None:\n        \"\"\"Save the initial conditions of the |Model| object handled by\n        each |Element| object.\"\"\"\n        for element in printtools.progressbar(self):\n            element.model.sequences.load_conditions()","return_type":"None","function_name":"Elements.load_conditions","stripped_code":"def load_conditions(self):\n        \"\"\"Save the initial conditions of the |Model| object handled by\n        each |Element| object.\"\"\"\n        for element in printtools.progressbar(self):\n            element.model.sequences.load_conditions()"}
{"code":"def schema_id(origin_did: str, name: str, version: str) -> str:\n    \"\"\"\n    Return schema identifier for input origin DID, schema name, and schema version.\n\n    :param origin_did: DID of schema originator\n    :param name: schema name\n    :param version: schema version\n    :return: schema identifier\n    \"\"\"\n\n    return '{}:2:{}:{}'.format(origin_did, name, version)","return_type":"str","function_name":"schema_id","stripped_code":"def schema_id(origin_did: str, name: str, version: str):\n    \"\"\"\n    Return schema identifier for input origin DID, schema name, and schema version.\n\n    :param origin_did: DID of schema originator\n    :param name: schema name\n    :param version: schema version\n    :return: schema identifier\n    \"\"\"\n\n    return '{}:2:{}:{}'.format(origin_did, name, version)"}
{"code":"def passport_series(self, year: int = None) -> str:\n        \"\"\"Generate random series of passport.\n\n        :param year: Year of manufacture.\n        :type year: int or None\n        :return: Series.\n\n        :Example:\n            02 15.\n        \"\"\"\n        if not year:\n            year = self.random.randint(10, 18)\n\n        region = self.random.randint(1, 99)\n        return '{:02d} {}'.format(region, year)","return_type":"str","function_name":"RussiaSpecProvider.passport_series","stripped_code":"def passport_series(self, year: int = None):\n        \"\"\"Generate random series of passport.\n\n        :param year: Year of manufacture.\n        :type year: int or None\n        :return: Series.\n\n        :Example:\n            02 15.\n        \"\"\"\n        if not year:\n            year = self.random.randint(10, 18)\n\n        region = self.random.randint(1, 99)\n        return '{:02d} {}'.format(region, year)"}
{"code":"def get_unstresses(stresses: List[int], count: int) -> List[int]:\n    \"\"\"\n    Given a list of stressed positions, and count of possible positions, return a list of\n    the unstressed positions.\n\n    :param stresses: a list of stressed positions\n    :param count: the number of possible positions\n    :return: a list of unstressed positions\n\n    >>> get_unstresses([0, 3, 6, 9, 12, 15], 17)\n    [1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16]\n    \"\"\"\n    return list(set(range(count)) - set(stresses))","return_type":"List[int]","function_name":"get_unstresses","stripped_code":"def get_unstresses(stresses: List[int], count: int):\n    \"\"\"\n    Given a list of stressed positions, and count of possible positions, return a list of\n    the unstressed positions.\n\n    :param stresses: a list of stressed positions\n    :param count: the number of possible positions\n    :return: a list of unstressed positions\n\n    >>> get_unstresses([0, 3, 6, 9, 12, 15], 17)\n    [1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16]\n    \"\"\"\n    return list(set(range(count)) - set(stresses))"}
{"code":"def on_backward_begin(self, smooth_loss:Tensor, **kwargs:Any)->None:\n        \"Record the loss before any other callback has a chance to modify it.\"\n        self.losses.append(smooth_loss)\n        if self.pbar is not None and hasattr(self.pbar,'child'):\n            self.pbar.child.comment = f'{smooth_loss:.4f}'","return_type":"None","function_name":"Recorder.on_backward_begin","stripped_code":"def on_backward_begin(self, smooth_loss:Tensor, **kwargs:Any):\n        \"Record the loss before any other callback has a chance to modify it.\"\n        self.losses.append(smooth_loss)\n        if self.pbar is not None and hasattr(self.pbar,'child'):\n            self.pbar.child.comment = f'{smooth_loss:.4f}'"}
{"code":"def fiscal_code(self, gender: Optional[Gender] = None) -> str:\n        \"\"\"Return a random fiscal code.\n\n        :param gender: Gender's enum object.\n        :return: Fiscal code.\n\n        Example:\n            RSSMRA66R05D612U\n        \"\"\"\n        code = ''.join(self.random.choices(string.ascii_uppercase, k=6))\n\n        code += self.random.custom_code(mask='##')\n\n        month_codes = self._data['fiscal_code']['month_codes']\n        code += self.random.choice(month_codes)\n\n        birth_day = self.random.randint(101, 131)\n        self._validate_enum(gender, Gender)\n        if gender == Gender.FEMALE:\n            birth_day += 40\n        code += str(birth_day)[1:]\n\n        city_letters = self._data['fiscal_code']['city_letters']\n        code += self.random.choice(city_letters)\n        code += self.random.custom_code(mask='###@')\n\n        return code","return_type":"str","function_name":"ItalySpecProvider.fiscal_code","stripped_code":"def fiscal_code(self, gender: Optional[Gender] = None):\n        \"\"\"Return a random fiscal code.\n\n        :param gender: Gender's enum object.\n        :return: Fiscal code.\n\n        Example:\n            RSSMRA66R05D612U\n        \"\"\"\n        code = ''.join(self.random.choices(string.ascii_uppercase, k=6))\n\n        code += self.random.custom_code(mask='##')\n\n        month_codes = self._data['fiscal_code']['month_codes']\n        code += self.random.choice(month_codes)\n\n        birth_day = self.random.randint(101, 131)\n        self._validate_enum(gender, Gender)\n        if gender == Gender.FEMALE:\n            birth_day += 40\n        code += str(birth_day)[1:]\n\n        city_letters = self._data['fiscal_code']['city_letters']\n        code += self.random.choice(city_letters)\n        code += self.random.custom_code(mask='###@')\n\n        return code"}
{"code":"def __recognize_user_class(self, node: yaml.Node,\n                               expected_type: Type) -> RecResult:\n        \"\"\"Recognize a user-defined class in the node.\n\n        This tries to recognize only exactly the specified class. It \\\n        recurses down into the class's attributes, but not to its \\\n        subclasses. See also __recognize_user_classes().\n\n        Args:\n            node: The node to recognize.\n            expected_type: A user-defined class.\n\n        Returns:\n            A list containing the user-defined class, or an empty list.\n        \"\"\"\n        logger.debug('Recognizing as a user-defined class')\n        loc_str = '{}{}'.format(node.start_mark, os.linesep)\n        if hasattr(expected_type, 'yatiml_recognize'):\n            try:\n                unode = UnknownNode(self, node)\n                expected_type.yatiml_recognize(unode)\n                return [expected_type], ''\n            except RecognitionError as e:\n                if len(e.args) > 0:\n                    message = ('Error recognizing a {}\\n{}because of the'\n                               ' following error(s): {}').format(\n                                   expected_type.__class__, loc_str,\n                                   indent(e.args[0], '    '))\n                else:\n                    message = 'Error recognizing a {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                return [], message\n        else:\n            if issubclass(expected_type, enum.Enum):\n                if (not isinstance(node, yaml.ScalarNode)\n                        or node.tag != 'tag:yaml.org,2002:str'):\n                    message = 'Expected an enum value from {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                    return [], message\n            elif (issubclass(expected_type, UserString)\n                  or issubclass(expected_type, str)):\n                if (not isinstance(node, yaml.ScalarNode)\n                        or node.tag != 'tag:yaml.org,2002:str'):\n                    message = 'Expected a string matching {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                    return [], message\n            else:\n                # auto-recognize based on constructor signature\n                if not isinstance(node, yaml.MappingNode):\n                    message = 'Expected a dict/mapping here\\n{}'.format(\n                        loc_str)\n                    return [], message\n\n                for attr_name, type_, required in class_subobjects(\n                        expected_type):\n                    cnode = Node(node)\n                    # try exact match first, dashes if that doesn't match\n                    for name in [attr_name, attr_name.replace('_', '-')]:\n                        if cnode.has_attribute(name):\n                            subnode = cnode.get_attribute(name)\n                            recognized_types, message = self.recognize(\n                                subnode.yaml_node, type_)\n                            if len(recognized_types) == 0:\n                                message = ('Failed when checking attribute'\n                                           ' {}:\\n{}').format(\n                                               name, indent(message, '    '))\n                                return [], message\n                            break\n                    else:\n                        if required:\n                            message = (\n                                'Error recognizing a {}\\n{}because it'\n                                ' is missing an attribute named {}').format(\n                                    expected_type.__name__, loc_str, attr_name)\n                            if '_' in attr_name:\n                                message += ' or maybe {}.\\n'.format(\n                                    attr_name.replace('_', '-'))\n                            else:\n                                message += '.\\n'\n                            return [], message\n\n            return [expected_type], ''","return_type":"RecResult","function_name":"Recognizer.__recognize_user_class","stripped_code":"def __recognize_user_class(self, node: yaml.Node,\n                               expected_type: Type):\n        \"\"\"Recognize a user-defined class in the node.\n\n        This tries to recognize only exactly the specified class. It \\\n        recurses down into the class's attributes, but not to its \\\n        subclasses. See also __recognize_user_classes().\n\n        Args:\n            node: The node to recognize.\n            expected_type: A user-defined class.\n\n        Returns:\n            A list containing the user-defined class, or an empty list.\n        \"\"\"\n        logger.debug('Recognizing as a user-defined class')\n        loc_str = '{}{}'.format(node.start_mark, os.linesep)\n        if hasattr(expected_type, 'yatiml_recognize'):\n            try:\n                unode = UnknownNode(self, node)\n                expected_type.yatiml_recognize(unode)\n                return [expected_type], ''\n            except RecognitionError as e:\n                if len(e.args) > 0:\n                    message = ('Error recognizing a {}\\n{}because of the'\n                               ' following error(s): {}').format(\n                                   expected_type.__class__, loc_str,\n                                   indent(e.args[0], '    '))\n                else:\n                    message = 'Error recognizing a {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                return [], message\n        else:\n            if issubclass(expected_type, enum.Enum):\n                if (not isinstance(node, yaml.ScalarNode)\n                        or node.tag != 'tag:yaml.org,2002:str'):\n                    message = 'Expected an enum value from {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                    return [], message\n            elif (issubclass(expected_type, UserString)\n                  or issubclass(expected_type, str)):\n                if (not isinstance(node, yaml.ScalarNode)\n                        or node.tag != 'tag:yaml.org,2002:str'):\n                    message = 'Expected a string matching {}\\n{}'.format(\n                        expected_type.__class__, loc_str)\n                    return [], message\n            else:\n                # auto-recognize based on constructor signature\n                if not isinstance(node, yaml.MappingNode):\n                    message = 'Expected a dict/mapping here\\n{}'.format(\n                        loc_str)\n                    return [], message\n\n                for attr_name, type_, required in class_subobjects(\n                        expected_type):\n                    cnode = Node(node)\n                    # try exact match first, dashes if that doesn't match\n                    for name in [attr_name, attr_name.replace('_', '-')]:\n                        if cnode.has_attribute(name):\n                            subnode = cnode.get_attribute(name)\n                            recognized_types, message = self.recognize(\n                                subnode.yaml_node, type_)\n                            if len(recognized_types) == 0:\n                                message = ('Failed when checking attribute'\n                                           ' {}:\\n{}').format(\n                                               name, indent(message, '    '))\n                                return [], message\n                            break\n                    else:\n                        if required:\n                            message = (\n                                'Error recognizing a {}\\n{}because it'\n                                ' is missing an attribute named {}').format(\n                                    expected_type.__name__, loc_str, attr_name)\n                            if '_' in attr_name:\n                                message += ' or maybe {}.\\n'.format(\n                                    attr_name.replace('_', '-'))\n                            else:\n                                message += '.\\n'\n                            return [], message\n\n            return [expected_type], ''"}
{"code":"def build_from_paths(paths: List[str], num_words: Optional[int] = None, min_count: int = 1,\n                     pad_to_multiple_of: Optional[int] = None) -> Vocab:\n    \"\"\"\n    Creates vocabulary from paths to a file in sentence-per-line format. A sentence is just a whitespace delimited\n    list of tokens. Note that special symbols like the beginning of sentence (BOS) symbol will be added to the\n    vocabulary.\n\n    :param paths: List of paths to files with one sentence per line.\n    :param num_words: Optional maximum number of words in the vocabulary.\n    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n    :param pad_to_multiple_of: If not None, pads the vocabulary to a size that is the next multiple of this int.\n    :return: Word-to-id mapping.\n    \"\"\"\n    with ExitStack() as stack:\n        logger.info(\"Building vocabulary from dataset(s): %s\", paths)\n        files = (stack.enter_context(utils.smart_open(path)) for path in paths)\n        return build_vocab(chain(*files), num_words, min_count, pad_to_multiple_of)","return_type":"Vocab","function_name":"build_from_paths","stripped_code":"def build_from_paths(paths: List[str], num_words: Optional[int] = None, min_count: int = 1,\n                     pad_to_multiple_of: Optional[int] = None):\n    \"\"\"\n    Creates vocabulary from paths to a file in sentence-per-line format. A sentence is just a whitespace delimited\n    list of tokens. Note that special symbols like the beginning of sentence (BOS) symbol will be added to the\n    vocabulary.\n\n    :param paths: List of paths to files with one sentence per line.\n    :param num_words: Optional maximum number of words in the vocabulary.\n    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n    :param pad_to_multiple_of: If not None, pads the vocabulary to a size that is the next multiple of this int.\n    :return: Word-to-id mapping.\n    \"\"\"\n    with ExitStack() as stack:\n        logger.info(\"Building vocabulary from dataset(s): %s\", paths)\n        files = (stack.enter_context(utils.smart_open(path)) for path in paths)\n        return build_vocab(chain(*files), num_words, min_count, pad_to_multiple_of)"}
{"code":"def sys_check_for_event(\n    mask: int, k: Optional[Key], m: Optional[Mouse]\n) -> int:\n    \"\"\"Check for and return an event.\n\n    Args:\n        mask (int): :any:`Event types` to wait for.\n        k (Optional[Key]): A tcod.Key instance which might be updated with\n                           an event.  Can be None.\n        m (Optional[Mouse]): A tcod.Mouse instance which might be updated\n                             with an event.  Can be None.\n\n    .. deprecated:: 9.3\n        Use the :any:`tcod.event.get` function to check for events.\n    \"\"\"\n    return int(\n        lib.TCOD_sys_check_for_event(\n            mask, k.key_p if k else ffi.NULL, m.mouse_p if m else ffi.NULL\n        )\n    )","return_type":"int","function_name":"sys_check_for_event","stripped_code":"def sys_check_for_event(\n    mask: int, k: Optional[Key], m: Optional[Mouse]\n):\n    \"\"\"Check for and return an event.\n\n    Args:\n        mask (int): :any:`Event types` to wait for.\n        k (Optional[Key]): A tcod.Key instance which might be updated with\n                           an event.  Can be None.\n        m (Optional[Mouse]): A tcod.Mouse instance which might be updated\n                             with an event.  Can be None.\n\n    .. deprecated:: 9.3\n        Use the :any:`tcod.event.get` function to check for events.\n    \"\"\"\n    return int(\n        lib.TCOD_sys_check_for_event(\n            mask, k.key_p if k else ffi.NULL, m.mouse_p if m else ffi.NULL\n        )\n    )"}
{"code":"def _qvm_run(self, quil_program, classical_addresses, trials,\n                 measurement_noise, gate_noise, random_seed) -> np.ndarray:\n        \"\"\"\n        Run a Forest ``run`` job on a QVM.\n\n        Users should use :py:func:`QVM.run` instead of calling this directly.\n        \"\"\"\n        payload = qvm_run_payload(quil_program, classical_addresses, trials,\n                                  measurement_noise, gate_noise, random_seed)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n\n        ram = response.json()\n\n        for k in ram.keys():\n            ram[k] = np.array(ram[k])\n\n        return ram","return_type":"np.ndarray","function_name":"ForestConnection._qvm_run","stripped_code":"def _qvm_run(self, quil_program, classical_addresses, trials,\n                 measurement_noise, gate_noise, random_seed):\n        \"\"\"\n        Run a Forest ``run`` job on a QVM.\n\n        Users should use :py:func:`QVM.run` instead of calling this directly.\n        \"\"\"\n        payload = qvm_run_payload(quil_program, classical_addresses, trials,\n                                  measurement_noise, gate_noise, random_seed)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n\n        ram = response.json()\n\n        for k in ram.keys():\n            ram[k] = np.array(ram[k])\n\n        return ram"}
{"code":"def get_cmd_output(*args, encoding: str = SYS_ENCODING) -> str:\n    \"\"\"\n    Returns text output of a command.\n    \"\"\"\n    log.debug(\"get_cmd_output(): args = {!r}\", args)\n    p = subprocess.Popen(args, stdout=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    return stdout.decode(encoding, errors='ignore')","return_type":"str","function_name":"get_cmd_output","stripped_code":"def get_cmd_output(*args, encoding: str = SYS_ENCODING):\n    \"\"\"\n    Returns text output of a command.\n    \"\"\"\n    log.debug(\"get_cmd_output(): args = {!r}\", args)\n    p = subprocess.Popen(args, stdout=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    return stdout.decode(encoding, errors='ignore')"}
{"code":"def bitcoin_address(self) -> str:\n        \"\"\"Generate a random bitcoin address.\n\n        :return: Bitcoin address.\n\n        :Example:\n            3EktnHQD7RiAE6uzMj2ZifT9YgRrkSgzQX\n        \"\"\"\n        type_ = self.random.choice(['1', '3'])\n        letters = string.ascii_letters + string.digits\n        return type_ + ''.join(\n            self.random.choice(letters) for _ in range(33))","return_type":"str","function_name":"Payment.bitcoin_address","stripped_code":"def bitcoin_address(self):\n        \"\"\"Generate a random bitcoin address.\n\n        :return: Bitcoin address.\n\n        :Example:\n            3EktnHQD7RiAE6uzMj2ZifT9YgRrkSgzQX\n        \"\"\"\n        type_ = self.random.choice(['1', '3'])\n        letters = string.ascii_letters + string.digits\n        return type_ + ''.join(\n            self.random.choice(letters) for _ in range(33))"}
{"code":"def snake_to_camel(s: str) -> str:\n    \"\"\"Convert string from snake case to camel case.\"\"\"\n\n    fragments = s.split('_')\n\n    return fragments[0] + ''.join(x.title() for x in fragments[1:])","return_type":"str","function_name":"snake_to_camel","stripped_code":"def snake_to_camel(s: str):\n    \"\"\"Convert string from snake case to camel case.\"\"\"\n\n    fragments = s.split('_')\n\n    return fragments[0] + ''.join(x.title() for x in fragments[1:])"}
{"code":"def hash_(token: str, hash_size: int) -> int:\n    \"\"\"Convert a token to a hash of given size.\n    Args:\n        token: a word\n        hash_size: hash size\n\n    Returns:\n        int, hashed token\n\n    \"\"\"\n    return murmurhash3_32(token, positive=True) % hash_size","return_type":"int","function_name":"hash_","stripped_code":"def hash_(token: str, hash_size: int):\n    \"\"\"Convert a token to a hash of given size.\n    Args:\n        token: a word\n        hash_size: hash size\n\n    Returns:\n        int, hashed token\n\n    \"\"\"\n    return murmurhash3_32(token, positive=True) % hash_size"}
{"code":"def persist(self, container: Container, image: str) -> None:\n        \"\"\"\n        Persists the state of a given container to a BugZoo image on this\n        server.\n\n        Parameters:\n            container: the container to persist.\n            image: the name of the Docker image that should be created.\n\n        Raises:\n            ImageAlreadyExists: if the image name is already in use by another\n                Docker image on this server.\n        \"\"\"\n        logger_c = logger.getChild(container.uid)\n        logger_c.debug(\"Persisting container as a Docker image: %s\", image)\n        try:\n            docker_container = self.__dockerc[container.uid]\n        except KeyError:\n            logger_c.exception(\"Failed to persist container: container no longer exists.\")  # noqa: pycodestyle\n            raise\n        try:\n            _ = self.__client_docker.images.get(image)\n            logger_c.error(\"Failed to persist container: image, '%s', already exists.\",  # noqa: pycodestyle\n                         image)\n            raise ImageAlreadyExists(image)\n        except docker.errors.ImageNotFound:\n            pass\n\n        cmd = \"docker commit {} {}\"\n        cmd = cmd.format(docker_container.id, image)\n        try:\n            subprocess.check_output(cmd, shell=True)\n        except subprocess.CalledProcessError:\n            logger.exception(\"Failed to persist container (%s) to image (%s).\",  # noqa: pycodestyle\n                             container.uid, image)\n            raise\n        logger_c.debug(\"Persisted container as a Docker image: %s\", image)","return_type":"None","function_name":"ContainerManager.persist","stripped_code":"def persist(self, container: Container, image: str):\n        \"\"\"\n        Persists the state of a given container to a BugZoo image on this\n        server.\n\n        Parameters:\n            container: the container to persist.\n            image: the name of the Docker image that should be created.\n\n        Raises:\n            ImageAlreadyExists: if the image name is already in use by another\n                Docker image on this server.\n        \"\"\"\n        logger_c = logger.getChild(container.uid)\n        logger_c.debug(\"Persisting container as a Docker image: %s\", image)\n        try:\n            docker_container = self.__dockerc[container.uid]\n        except KeyError:\n            logger_c.exception(\"Failed to persist container: container no longer exists.\")  # noqa: pycodestyle\n            raise\n        try:\n            _ = self.__client_docker.images.get(image)\n            logger_c.error(\"Failed to persist container: image, '%s', already exists.\",  # noqa: pycodestyle\n                         image)\n            raise ImageAlreadyExists(image)\n        except docker.errors.ImageNotFound:\n            pass\n\n        cmd = \"docker commit {} {}\"\n        cmd = cmd.format(docker_container.id, image)\n        try:\n            subprocess.check_output(cmd, shell=True)\n        except subprocess.CalledProcessError:\n            logger.exception(\"Failed to persist container (%s) to image (%s).\",  # noqa: pycodestyle\n                             container.uid, image)\n            raise\n        logger_c.debug(\"Persisted container as a Docker image: %s\", image)"}
{"code":"def reply_media_group(\n        self,\n        media: List[Union[\"pyrogram.InputMediaPhoto\", \"pyrogram.InputMediaVideo\"]],\n        quote: bool = None,\n        disable_notification: bool = None,\n        reply_to_message_id: int = None\n    ) -> \"Message\":\n        \"\"\"Bound method *reply_media_group* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_media_group(\n                chat_id=message.chat.id,\n                media=list_of_media\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_media_group(list_of_media)\n\n        Args:\n            media (``list``):\n                A list containing either :obj:`InputMediaPhoto <pyrogram.InputMediaPhoto>` or\n                :obj:`InputMediaVideo <pyrogram.InputMediaVideo>` objects\n                describing photos and videos to be sent, must include 2\u201310 items.\n\n            quote (``bool``, *optional*):\n                If ``True``, the message will be sent as a reply to this message.\n                If *reply_to_message_id* is passed, this parameter will be ignored.\n                Defaults to ``True`` in group chats and ``False`` in private chats.\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message.\n\n        Returns:\n            On success, a :obj:`Messages <pyrogram.Messages>` object is returned containing all the\n            single messages sent.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        if quote is None:\n            quote = self.chat.type != \"private\"\n\n        if reply_to_message_id is None and quote:\n            reply_to_message_id = self.message_id\n\n        return self._client.send_media_group(\n            chat_id=self.chat.id,\n            media=media,\n            disable_notification=disable_notification,\n            reply_to_message_id=reply_to_message_id\n        )","return_type":"\"Message\"","function_name":"Message.reply_media_group","stripped_code":"def reply_media_group(\n        self,\n        media: List[Union[\"pyrogram.InputMediaPhoto\", \"pyrogram.InputMediaVideo\"]],\n        quote: bool = None,\n        disable_notification: bool = None,\n        reply_to_message_id: int = None\n    ):\n        \"\"\"Bound method *reply_media_group* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_media_group(\n                chat_id=message.chat.id,\n                media=list_of_media\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_media_group(list_of_media)\n\n        Args:\n            media (``list``):\n                A list containing either :obj:`InputMediaPhoto <pyrogram.InputMediaPhoto>` or\n                :obj:`InputMediaVideo <pyrogram.InputMediaVideo>` objects\n                describing photos and videos to be sent, must include 2\u201310 items.\n\n            quote (``bool``, *optional*):\n                If ``True``, the message will be sent as a reply to this message.\n                If *reply_to_message_id* is passed, this parameter will be ignored.\n                Defaults to ``True`` in group chats and ``False`` in private chats.\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message.\n\n        Returns:\n            On success, a :obj:`Messages <pyrogram.Messages>` object is returned containing all the\n            single messages sent.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        if quote is None:\n            quote = self.chat.type != \"private\"\n\n        if reply_to_message_id is None and quote:\n            reply_to_message_id = self.message_id\n\n        return self._client.send_media_group(\n            chat_id=self.chat.id,\n            media=media,\n            disable_notification=disable_notification,\n            reply_to_message_id=reply_to_message_id\n        )"}
{"code":"def in_pathing_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if a unit can pass through a grid point. \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.pathing_grid[pos] == 0","return_type":"bool","function_name":"BotAI.in_pathing_grid","stripped_code":"def in_pathing_grid(self, pos: Union[Point2, Point3, Unit]):\n        \"\"\" Returns True if a unit can pass through a grid point. \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.pathing_grid[pos] == 0"}
{"code":"def street_number(self, maximum: int = 1400) -> str:\n        \"\"\"Generate a random street number.\n\n        :param maximum: Maximum value.\n        :return: Street number.\n        \"\"\"\n        return str(self.random.randint(1, maximum))","return_type":"str","function_name":"Address.street_number","stripped_code":"def street_number(self, maximum: int = 1400):\n        \"\"\"Generate a random street number.\n\n        :param maximum: Maximum value.\n        :return: Street number.\n        \"\"\"\n        return str(self.random.randint(1, maximum))"}
{"code":"def _handle_api_result(result: Optional[Dict[str, Any]]) -> Any:\n    \"\"\"\n    Retrieve 'data' field from the API result object.\n\n    :param result: API result that received from HTTP API\n    :return: the 'data' field in result object\n    :raise ActionFailed: the 'status' field is 'failed'\n    \"\"\"\n    if isinstance(result, dict):\n        if result.get('status') == 'failed':\n            raise ActionFailed(retcode=result.get('retcode'))\n        return result.get('data')","return_type":"Any","function_name":"_handle_api_result","stripped_code":"def _handle_api_result(result: Optional[Dict[str, Any]]):\n    \"\"\"\n    Retrieve 'data' field from the API result object.\n\n    :param result: API result that received from HTTP API\n    :return: the 'data' field in result object\n    :raise ActionFailed: the 'status' field is 'failed'\n    \"\"\"\n    if isinstance(result, dict):\n        if result.get('status') == 'failed':\n            raise ActionFailed(retcode=result.get('retcode'))\n        return result.get('data')"}
{"code":"def _deploy_and_remember(\n            self,\n            contract_name: str,\n            arguments: List,\n            deployed_contracts: 'DeployedContracts',\n    ) -> Contract:\n        \"\"\" Deploys contract_name with arguments and store the result in deployed_contracts. \"\"\"\n        receipt = self.deploy(contract_name, arguments)\n        deployed_contracts['contracts'][contract_name] = _deployed_data_from_receipt(\n            receipt=receipt,\n            constructor_arguments=arguments,\n        )\n        return self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=deployed_contracts['contracts'][contract_name]['address'],\n        )","return_type":"Contract","function_name":"ContractDeployer._deploy_and_remember","stripped_code":"def _deploy_and_remember(\n            self,\n            contract_name: str,\n            arguments: List,\n            deployed_contracts: 'DeployedContracts',\n    ):\n        \"\"\" Deploys contract_name with arguments and store the result in deployed_contracts. \"\"\"\n        receipt = self.deploy(contract_name, arguments)\n        deployed_contracts['contracts'][contract_name] = _deployed_data_from_receipt(\n            receipt=receipt,\n            constructor_arguments=arguments,\n        )\n        return self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=deployed_contracts['contracts'][contract_name]['address'],\n        )"}
{"code":"def evaluate(model_path: str, stream_name: str, config_path: Optional[str], cl_arguments: Iterable[str],\n             output_root: str) -> None:\n    \"\"\"\n    Evaluate the given model on the specified data stream.\n\n    Configuration is updated by the respective predict.stream_name section, in particular:\n        - hooks section is entirely replaced\n        - model and dataset sections are updated\n\n    :param model_path: path to the model to be evaluated\n    :param stream_name: data stream name to be evaluated\n    :param config_path: path to the config to be used, if not specified infer the path from ``model_path``\n    :param cl_arguments: additional command line arguments which will update the configuration\n    :param output_root: output root in which the training directory will be created\n    \"\"\"\n    config = None\n\n    try:\n        model_dir = path.dirname(model_path) if not path.isdir(model_path) else model_path\n        config_path = find_config(model_dir if config_path is None else config_path)\n        config = load_config(config_file=config_path, additional_args=cl_arguments)\n\n        if stream_name == CXF_PREDICT_STREAM and stream_name in config:  # old style ``cxflow predict ...``\n            logging.warning('Old style ``predict`` configuration section is deprecated and will not be supported, '\n                            'use ``eval.predict`` section instead.')\n            config['eval'] = {'predict': config['predict']}\n\n        if 'eval' in config and stream_name in config['eval']:\n            update_section = config['eval'][stream_name]\n            for subsection in ['dataset', 'model', 'main_loop']:\n                if subsection in update_section:\n                    config[subsection].update(update_section[subsection])\n            if 'hooks' in update_section:\n                config['hooks'] = update_section['hooks']\n            else:\n                logging.warning('Config does not contain `eval.%s.hooks` section. '\n                                'No hook will be employed during the evaluation.', stream_name)\n                config['hooks'] = []\n\n        validate_config(config)\n\n        logging.debug('\\tLoaded config: %s', config)\n    except Exception as ex:  # pylint: disable=broad-except\n        fallback('Loading config failed', ex)\n\n    run(config=config, output_root=output_root, restore_from=model_path, eval=stream_name)","return_type":"None","function_name":"evaluate","stripped_code":"def evaluate(model_path: str, stream_name: str, config_path: Optional[str], cl_arguments: Iterable[str],\n             output_root: str):\n    \"\"\"\n    Evaluate the given model on the specified data stream.\n\n    Configuration is updated by the respective predict.stream_name section, in particular:\n        - hooks section is entirely replaced\n        - model and dataset sections are updated\n\n    :param model_path: path to the model to be evaluated\n    :param stream_name: data stream name to be evaluated\n    :param config_path: path to the config to be used, if not specified infer the path from ``model_path``\n    :param cl_arguments: additional command line arguments which will update the configuration\n    :param output_root: output root in which the training directory will be created\n    \"\"\"\n    config = None\n\n    try:\n        model_dir = path.dirname(model_path) if not path.isdir(model_path) else model_path\n        config_path = find_config(model_dir if config_path is None else config_path)\n        config = load_config(config_file=config_path, additional_args=cl_arguments)\n\n        if stream_name == CXF_PREDICT_STREAM and stream_name in config:  # old style ``cxflow predict ...``\n            logging.warning('Old style ``predict`` configuration section is deprecated and will not be supported, '\n                            'use ``eval.predict`` section instead.')\n            config['eval'] = {'predict': config['predict']}\n\n        if 'eval' in config and stream_name in config['eval']:\n            update_section = config['eval'][stream_name]\n            for subsection in ['dataset', 'model', 'main_loop']:\n                if subsection in update_section:\n                    config[subsection].update(update_section[subsection])\n            if 'hooks' in update_section:\n                config['hooks'] = update_section['hooks']\n            else:\n                logging.warning('Config does not contain `eval.%s.hooks` section. '\n                                'No hook will be employed during the evaluation.', stream_name)\n                config['hooks'] = []\n\n        validate_config(config)\n\n        logging.debug('\\tLoaded config: %s', config)\n    except Exception as ex:  # pylint: disable=broad-except\n        fallback('Loading config failed', ex)\n\n    run(config=config, output_root=output_root, restore_from=model_path, eval=stream_name)"}
{"code":"def clean_expired_user_attempts(attempt_time: datetime = None) -> int:\n    \"\"\"\n    Clean expired user attempts from the database.\n    \"\"\"\n\n    if settings.AXES_COOLOFF_TIME is None:\n        log.debug('AXES: Skipping clean for expired access attempts because no AXES_COOLOFF_TIME is configured')\n        return 0\n\n    threshold = get_cool_off_threshold(attempt_time)\n    count, _ = AccessAttempt.objects.filter(attempt_time__lt=threshold).delete()\n    log.info('AXES: Cleaned up %s expired access attempts from database that were older than %s', count, threshold)\n    return count","return_type":"int","function_name":"clean_expired_user_attempts","stripped_code":"def clean_expired_user_attempts(attempt_time: datetime = None):\n    \"\"\"\n    Clean expired user attempts from the database.\n    \"\"\"\n\n    if settings.AXES_COOLOFF_TIME is None:\n        log.debug('AXES: Skipping clean for expired access attempts because no AXES_COOLOFF_TIME is configured')\n        return 0\n\n    threshold = get_cool_off_threshold(attempt_time)\n    count, _ = AccessAttempt.objects.filter(attempt_time__lt=threshold).delete()\n    log.info('AXES: Cleaned up %s expired access attempts from database that were older than %s', count, threshold)\n    return count"}
{"code":"def is_special_unitary(\n        matrix: np.ndarray,\n        *,\n        rtol: float = 1e-5,\n        atol: float = 1e-8) -> bool:\n    \"\"\"Determines if a matrix is approximately unitary with unit determinant.\n\n    A matrix is special-unitary if it is square and its adjoint is its inverse\n    and its determinant is one.\n\n    Args:\n        matrix: The matrix to check.\n        rtol: The per-matrix-entry relative tolerance on equality.\n        atol: The per-matrix-entry absolute tolerance on equality.\n    Returns:\n        Whether the matrix is unitary with unit determinant within the given\n        tolerance.\n    \"\"\"\n    return (is_unitary(matrix, rtol=rtol, atol=atol) and\n            (matrix.shape[0] == 0 or\n             np.allclose(np.linalg.det(matrix), 1, rtol=rtol, atol=atol)))","return_type":"bool","function_name":"is_special_unitary","stripped_code":"def is_special_unitary(\n        matrix: np.ndarray,\n        *,\n        rtol: float = 1e-5,\n        atol: float = 1e-8):\n    \"\"\"Determines if a matrix is approximately unitary with unit determinant.\n\n    A matrix is special-unitary if it is square and its adjoint is its inverse\n    and its determinant is one.\n\n    Args:\n        matrix: The matrix to check.\n        rtol: The per-matrix-entry relative tolerance on equality.\n        atol: The per-matrix-entry absolute tolerance on equality.\n    Returns:\n        Whether the matrix is unitary with unit determinant within the given\n        tolerance.\n    \"\"\"\n    return (is_unitary(matrix, rtol=rtol, atol=atol) and\n            (matrix.shape[0] == 0 or\n             np.allclose(np.linalg.det(matrix), 1, rtol=rtol, atol=atol)))"}
{"code":"def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )","return_type":"GeneratedPyAST","function_name":"__deftype_method_recur_to_py_ast","stripped_code":"def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n):\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"code":"def _representable(value: Any) -> bool:\n    \"\"\"\n    Check whether we want to represent the value in the error message on contract breach.\n\n    We do not want to represent classes, methods, modules and functions.\n\n    :param value: value related to an AST node\n    :return: True if we want to represent it in the violation error\n    \"\"\"\n    return not inspect.isclass(value) and not inspect.isfunction(value) and not inspect.ismethod(value) and not \\\n        inspect.ismodule(value) and not inspect.isbuiltin(value)","return_type":"bool","function_name":"_representable","stripped_code":"def _representable(value: Any):\n    \"\"\"\n    Check whether we want to represent the value in the error message on contract breach.\n\n    We do not want to represent classes, methods, modules and functions.\n\n    :param value: value related to an AST node\n    :return: True if we want to represent it in the violation error\n    \"\"\"\n    return not inspect.isclass(value) and not inspect.isfunction(value) and not inspect.ismethod(value) and not \\\n        inspect.ismodule(value) and not inspect.isbuiltin(value)"}
{"code":"def bitwise_xor(bs0: str, bs1: str) -> str:\n    \"\"\"\n    A helper to calculate the bitwise XOR of two bit string\n\n    :param bs0: String of 0's and 1's representing a number in binary representations\n    :param bs1: String of 0's and 1's representing a number in binary representations\n    :return: String of 0's and 1's representing the XOR between bs0 and bs1\n    \"\"\"\n    if len(bs0) != len(bs1):\n        raise ValueError(\"Bit strings are not of equal length\")\n    n_bits = len(bs0)\n    return PADDED_BINARY_BIT_STRING.format(xor(int(bs0, 2), int(bs1, 2)), n_bits)","return_type":"str","function_name":"bitwise_xor","stripped_code":"def bitwise_xor(bs0: str, bs1: str):\n    \"\"\"\n    A helper to calculate the bitwise XOR of two bit string\n\n    :param bs0: String of 0's and 1's representing a number in binary representations\n    :param bs1: String of 0's and 1's representing a number in binary representations\n    :return: String of 0's and 1's representing the XOR between bs0 and bs1\n    \"\"\"\n    if len(bs0) != len(bs1):\n        raise ValueError(\"Bit strings are not of equal length\")\n    n_bits = len(bs0)\n    return PADDED_BINARY_BIT_STRING.format(xor(int(bs0, 2), int(bs1, 2)), n_bits)"}
{"code":"def major_extent(self) -> complex:\n        \"\"\"Maximum deviation from null.\"\"\"\n        return max((self.max() - self.null, self.null - self.min()))","return_type":"complex","function_name":"Channel.major_extent","stripped_code":"def major_extent(self):\n        \"\"\"Maximum deviation from null.\"\"\"\n        return max((self.max() - self.null, self.null - self.min()))"}
{"code":"def tree_to_gexf(tree:'BubbleTree') -> str:\n    \"\"\"Compute the gexf representation of given power graph,\n    and push it into given file.\n\n    See https://gephi.org/gexf/format/index.html\n    for format doc.\n\n    \"\"\"\n    output_nodes, output_edges = '', ''\n\n    def build_node(node:str) -> str:\n        \"\"\"Yield strings describing given node, recursively\"\"\"\n        if tree.inclusions[node]:  # it's a powernode\n            yield '<node id=\"{}\" label=\"{}\">'.format(node, node)\n            yield '<nodes>'\n            for sub in tree.inclusions[node]:\n                yield from build_node(sub)\n            yield '</nodes>'\n            yield '</node>'\n        else:  # it's a regular node\n            yield '<node id=\"{}\" label=\"{}\"/>'.format(node, node)\n        return\n\n    # build full hierarchy from the roots\n    output_nodes += '\\n'.join('\\n'.join(build_node(root)) for root in tree.roots)\n\n    # # add the edges to the final graph\n    for idx, (source, targets) in enumerate(tree.edges.items()):\n        for target in targets:\n            if source <= target:  # edges dict is complete. This avoid multiple edges.\n                output_edges += '<edge id=\"{}\" source=\"{}\" target=\"{}\" />\\n'.format(idx, source, target)\n\n    return GEXF_TEMPLATE.format(\n        'directed' if tree.oriented else 'undirected',\n        output_nodes,\n        output_edges\n    )","return_type":"str","function_name":"tree_to_gexf","stripped_code":"def tree_to_gexf(tree:'BubbleTree'):\n    \"\"\"Compute the gexf representation of given power graph,\n    and push it into given file.\n\n    See https://gephi.org/gexf/format/index.html\n    for format doc.\n\n    \"\"\"\n    output_nodes, output_edges = '', ''\n\n    def build_node(node:str) -> str:\n        \"\"\"Yield strings describing given node, recursively\"\"\"\n        if tree.inclusions[node]:  # it's a powernode\n            yield '<node id=\"{}\" label=\"{}\">'.format(node, node)\n            yield '<nodes>'\n            for sub in tree.inclusions[node]:\n                yield from build_node(sub)\n            yield '</nodes>'\n            yield '</node>'\n        else:  # it's a regular node\n            yield '<node id=\"{}\" label=\"{}\"/>'.format(node, node)\n        return\n\n    # build full hierarchy from the roots\n    output_nodes += '\\n'.join('\\n'.join(build_node(root)) for root in tree.roots)\n\n    # # add the edges to the final graph\n    for idx, (source, targets) in enumerate(tree.edges.items()):\n        for target in targets:\n            if source <= target:  # edges dict is complete. This avoid multiple edges.\n                output_edges += '<edge id=\"{}\" source=\"{}\" target=\"{}\" />\\n'.format(idx, source, target)\n\n    return GEXF_TEMPLATE.format(\n        'directed' if tree.oriented else 'undirected',\n        output_nodes,\n        output_edges\n    )"}
{"code":"def parse_item(location: str, item_type: Type[T], item_name_for_log: str = None,\n               file_mapping_conf: FileMappingConfiguration = None,\n               logger: Logger = default_logger, lazy_mfcollection_parsing: bool = False) -> T:\n    \"\"\"\n    Creates a RootParser() and calls its parse_item() method\n\n    :param location:\n    :param item_type:\n    :param item_name_for_log:\n    :param file_mapping_conf:\n    :param logger:\n    :param lazy_mfcollection_parsing:\n    :return:\n    \"\"\"\n    rp = _create_parser_from_default(logger)\n    opts = create_parser_options(lazy_mfcollection_parsing=lazy_mfcollection_parsing)\n    return rp.parse_item(location, item_type, item_name_for_log=item_name_for_log, file_mapping_conf=file_mapping_conf,\n                         options=opts)","return_type":"T","function_name":"parse_item","stripped_code":"def parse_item(location: str, item_type: Type[T], item_name_for_log: str = None,\n               file_mapping_conf: FileMappingConfiguration = None,\n               logger: Logger = default_logger, lazy_mfcollection_parsing: bool = False):\n    \"\"\"\n    Creates a RootParser() and calls its parse_item() method\n\n    :param location:\n    :param item_type:\n    :param item_name_for_log:\n    :param file_mapping_conf:\n    :param logger:\n    :param lazy_mfcollection_parsing:\n    :return:\n    \"\"\"\n    rp = _create_parser_from_default(logger)\n    opts = create_parser_options(lazy_mfcollection_parsing=lazy_mfcollection_parsing)\n    return rp.parse_item(location, item_type, item_name_for_log=item_name_for_log, file_mapping_conf=file_mapping_conf,\n                         options=opts)"}
{"code":"def _decorate_namespace_function(bases: List[type], namespace: MutableMapping[str, Any], key: str) -> None:\n    \"\"\"Collect preconditions and postconditions from the bases and decorate the function at the ``key``.\"\"\"\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-locals\n\n    value = namespace[key]\n    assert inspect.isfunction(value) or isinstance(value, (staticmethod, classmethod))\n\n    # Determine the function to be decorated\n    if inspect.isfunction(value):\n        func = value\n    elif isinstance(value, (staticmethod, classmethod)):\n        func = value.__func__\n    else:\n        raise NotImplementedError(\"Unexpected value for a function: {}\".format(value))\n\n    # Collect preconditions and postconditions of the function\n    preconditions = []  # type: List[List[Contract]]\n    snapshots = []  # type: List[Snapshot]\n    postconditions = []  # type: List[Contract]\n\n    contract_checker = icontract._checkers.find_checker(func=func)\n    if contract_checker is not None:\n        preconditions = contract_checker.__preconditions__  # type: ignore\n        snapshots = contract_checker.__postcondition_snapshots__  # type: ignore\n        postconditions = contract_checker.__postconditions__  # type: ignore\n\n    # Collect the preconditions and postconditions from bases.\n    #\n    # Preconditions and postconditions of __init__ of base classes are deliberately ignored (and not collapsed) since\n    # initialization is an operation specific to the concrete class and does not relate to the class hierarchy.\n    if key not in ['__init__']:\n        base_preconditions = []  # type: List[List[Contract]]\n        base_snapshots = []  # type: List[Snapshot]\n        base_postconditions = []  # type: List[Contract]\n\n        bases_have_func = False\n        for base in bases:\n            if hasattr(base, key):\n                bases_have_func = True\n\n                # Check if there is a checker function in the base class\n                base_func = getattr(base, key)\n                base_contract_checker = icontract._checkers.find_checker(func=base_func)\n\n                # Ignore functions which don't have preconditions or postconditions\n                if base_contract_checker is not None:\n                    base_preconditions.extend(base_contract_checker.__preconditions__)  # type: ignore\n                    base_snapshots.extend(base_contract_checker.__postcondition_snapshots__)  # type: ignore\n                    base_postconditions.extend(base_contract_checker.__postconditions__)  # type: ignore\n\n        # Collapse preconditions and postconditions from the bases with the the function's own ones\n        preconditions = _collapse_preconditions(\n            base_preconditions=base_preconditions,\n            bases_have_func=bases_have_func,\n            preconditions=preconditions,\n            func=func)\n\n        snapshots = _collapse_snapshots(base_snapshots=base_snapshots, snapshots=snapshots)\n\n        postconditions = _collapse_postconditions(\n            base_postconditions=base_postconditions, postconditions=postconditions)\n\n    if preconditions or postconditions:\n        if contract_checker is None:\n            contract_checker = icontract._checkers.decorate_with_checker(func=func)\n\n            # Replace the function with the function decorated with contract checks\n            if inspect.isfunction(value):\n                namespace[key] = contract_checker\n            elif isinstance(value, staticmethod):\n                namespace[key] = staticmethod(contract_checker)\n\n            elif isinstance(value, classmethod):\n                namespace[key] = classmethod(contract_checker)\n\n            else:\n                raise NotImplementedError(\"Unexpected value for a function: {}\".format(value))\n\n        # Override the preconditions and postconditions\n        contract_checker.__preconditions__ = preconditions  # type: ignore\n        contract_checker.__postcondition_snapshots__ = snapshots  # type: ignore\n        contract_checker.__postconditions__ = postconditions","return_type":"None","function_name":"_decorate_namespace_function","stripped_code":"def _decorate_namespace_function(bases: List[type], namespace: MutableMapping[str, Any], key: str):\n    \"\"\"Collect preconditions and postconditions from the bases and decorate the function at the ``key``.\"\"\"\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-locals\n\n    value = namespace[key]\n    assert inspect.isfunction(value) or isinstance(value, (staticmethod, classmethod))\n\n    # Determine the function to be decorated\n    if inspect.isfunction(value):\n        func = value\n    elif isinstance(value, (staticmethod, classmethod)):\n        func = value.__func__\n    else:\n        raise NotImplementedError(\"Unexpected value for a function: {}\".format(value))\n\n    # Collect preconditions and postconditions of the function\n    preconditions = []  # type: List[List[Contract]]\n    snapshots = []  # type: List[Snapshot]\n    postconditions = []  # type: List[Contract]\n\n    contract_checker = icontract._checkers.find_checker(func=func)\n    if contract_checker is not None:\n        preconditions = contract_checker.__preconditions__  # type: ignore\n        snapshots = contract_checker.__postcondition_snapshots__  # type: ignore\n        postconditions = contract_checker.__postconditions__  # type: ignore\n\n    # Collect the preconditions and postconditions from bases.\n    #\n    # Preconditions and postconditions of __init__ of base classes are deliberately ignored (and not collapsed) since\n    # initialization is an operation specific to the concrete class and does not relate to the class hierarchy.\n    if key not in ['__init__']:\n        base_preconditions = []  # type: List[List[Contract]]\n        base_snapshots = []  # type: List[Snapshot]\n        base_postconditions = []  # type: List[Contract]\n\n        bases_have_func = False\n        for base in bases:\n            if hasattr(base, key):\n                bases_have_func = True\n\n                # Check if there is a checker function in the base class\n                base_func = getattr(base, key)\n                base_contract_checker = icontract._checkers.find_checker(func=base_func)\n\n                # Ignore functions which don't have preconditions or postconditions\n                if base_contract_checker is not None:\n                    base_preconditions.extend(base_contract_checker.__preconditions__)  # type: ignore\n                    base_snapshots.extend(base_contract_checker.__postcondition_snapshots__)  # type: ignore\n                    base_postconditions.extend(base_contract_checker.__postconditions__)  # type: ignore\n\n        # Collapse preconditions and postconditions from the bases with the the function's own ones\n        preconditions = _collapse_preconditions(\n            base_preconditions=base_preconditions,\n            bases_have_func=bases_have_func,\n            preconditions=preconditions,\n            func=func)\n\n        snapshots = _collapse_snapshots(base_snapshots=base_snapshots, snapshots=snapshots)\n\n        postconditions = _collapse_postconditions(\n            base_postconditions=base_postconditions, postconditions=postconditions)\n\n    if preconditions or postconditions:\n        if contract_checker is None:\n            contract_checker = icontract._checkers.decorate_with_checker(func=func)\n\n            # Replace the function with the function decorated with contract checks\n            if inspect.isfunction(value):\n                namespace[key] = contract_checker\n            elif isinstance(value, staticmethod):\n                namespace[key] = staticmethod(contract_checker)\n\n            elif isinstance(value, classmethod):\n                namespace[key] = classmethod(contract_checker)\n\n            else:\n                raise NotImplementedError(\"Unexpected value for a function: {}\".format(value))\n\n        # Override the preconditions and postconditions\n        contract_checker.__preconditions__ = preconditions  # type: ignore\n        contract_checker.__postcondition_snapshots__ = snapshots  # type: ignore\n        contract_checker.__postconditions__ = postconditions"}
{"code":"def is_unknown(val: str) -> bool:\n    \"\"\"\n    Returns True if val contains only '/' characters\n    \"\"\"\n    for char in ('/', 'X'):\n        if val == char * len(val):\n            return True\n    return False","return_type":"bool","function_name":"is_unknown","stripped_code":"def is_unknown(val: str):\n    \"\"\"\n    Returns True if val contains only '/' characters\n    \"\"\"\n    for char in ('/', 'X'):\n        if val == char * len(val):\n            return True\n    return False"}
{"code":"def build_node_key_search(query, key) -> NodePredicate:\n    \"\"\"Build a node filter for nodes whose values for the given key are superstrings of the query string(s).\n\n    :param query: The query string or strings to check if they're in the node name\n    :type query: str or iter[str]\n    :param str key: The key for the node data dictionary. Should refer only to entries that have str values\n    \"\"\"\n    if isinstance(query, str):\n        return build_node_data_search(key, lambda s: query.lower() in s.lower())\n\n    if isinstance(query, Iterable):\n        return build_node_data_search(key, lambda s: any(q.lower() in s.lower() for q in query))\n\n    raise TypeError('query is wrong type: %s', query)","return_type":"NodePredicate","function_name":"build_node_key_search","stripped_code":"def build_node_key_search(query, key):\n    \"\"\"Build a node filter for nodes whose values for the given key are superstrings of the query string(s).\n\n    :param query: The query string or strings to check if they're in the node name\n    :type query: str or iter[str]\n    :param str key: The key for the node data dictionary. Should refer only to entries that have str values\n    \"\"\"\n    if isinstance(query, str):\n        return build_node_data_search(key, lambda s: query.lower() in s.lower())\n\n    if isinstance(query, Iterable):\n        return build_node_data_search(key, lambda s: any(q.lower() in s.lower() for q in query))\n\n    raise TypeError('query is wrong type: %s', query)"}
{"code":"def _collect_io(self) -> None:\n        \"\"\"\n        Collect inputs/outputs from all child statements\n        to :py:attr:`~_input` / :py:attr:`_output` attribure on this object\n        \"\"\"\n        in_add = self._inputs.extend\n        out_add = self._outputs.extend\n\n        for stm in self._iter_stms():\n            in_add(stm._inputs)\n            out_add(stm._outputs)","return_type":"None","function_name":"HdlStatement._collect_io","stripped_code":"def _collect_io(self):\n        \"\"\"\n        Collect inputs/outputs from all child statements\n        to :py:attr:`~_input` / :py:attr:`_output` attribure on this object\n        \"\"\"\n        in_add = self._inputs.extend\n        out_add = self._outputs.extend\n\n        for stm in self._iter_stms():\n            in_add(stm._inputs)\n            out_add(stm._outputs)"}
{"code":"def get_edge_by_name(self, source_name: str, target_name: str) -> Optional[Edge]:\r\n        \"\"\"\r\n        Returns the edge connecting the nodes with the specified names if such an edge exists.\r\n\r\n        Arguments:\r\n            source_name (str): The name of one of the endpoints of queried edge.\r\n            target_name (str): The name of the other endpoint of the queried edge.\r\n\r\n        Returns:\r\n            The edge connecting the nodes with the specified names\r\n            or `None` if no such node exists.\r\n        \"\"\"\r\n        nodes: NodeList = self._graph.nodes\r\n        source: Optional[Node] = nodes.get_node_by_name(source_name)\r\n        if source is None:\r\n            return None\r\n        target: Optional[Node] = nodes.get_node_by_name(target_name)\r\n        if target is None:\r\n            return None\r\n        return self.get_edge_by_index(source.index, target.index)","return_type":"Optional[Edge]","function_name":"EdgeList.get_edge_by_name","stripped_code":"def get_edge_by_name(self, source_name: str, target_name: str):\r\n        \"\"\"\r\n        Returns the edge connecting the nodes with the specified names if such an edge exists.\r\n\r\n        Arguments:\r\n            source_name (str): The name of one of the endpoints of queried edge.\r\n            target_name (str): The name of the other endpoint of the queried edge.\r\n\r\n        Returns:\r\n            The edge connecting the nodes with the specified names\r\n            or `None` if no such node exists.\r\n        \"\"\"\r\n        nodes: NodeList = self._graph.nodes\r\n        source: Optional[Node] = nodes.get_node_by_name(source_name)\r\n        if source is None:\r\n            return None\r\n        target: Optional[Node] = nodes.get_node_by_name(target_name)\r\n        if target is None:\r\n            return None\r\n        return self.get_edge_by_index(source.index, target.index)"}
{"code":"def get_output(cls, response: requests.Response) -> str:\n        \"\"\"\n        Extracts the senza cli output from the response\n        \"\"\"\n        output = response.headers['X-Lizzy-Output']  # type: str\n        output = output.replace('\\\\n', '\\n')  # unescape new lines\n        lines = ('[AGENT] {}'.format(line) for line in output.splitlines())\n        return '\\n'.join(lines)","return_type":"str","function_name":"Lizzy.get_output","stripped_code":"def get_output(cls, response: requests.Response):\n        \"\"\"\n        Extracts the senza cli output from the response\n        \"\"\"\n        output = response.headers['X-Lizzy-Output']  # type: str\n        output = output.replace('\\\\n', '\\n')  # unescape new lines\n        lines = ('[AGENT] {}'.format(line) for line in output.splitlines())\n        return '\\n'.join(lines)"}
{"code":"def crossplat_loop_run(coro) -> Any:\n    \"\"\"Cross-platform method for running a subprocess-spawning coroutine.\"\"\"\n    if sys.platform == 'win32':\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n        loop = asyncio.ProactorEventLoop()\n    else:\n        loop = asyncio.new_event_loop()\n\n    asyncio.set_event_loop(loop)\n    with contextlib.closing(loop):\n        return loop.run_until_complete(coro)","return_type":"Any","function_name":"crossplat_loop_run","stripped_code":"def crossplat_loop_run(coro):\n    \"\"\"Cross-platform method for running a subprocess-spawning coroutine.\"\"\"\n    if sys.platform == 'win32':\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n        loop = asyncio.ProactorEventLoop()\n    else:\n        loop = asyncio.new_event_loop()\n\n    asyncio.set_event_loop(loop)\n    with contextlib.closing(loop):\n        return loop.run_until_complete(coro)"}
{"code":"def init(library: typing.Union[str, types.ModuleType]) -> None:\n    '''\n    Must be called at some point after import and before your event loop\n    is run.\n\n    Populates the asynclib instance of _AsyncLib with methods relevant to the\n    async library you are using.\n\n    The supported libraries at the moment are:\n    - curio\n    - trio\n\n    Args:\n        library (str or module): Either the module name as a string or the\n                                 imported module itself. E.g. ``multio.init(curio)``.\n    '''\n    if isinstance(library, types.ModuleType):\n        library = library.__name__\n\n    if library not in manager._handlers:\n        raise ValueError(\"Possible values are <{}>, not <{}>\".format(manager._handlers.keys(),\n                                                                     library))\n\n    manager.init(library, asynclib)\n    asynclib.lib_name = library\n    asynclib._init = True","return_type":"None","function_name":"init","stripped_code":"def init(library: typing.Union[str, types.ModuleType]):\n    '''\n    Must be called at some point after import and before your event loop\n    is run.\n\n    Populates the asynclib instance of _AsyncLib with methods relevant to the\n    async library you are using.\n\n    The supported libraries at the moment are:\n    - curio\n    - trio\n\n    Args:\n        library (str or module): Either the module name as a string or the\n                                 imported module itself. E.g. ``multio.init(curio)``.\n    '''\n    if isinstance(library, types.ModuleType):\n        library = library.__name__\n\n    if library not in manager._handlers:\n        raise ValueError(\"Possible values are <{}>, not <{}>\".format(manager._handlers.keys(),\n                                                                     library))\n\n    manager.init(library, asynclib)\n    asynclib.lib_name = library\n    asynclib._init = True"}
{"code":"def check_concurrency(self) -> bool:\n        \"\"\"Checks the concurrency of the operation run.\n\n        Checks the concurrency of the operation run\n        to validate if we can start a new operation run.\n\n        Returns:\n            boolean: Whether to start a new operation run or not.\n        \"\"\"\n        if not self.operation.concurrency:  # No concurrency set\n            return True\n\n        ops_count = self.operation.runs.filter(\n            status__status__in=self.STATUSES.RUNNING_STATUS).count()\n        return ops_count < self.operation.concurrency","return_type":"bool","function_name":"OperationRun.check_concurrency","stripped_code":"def check_concurrency(self):\n        \"\"\"Checks the concurrency of the operation run.\n\n        Checks the concurrency of the operation run\n        to validate if we can start a new operation run.\n\n        Returns:\n            boolean: Whether to start a new operation run or not.\n        \"\"\"\n        if not self.operation.concurrency:  # No concurrency set\n            return True\n\n        ops_count = self.operation.runs.filter(\n            status__status__in=self.STATUSES.RUNNING_STATUS).count()\n        return ops_count < self.operation.concurrency"}
{"code":"def as_bel(self) -> str:\n        \"\"\"Return this fusion as a BEL string.\"\"\"\n        return '{}(fus({}:{}, \"{}\", {}:{}, \"{}\"))'.format(\n            self._func,\n            self.partner_5p.namespace,\n            self.partner_5p._priority_id,\n            self.range_5p.as_bel(),\n            self.partner_3p.namespace,\n            self.partner_3p._priority_id,\n            self.range_3p.as_bel(),\n        )","return_type":"str","function_name":"FusionBase.as_bel","stripped_code":"def as_bel(self):\n        \"\"\"Return this fusion as a BEL string.\"\"\"\n        return '{}(fus({}:{}, \"{}\", {}:{}, \"{}\"))'.format(\n            self._func,\n            self.partner_5p.namespace,\n            self.partner_5p._priority_id,\n            self.range_5p.as_bel(),\n            self.partner_3p.namespace,\n            self.partner_3p._priority_id,\n            self.range_3p.as_bel(),\n        )"}
{"code":"def _prune_subdirs(dir_: str) -> None:\n    \"\"\"\n    Delete all subdirs in training log dirs.\n\n    :param dir_: dir with training log dirs\n    \"\"\"\n    for logdir in [path.join(dir_, f) for f in listdir(dir_) if is_train_dir(path.join(dir_, f))]:\n        for subdir in [path.join(logdir, f) for f in listdir(logdir) if path.isdir(path.join(logdir, f))]:\n            _safe_rmtree(subdir)","return_type":"None","function_name":"_prune_subdirs","stripped_code":"def _prune_subdirs(dir_: str):\n    \"\"\"\n    Delete all subdirs in training log dirs.\n\n    :param dir_: dir with training log dirs\n    \"\"\"\n    for logdir in [path.join(dir_, f) for f in listdir(dir_) if is_train_dir(path.join(dir_, f))]:\n        for subdir in [path.join(logdir, f) for f in listdir(logdir) if path.isdir(path.join(logdir, f))]:\n            _safe_rmtree(subdir)"}
{"code":"def image(\n        rendered_path: str,\n        width: int = None,\n        height: int = None,\n        justify: str = None\n) -> str:\n    \"\"\"Renders an image block\"\"\"\n    environ.abort_thread()\n    return templating.render_template(\n        'image.html',\n        path=rendered_path,\n        width=width,\n        height=height,\n        justification=(justify or 'left').lower()\n    )","return_type":"str","function_name":"image","stripped_code":"def image(\n        rendered_path: str,\n        width: int = None,\n        height: int = None,\n        justify: str = None\n):\n    \"\"\"Renders an image block\"\"\"\n    environ.abort_thread()\n    return templating.render_template(\n        'image.html',\n        path=rendered_path,\n        width=width,\n        height=height,\n        justification=(justify or 'left').lower()\n    )"}
{"code":"def register_encoder(self, lookup: Lookup, encoder: Encoder, label: str=None) -> None:\n        \"\"\"\n        Registers the given ``encoder`` under the given ``lookup``.  A unique\n        string label may be optionally provided that can be used to refer to\n        the registration by name.  For more information about arguments, refer\n        to :any:`register`.\n        \"\"\"\n        self._register_coder(self._encoders, lookup, encoder, label=label)","return_type":"None","function_name":"ABIRegistry.register_encoder","stripped_code":"def register_encoder(self, lookup: Lookup, encoder: Encoder, label: str=None):\n        \"\"\"\n        Registers the given ``encoder`` under the given ``lookup``.  A unique\n        string label may be optionally provided that can be used to refer to\n        the registration by name.  For more information about arguments, refer\n        to :any:`register`.\n        \"\"\"\n        self._register_coder(self._encoders, lookup, encoder, label=label)"}
{"code":"def write(self, learn:Learner, trn_batch:Tuple, val_batch:Tuple, iteration:int, tbwriter:SummaryWriter)->None:\n        \"Writes training and validation batch images to Tensorboard.\"\n        self._write_for_dstype(learn=learn, batch=val_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Valid)\n        self._write_for_dstype(learn=learn, batch=trn_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Train)","return_type":"None","function_name":"ImageTBWriter.write","stripped_code":"def write(self, learn:Learner, trn_batch:Tuple, val_batch:Tuple, iteration:int, tbwriter:SummaryWriter):\n        \"Writes training and validation batch images to Tensorboard.\"\n        self._write_for_dstype(learn=learn, batch=val_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Valid)\n        self._write_for_dstype(learn=learn, batch=trn_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Train)"}
{"code":"def classes(self) -> Iterator[str]:\n        \"\"\"Yield the name of all classes discovered in the path map.\"\"\"\n        yield from (\n            c[:-6]\n            for c in self.path_map.keys() if c.endswith('.class')\n        )","return_type":"Iterator[str]","function_name":"ClassLoader.classes","stripped_code":"def classes(self):\n        \"\"\"Yield the name of all classes discovered in the path map.\"\"\"\n        yield from (\n            c[:-6]\n            for c in self.path_map.keys() if c.endswith('.class')\n        )"}
{"code":"def perform_batch_reply(\n            self,\n            *,\n            callback: Callable[..., str],\n            lookback_limit: int,\n            target_handle: str,\n    ) -> List[OutputRecord]:\n        \"\"\"\n        Performs batch reply on target account.\n        Looks up the recent messages of the target user,\n        applies the callback,\n        and replies with\n        what the callback generates.\n\n        :param callback: a callback taking a message id,\n            message contents,\n            and optional extra keys,\n            and returning a message string.\n        :param target: the id of the target account.\n        :param lookback_limit: a lookback limit of how many messages to consider.\n        :returns: list of output records,\n            each corresponding to either a single post,\n            or an error.\n        \"\"\"\n        self.log.info(f\"Attempting to batch reply to mastodon user {target_handle}\")\n\n        # target handle should be able to be provided either as @user or @user@domain\n        # note that this produces an empty first chunk\n        handle_chunks = target_handle.split(\"@\")\n        target_base_handle = handle_chunks[1]\n\n        records: List[OutputRecord] = []\n        our_id = self.api.account_verify_credentials()[\"id\"]\n\n        # be careful here - we're using a search to do this,\n        # and if we're not careful we'll pull up people just mentioning the target.\n        possible_accounts = self.api.account_search(target_handle, following=True)\n        their_id = None\n        for account in possible_accounts:\n            if account[\"username\"] == target_base_handle:\n                their_id = account[\"id\"]\n                break\n\n        if their_id is None:\n            return [self.handle_error(f\"Could not find target handle {target_handle}!\", None)]\n\n        statuses = self.api.account_statuses(their_id, limit=lookback_limit)\n        for status in statuses:\n\n            status_id = status.id\n\n            # find possible replies we've made.\n            our_statuses = self.api.account_statuses(our_id, since_id=status_id)\n            in_reply_to_ids = list(map(lambda x: x.in_reply_to_id, our_statuses))\n            if status_id not in in_reply_to_ids:\n\n                encoded_status_text = re.sub(self.html_re, \"\", status.content)\n                status_text = html.unescape(encoded_status_text)\n\n                message = callback(message_id=status_id, message=status_text, extra_keys={})\n                self.log.info(f\"Replying {message} to status {status_id} from {target_handle}.\")\n                try:\n                    new_status = self.api.status_post(status=message, in_reply_to_id=status_id)\n\n                    records.append(TootRecord(record_data={\n                        \"toot_id\": new_status.id,\n                        \"in_reply_to\": target_handle,\n                        \"in_reply_to_id\": status_id,\n                        \"text\": message,\n                    }))\n\n                except mastodon.MastodonError as e:\n                    records.append(\n                        self.handle_error((f\"Bot {self.bot_name} encountered an error when \"\n                                           f\"sending post {message} during a batch reply \"\n                                           f\":\\n{e}\\n\"),\n                                          e))\n            else:\n                self.log.info(f\"Not replying to status {status_id} from {target_handle} \"\n                              f\"- we already replied.\")\n\n        return records","return_type":"List[OutputRecord]","function_name":"MastodonSkeleton.perform_batch_reply","stripped_code":"def perform_batch_reply(\n            self,\n            *,\n            callback: Callable[..., str],\n            lookback_limit: int,\n            target_handle: str,\n    ):\n        \"\"\"\n        Performs batch reply on target account.\n        Looks up the recent messages of the target user,\n        applies the callback,\n        and replies with\n        what the callback generates.\n\n        :param callback: a callback taking a message id,\n            message contents,\n            and optional extra keys,\n            and returning a message string.\n        :param target: the id of the target account.\n        :param lookback_limit: a lookback limit of how many messages to consider.\n        :returns: list of output records,\n            each corresponding to either a single post,\n            or an error.\n        \"\"\"\n        self.log.info(f\"Attempting to batch reply to mastodon user {target_handle}\")\n\n        # target handle should be able to be provided either as @user or @user@domain\n        # note that this produces an empty first chunk\n        handle_chunks = target_handle.split(\"@\")\n        target_base_handle = handle_chunks[1]\n\n        records: List[OutputRecord] = []\n        our_id = self.api.account_verify_credentials()[\"id\"]\n\n        # be careful here - we're using a search to do this,\n        # and if we're not careful we'll pull up people just mentioning the target.\n        possible_accounts = self.api.account_search(target_handle, following=True)\n        their_id = None\n        for account in possible_accounts:\n            if account[\"username\"] == target_base_handle:\n                their_id = account[\"id\"]\n                break\n\n        if their_id is None:\n            return [self.handle_error(f\"Could not find target handle {target_handle}!\", None)]\n\n        statuses = self.api.account_statuses(their_id, limit=lookback_limit)\n        for status in statuses:\n\n            status_id = status.id\n\n            # find possible replies we've made.\n            our_statuses = self.api.account_statuses(our_id, since_id=status_id)\n            in_reply_to_ids = list(map(lambda x: x.in_reply_to_id, our_statuses))\n            if status_id not in in_reply_to_ids:\n\n                encoded_status_text = re.sub(self.html_re, \"\", status.content)\n                status_text = html.unescape(encoded_status_text)\n\n                message = callback(message_id=status_id, message=status_text, extra_keys={})\n                self.log.info(f\"Replying {message} to status {status_id} from {target_handle}.\")\n                try:\n                    new_status = self.api.status_post(status=message, in_reply_to_id=status_id)\n\n                    records.append(TootRecord(record_data={\n                        \"toot_id\": new_status.id,\n                        \"in_reply_to\": target_handle,\n                        \"in_reply_to_id\": status_id,\n                        \"text\": message,\n                    }))\n\n                except mastodon.MastodonError as e:\n                    records.append(\n                        self.handle_error((f\"Bot {self.bot_name} encountered an error when \"\n                                           f\"sending post {message} during a batch reply \"\n                                           f\":\\n{e}\\n\"),\n                                          e))\n            else:\n                self.log.info(f\"Not replying to status {status_id} from {target_handle} \"\n                              f\"- we already replied.\")\n\n        return records"}
{"code":"def verify_pop(pop: ProofOfPossession, ver_key: VerKey, gen: Generator) -> bool:\n        \"\"\"\n        Verifies the proof of possession and returns true - if signature valid or false otherwise.\n\n        :param: pop - Proof of possession\n        :param: ver_key - Verification key\n        :param: gen - Generator point\n        :return: true if signature valid\n        \"\"\"\n\n        logger = logging.getLogger(__name__)\n        logger.debug(\"Bls::verify_pop: >>> pop: %r, ver_key: %r, gen: %r\",\n                     pop,\n                     ver_key,\n                     gen)\n\n        valid = c_bool()\n        do_call('indy_crypto_bsl_verify_pop',\n                pop.c_instance,\n                ver_key.c_instance,\n                gen.c_instance,\n                byref(valid))\n\n        res = valid\n        logger.debug(\"Bls::verify_pop: <<< res: %r\", res)\n        return res","return_type":"bool","function_name":"Bls.verify_pop","stripped_code":"def verify_pop(pop: ProofOfPossession, ver_key: VerKey, gen: Generator):\n        \"\"\"\n        Verifies the proof of possession and returns true - if signature valid or false otherwise.\n\n        :param: pop - Proof of possession\n        :param: ver_key - Verification key\n        :param: gen - Generator point\n        :return: true if signature valid\n        \"\"\"\n\n        logger = logging.getLogger(__name__)\n        logger.debug(\"Bls::verify_pop: >>> pop: %r, ver_key: %r, gen: %r\",\n                     pop,\n                     ver_key,\n                     gen)\n\n        valid = c_bool()\n        do_call('indy_crypto_bsl_verify_pop',\n                pop.c_instance,\n                ver_key.c_instance,\n                gen.c_instance,\n                byref(valid))\n\n        res = valid\n        logger.debug(\"Bls::verify_pop: <<< res: %r\", res)\n        return res"}
{"code":"def collect_conflicts_between(\n    context: ValidationContext,\n    conflicts: List[Conflict],\n    cached_fields_and_fragment_names: Dict,\n    compared_fragment_pairs: \"PairSet\",\n    parent_fields_are_mutually_exclusive: bool,\n    field_map1: NodeAndDefCollection,\n    field_map2: NodeAndDefCollection,\n) -> None:\n    \"\"\"Collect all Conflicts between two collections of fields.\n\n    This is similar to, but different from the `collectConflictsWithin` function above.\n    This check assumes that `collectConflictsWithin` has already been called on each\n    provided collection of fields. This is true because this validator traverses each\n    individual selection set.\n    \"\"\"\n    # A field map is a keyed collection, where each key represents a response name and\n    # the value at that key is a list of all fields which provide that response name.\n    # For any response name which appears in both provided field maps, each field from\n    # the first field map must be compared to every field in the second field map to\n    # find potential conflicts.\n    for response_name, fields1 in field_map1.items():\n        fields2 = field_map2.get(response_name)\n        if fields2:\n            for field1 in fields1:\n                for field2 in fields2:\n                    conflict = find_conflict(\n                        context,\n                        cached_fields_and_fragment_names,\n                        compared_fragment_pairs,\n                        parent_fields_are_mutually_exclusive,\n                        response_name,\n                        field1,\n                        field2,\n                    )\n                    if conflict:\n                        conflicts.append(conflict)","return_type":"None","function_name":"collect_conflicts_between","stripped_code":"def collect_conflicts_between(\n    context: ValidationContext,\n    conflicts: List[Conflict],\n    cached_fields_and_fragment_names: Dict,\n    compared_fragment_pairs: \"PairSet\",\n    parent_fields_are_mutually_exclusive: bool,\n    field_map1: NodeAndDefCollection,\n    field_map2: NodeAndDefCollection,\n):\n    \"\"\"Collect all Conflicts between two collections of fields.\n\n    This is similar to, but different from the `collectConflictsWithin` function above.\n    This check assumes that `collectConflictsWithin` has already been called on each\n    provided collection of fields. This is true because this validator traverses each\n    individual selection set.\n    \"\"\"\n    # A field map is a keyed collection, where each key represents a response name and\n    # the value at that key is a list of all fields which provide that response name.\n    # For any response name which appears in both provided field maps, each field from\n    # the first field map must be compared to every field in the second field map to\n    # find potential conflicts.\n    for response_name, fields1 in field_map1.items():\n        fields2 = field_map2.get(response_name)\n        if fields2:\n            for field1 in fields1:\n                for field2 in fields2:\n                    conflict = find_conflict(\n                        context,\n                        cached_fields_and_fragment_names,\n                        compared_fragment_pairs,\n                        parent_fields_are_mutually_exclusive,\n                        response_name,\n                        field1,\n                        field2,\n                    )\n                    if conflict:\n                        conflicts.append(conflict)"}
{"code":"def get_cartesian_coords(self, fractional_coords: Vector3Like) -> np.ndarray:\n        \"\"\"\n        Returns the cartesian coordinates given fractional coordinates.\n\n        Args:\n            fractional_coords (3x1 array): Fractional coords.\n\n        Returns:\n            Cartesian coordinates\n        \"\"\"\n        return dot(fractional_coords, self._matrix)","return_type":"np.ndarray","function_name":"Lattice.get_cartesian_coords","stripped_code":"def get_cartesian_coords(self, fractional_coords: Vector3Like):\n        \"\"\"\n        Returns the cartesian coordinates given fractional coordinates.\n\n        Args:\n            fractional_coords (3x1 array): Fractional coords.\n\n        Returns:\n            Cartesian coordinates\n        \"\"\"\n        return dot(fractional_coords, self._matrix)"}
{"code":"def nhs_check_digit(ninedigits: Union[str, List[Union[str, int]]]) -> int:\n    \"\"\"\n    Calculates an NHS number check digit.\n\n    Args:\n        ninedigits: string or list\n\n    Returns:\n        check digit\n\n    Method:\n\n    1. Multiply each of the first nine digits by the corresponding\n       digit weighting (see :const:`NHS_DIGIT_WEIGHTINGS`).\n    2. Sum the results.\n    3. Take remainder after division by 11.\n    4. Subtract the remainder from 11\n    5. If this is 11, use 0 instead\n       If it's 10, the number is invalid\n       If it doesn't match the actual check digit, the number is invalid\n\n    \"\"\"\n    if len(ninedigits) != 9 or not all(str(x).isdigit() for x in ninedigits):\n        raise ValueError(\"bad string to nhs_check_digit\")\n    check_digit = 11 - (sum([\n        int(d) * f\n        for (d, f) in zip(ninedigits, NHS_DIGIT_WEIGHTINGS)\n    ]) % 11)\n    # ... % 11 yields something in the range 0-10\n    # ... 11 - that yields something in the range 1-11\n    if check_digit == 11:\n        check_digit = 0\n    return check_digit","return_type":"int","function_name":"nhs_check_digit","stripped_code":"def nhs_check_digit(ninedigits: Union[str, List[Union[str, int]]]):\n    \"\"\"\n    Calculates an NHS number check digit.\n\n    Args:\n        ninedigits: string or list\n\n    Returns:\n        check digit\n\n    Method:\n\n    1. Multiply each of the first nine digits by the corresponding\n       digit weighting (see :const:`NHS_DIGIT_WEIGHTINGS`).\n    2. Sum the results.\n    3. Take remainder after division by 11.\n    4. Subtract the remainder from 11\n    5. If this is 11, use 0 instead\n       If it's 10, the number is invalid\n       If it doesn't match the actual check digit, the number is invalid\n\n    \"\"\"\n    if len(ninedigits) != 9 or not all(str(x).isdigit() for x in ninedigits):\n        raise ValueError(\"bad string to nhs_check_digit\")\n    check_digit = 11 - (sum([\n        int(d) * f\n        for (d, f) in zip(ninedigits, NHS_DIGIT_WEIGHTINGS)\n    ]) % 11)\n    # ... % 11 yields something in the range 0-10\n    # ... 11 - that yields something in the range 1-11\n    if check_digit == 11:\n        check_digit = 0\n    return check_digit"}
{"code":"def _ascii_tree(self, indent: str, no_types: bool, val_count: bool) -> str:\n        \"\"\"Return the receiver's subtree as ASCII art.\"\"\"\n        def suffix(sn):\n            return f\" {{{sn.val_count}}}\\n\" if val_count else \"\\n\"\n        if not self.children:\n            return \"\"\n        cs = []\n        for c in self.children:\n            cs.extend(c._flatten())\n        cs.sort(key=lambda x: x.qual_name)\n        res = \"\"\n        for c in cs[:-1]:\n            res += (indent + c._tree_line(no_types) + suffix(c) +\n                    c._ascii_tree(indent + \"|  \", no_types, val_count))\n        return (res + indent + cs[-1]._tree_line(no_types) + suffix(cs[-1]) +\n                cs[-1]._ascii_tree(indent + \"   \", no_types, val_count))","return_type":"str","function_name":"InternalNode._ascii_tree","stripped_code":"def _ascii_tree(self, indent: str, no_types: bool, val_count: bool):\n        \"\"\"Return the receiver's subtree as ASCII art.\"\"\"\n        def suffix(sn):\n            return f\" {{{sn.val_count}}}\\n\" if val_count else \"\\n\"\n        if not self.children:\n            return \"\"\n        cs = []\n        for c in self.children:\n            cs.extend(c._flatten())\n        cs.sort(key=lambda x: x.qual_name)\n        res = \"\"\n        for c in cs[:-1]:\n            res += (indent + c._tree_line(no_types) + suffix(c) +\n                    c._ascii_tree(indent + \"|  \", no_types, val_count))\n        return (res + indent + cs[-1]._tree_line(no_types) + suffix(cs[-1]) +\n                cs[-1]._ascii_tree(indent + \"   \", no_types, val_count))"}
{"code":"def sys_register_SDL_renderer(callback: Callable[[Any], None]) -> None:\n    \"\"\"Register a custom randering function with libtcod.\n\n    Note:\n        This callback will only be called by the SDL renderer.\n\n    The callack will receive a :any:`CData <ffi-cdata>` void* to an\n    SDL_Surface* struct.\n\n    The callback is called on every call to :any:`tcod.console_flush`.\n\n    Args:\n        callback Callable[[CData], None]:\n            A function which takes a single argument.\n    \"\"\"\n    with _PropagateException() as propagate:\n\n        @ffi.def_extern(onerror=propagate)  # type: ignore\n        def _pycall_sdl_hook(sdl_surface: Any) -> None:\n            callback(sdl_surface)\n\n        lib.TCOD_sys_register_SDL_renderer(lib._pycall_sdl_hook)","return_type":"None","function_name":"sys_register_SDL_renderer","stripped_code":"def sys_register_SDL_renderer(callback: Callable[[Any], None]):\n    \"\"\"Register a custom randering function with libtcod.\n\n    Note:\n        This callback will only be called by the SDL renderer.\n\n    The callack will receive a :any:`CData <ffi-cdata>` void* to an\n    SDL_Surface* struct.\n\n    The callback is called on every call to :any:`tcod.console_flush`.\n\n    Args:\n        callback Callable[[CData], None]:\n            A function which takes a single argument.\n    \"\"\"\n    with _PropagateException() as propagate:\n\n        @ffi.def_extern(onerror=propagate)  # type: ignore\n        def _pycall_sdl_hook(sdl_surface: Any) -> None:\n            callback(sdl_surface)\n\n        lib.TCOD_sys_register_SDL_renderer(lib._pycall_sdl_hook)"}
{"code":"def close(self, force=False) -> None:\n        \"\"\"\n        Closes the file if the file was opened by :class:`File`,\n        if not, this does nothing.\n\n        Parameters\n        ----------\n        force: bool\n            If set to :class:`True`, force close every file.\n\n        \"\"\"\n        self.fp.close = self._close\n        if self._manual_opened or force:\n            self.fp.close()","return_type":"None","function_name":"File.close","stripped_code":"def close(self, force=False):\n        \"\"\"\n        Closes the file if the file was opened by :class:`File`,\n        if not, this does nothing.\n\n        Parameters\n        ----------\n        force: bool\n            If set to :class:`True`, force close every file.\n\n        \"\"\"\n        self.fp.close = self._close\n        if self._manual_opened or force:\n            self.fp.close()"}
{"code":"def GetChildren(self) -> list:\n        \"\"\"\n        Call IUIAutomationTextRange::GetChildren.\n        textAttributeId: int, a value in class `TextAttributeId`.\n        Return list, a list of `Control` subclasses, embedded objects that fall within the text range..\n        Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtextrange-getchildren\n        \"\"\"\n        eleArray = self.textRange.GetChildren()\n        if eleArray:\n            controls = []\n            for i in range(eleArray.Length):\n                ele = eleArray.GetElement(i)\n                con = Control.CreateControlFromElement(element=ele)\n                if con:\n                    controls.append(con)\n            return controls\n        return []","return_type":"list","function_name":"TextRange.GetChildren","stripped_code":"def GetChildren(self):\n        \"\"\"\n        Call IUIAutomationTextRange::GetChildren.\n        textAttributeId: int, a value in class `TextAttributeId`.\n        Return list, a list of `Control` subclasses, embedded objects that fall within the text range..\n        Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtextrange-getchildren\n        \"\"\"\n        eleArray = self.textRange.GetChildren()\n        if eleArray:\n            controls = []\n            for i in range(eleArray.Length):\n                ele = eleArray.GetElement(i)\n                con = Control.CreateControlFromElement(element=ele)\n                if con:\n                    controls.append(con)\n            return controls\n        return []"}
{"code":"def trunc_normal_(x:Tensor, mean:float=0., std:float=1.) -> Tensor:\n    \"Truncated normal initialization.\"\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)","return_type":"Tensor","function_name":"trunc_normal_","stripped_code":"def trunc_normal_(x:Tensor, mean:float=0., std:float=1.):\n    \"Truncated normal initialization.\"\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)"}
{"code":"def groups(self) -> Set[str]:\n        \"\"\"The set of option-groups created by ``define``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        return set(opt.group_name for opt in self._options.values())","return_type":"Set[str]","function_name":"OptionParser.groups","stripped_code":"def groups(self):\n        \"\"\"The set of option-groups created by ``define``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        return set(opt.group_name for opt in self._options.values())"}
{"code":"def example(self) -> str:\n        \"\"\" Same as str(self), except the color codes are actually used. \"\"\"\n        if self.rgb_mode:\n            colorcode = '\\033[38;2;{};{};{}m'.format(*self.rgb)\n        else:\n            colorcode = '\\033[38;5;{}m'.format(self.code)\n        return '{code}{s}\\033[0m'.format(code=colorcode, s=self)","return_type":"str","function_name":"ColorCode.example","stripped_code":"def example(self):\n        \"\"\" Same as str(self), except the color codes are actually used. \"\"\"\n        if self.rgb_mode:\n            colorcode = '\\033[38;2;{};{};{}m'.format(*self.rgb)\n        else:\n            colorcode = '\\033[38;5;{}m'.format(self.code)\n        return '{code}{s}\\033[0m'.format(code=colorcode, s=self)"}
{"code":"def tensormul(tensor0: BKTensor, tensor1: BKTensor,\n              indices: typing.List[int]) -> BKTensor:\n    r\"\"\"\n    Generalization of matrix multiplication to product tensors.\n\n    A state vector in product tensor representation has N dimension, one for\n    each contravariant index, e.g. for 3-qubit states\n    :math:`B^{b_0,b_1,b_2}`. An operator has K dimensions, K/2 for\n    contravariant indices (e.g. ket components) and K/2 for covariant (bra)\n    indices, e.g. :math:`A^{a_0,a_1}_{a_2,a_3}` for a 2-qubit gate. The given\n    indices of A are contracted against B, replacing the given positions.\n\n    E.g. ``tensormul(A, B, [0,2])`` is equivalent to\n\n    .. math::\n\n        C^{a_0,b_1,a_1} =\\sum_{i_0,i_1} A^{a_0,a_1}_{i_0,i_1} B^{i_0,b_1,i_1}\n\n    Args:\n        tensor0: A tensor product representation of a gate\n        tensor1: A tensor product representation of a gate or state\n        indices: List of indices of tensor1 on which to act.\n    Returns:\n        Resultant state or gate tensor\n\n    \"\"\"\n\n    # Note: This method is the critical computational core of QuantumFlow\n    # We currently have two implementations, one that uses einsum, the other\n    # using matrix multiplication\n    #\n    # numpy:\n    #   einsum is much faster particularly for small numbers of qubits\n    # tensorflow:\n    #   Little different is performance, but einsum would restrict the\n    #   maximum number of qubits to 26 (Because tensorflow only allows 26\n    #   einsum subscripts at present]\n    # torch:\n    #   einsum is slower than matmul\n\n    N = rank(tensor1)\n    K = rank(tensor0) // 2\n    assert K == len(indices)\n\n    out = list(EINSUM_SUBSCRIPTS[0:N])\n    left_in = list(EINSUM_SUBSCRIPTS[N:N+K])\n    left_out = [out[idx] for idx in indices]\n    right = list(EINSUM_SUBSCRIPTS[0:N])\n    for idx, s in zip(indices, left_in):\n        right[idx] = s\n\n    subscripts = ''.join(left_out + left_in + [','] + right + ['->'] + out)\n    # print('>>>', K, N, subscripts)\n\n    tensor = einsum(subscripts, tensor0, tensor1)\n    return tensor","return_type":"BKTensor","function_name":"tensormul","stripped_code":"def tensormul(tensor0: BKTensor, tensor1: BKTensor,\n              indices: typing.List[int]):\n    r\"\"\"\n    Generalization of matrix multiplication to product tensors.\n\n    A state vector in product tensor representation has N dimension, one for\n    each contravariant index, e.g. for 3-qubit states\n    :math:`B^{b_0,b_1,b_2}`. An operator has K dimensions, K/2 for\n    contravariant indices (e.g. ket components) and K/2 for covariant (bra)\n    indices, e.g. :math:`A^{a_0,a_1}_{a_2,a_3}` for a 2-qubit gate. The given\n    indices of A are contracted against B, replacing the given positions.\n\n    E.g. ``tensormul(A, B, [0,2])`` is equivalent to\n\n    .. math::\n\n        C^{a_0,b_1,a_1} =\\sum_{i_0,i_1} A^{a_0,a_1}_{i_0,i_1} B^{i_0,b_1,i_1}\n\n    Args:\n        tensor0: A tensor product representation of a gate\n        tensor1: A tensor product representation of a gate or state\n        indices: List of indices of tensor1 on which to act.\n    Returns:\n        Resultant state or gate tensor\n\n    \"\"\"\n\n    # Note: This method is the critical computational core of QuantumFlow\n    # We currently have two implementations, one that uses einsum, the other\n    # using matrix multiplication\n    #\n    # numpy:\n    #   einsum is much faster particularly for small numbers of qubits\n    # tensorflow:\n    #   Little different is performance, but einsum would restrict the\n    #   maximum number of qubits to 26 (Because tensorflow only allows 26\n    #   einsum subscripts at present]\n    # torch:\n    #   einsum is slower than matmul\n\n    N = rank(tensor1)\n    K = rank(tensor0) // 2\n    assert K == len(indices)\n\n    out = list(EINSUM_SUBSCRIPTS[0:N])\n    left_in = list(EINSUM_SUBSCRIPTS[N:N+K])\n    left_out = [out[idx] for idx in indices]\n    right = list(EINSUM_SUBSCRIPTS[0:N])\n    for idx, s in zip(indices, left_in):\n        right[idx] = s\n\n    subscripts = ''.join(left_out + left_in + [','] + right + ['->'] + out)\n    # print('>>>', K, N, subscripts)\n\n    tensor = einsum(subscripts, tensor0, tensor1)\n    return tensor"}
{"code":"def clear_tags(self) -> dict:\n        \"\"\"\n        Accessor for record tags (metadata) stored in the clear.\n\n        :return: record tags stored in the clear\n        \"\"\"\n\n        return {t: self.tags[t] for t in (self.tags or {}) if t.startswith('~')} or None","return_type":"dict","function_name":"StorageRecord.clear_tags","stripped_code":"def clear_tags(self):\n        \"\"\"\n        Accessor for record tags (metadata) stored in the clear.\n\n        :return: record tags stored in the clear\n        \"\"\"\n\n        return {t: self.tags[t] for t in (self.tags or {}) if t.startswith('~')} or None"}
{"code":"def validate_request_signature(\n    body: str, headers: MutableMapping, signing_secret: str\n) -> None:\n    \"\"\"\n    Validate incoming request signature using the application signing secret.\n\n    Contrary to the ``team_id`` and ``verification_token`` verification this method is not called by ``slack-sansio`` when creating object from incoming HTTP request. Because the body of the request needs to be provided as text and not decoded as json beforehand.\n\n    Args:\n        body: Raw request body\n        headers: Request headers\n        signing_secret: Application signing_secret\n\n    Raise:\n        :class:`slack.exceptions.InvalidSlackSignature`: when provided and calculated signature do not match\n        :class:`slack.exceptions.InvalidTimestamp`: when incoming request timestamp is more than 5 minutes old\n    \"\"\"\n\n    request_timestamp = int(headers[\"X-Slack-Request-Timestamp\"])\n\n    if (int(time.time()) - request_timestamp) > (60 * 5):\n        raise exceptions.InvalidTimestamp(timestamp=request_timestamp)\n\n    slack_signature = headers[\"X-Slack-Signature\"]\n    calculated_signature = (\n        \"v0=\"\n        + hmac.new(\n            signing_secret.encode(\"utf-8\"),\n            f\"\"\"v0:{headers[\"X-Slack-Request-Timestamp\"]}:{body}\"\"\".encode(\"utf-8\"),\n            digestmod=hashlib.sha256,\n        ).hexdigest()\n    )\n\n    if not hmac.compare_digest(slack_signature, calculated_signature):\n        raise exceptions.InvalidSlackSignature(slack_signature, calculated_signature)","return_type":"None","function_name":"validate_request_signature","stripped_code":"def validate_request_signature(\n    body: str, headers: MutableMapping, signing_secret: str\n):\n    \"\"\"\n    Validate incoming request signature using the application signing secret.\n\n    Contrary to the ``team_id`` and ``verification_token`` verification this method is not called by ``slack-sansio`` when creating object from incoming HTTP request. Because the body of the request needs to be provided as text and not decoded as json beforehand.\n\n    Args:\n        body: Raw request body\n        headers: Request headers\n        signing_secret: Application signing_secret\n\n    Raise:\n        :class:`slack.exceptions.InvalidSlackSignature`: when provided and calculated signature do not match\n        :class:`slack.exceptions.InvalidTimestamp`: when incoming request timestamp is more than 5 minutes old\n    \"\"\"\n\n    request_timestamp = int(headers[\"X-Slack-Request-Timestamp\"])\n\n    if (int(time.time()) - request_timestamp) > (60 * 5):\n        raise exceptions.InvalidTimestamp(timestamp=request_timestamp)\n\n    slack_signature = headers[\"X-Slack-Signature\"]\n    calculated_signature = (\n        \"v0=\"\n        + hmac.new(\n            signing_secret.encode(\"utf-8\"),\n            f\"\"\"v0:{headers[\"X-Slack-Request-Timestamp\"]}:{body}\"\"\".encode(\"utf-8\"),\n            digestmod=hashlib.sha256,\n        ).hexdigest()\n    )\n\n    if not hmac.compare_digest(slack_signature, calculated_signature):\n        raise exceptions.InvalidSlackSignature(slack_signature, calculated_signature)"}
{"code":"def _set_residual_probability(p: np.ndarray) -> np.ndarray:\n    \"\"\"Turns any use of `RESIDUAL_CHOICE` into a residual probability.\n\n    Parameters\n    ----------\n    p :\n        Array where each row is a set of probability weights and potentially\n        a `RESIDUAL_CHOICE` placeholder.\n\n    Returns\n    -------\n    np.ndarray\n        Array where each row is a set of normalized probability weights.\n    \"\"\"\n    residual_mask = p == RESIDUAL_CHOICE\n    if residual_mask.any():  # I.E. if we have any placeholders.\n        if np.any(np.sum(residual_mask, axis=1) - 1):\n            raise RandomnessError(\n                'More than one residual choice supplied for a single set of weights. Weights: {}.'.format(p))\n\n        p[residual_mask] = 0\n        residual_p = 1 - np.sum(p, axis=1)  # Probabilities sum to 1.\n\n        if np.any(residual_p < 0):  # We got un-normalized probability weights.\n            raise RandomnessError(\n                'Residual choice supplied with weights that summed to more than 1. Weights: {}.'.format(p))\n\n        p[residual_mask] = residual_p\n    return p","return_type":"np.ndarray","function_name":"_set_residual_probability","stripped_code":"def _set_residual_probability(p: np.ndarray):\n    \"\"\"Turns any use of `RESIDUAL_CHOICE` into a residual probability.\n\n    Parameters\n    ----------\n    p :\n        Array where each row is a set of probability weights and potentially\n        a `RESIDUAL_CHOICE` placeholder.\n\n    Returns\n    -------\n    np.ndarray\n        Array where each row is a set of normalized probability weights.\n    \"\"\"\n    residual_mask = p == RESIDUAL_CHOICE\n    if residual_mask.any():  # I.E. if we have any placeholders.\n        if np.any(np.sum(residual_mask, axis=1) - 1):\n            raise RandomnessError(\n                'More than one residual choice supplied for a single set of weights. Weights: {}.'.format(p))\n\n        p[residual_mask] = 0\n        residual_p = 1 - np.sum(p, axis=1)  # Probabilities sum to 1.\n\n        if np.any(residual_p < 0):  # We got un-normalized probability weights.\n            raise RandomnessError(\n                'Residual choice supplied with weights that summed to more than 1. Weights: {}.'.format(p))\n\n        p[residual_mask] = residual_p\n    return p"}
{"code":"def get_ip_info(ip: str, exceptions: bool=False, timeout: int=10) -> tuple:\n    \"\"\"\n    Returns (ip, country_code, host) tuple of the IP address.\n    :param ip: IP address\n    :param exceptions: Raise Exception or not\n    :param timeout: Timeout in seconds. Note that timeout only affects geo IP part, not getting host name.\n    :return: (ip, country_code, host)\n    \"\"\"\n    import traceback\n    import socket\n    if not ip:  # localhost\n        return None, '', ''\n    host = ''\n    country_code = get_geo_ip(ip, exceptions=exceptions, timeout=timeout).get('country_code', '')\n    try:\n        res = socket.gethostbyaddr(ip)\n        host = res[0][:255] if ip else ''\n    except Exception as e:\n        msg = 'socket.gethostbyaddr({}) failed: {}'.format(ip, traceback.format_exc())\n        logger.error(msg)\n        if exceptions:\n            raise e\n    return ip, country_code, host","return_type":"tuple","function_name":"get_ip_info","stripped_code":"def get_ip_info(ip: str, exceptions: bool=False, timeout: int=10):\n    \"\"\"\n    Returns (ip, country_code, host) tuple of the IP address.\n    :param ip: IP address\n    :param exceptions: Raise Exception or not\n    :param timeout: Timeout in seconds. Note that timeout only affects geo IP part, not getting host name.\n    :return: (ip, country_code, host)\n    \"\"\"\n    import traceback\n    import socket\n    if not ip:  # localhost\n        return None, '', ''\n    host = ''\n    country_code = get_geo_ip(ip, exceptions=exceptions, timeout=timeout).get('country_code', '')\n    try:\n        res = socket.gethostbyaddr(ip)\n        host = res[0][:255] if ip else ''\n    except Exception as e:\n        msg = 'socket.gethostbyaddr({}) failed: {}'.format(ip, traceback.format_exc())\n        logger.error(msg)\n        if exceptions:\n            raise e\n    return ip, country_code, host"}
{"code":"def create_session(\n        self,\n        kind: SessionKind,\n        proxy_user: str = None,\n        jars: List[str] = None,\n        py_files: List[str] = None,\n        files: List[str] = None,\n        driver_memory: str = None,\n        driver_cores: int = None,\n        executor_memory: str = None,\n        executor_cores: int = None,\n        num_executors: int = None,\n        archives: List[str] = None,\n        queue: str = None,\n        name: str = None,\n        spark_conf: Dict[str, Any] = None,\n    ) -> Session:\n        \"\"\"Create a new session in Livy.\n\n        The py_files, files, jars and archives arguments are lists of URLs,\n        e.g. [\"s3://bucket/object\", \"hdfs://path/to/file\", ...] and must be\n        reachable by the Spark driver process.  If the provided URL has no\n        scheme, it's considered to be relative to the default file system\n        configured in the Livy server.\n\n        URLs in the py_files argument are copied to a temporary staging area\n        and inserted into Python's sys.path ahead of the standard library\n        paths.  This allows you to import .py, .zip and .egg files in Python.\n\n        URLs for jars, py_files, files and archives arguments are all copied\n        to the same working directory on the Spark cluster.\n\n        The driver_memory and executor_memory arguments have the same format\n        as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n        (e.g. 512m, 2g).\n\n        See https://spark.apache.org/docs/latest/configuration.html for more\n        information on Spark configuration properties.\n\n        :param kind: The kind of session to create.\n        :param proxy_user: User to impersonate when starting the session.\n        :param jars: URLs of jars to be used in this session.\n        :param py_files: URLs of Python files to be used in this session.\n        :param files: URLs of files to be used in this session.\n        :param driver_memory: Amount of memory to use for the driver process\n            (e.g. '512m').\n        :param driver_cores: Number of cores to use for the driver process.\n        :param executor_memory: Amount of memory to use per executor process\n            (e.g. '512m').\n        :param executor_cores: Number of cores to use for each executor.\n        :param num_executors: Number of executors to launch for this session.\n        :param archives: URLs of archives to be used in this session.\n        :param queue: The name of the YARN queue to which submitted.\n        :param name: The name of this session.\n        :param spark_conf: Spark configuration properties.\n        \"\"\"\n        if self.legacy_server():\n            valid_kinds = VALID_LEGACY_SESSION_KINDS\n        else:\n            valid_kinds = VALID_SESSION_KINDS\n\n        if kind not in valid_kinds:\n            raise ValueError(\n                f\"{kind} is not a valid session kind for a Livy server of \"\n                f\"this version (should be one of {valid_kinds})\"\n            )\n\n        body = {\"kind\": kind.value}\n        if proxy_user is not None:\n            body[\"proxyUser\"] = proxy_user\n        if jars is not None:\n            body[\"jars\"] = jars\n        if py_files is not None:\n            body[\"pyFiles\"] = py_files\n        if files is not None:\n            body[\"files\"] = files\n        if driver_memory is not None:\n            body[\"driverMemory\"] = driver_memory\n        if driver_cores is not None:\n            body[\"driverCores\"] = driver_cores\n        if executor_memory is not None:\n            body[\"executorMemory\"] = executor_memory\n        if executor_cores is not None:\n            body[\"executorCores\"] = executor_cores\n        if num_executors is not None:\n            body[\"numExecutors\"] = num_executors\n        if archives is not None:\n            body[\"archives\"] = archives\n        if queue is not None:\n            body[\"queue\"] = queue\n        if name is not None:\n            body[\"name\"] = name\n        if spark_conf is not None:\n            body[\"conf\"] = spark_conf\n\n        data = self._client.post(\"/sessions\", data=body)\n        return Session.from_json(data)","return_type":"Session","function_name":"LivyClient.create_session","stripped_code":"def create_session(\n        self,\n        kind: SessionKind,\n        proxy_user: str = None,\n        jars: List[str] = None,\n        py_files: List[str] = None,\n        files: List[str] = None,\n        driver_memory: str = None,\n        driver_cores: int = None,\n        executor_memory: str = None,\n        executor_cores: int = None,\n        num_executors: int = None,\n        archives: List[str] = None,\n        queue: str = None,\n        name: str = None,\n        spark_conf: Dict[str, Any] = None,\n    ):\n        \"\"\"Create a new session in Livy.\n\n        The py_files, files, jars and archives arguments are lists of URLs,\n        e.g. [\"s3://bucket/object\", \"hdfs://path/to/file\", ...] and must be\n        reachable by the Spark driver process.  If the provided URL has no\n        scheme, it's considered to be relative to the default file system\n        configured in the Livy server.\n\n        URLs in the py_files argument are copied to a temporary staging area\n        and inserted into Python's sys.path ahead of the standard library\n        paths.  This allows you to import .py, .zip and .egg files in Python.\n\n        URLs for jars, py_files, files and archives arguments are all copied\n        to the same working directory on the Spark cluster.\n\n        The driver_memory and executor_memory arguments have the same format\n        as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\")\n        (e.g. 512m, 2g).\n\n        See https://spark.apache.org/docs/latest/configuration.html for more\n        information on Spark configuration properties.\n\n        :param kind: The kind of session to create.\n        :param proxy_user: User to impersonate when starting the session.\n        :param jars: URLs of jars to be used in this session.\n        :param py_files: URLs of Python files to be used in this session.\n        :param files: URLs of files to be used in this session.\n        :param driver_memory: Amount of memory to use for the driver process\n            (e.g. '512m').\n        :param driver_cores: Number of cores to use for the driver process.\n        :param executor_memory: Amount of memory to use per executor process\n            (e.g. '512m').\n        :param executor_cores: Number of cores to use for each executor.\n        :param num_executors: Number of executors to launch for this session.\n        :param archives: URLs of archives to be used in this session.\n        :param queue: The name of the YARN queue to which submitted.\n        :param name: The name of this session.\n        :param spark_conf: Spark configuration properties.\n        \"\"\"\n        if self.legacy_server():\n            valid_kinds = VALID_LEGACY_SESSION_KINDS\n        else:\n            valid_kinds = VALID_SESSION_KINDS\n\n        if kind not in valid_kinds:\n            raise ValueError(\n                f\"{kind} is not a valid session kind for a Livy server of \"\n                f\"this version (should be one of {valid_kinds})\"\n            )\n\n        body = {\"kind\": kind.value}\n        if proxy_user is not None:\n            body[\"proxyUser\"] = proxy_user\n        if jars is not None:\n            body[\"jars\"] = jars\n        if py_files is not None:\n            body[\"pyFiles\"] = py_files\n        if files is not None:\n            body[\"files\"] = files\n        if driver_memory is not None:\n            body[\"driverMemory\"] = driver_memory\n        if driver_cores is not None:\n            body[\"driverCores\"] = driver_cores\n        if executor_memory is not None:\n            body[\"executorMemory\"] = executor_memory\n        if executor_cores is not None:\n            body[\"executorCores\"] = executor_cores\n        if num_executors is not None:\n            body[\"numExecutors\"] = num_executors\n        if archives is not None:\n            body[\"archives\"] = archives\n        if queue is not None:\n            body[\"queue\"] = queue\n        if name is not None:\n            body[\"name\"] = name\n        if spark_conf is not None:\n            body[\"conf\"] = spark_conf\n\n        data = self._client.post(\"/sessions\", data=body)\n        return Session.from_json(data)"}
{"code":"def _latex_format(obj: Any) -> str:\n    \"\"\"Format an object as a latex string.\"\"\"\n    if isinstance(obj, float):\n        try:\n            return sympy.latex(symbolize(obj))\n        except ValueError:\n            return \"{0:.4g}\".format(obj)\n\n    return str(obj)","return_type":"str","function_name":"_latex_format","stripped_code":"def _latex_format(obj: Any):\n    \"\"\"Format an object as a latex string.\"\"\"\n    if isinstance(obj, float):\n        try:\n            return sympy.latex(symbolize(obj))\n        except ValueError:\n            return \"{0:.4g}\".format(obj)\n\n    return str(obj)"}
{"code":"def import_string(dotted_path: str) -> Any:\n    \"\"\"\n    Stolen approximately from django. Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import fails.\n    \"\"\"\n    try:\n        module_path, class_name = dotted_path.strip(' ').rsplit('.', 1)\n    except ValueError as e:\n        raise ImportError(f'\"{dotted_path}\" doesn\\'t look like a module path') from e\n\n    module = import_module(module_path)\n    try:\n        return getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(f'Module \"{module_path}\" does not define a \"{class_name}\" attribute') from e","return_type":"Any","function_name":"import_string","stripped_code":"def import_string(dotted_path: str):\n    \"\"\"\n    Stolen approximately from django. Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import fails.\n    \"\"\"\n    try:\n        module_path, class_name = dotted_path.strip(' ').rsplit('.', 1)\n    except ValueError as e:\n        raise ImportError(f'\"{dotted_path}\" doesn\\'t look like a module path') from e\n\n    module = import_module(module_path)\n    try:\n        return getattr(module, class_name)\n    except AttributeError as e:\n        raise ImportError(f'Module \"{module_path}\" does not define a \"{class_name}\" attribute') from e"}
{"code":"def end(self, value: typing.Union[float, typing.Tuple[float, float]]) -> None:\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        ...","return_type":"None","function_name":"Graphic.end","stripped_code":"def end(self, value: typing.Union[float, typing.Tuple[float, float]]):\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        ..."}
{"code":"def read_10x_h5(filename, genome=None, gex_only=True) -> AnnData:\n    \"\"\"Read 10x-Genomics-formatted hdf5 file.\n\n    Parameters\n    ----------\n    filename : `str` | :class:`~pathlib.Path`\n        Filename.\n    genome : `str`, optional (default: ``None``)\n        Filter expression to this genes within this genome. For legacy 10x h5\n        files, this must be provided if the data contains more than one genome.\n    gex_only : `bool`, optional (default: `True`)\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n\n    Returns\n    -------\n    Annotated data matrix, where obsevations/cells are named by their\n    barcode and variables/genes by gene name. The data matrix is stored in\n    `adata.X`, cell names in `adata.obs_names` and gene names in\n    `adata.var_names`. The gene IDs are stored in `adata.var['gene_ids']`.\n    The feature types are stored in `adata.var['feature_types']`\n    \"\"\"\n    logg.info('reading', filename, r=True, end=' ')\n    with tables.open_file(str(filename), 'r') as f:\n        v3 = '/matrix' in f\n    if v3:\n        adata = _read_v3_10x_h5(filename)\n        if genome:\n            if genome not in adata.var['genome'].values:\n                raise ValueError(\n                    \"Could not find data corresponding to genome '{genome}' in '{filename}'. \"\n                    \"Available genomes are: {avail}.\"\n                    .format(\n                        genome=genome, filename=filename,\n                        avail=list(adata.var[\"genome\"].unique()),\n                    )\n                )\n            adata = adata[:, list(map(lambda x: x == str(genome), adata.var['genome']))]\n        if gex_only:\n            adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]\n        return adata\n    else:\n        return _read_legacy_10x_h5(filename, genome=genome)","return_type":"AnnData","function_name":"read_10x_h5","stripped_code":"def read_10x_h5(filename, genome=None, gex_only=True):\n    \"\"\"Read 10x-Genomics-formatted hdf5 file.\n\n    Parameters\n    ----------\n    filename : `str` | :class:`~pathlib.Path`\n        Filename.\n    genome : `str`, optional (default: ``None``)\n        Filter expression to this genes within this genome. For legacy 10x h5\n        files, this must be provided if the data contains more than one genome.\n    gex_only : `bool`, optional (default: `True`)\n        Only keep 'Gene Expression' data and ignore other feature types,\n        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'\n\n    Returns\n    -------\n    Annotated data matrix, where obsevations/cells are named by their\n    barcode and variables/genes by gene name. The data matrix is stored in\n    `adata.X`, cell names in `adata.obs_names` and gene names in\n    `adata.var_names`. The gene IDs are stored in `adata.var['gene_ids']`.\n    The feature types are stored in `adata.var['feature_types']`\n    \"\"\"\n    logg.info('reading', filename, r=True, end=' ')\n    with tables.open_file(str(filename), 'r') as f:\n        v3 = '/matrix' in f\n    if v3:\n        adata = _read_v3_10x_h5(filename)\n        if genome:\n            if genome not in adata.var['genome'].values:\n                raise ValueError(\n                    \"Could not find data corresponding to genome '{genome}' in '{filename}'. \"\n                    \"Available genomes are: {avail}.\"\n                    .format(\n                        genome=genome, filename=filename,\n                        avail=list(adata.var[\"genome\"].unique()),\n                    )\n                )\n            adata = adata[:, list(map(lambda x: x == str(genome), adata.var['genome']))]\n        if gex_only:\n            adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]\n        return adata\n    else:\n        return _read_legacy_10x_h5(filename, genome=genome)"}
{"code":"def patch(self, patch: int) -> None:\n        \"\"\"\n        param patch\n\n        Patch version number property. Must be a non-negative integer.\n        \"\"\"\n        self.filter_negatives(patch)\n        self._patch = patch","return_type":"None","function_name":"SemVer.patch","stripped_code":"def patch(self, patch: int):\n        \"\"\"\n        param patch\n\n        Patch version number property. Must be a non-negative integer.\n        \"\"\"\n        self.filter_negatives(patch)\n        self._patch = patch"}
{"code":"def convert_attrs_to_lowercase(obj: Any, attrs: Iterable[str]) -> None:\n    \"\"\"\n    Converts the specified attributes of an object to lower case, modifying\n    the object in place.\n    \"\"\"\n    for a in attrs:\n        value = getattr(obj, a)\n        if value is None:\n            continue\n        setattr(obj, a, value.lower())","return_type":"None","function_name":"convert_attrs_to_lowercase","stripped_code":"def convert_attrs_to_lowercase(obj: Any, attrs: Iterable[str]):\n    \"\"\"\n    Converts the specified attributes of an object to lower case, modifying\n    the object in place.\n    \"\"\"\n    for a in attrs:\n        value = getattr(obj, a)\n        if value is None:\n            continue\n        setattr(obj, a, value.lower())"}
{"code":"def _disable_prometheus_process_collector(self) -> None:\n        \"\"\"\n        There is a bug in SDC' Docker implementation and intolerable prometheus_client code, due to which\n        its process_collector will fail.\n\n        See https://github.com/prometheus/client_python/issues/80\n        \"\"\"\n        logger.info(\"Removing prometheus process collector\")\n        try:\n            core.REGISTRY.unregister(PROCESS_COLLECTOR)\n        except KeyError:\n            logger.debug(\"PROCESS_COLLECTOR already removed from prometheus\")","return_type":"None","function_name":"Metrics._disable_prometheus_process_collector","stripped_code":"def _disable_prometheus_process_collector(self):\n        \"\"\"\n        There is a bug in SDC' Docker implementation and intolerable prometheus_client code, due to which\n        its process_collector will fail.\n\n        See https://github.com/prometheus/client_python/issues/80\n        \"\"\"\n        logger.info(\"Removing prometheus process collector\")\n        try:\n            core.REGISTRY.unregister(PROCESS_COLLECTOR)\n        except KeyError:\n            logger.debug(\"PROCESS_COLLECTOR already removed from prometheus\")"}
{"code":"def leave_multicast(self, universe: int) -> None:\n        \"\"\"\n        Try to leave the multicast group with the specified universe. This does not throw any exception if the group\n        could not be leaved.\n        :param universe: the universe to leave the multicast group.\n        The network hardware has to support the multicast feature!\n        \"\"\"\n        try:\n            self.sock.setsockopt(socket.SOL_IP, socket.IP_DROP_MEMBERSHIP,\n                                 socket.inet_aton(calculate_multicast_addr(universe)) +\n                                 socket.inet_aton(self._bindAddress))\n        except:  # try to leave the multicast group for the universe\n            pass","return_type":"None","function_name":"sACNreceiver.leave_multicast","stripped_code":"def leave_multicast(self, universe: int):\n        \"\"\"\n        Try to leave the multicast group with the specified universe. This does not throw any exception if the group\n        could not be leaved.\n        :param universe: the universe to leave the multicast group.\n        The network hardware has to support the multicast feature!\n        \"\"\"\n        try:\n            self.sock.setsockopt(socket.SOL_IP, socket.IP_DROP_MEMBERSHIP,\n                                 socket.inet_aton(calculate_multicast_addr(universe)) +\n                                 socket.inet_aton(self._bindAddress))\n        except:  # try to leave the multicast group for the universe\n            pass"}
{"code":"def drop_bel_namespace(self) -> Optional[Namespace]:\n        \"\"\"Remove the default namespace if it exists.\"\"\"\n        namespace = self._get_default_namespace()\n\n        if namespace is not None:\n            for entry in tqdm(namespace.entries, desc=f'deleting entries in {self._get_namespace_name()}'):\n                self.session.delete(entry)\n            self.session.delete(namespace)\n\n            log.info('committing deletions')\n            self.session.commit()\n            return namespace","return_type":"Optional[Namespace]","function_name":"BELNamespaceManagerMixin.drop_bel_namespace","stripped_code":"def drop_bel_namespace(self):\n        \"\"\"Remove the default namespace if it exists.\"\"\"\n        namespace = self._get_default_namespace()\n\n        if namespace is not None:\n            for entry in tqdm(namespace.entries, desc=f'deleting entries in {self._get_namespace_name()}'):\n                self.session.delete(entry)\n            self.session.delete(namespace)\n\n            log.info('committing deletions')\n            self.session.commit()\n            return namespace"}
{"code":"def in_reply_to(self) -> Optional[UnstructuredHeader]:\n        \"\"\"The ``In-Reply-To`` header.\"\"\"\n        try:\n            return cast(UnstructuredHeader, self[b'in-reply-to'][0])\n        except (KeyError, IndexError):\n            return None","return_type":"Optional[UnstructuredHeader]","function_name":"ParsedHeaders.in_reply_to","stripped_code":"def in_reply_to(self):\n        \"\"\"The ``In-Reply-To`` header.\"\"\"\n        try:\n            return cast(UnstructuredHeader, self[b'in-reply-to'][0])\n        except (KeyError, IndexError):\n            return None"}
{"code":"def sort(coll, f=None) -> Optional[ISeq]:\n    \"\"\"Return a sorted sequence of the elements in coll. If a comparator\n    function f is provided, compare elements in coll using f.\"\"\"\n    return to_seq(sorted(coll, key=Maybe(f).map(functools.cmp_to_key).value))","return_type":"Optional[ISeq]","function_name":"sort","stripped_code":"def sort(coll, f=None):\n    \"\"\"Return a sorted sequence of the elements in coll. If a comparator\n    function f is provided, compare elements in coll using f.\"\"\"\n    return to_seq(sorted(coll, key=Maybe(f).map(functools.cmp_to_key).value))"}
{"code":"def run(self) -> None:\n        \"\"\"Runs the worker and consumes messages from RabbitMQ.\n        Returns only after `shutdown()` is called.\n\n        \"\"\"\n        if self._logging_level:\n            logging.basicConfig(\n                level=getattr(logging, self._logging_level.upper()),\n                format=\"%(levelname).1s %(name)s.%(funcName)s:%(lineno)d - %(message)s\")\n\n        signal.signal(signal.SIGINT, self._handle_sigint)\n        signal.signal(signal.SIGTERM, self._handle_sigterm)\n        if platform.system() != 'Windows':\n            # These features will not be available on Windows, but that is OK.\n            # Read this issue for more details:\n            # https://github.com/cenkalti/kuyruk/issues/54\n            signal.signal(signal.SIGHUP, self._handle_sighup)\n            signal.signal(signal.SIGUSR1, self._handle_sigusr1)\n            signal.signal(signal.SIGUSR2, self._handle_sigusr2)\n\n        self._started_at = os.times().elapsed\n\n        for t in self._threads:\n            t.start()\n\n        try:\n            signals.worker_start.send(self.kuyruk, worker=self)\n            self._consume_messages()\n            signals.worker_shutdown.send(self.kuyruk, worker=self)\n        finally:\n            self.shutdown_pending.set()\n            for t in self._threads:\n                t.join()\n\n        logger.debug(\"End run worker\")","return_type":"None","function_name":"Worker.run","stripped_code":"def run(self):\n        \"\"\"Runs the worker and consumes messages from RabbitMQ.\n        Returns only after `shutdown()` is called.\n\n        \"\"\"\n        if self._logging_level:\n            logging.basicConfig(\n                level=getattr(logging, self._logging_level.upper()),\n                format=\"%(levelname).1s %(name)s.%(funcName)s:%(lineno)d - %(message)s\")\n\n        signal.signal(signal.SIGINT, self._handle_sigint)\n        signal.signal(signal.SIGTERM, self._handle_sigterm)\n        if platform.system() != 'Windows':\n            # These features will not be available on Windows, but that is OK.\n            # Read this issue for more details:\n            # https://github.com/cenkalti/kuyruk/issues/54\n            signal.signal(signal.SIGHUP, self._handle_sighup)\n            signal.signal(signal.SIGUSR1, self._handle_sigusr1)\n            signal.signal(signal.SIGUSR2, self._handle_sigusr2)\n\n        self._started_at = os.times().elapsed\n\n        for t in self._threads:\n            t.start()\n\n        try:\n            signals.worker_start.send(self.kuyruk, worker=self)\n            self._consume_messages()\n            signals.worker_shutdown.send(self.kuyruk, worker=self)\n        finally:\n            self.shutdown_pending.set()\n            for t in self._threads:\n                t.join()\n\n        logger.debug(\"End run worker\")"}
{"code":"def BooleanTake(input_vertex: vertex_constructor_param_types, index: Collection[int], label: Optional[str]=None) -> Vertex:\n    \"\"\"\n    A vertex that extracts a scalar at a given index\n    \n    :param input_vertex: the input vertex to extract from\n    :param index: the index to extract at\n    \"\"\"\n    return Boolean(context.jvm_view().BooleanTakeVertex, label, cast_to_vertex(input_vertex), cast_to_long_array(index))","return_type":"Vertex","function_name":"BooleanTake","stripped_code":"def BooleanTake(input_vertex: vertex_constructor_param_types, index: Collection[int], label: Optional[str]=None):\n    \"\"\"\n    A vertex that extracts a scalar at a given index\n    \n    :param input_vertex: the input vertex to extract from\n    :param index: the index to extract at\n    \"\"\"\n    return Boolean(context.jvm_view().BooleanTakeVertex, label, cast_to_vertex(input_vertex), cast_to_long_array(index))"}
{"code":"def availability_pdf() -> bool:\n    \"\"\"\n    Is a PDF-to-text tool available?\n    \"\"\"\n    pdftotext = tools['pdftotext']\n    if pdftotext:\n        return True\n    elif pdfminer:\n        log.warning(\"PDF conversion: pdftotext missing; \"\n                    \"using pdfminer (less efficient)\")\n        return True\n    else:\n        return False","return_type":"bool","function_name":"availability_pdf","stripped_code":"def availability_pdf():\n    \"\"\"\n    Is a PDF-to-text tool available?\n    \"\"\"\n    pdftotext = tools['pdftotext']\n    if pdftotext:\n        return True\n    elif pdfminer:\n        log.warning(\"PDF conversion: pdftotext missing; \"\n                    \"using pdfminer (less efficient)\")\n        return True\n    else:\n        return False"}
{"code":"def get_annotations(self) -> Dict:\n        \"\"\"Get the current annotations.\"\"\"\n        return {\n            EVIDENCE: self.evidence,\n            CITATION: self.citation.copy(),\n            ANNOTATIONS: self.annotations.copy()\n        }","return_type":"Dict","function_name":"ControlParser.get_annotations","stripped_code":"def get_annotations(self):\n        \"\"\"Get the current annotations.\"\"\"\n        return {\n            EVIDENCE: self.evidence,\n            CITATION: self.citation.copy(),\n            ANNOTATIONS: self.annotations.copy()\n        }"}
{"code":"def unban_chat_member(\n        self,\n        chat_id: Union[int, str],\n        user_id: Union[int, str]\n    ) -> bool:\n        \"\"\"Use this method to unban a previously kicked user in a supergroup or channel.\n        The user will **not** return to the group or channel automatically, but will be able to join via link, etc.\n        You must be an administrator for this to work.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n\n            user_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target user.\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        self.send(\n            functions.channels.EditBanned(\n                channel=self.resolve_peer(chat_id),\n                user_id=self.resolve_peer(user_id),\n                banned_rights=types.ChatBannedRights(\n                    until_date=0\n                )\n            )\n        )\n\n        return True","return_type":"bool","function_name":"UnbanChatMember.unban_chat_member","stripped_code":"def unban_chat_member(\n        self,\n        chat_id: Union[int, str],\n        user_id: Union[int, str]\n    ):\n        \"\"\"Use this method to unban a previously kicked user in a supergroup or channel.\n        The user will **not** return to the group or channel automatically, but will be able to join via link, etc.\n        You must be an administrator for this to work.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n\n            user_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target user.\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        self.send(\n            functions.channels.EditBanned(\n                channel=self.resolve_peer(chat_id),\n                user_id=self.resolve_peer(user_id),\n                banned_rights=types.ChatBannedRights(\n                    until_date=0\n                )\n            )\n        )\n\n        return True"}
{"code":"def load_forcing_grid(path_runcontrol: str, grid: int)->pd.DataFrame:\n    '''Load forcing data for a specific grid included in the index of `df_state_init </data-structure/supy-io.ipynb#df_state_init:-model-initial-states>`.\n\n    Parameters\n    ----------\n    path_runcontrol : str\n        Path to SUEWS :ref:`RunControl.nml <suews:RunControl.nml>`\n    grid : int\n        Grid number\n\n    Returns\n    -------\n    df_forcing: pandas.DataFrame\n        Forcing data. See `df_forcing_var` for details.\n\n    Examples\n    --------\n    >>> path_runcontrol = \"~/SUEWS_sims/RunControl.nml\"  # a valid path to `RunControl.nml`\n    >>> df_state_init = supy.init_supy(path_runcontrol) # get `df_state_init`\n    >>> grid = df_state_init.index[0] # first grid number included in `df_state_init`\n    >>> df_forcing = supy.load_forcing_grid(path_runcontrol, grid) # get df_forcing\n\n\n    '''\n\n    try:\n        path_runcontrol = Path(path_runcontrol).expanduser().resolve()\n    except FileNotFoundError:\n        print('{path} does not exists!'.format(path=path_runcontrol))\n    else:\n        dict_mod_cfg = load_SUEWS_dict_ModConfig(path_runcontrol)\n        df_state_init = init_supy(path_runcontrol)\n\n        # load setting variables from dict_mod_cfg\n        (\n            filecode,\n            kdownzen,\n            tstep_met_in,\n            tstep_ESTM_in,\n            multiplemetfiles,\n            multipleestmfiles,\n            dir_input_cfg\n        ) = (dict_mod_cfg[x] for x in\n             [\n            'filecode',\n            'kdownzen',\n            'resolutionfilesin',\n            'resolutionfilesinestm',\n            'multiplemetfiles',\n            'multipleestmfiles',\n            'fileinputpath'\n        ]\n        )\n        tstep_mod, lat, lon, alt, timezone = df_state_init.loc[\n            grid,\n            [(x, '0') for x in ['tstep', 'lat', 'lng', 'alt', 'timezone']]\n        ].values\n\n        path_site = path_runcontrol.parent\n        path_input = path_site / dict_mod_cfg['fileinputpath']\n\n        # load raw data\n        # met forcing\n        df_forcing_met = load_SUEWS_Forcing_met_df_raw(\n            path_input, filecode, grid, tstep_met_in, multiplemetfiles)\n\n        # resample raw data from tstep_in to tstep_mod\n        df_forcing_met_tstep = resample_forcing_met(\n            df_forcing_met, tstep_met_in, tstep_mod,\n            lat, lon, alt, timezone, kdownzen)\n\n        # merge forcing datasets (met and ESTM)\n        df_forcing_tstep = df_forcing_met_tstep.copy()\n\n        # disable the AnOHM and ESTM components for now and for better performance\n        # |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n        # TS 28 Dec 2018\n        # pack all records of `id` into `metforcingdata_grid` for AnOHM\n        # df_grp = df_forcing_tstep.groupby('id')\n        # dict_id_all = {xid: df_grp.get_group(xid)\n        #                for xid in df_forcing_tstep['id'].unique()}\n        # id_all = df_forcing_tstep['id'].apply(lambda xid: dict_id_all[xid])\n        # df_forcing_tstep = df_forcing_tstep.merge(\n        #     id_all.to_frame(name='metforcingdata_grid'),\n        #     left_index=True,\n        #     right_index=True)\n        # # add Ts forcing for ESTM\n        # if np.asscalar(df_state_init.iloc[0]['storageheatmethod'].values) == 4:\n        #     # load ESTM forcing\n        #     df_forcing_estm = load_SUEWS_Forcing_ESTM_df_raw(\n        #         path_input, filecode, grid, tstep_ESTM_in, multipleestmfiles)\n        #     # resample raw data from tstep_in to tstep_mod\n        #     df_forcing_estm_tstep = resample_linear(\n        #         df_forcing_estm, tstep_met_in, tstep_mod)\n        #     df_forcing_tstep = df_forcing_tstep.merge(\n        #         df_forcing_estm_tstep,\n        #         left_on=['iy', 'id', 'it', 'imin'],\n        #         right_on=['iy', 'id', 'it', 'imin'])\n        #     # insert `ts5mindata_ir` into df_forcing_tstep\n        #     ts_col = df_forcing_estm.columns[4:]\n        #     df_forcing_tstep['ts5mindata_ir'] = (\n        #         df_forcing_tstep.loc[:, ts_col].values.tolist())\n        #     df_forcing_tstep['ts5mindata_ir'] = df_forcing_tstep[\n        #         'ts5mindata_ir'].map(lambda x: np.array(x, order='F'))\n        # else:\n        #     # insert some placeholder values\n        #     df_forcing_tstep['ts5mindata_ir'] = df_forcing_tstep['Tair']\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        # disable the AnOHM and ESTM components for now and for better performance\n\n        # coerced precision here to prevent numerical errors inside Fortran\n        df_forcing = np.around(df_forcing_tstep, decimals=10)\n        # new columns for later use in main calculation\n        df_forcing[['iy', 'id', 'it', 'imin']] = df_forcing[[\n            'iy', 'id', 'it', 'imin']].astype(np.int64)\n\n    return df_forcing","return_type":"pd.DataFrame","function_name":"load_forcing_grid","stripped_code":"def load_forcing_grid(path_runcontrol: str, grid: int):\n    '''Load forcing data for a specific grid included in the index of `df_state_init </data-structure/supy-io.ipynb#df_state_init:-model-initial-states>`.\n\n    Parameters\n    ----------\n    path_runcontrol : str\n        Path to SUEWS :ref:`RunControl.nml <suews:RunControl.nml>`\n    grid : int\n        Grid number\n\n    Returns\n    -------\n    df_forcing: pandas.DataFrame\n        Forcing data. See `df_forcing_var` for details.\n\n    Examples\n    --------\n    >>> path_runcontrol = \"~/SUEWS_sims/RunControl.nml\"  # a valid path to `RunControl.nml`\n    >>> df_state_init = supy.init_supy(path_runcontrol) # get `df_state_init`\n    >>> grid = df_state_init.index[0] # first grid number included in `df_state_init`\n    >>> df_forcing = supy.load_forcing_grid(path_runcontrol, grid) # get df_forcing\n\n\n    '''\n\n    try:\n        path_runcontrol = Path(path_runcontrol).expanduser().resolve()\n    except FileNotFoundError:\n        print('{path} does not exists!'.format(path=path_runcontrol))\n    else:\n        dict_mod_cfg = load_SUEWS_dict_ModConfig(path_runcontrol)\n        df_state_init = init_supy(path_runcontrol)\n\n        # load setting variables from dict_mod_cfg\n        (\n            filecode,\n            kdownzen,\n            tstep_met_in,\n            tstep_ESTM_in,\n            multiplemetfiles,\n            multipleestmfiles,\n            dir_input_cfg\n        ) = (dict_mod_cfg[x] for x in\n             [\n            'filecode',\n            'kdownzen',\n            'resolutionfilesin',\n            'resolutionfilesinestm',\n            'multiplemetfiles',\n            'multipleestmfiles',\n            'fileinputpath'\n        ]\n        )\n        tstep_mod, lat, lon, alt, timezone = df_state_init.loc[\n            grid,\n            [(x, '0') for x in ['tstep', 'lat', 'lng', 'alt', 'timezone']]\n        ].values\n\n        path_site = path_runcontrol.parent\n        path_input = path_site / dict_mod_cfg['fileinputpath']\n\n        # load raw data\n        # met forcing\n        df_forcing_met = load_SUEWS_Forcing_met_df_raw(\n            path_input, filecode, grid, tstep_met_in, multiplemetfiles)\n\n        # resample raw data from tstep_in to tstep_mod\n        df_forcing_met_tstep = resample_forcing_met(\n            df_forcing_met, tstep_met_in, tstep_mod,\n            lat, lon, alt, timezone, kdownzen)\n\n        # merge forcing datasets (met and ESTM)\n        df_forcing_tstep = df_forcing_met_tstep.copy()\n\n        # disable the AnOHM and ESTM components for now and for better performance\n        # |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n        # TS 28 Dec 2018\n        # pack all records of `id` into `metforcingdata_grid` for AnOHM\n        # df_grp = df_forcing_tstep.groupby('id')\n        # dict_id_all = {xid: df_grp.get_group(xid)\n        #                for xid in df_forcing_tstep['id'].unique()}\n        # id_all = df_forcing_tstep['id'].apply(lambda xid: dict_id_all[xid])\n        # df_forcing_tstep = df_forcing_tstep.merge(\n        #     id_all.to_frame(name='metforcingdata_grid'),\n        #     left_index=True,\n        #     right_index=True)\n        # # add Ts forcing for ESTM\n        # if np.asscalar(df_state_init.iloc[0]['storageheatmethod'].values) == 4:\n        #     # load ESTM forcing\n        #     df_forcing_estm = load_SUEWS_Forcing_ESTM_df_raw(\n        #         path_input, filecode, grid, tstep_ESTM_in, multipleestmfiles)\n        #     # resample raw data from tstep_in to tstep_mod\n        #     df_forcing_estm_tstep = resample_linear(\n        #         df_forcing_estm, tstep_met_in, tstep_mod)\n        #     df_forcing_tstep = df_forcing_tstep.merge(\n        #         df_forcing_estm_tstep,\n        #         left_on=['iy', 'id', 'it', 'imin'],\n        #         right_on=['iy', 'id', 'it', 'imin'])\n        #     # insert `ts5mindata_ir` into df_forcing_tstep\n        #     ts_col = df_forcing_estm.columns[4:]\n        #     df_forcing_tstep['ts5mindata_ir'] = (\n        #         df_forcing_tstep.loc[:, ts_col].values.tolist())\n        #     df_forcing_tstep['ts5mindata_ir'] = df_forcing_tstep[\n        #         'ts5mindata_ir'].map(lambda x: np.array(x, order='F'))\n        # else:\n        #     # insert some placeholder values\n        #     df_forcing_tstep['ts5mindata_ir'] = df_forcing_tstep['Tair']\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        # disable the AnOHM and ESTM components for now and for better performance\n\n        # coerced precision here to prevent numerical errors inside Fortran\n        df_forcing = np.around(df_forcing_tstep, decimals=10)\n        # new columns for later use in main calculation\n        df_forcing[['iy', 'id', 'it', 'imin']] = df_forcing[[\n            'iy', 'id', 'it', 'imin']].astype(np.int64)\n\n    return df_forcing"}
{"code":"def _verify_deployed_contract(\n            self,\n            deployment_data: DeployedContracts,\n            contract_name: str,\n    ) -> Contract:\n        \"\"\" Verify deployment info against the chain\n\n        Verifies:\n        - the runtime bytecode - precompiled data against the chain\n        - information stored in deployment_*.json against the chain,\n        except for the constructor arguments, which have to be checked\n        separately.\n\n        Returns: (onchain_instance, constructor_arguments)\n        \"\"\"\n        contracts = deployment_data['contracts']\n\n        contract_address = contracts[contract_name]['address']\n        contract_instance = self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=contract_address,\n        )\n\n        # Check that the deployed bytecode matches the precompiled data\n        blockchain_bytecode = self.web3.eth.getCode(contract_address).hex()\n        compiled_bytecode = self.contract_manager.get_runtime_hexcode(contract_name)\n        assert blockchain_bytecode == compiled_bytecode\n\n        print(\n            f'{contract_name} at {contract_address} '\n            f'matches the compiled data from contracts.json',\n        )\n\n        # Check blockchain transaction hash & block information\n        receipt = self.web3.eth.getTransactionReceipt(\n            contracts[contract_name]['transaction_hash'],\n        )\n        assert receipt['blockNumber'] == contracts[contract_name]['block_number'], (\n            f'We have block_number {contracts[contract_name][\"block_number\"]} in the deployment '\n            f'info, but {receipt[\"blockNumber\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['gasUsed'] == contracts[contract_name]['gas_cost'], (\n            f'We have gasUsed {contracts[contract_name][\"gas_cost\"]} in the deployment info, '\n            f'but {receipt[\"gasUsed\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['contractAddress'] == contracts[contract_name]['address'], (\n            f'We have contractAddress {contracts[contract_name][\"address\"]} in the deployment info'\n            f' but {receipt[\"contractAddress\"]} in the transaction receipt from web3.'\n        )\n\n        # Check the contract version\n        version = contract_instance.functions.contract_version().call()\n        assert version == deployment_data['contracts_version'], \\\n            f'got {version} expected {deployment_data[\"contracts_version\"]}.' \\\n            f'contract_manager has contracts_version {self.contract_manager.contracts_version}'\n\n        return contract_instance, contracts[contract_name]['constructor_arguments']","return_type":"Contract","function_name":"ContractVerifier._verify_deployed_contract","stripped_code":"def _verify_deployed_contract(\n            self,\n            deployment_data: DeployedContracts,\n            contract_name: str,\n    ):\n        \"\"\" Verify deployment info against the chain\n\n        Verifies:\n        - the runtime bytecode - precompiled data against the chain\n        - information stored in deployment_*.json against the chain,\n        except for the constructor arguments, which have to be checked\n        separately.\n\n        Returns: (onchain_instance, constructor_arguments)\n        \"\"\"\n        contracts = deployment_data['contracts']\n\n        contract_address = contracts[contract_name]['address']\n        contract_instance = self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=contract_address,\n        )\n\n        # Check that the deployed bytecode matches the precompiled data\n        blockchain_bytecode = self.web3.eth.getCode(contract_address).hex()\n        compiled_bytecode = self.contract_manager.get_runtime_hexcode(contract_name)\n        assert blockchain_bytecode == compiled_bytecode\n\n        print(\n            f'{contract_name} at {contract_address} '\n            f'matches the compiled data from contracts.json',\n        )\n\n        # Check blockchain transaction hash & block information\n        receipt = self.web3.eth.getTransactionReceipt(\n            contracts[contract_name]['transaction_hash'],\n        )\n        assert receipt['blockNumber'] == contracts[contract_name]['block_number'], (\n            f'We have block_number {contracts[contract_name][\"block_number\"]} in the deployment '\n            f'info, but {receipt[\"blockNumber\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['gasUsed'] == contracts[contract_name]['gas_cost'], (\n            f'We have gasUsed {contracts[contract_name][\"gas_cost\"]} in the deployment info, '\n            f'but {receipt[\"gasUsed\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['contractAddress'] == contracts[contract_name]['address'], (\n            f'We have contractAddress {contracts[contract_name][\"address\"]} in the deployment info'\n            f' but {receipt[\"contractAddress\"]} in the transaction receipt from web3.'\n        )\n\n        # Check the contract version\n        version = contract_instance.functions.contract_version().call()\n        assert version == deployment_data['contracts_version'], \\\n            f'got {version} expected {deployment_data[\"contracts_version\"]}.' \\\n            f'contract_manager has contracts_version {self.contract_manager.contracts_version}'\n\n        return contract_instance, contracts[contract_name]['constructor_arguments']"}
{"code":"def get_income_total(self) -> Decimal:\n        \"\"\" Sum of all income = sum of balances of all income accounts. \"\"\"\n        accounts = self.get_income_accounts()\n        # log(DEBUG, \"income accounts: %s\", accounts)\n        income = Decimal(0)\n        for acct in accounts:\n            income += acct.get_balance()\n        return income","return_type":"Decimal","function_name":"SecurityAggregate.get_income_total","stripped_code":"def get_income_total(self):\n        \"\"\" Sum of all income = sum of balances of all income accounts. \"\"\"\n        accounts = self.get_income_accounts()\n        # log(DEBUG, \"income accounts: %s\", accounts)\n        income = Decimal(0)\n        for acct in accounts:\n            income += acct.get_balance()\n        return income"}
{"code":"def visit_Call(self, node: parsing.Call) -> ast.expr:\n        \"\"\"Generates python code calling the function.\n\n        fn(*args)\n        \"\"\"\n        return ast.Call(\n            ast.Attribute(\n                ast.Name('self', ast.Load),\n                node.callObject.__name__,\n                ast.Load()),\n            [ast.Str(param) for param in node.params],\n            [],\n            None,\n            None)","return_type":"ast.expr","function_name":"RuleVisitor.visit_Call","stripped_code":"def visit_Call(self, node: parsing.Call):\n        \"\"\"Generates python code calling the function.\n\n        fn(*args)\n        \"\"\"\n        return ast.Call(\n            ast.Attribute(\n                ast.Name('self', ast.Load),\n                node.callObject.__name__,\n                ast.Load()),\n            [ast.Str(param) for param in node.params],\n            [],\n            None,\n            None)"}
{"code":"def experimental_design(self) -> Any:\n        \"\"\"Return a markdown summary of the samples on this sample sheet.\n\n        This property supports displaying rendered markdown only when running\n        within an IPython interpreter. If we are not running in an IPython\n        interpreter, then print out a nicely formatted ASCII table.\n\n        Returns:\n            Markdown, str: A visual table of IDs and names for all samples.\n\n        \"\"\"\n        if not self.samples:\n            raise ValueError('No samples in sample sheet')\n\n        markdown = tabulate(\n            [[getattr(s, h, '') for h in DESIGN_HEADER] for s in self.samples],\n            headers=DESIGN_HEADER,\n            tablefmt='pipe',\n        )\n\n        return maybe_render_markdown(markdown)","return_type":"Any","function_name":"SampleSheet.experimental_design","stripped_code":"def experimental_design(self):\n        \"\"\"Return a markdown summary of the samples on this sample sheet.\n\n        This property supports displaying rendered markdown only when running\n        within an IPython interpreter. If we are not running in an IPython\n        interpreter, then print out a nicely formatted ASCII table.\n\n        Returns:\n            Markdown, str: A visual table of IDs and names for all samples.\n\n        \"\"\"\n        if not self.samples:\n            raise ValueError('No samples in sample sheet')\n\n        markdown = tabulate(\n            [[getattr(s, h, '') for h in DESIGN_HEADER] for s in self.samples],\n            headers=DESIGN_HEADER,\n            tablefmt='pipe',\n        )\n\n        return maybe_render_markdown(markdown)"}
{"code":"def get_client_username(request: AxesHttpRequest, credentials: dict = None) -> str:\n    \"\"\"\n    Resolve client username from the given request or credentials if supplied.\n\n    The order of preference for fetching the username is as follows:\n\n    1. If configured, use ``AXES_USERNAME_CALLABLE``, and supply ``request, credentials`` as arguments\n    2. If given, use ``credentials`` and fetch username from ``AXES_USERNAME_FORM_FIELD`` (defaults to ``username``)\n    3. Use request.POST and fetch username from ``AXES_USERNAME_FORM_FIELD`` (defaults to ``username``)\n\n    :param request: incoming Django ``HttpRequest`` or similar object from authentication backend or other source\n    :param credentials: incoming credentials ``dict`` or similar object from authentication backend or other source\n    \"\"\"\n\n    if settings.AXES_USERNAME_CALLABLE:\n        log.debug('Using settings.AXES_USERNAME_CALLABLE to get username')\n\n        if callable(settings.AXES_USERNAME_CALLABLE):\n            return settings.AXES_USERNAME_CALLABLE(request, credentials)\n        if isinstance(settings.AXES_USERNAME_CALLABLE, str):\n            return import_string(settings.AXES_USERNAME_CALLABLE)(request, credentials)\n        raise TypeError('settings.AXES_USERNAME_CALLABLE needs to be a string, callable, or None.')\n\n    if credentials:\n        log.debug('Using parameter credentials to get username with key settings.AXES_USERNAME_FORM_FIELD')\n        return credentials.get(settings.AXES_USERNAME_FORM_FIELD, None)\n\n    log.debug('Using parameter request.POST to get username with key settings.AXES_USERNAME_FORM_FIELD')\n    return request.POST.get(settings.AXES_USERNAME_FORM_FIELD, None)","return_type":"str","function_name":"get_client_username","stripped_code":"def get_client_username(request: AxesHttpRequest, credentials: dict = None):\n    \"\"\"\n    Resolve client username from the given request or credentials if supplied.\n\n    The order of preference for fetching the username is as follows:\n\n    1. If configured, use ``AXES_USERNAME_CALLABLE``, and supply ``request, credentials`` as arguments\n    2. If given, use ``credentials`` and fetch username from ``AXES_USERNAME_FORM_FIELD`` (defaults to ``username``)\n    3. Use request.POST and fetch username from ``AXES_USERNAME_FORM_FIELD`` (defaults to ``username``)\n\n    :param request: incoming Django ``HttpRequest`` or similar object from authentication backend or other source\n    :param credentials: incoming credentials ``dict`` or similar object from authentication backend or other source\n    \"\"\"\n\n    if settings.AXES_USERNAME_CALLABLE:\n        log.debug('Using settings.AXES_USERNAME_CALLABLE to get username')\n\n        if callable(settings.AXES_USERNAME_CALLABLE):\n            return settings.AXES_USERNAME_CALLABLE(request, credentials)\n        if isinstance(settings.AXES_USERNAME_CALLABLE, str):\n            return import_string(settings.AXES_USERNAME_CALLABLE)(request, credentials)\n        raise TypeError('settings.AXES_USERNAME_CALLABLE needs to be a string, callable, or None.')\n\n    if credentials:\n        log.debug('Using parameter credentials to get username with key settings.AXES_USERNAME_FORM_FIELD')\n        return credentials.get(settings.AXES_USERNAME_FORM_FIELD, None)\n\n    log.debug('Using parameter request.POST to get username with key settings.AXES_USERNAME_FORM_FIELD')\n    return request.POST.get(settings.AXES_USERNAME_FORM_FIELD, None)"}
{"code":"def find_java_home(cratedb_version: tuple) -> str:\n    \"\"\" Return a path to a JAVA_HOME suites for the given CrateDB version \"\"\"\n    if MIN_VERSION_FOR_JVM11 <= cratedb_version < (4, 0):\n        # Supports 8 to 11+, use whatever is set\n        return os.environ.get('JAVA_HOME', '')\n    if cratedb_version < MIN_VERSION_FOR_JVM11:\n        return _find_matching_java_home(lambda ver: ver[0] == 8)\n    else:\n        return _find_matching_java_home(lambda ver: ver[0] >= 11)","return_type":"str","function_name":"find_java_home","stripped_code":"def find_java_home(cratedb_version: tuple):\n    \"\"\" Return a path to a JAVA_HOME suites for the given CrateDB version \"\"\"\n    if MIN_VERSION_FOR_JVM11 <= cratedb_version < (4, 0):\n        # Supports 8 to 11+, use whatever is set\n        return os.environ.get('JAVA_HOME', '')\n    if cratedb_version < MIN_VERSION_FOR_JVM11:\n        return _find_matching_java_home(lambda ver: ver[0] == 8)\n    else:\n        return _find_matching_java_home(lambda ver: ver[0] >= 11)"}
{"code":"def hex2termhex(hexval: str, allow_short: bool = False) -> str:\n    \"\"\" Convert a hex value into the nearest terminal color matched hex. \"\"\"\n    return rgb2termhex(*hex2rgb(hexval, allow_short=allow_short))","return_type":"str","function_name":"hex2termhex","stripped_code":"def hex2termhex(hexval: str, allow_short: bool = False):\n    \"\"\" Convert a hex value into the nearest terminal color matched hex. \"\"\"\n    return rgb2termhex(*hex2rgb(hexval, allow_short=allow_short))"}
{"code":"def do_program(self, program: Program) -> 'AbstractQuantumSimulator':\n        \"\"\"\n        Perform a sequence of gates contained within a program.\n\n        :param program: The program\n        :return: self\n        \"\"\"\n        for gate in program:\n            if not isinstance(gate, Gate):\n                raise ValueError(\"Can only compute the simulate a program composed of `Gate`s\")\n            self.do_gate(gate)\n        return self","return_type":"'AbstractQuantumSimulator'","function_name":"AbstractQuantumSimulator.do_program","stripped_code":"def do_program(self, program: Program):\n        \"\"\"\n        Perform a sequence of gates contained within a program.\n\n        :param program: The program\n        :return: self\n        \"\"\"\n        for gate in program:\n            if not isinstance(gate, Gate):\n                raise ValueError(\"Can only compute the simulate a program composed of `Gate`s\")\n            self.do_gate(gate)\n        return self"}
{"code":"def normalize_attr_strings(a: np.ndarray) -> np.ndarray:\n\t\"\"\"\n\tTake an np.ndarray of all kinds of string-like elements, and return an array of ascii (np.string_) objects\n\t\"\"\"\n\tif np.issubdtype(a.dtype, np.object_):\n\t\t# if np.all([type(x) is str for x in a]) or np.all([type(x) is np.str_ for x in a]) or np.all([type(x) is np.unicode_ for x in a]):\n\t\tif np.all([(type(x) is str or type(x) is np.str_ or type(x) is np.unicode_) for x in a]):\n\t\t\treturn np.array([x.encode('ascii', 'xmlcharrefreplace') for x in a])\n\t\telif np.all([type(x) is np.string_ for x in a]) or np.all([type(x) is np.bytes_ for x in a]):\n\t\t\treturn a.astype(\"string_\")\n\t\telse:\n\t\t\tlogging.debug(f\"Attribute contains mixed object types ({np.unique([str(type(x)) for x in a])}); casting all to string\")\n\t\t\treturn np.array([str(x) for x in a], dtype=\"string_\")\n\telif np.issubdtype(a.dtype, np.string_) or np.issubdtype(a.dtype, np.object_):\n\t\treturn a\n\telif np.issubdtype(a.dtype, np.str_) or np.issubdtype(a.dtype, np.unicode_):\n\t\treturn np.array([x.encode('ascii', 'xmlcharrefreplace') for x in a])\n\telse:\n\t\traise ValueError(\"String values must be object, ascii or unicode.\")","return_type":"np.ndarray","function_name":"normalize_attr_strings","stripped_code":"def normalize_attr_strings(a: np.ndarray):\n\t\"\"\"\n\tTake an np.ndarray of all kinds of string-like elements, and return an array of ascii (np.string_) objects\n\t\"\"\"\n\tif np.issubdtype(a.dtype, np.object_):\n\t\t# if np.all([type(x) is str for x in a]) or np.all([type(x) is np.str_ for x in a]) or np.all([type(x) is np.unicode_ for x in a]):\n\t\tif np.all([(type(x) is str or type(x) is np.str_ or type(x) is np.unicode_) for x in a]):\n\t\t\treturn np.array([x.encode('ascii', 'xmlcharrefreplace') for x in a])\n\t\telif np.all([type(x) is np.string_ for x in a]) or np.all([type(x) is np.bytes_ for x in a]):\n\t\t\treturn a.astype(\"string_\")\n\t\telse:\n\t\t\tlogging.debug(f\"Attribute contains mixed object types ({np.unique([str(type(x)) for x in a])}); casting all to string\")\n\t\t\treturn np.array([str(x) for x in a], dtype=\"string_\")\n\telif np.issubdtype(a.dtype, np.string_) or np.issubdtype(a.dtype, np.object_):\n\t\treturn a\n\telif np.issubdtype(a.dtype, np.str_) or np.issubdtype(a.dtype, np.unicode_):\n\t\treturn np.array([x.encode('ascii', 'xmlcharrefreplace') for x in a])\n\telse:\n\t\traise ValueError(\"String values must be object, ascii or unicode.\")"}
{"code":"def check_column_linked_id(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    column: str,\n    target_df: DataFrame,\n    target_column: Optional[str] = None,\n    *,\n    column_required: bool = True,\n) -> List:\n    \"\"\"\n    A modified version of :func:`check_column_id`.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    column : string\n        A column of ``df``\n    column_required : boolean\n        ``True`` if and only if ``column`` is required\n        (and not optional) by the GTFS\n    target_df : DataFrame\n        A GTFS table\n    target_column : string\n        A column of ``target_df``; defaults to ``column_name``\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Record indices of ``df`` where the following condition is\n        violated: ``column`` contain IDs that are valid strings and are\n        present in ``target_df`` under the ``target_column`` name.\n        If the list of indices is nonempty, append to the problems the\n        item ``[type_, problem, table, indices]``; otherwise do not\n        append anything.\n\n        If not ``column_required``, then NaN entries will be ignored\n        in the checking.\n\n    \"\"\"\n    if target_column is None:\n        target_column = column\n\n    f = df.copy()\n\n    if target_df is None:\n        g = pd.DataFrame()\n        g[target_column] = np.nan\n    else:\n        g = target_df.copy()\n\n    if target_column not in g.columns:\n        g[target_column] = np.nan\n\n    if not column_required:\n        if column not in f.columns:\n            f[column] = np.nan\n        f = f.dropna(subset=[column])\n        g = g.dropna(subset=[target_column])\n\n    cond = ~f[column].isin(g[target_column])\n    problems = check_table(problems, table, f, cond, f\"Undefined {column}\")\n\n    return problems","return_type":"List","function_name":"check_column_linked_id","stripped_code":"def check_column_linked_id(\n    problems: List,\n    table: str,\n    df: DataFrame,\n    column: str,\n    target_df: DataFrame,\n    target_column: Optional[str] = None,\n    *,\n    column_required: bool = True,\n):\n    \"\"\"\n    A modified version of :func:`check_column_id`.\n\n    Parameters\n    ----------\n    problems : list\n        A four-tuple containing\n\n        1. A problem type (string) equal to ``'error'`` or ``'warning'``;\n           ``'error'`` means the GTFS is violated;\n           ``'warning'`` means there is a problem but it is not a\n           GTFS violation\n        2. A message (string) that describes the problem\n        3. A GTFS table name, e.g. ``'routes'``, in which the problem\n           occurs\n        4. A list of rows (integers) of the table's DataFrame where the\n           problem occurs\n\n    table : string\n        Name of a GTFS table\n    df : DataFrame\n        The GTFS table corresponding to ``table``\n    column : string\n        A column of ``df``\n    column_required : boolean\n        ``True`` if and only if ``column`` is required\n        (and not optional) by the GTFS\n    target_df : DataFrame\n        A GTFS table\n    target_column : string\n        A column of ``target_df``; defaults to ``column_name``\n\n    Returns\n    -------\n    list\n        The ``problems`` list extended as follows.\n        Record indices of ``df`` where the following condition is\n        violated: ``column`` contain IDs that are valid strings and are\n        present in ``target_df`` under the ``target_column`` name.\n        If the list of indices is nonempty, append to the problems the\n        item ``[type_, problem, table, indices]``; otherwise do not\n        append anything.\n\n        If not ``column_required``, then NaN entries will be ignored\n        in the checking.\n\n    \"\"\"\n    if target_column is None:\n        target_column = column\n\n    f = df.copy()\n\n    if target_df is None:\n        g = pd.DataFrame()\n        g[target_column] = np.nan\n    else:\n        g = target_df.copy()\n\n    if target_column not in g.columns:\n        g[target_column] = np.nan\n\n    if not column_required:\n        if column not in f.columns:\n            f[column] = np.nan\n        f = f.dropna(subset=[column])\n        g = g.dropna(subset=[target_column])\n\n    cond = ~f[column].isin(g[target_column])\n    problems = check_table(problems, table, f, cond, f\"Undefined {column}\")\n\n    return problems"}
{"code":"def visit_comprehension(self, node: AST,\n                            dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s representation as comprehension.\"\"\"\n        target = node.target\n        try:\n            elts = target.elts          # we have a tuple of names\n        except AttributeError:\n            names = self.visit(target)\n        else:\n            names = ', '.join(self.visit(elt) for elt in elts)\n        src = f\"for {names} in {self.visit(node.iter)}\"\n        if node.ifs:\n            src += f\" {' '.join('if ' + self.visit(if_) for if_ in node.ifs)}\"\n        return src","return_type":"str","function_name":"SourceGenerator.visit_comprehension","stripped_code":"def visit_comprehension(self, node: AST,\n                            dfltChaining: bool = True):\n        \"\"\"Return `node`s representation as comprehension.\"\"\"\n        target = node.target\n        try:\n            elts = target.elts          # we have a tuple of names\n        except AttributeError:\n            names = self.visit(target)\n        else:\n            names = ', '.join(self.visit(elt) for elt in elts)\n        src = f\"for {names} in {self.visit(node.iter)}\"\n        if node.ifs:\n            src += f\" {' '.join('if ' + self.visit(if_) for if_ in node.ifs)}\"\n        return src"}
{"code":"def deck_spawn(provider: Provider, deck: Deck, inputs: dict,\n               change_address: str, locktime: int=0) -> Transaction:\n\n    '''Creates Deck spawn raw transaction.\n\n       : key - Kutil object which we'll use to sign the tx\n       : deck - Deck object\n       : card - CardTransfer object\n       : inputs - utxos (has to be owned by deck issuer)\n       : change_address - address to send the change to\n       : locktime - tx locked until block n=int\n    '''\n\n    network_params = net_query(deck.network)\n    pa_params = param_query(deck.network)\n\n    if deck.production:\n        p2th_addr = pa_params.P2TH_addr\n    else:\n        p2th_addr = pa_params.test_P2TH_addr\n\n    #  first round of txn making is done by presuming minimal fee\n    change_sum = Decimal(inputs['total'] - network_params.min_tx_fee - pa_params.P2TH_fee)\n\n    txouts = [\n        tx_output(network=deck.network, value=pa_params.P2TH_fee,\n                  n=0, script=p2pkh_script(address=p2th_addr,\n                                           network=deck.network)),  # p2th\n\n        tx_output(network=deck.network, value=Decimal(0),\n                  n=1, script=nulldata_script(deck.metainfo_to_protobuf)),  # op_return\n\n        tx_output(network=deck.network, value=change_sum,\n                  n=2, script=p2pkh_script(address=change_address,\n                                           network=deck.network))  # change\n              ]\n\n    unsigned_tx = make_raw_transaction(network=deck.network,\n                                       inputs=inputs['utxos'],\n                                       outputs=txouts,\n                                       locktime=Locktime(locktime)\n                                       )\n    return unsigned_tx","return_type":"Transaction","function_name":"deck_spawn","stripped_code":"def deck_spawn(provider: Provider, deck: Deck, inputs: dict,\n               change_address: str, locktime: int=0):\n\n    '''Creates Deck spawn raw transaction.\n\n       : key - Kutil object which we'll use to sign the tx\n       : deck - Deck object\n       : card - CardTransfer object\n       : inputs - utxos (has to be owned by deck issuer)\n       : change_address - address to send the change to\n       : locktime - tx locked until block n=int\n    '''\n\n    network_params = net_query(deck.network)\n    pa_params = param_query(deck.network)\n\n    if deck.production:\n        p2th_addr = pa_params.P2TH_addr\n    else:\n        p2th_addr = pa_params.test_P2TH_addr\n\n    #  first round of txn making is done by presuming minimal fee\n    change_sum = Decimal(inputs['total'] - network_params.min_tx_fee - pa_params.P2TH_fee)\n\n    txouts = [\n        tx_output(network=deck.network, value=pa_params.P2TH_fee,\n                  n=0, script=p2pkh_script(address=p2th_addr,\n                                           network=deck.network)),  # p2th\n\n        tx_output(network=deck.network, value=Decimal(0),\n                  n=1, script=nulldata_script(deck.metainfo_to_protobuf)),  # op_return\n\n        tx_output(network=deck.network, value=change_sum,\n                  n=2, script=p2pkh_script(address=change_address,\n                                           network=deck.network))  # change\n              ]\n\n    unsigned_tx = make_raw_transaction(network=deck.network,\n                                       inputs=inputs['utxos'],\n                                       outputs=txouts,\n                                       locktime=Locktime(locktime)\n                                       )\n    return unsigned_tx"}
{"code":"def get_active_trips_df(trip_times: DataFrame) -> DataFrame:\n    \"\"\"\n    Count the number of trips in ``trip_times`` that are active\n    at any given time.\n\n    Parameters\n    ----------\n    trip_times : DataFrame\n        Contains columns\n\n        - start_time: start time of the trip in seconds past midnight\n        - end_time: end time of the trip in seconds past midnight\n\n    Returns\n    -------\n    Series\n        index is times from midnight when trips start and end,\n        values are number of active trips for that time\n\n    \"\"\"\n    active_trips = (\n        pd.concat(\n            [\n                pd.Series(1, trip_times.start_time),  # departed add 1\n                pd.Series(-1, trip_times.end_time),  # arrived subtract 1\n            ]\n        )\n        .groupby(level=0, sort=True)\n        .sum()\n        .cumsum()\n        .ffill()\n    )\n    return active_trips","return_type":"DataFrame","function_name":"get_active_trips_df","stripped_code":"def get_active_trips_df(trip_times: DataFrame):\n    \"\"\"\n    Count the number of trips in ``trip_times`` that are active\n    at any given time.\n\n    Parameters\n    ----------\n    trip_times : DataFrame\n        Contains columns\n\n        - start_time: start time of the trip in seconds past midnight\n        - end_time: end time of the trip in seconds past midnight\n\n    Returns\n    -------\n    Series\n        index is times from midnight when trips start and end,\n        values are number of active trips for that time\n\n    \"\"\"\n    active_trips = (\n        pd.concat(\n            [\n                pd.Series(1, trip_times.start_time),  # departed add 1\n                pd.Series(-1, trip_times.end_time),  # arrived subtract 1\n            ]\n        )\n        .groupby(level=0, sort=True)\n        .sum()\n        .cumsum()\n        .ffill()\n    )\n    return active_trips"}
{"code":"def _node_has_variant(node: BaseEntity, variant: str) -> bool:\n    \"\"\"Return true if the node has at least one of the given variant.\n\n    :param variant: :data:`PMOD`, :data:`HGVS`, :data:`GMOD`, or :data:`FRAGMENT`\n    \"\"\"\n    return VARIANTS in node and any(\n        variant_dict[KIND] == variant\n        for variant_dict in node[VARIANTS]\n    )","return_type":"bool","function_name":"_node_has_variant","stripped_code":"def _node_has_variant(node: BaseEntity, variant: str):\n    \"\"\"Return true if the node has at least one of the given variant.\n\n    :param variant: :data:`PMOD`, :data:`HGVS`, :data:`GMOD`, or :data:`FRAGMENT`\n    \"\"\"\n    return VARIANTS in node and any(\n        variant_dict[KIND] == variant\n        for variant_dict in node[VARIANTS]\n    )"}
{"code":"def _resolve_class(self, clazz: Any) -> object:\n        \"\"\"\n        Constructs an instance based on the function annotations on an object's\n        __init__ method\n        :argument clazz the class to instantiate\n        :return the instantiated class\n        \"\"\"\n        arguments = {}\n        if hasattr(clazz.__init__, '__annotations__'):\n            annotations = clazz.__init__.__annotations__\n            for name, type_hint in annotations.items():\n                if name != 'return':\n                    arguments[name] = self.resolve(type_hint)\n        return clazz(**arguments)","return_type":"object","function_name":"Container._resolve_class","stripped_code":"def _resolve_class(self, clazz: Any):\n        \"\"\"\n        Constructs an instance based on the function annotations on an object's\n        __init__ method\n        :argument clazz the class to instantiate\n        :return the instantiated class\n        \"\"\"\n        arguments = {}\n        if hasattr(clazz.__init__, '__annotations__'):\n            annotations = clazz.__init__.__annotations__\n            for name, type_hint in annotations.items():\n                if name != 'return':\n                    arguments[name] = self.resolve(type_hint)\n        return clazz(**arguments)"}
{"code":"def get_node_list(self) -> list:\n        \"\"\"Get a list of nodes.\n\n        Only the manager nodes can retrieve all the nodes\n\n        Returns:\n            list, all the ids of the nodes in swarm\n\n        \"\"\"\n        # Initialising empty list\n        nodes = []\n\n        # Raise an exception if we are not a manager\n        if not self._manager:\n            raise RuntimeError('Only the Swarm manager node '\n                               'can retrieve all the nodes.')\n\n        node_list = self._client.nodes.list()\n        for n_list in node_list:\n            nodes.append(n_list.id)\n        return nodes","return_type":"list","function_name":"DockerSwarmClient.get_node_list","stripped_code":"def get_node_list(self):\n        \"\"\"Get a list of nodes.\n\n        Only the manager nodes can retrieve all the nodes\n\n        Returns:\n            list, all the ids of the nodes in swarm\n\n        \"\"\"\n        # Initialising empty list\n        nodes = []\n\n        # Raise an exception if we are not a manager\n        if not self._manager:\n            raise RuntimeError('Only the Swarm manager node '\n                               'can retrieve all the nodes.')\n\n        node_list = self._client.nodes.list()\n        for n_list in node_list:\n            nodes.append(n_list.id)\n        return nodes"}
{"code":"def compare_python_to_reference_murmur3_64(data: Any, seed: int = 0) -> None:\n    \"\"\"\n    Checks the pure Python implementation of 64-bit murmur3 against the\n    ``mmh3`` C-based module.\n\n    Args:\n        data: data to hash\n        seed: seed\n\n    Raises:\n        AssertionError: if the two calculations don't match\n\n    \"\"\"\n    assert mmh3, \"Need mmh3 module\"\n    c_data = to_str(data)\n    c_signed_low, c_signed_high = mmh3.hash64(c_data, seed=seed,\n                                              x64arch=IS_64_BIT)\n    py_data = to_bytes(c_data)\n    py_signed_low, py_signed_high = pymmh3_hash64(py_data, seed=seed)\n    preamble = \"Hashing {data} with MurmurHash3/64-bit values from 128-bit \" \\\n               \"hash/seed={seed}\".format(data=repr(data), seed=seed)\n    if c_signed_low == py_signed_low and c_signed_high == py_signed_high:\n        print(preamble + \" -> (low={low}, high={high}): OK\".format(\n            low=c_signed_low, high=c_signed_high))\n    else:\n        raise AssertionError(\n            preamble +\n            \"; mmh3 says {c_data} -> (low={c_low}, high={c_high}), Python \"\n            \"version says {py_data} -> (low={py_low}, high={py_high})\".format(\n                c_data=repr(c_data),\n                c_low=c_signed_low,\n                c_high=c_signed_high,\n                py_data=repr(py_data),\n                py_low=py_signed_low,\n                py_high=py_signed_high))","return_type":"None","function_name":"compare_python_to_reference_murmur3_64","stripped_code":"def compare_python_to_reference_murmur3_64(data: Any, seed: int = 0):\n    \"\"\"\n    Checks the pure Python implementation of 64-bit murmur3 against the\n    ``mmh3`` C-based module.\n\n    Args:\n        data: data to hash\n        seed: seed\n\n    Raises:\n        AssertionError: if the two calculations don't match\n\n    \"\"\"\n    assert mmh3, \"Need mmh3 module\"\n    c_data = to_str(data)\n    c_signed_low, c_signed_high = mmh3.hash64(c_data, seed=seed,\n                                              x64arch=IS_64_BIT)\n    py_data = to_bytes(c_data)\n    py_signed_low, py_signed_high = pymmh3_hash64(py_data, seed=seed)\n    preamble = \"Hashing {data} with MurmurHash3/64-bit values from 128-bit \" \\\n               \"hash/seed={seed}\".format(data=repr(data), seed=seed)\n    if c_signed_low == py_signed_low and c_signed_high == py_signed_high:\n        print(preamble + \" -> (low={low}, high={high}): OK\".format(\n            low=c_signed_low, high=c_signed_high))\n    else:\n        raise AssertionError(\n            preamble +\n            \"; mmh3 says {c_data} -> (low={c_low}, high={c_high}), Python \"\n            \"version says {py_data} -> (low={py_low}, high={py_high})\".format(\n                c_data=repr(c_data),\n                c_low=c_signed_low,\n                c_high=c_signed_high,\n                py_data=repr(py_data),\n                py_low=py_signed_low,\n                py_high=py_signed_high))"}
{"code":"def visit_Expr(self, node: ast.Expr) -> Any:\n        \"\"\"Visit the node's ``value``.\"\"\"\n        result = self.visit(node=node.value)\n\n        self.recomputed_values[node] = result\n        return result","return_type":"Any","function_name":"Visitor.visit_Expr","stripped_code":"def visit_Expr(self, node: ast.Expr):\n        \"\"\"Visit the node's ``value``.\"\"\"\n        result = self.visit(node=node.value)\n\n        self.recomputed_values[node] = result\n        return result"}
{"code":"def write(\n    contents: str,\n    path: Union[str, pathlib.Path],\n    verbose: bool = False,\n    logger_func=None,\n) -> bool:\n    \"\"\"\n    Writes ``contents`` to ``path``.\n\n    Checks if ``path`` already exists and only write out new contents if the\n    old contents do not match.\n\n    Creates any intermediate missing directories.\n\n    :param contents: the file contents to write\n    :param path: the path to write to\n    :param verbose: whether to print output\n    \"\"\"\n    print_func = logger_func or print\n    path = pathlib.Path(path)\n    if path.exists():\n        with path.open(\"r\") as file_pointer:\n            old_contents = file_pointer.read()\n        if old_contents == contents:\n            if verbose:\n                print_func(\"preserved {}\".format(path))\n            return False\n        else:\n            with path.open(\"w\") as file_pointer:\n                file_pointer.write(contents)\n            if verbose:\n                print_func(\"rewrote {}\".format(path))\n            return True\n    elif not path.exists():\n        if not path.parent.exists():\n            path.parent.mkdir(parents=True)\n        with path.open(\"w\") as file_pointer:\n            file_pointer.write(contents)\n        if verbose:\n            print_func(\"wrote {}\".format(path))\n    return True","return_type":"bool","function_name":"write","stripped_code":"def write(\n    contents: str,\n    path: Union[str, pathlib.Path],\n    verbose: bool = False,\n    logger_func=None,\n):\n    \"\"\"\n    Writes ``contents`` to ``path``.\n\n    Checks if ``path`` already exists and only write out new contents if the\n    old contents do not match.\n\n    Creates any intermediate missing directories.\n\n    :param contents: the file contents to write\n    :param path: the path to write to\n    :param verbose: whether to print output\n    \"\"\"\n    print_func = logger_func or print\n    path = pathlib.Path(path)\n    if path.exists():\n        with path.open(\"r\") as file_pointer:\n            old_contents = file_pointer.read()\n        if old_contents == contents:\n            if verbose:\n                print_func(\"preserved {}\".format(path))\n            return False\n        else:\n            with path.open(\"w\") as file_pointer:\n                file_pointer.write(contents)\n            if verbose:\n                print_func(\"rewrote {}\".format(path))\n            return True\n    elif not path.exists():\n        if not path.parent.exists():\n            path.parent.mkdir(parents=True)\n        with path.open(\"w\") as file_pointer:\n            file_pointer.write(contents)\n        if verbose:\n            print_func(\"wrote {}\".format(path))\n    return True"}
{"code":"def unload(self, source: Source) -> None:\n        \"\"\"\n        Unloads a registered source, causing all of its associated bugs, tools,\n        and blueprints to also be unloaded. If the given source is not loaded,\n        this function will do nothing.\n        \"\"\"\n        logger.info('unloading source: %s', source.name)\n        try:\n            contents = self.contents(source)\n            del self.__contents[source.name]\n            del self.__sources[source.name]\n\n            for name in contents.bugs:\n                bug = self.__installation.bugs[name]\n                self.__installation.bugs.remove(bug)\n            for name in contents.blueprints:\n                blueprint = self.__installation.build[name]\n                self.__installation.build.remove(blueprint)\n            for name in contents.tools:\n                tool = self.__installation.tools[name]\n                self.__installation.tools.remove(tool)\n        except KeyError:\n            pass\n        logger.info('unloaded source: %s', source.name)","return_type":"None","function_name":"SourceManager.unload","stripped_code":"def unload(self, source: Source):\n        \"\"\"\n        Unloads a registered source, causing all of its associated bugs, tools,\n        and blueprints to also be unloaded. If the given source is not loaded,\n        this function will do nothing.\n        \"\"\"\n        logger.info('unloading source: %s', source.name)\n        try:\n            contents = self.contents(source)\n            del self.__contents[source.name]\n            del self.__sources[source.name]\n\n            for name in contents.bugs:\n                bug = self.__installation.bugs[name]\n                self.__installation.bugs.remove(bug)\n            for name in contents.blueprints:\n                blueprint = self.__installation.build[name]\n                self.__installation.build.remove(blueprint)\n            for name in contents.tools:\n                tool = self.__installation.tools[name]\n                self.__installation.tools.remove(tool)\n        except KeyError:\n            pass\n        logger.info('unloaded source: %s', source.name)"}
{"code":"def force_horizontal_padding_after(\n            self, index: int, padding: Union[int, float]) -> None:\n        \"\"\"Change the padding after the given column.\"\"\"\n        self.horizontal_padding[index] = padding","return_type":"None","function_name":"TextDiagramDrawer.force_horizontal_padding_after","stripped_code":"def force_horizontal_padding_after(\n            self, index: int, padding: Union[int, float]):\n        \"\"\"Change the padding after the given column.\"\"\"\n        self.horizontal_padding[index] = padding"}
{"code":"def upload_bel_namespace(self, update: bool = False) -> Namespace:\n        \"\"\"Upload the namespace to the PyBEL database.\n\n        :param update: Should the namespace be updated first?\n        \"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        namespace = self._get_default_namespace()\n\n        if namespace is None:\n            log.info('making namespace for %s', self._get_namespace_name())\n            return self._make_namespace()\n\n        if update:\n            self._update_namespace(namespace)\n\n        return namespace","return_type":"Namespace","function_name":"BELNamespaceManagerMixin.upload_bel_namespace","stripped_code":"def upload_bel_namespace(self, update: bool = False):\n        \"\"\"Upload the namespace to the PyBEL database.\n\n        :param update: Should the namespace be updated first?\n        \"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        namespace = self._get_default_namespace()\n\n        if namespace is None:\n            log.info('making namespace for %s', self._get_namespace_name())\n            return self._make_namespace()\n\n        if update:\n            self._update_namespace(namespace)\n\n        return namespace"}
{"code":"def logs_blocks_sanity_check(from_block: BlockSpecification, to_block: BlockSpecification) -> None:\n    \"\"\"Checks that the from/to blocks passed onto log calls contain only appropriate types\"\"\"\n    is_valid_from = isinstance(from_block, int) or isinstance(from_block, str)\n    assert is_valid_from, 'event log from block can be integer or latest,pending, earliest'\n    is_valid_to = isinstance(to_block, int) or isinstance(to_block, str)\n    assert is_valid_to, 'event log to block can be integer or latest,pending, earliest'","return_type":"None","function_name":"logs_blocks_sanity_check","stripped_code":"def logs_blocks_sanity_check(from_block: BlockSpecification, to_block: BlockSpecification):\n    \"\"\"Checks that the from/to blocks passed onto log calls contain only appropriate types\"\"\"\n    is_valid_from = isinstance(from_block, int) or isinstance(from_block, str)\n    assert is_valid_from, 'event log from block can be integer or latest,pending, earliest'\n    is_valid_to = isinstance(to_block, int) or isinstance(to_block, str)\n    assert is_valid_to, 'event log to block can be integer or latest,pending, earliest'"}
{"code":"def get_by_symbol(self, symbol: str) -> Commodity:\n        \"\"\" Loads currency by symbol \"\"\"\n        assert isinstance(symbol, str)\n\n        query = (\n            self.currencies_query\n                .filter(Commodity.mnemonic == symbol)\n        )\n        return query.one()","return_type":"Commodity","function_name":"CurrenciesAggregate.get_by_symbol","stripped_code":"def get_by_symbol(self, symbol: str):\n        \"\"\" Loads currency by symbol \"\"\"\n        assert isinstance(symbol, str)\n\n        query = (\n            self.currencies_query\n                .filter(Commodity.mnemonic == symbol)\n        )\n        return query.one()"}
{"code":"def check_page_for_warnings(html: str) -> None:\n    \"\"\"\n    Checks if is any warnings on page if so raises an exception\n    \"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    warnings = soup.find_all('div', {'class': 'service_msg_warning'})\n    if warnings:\n        exception_msg = '; '.join((warning.get_text() for warning in warnings))\n        raise VVKPageWarningException(exception_msg)","return_type":"None","function_name":"check_page_for_warnings","stripped_code":"def check_page_for_warnings(html: str):\n    \"\"\"\n    Checks if is any warnings on page if so raises an exception\n    \"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    warnings = soup.find_all('div', {'class': 'service_msg_warning'})\n    if warnings:\n        exception_msg = '; '.join((warning.get_text() for warning in warnings))\n        raise VVKPageWarningException(exception_msg)"}
{"code":"def visit_BinOp(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s operator and operands as inlined expression.\"\"\"\n        op = node.op\n        with self.op_man(op):\n            if isinstance(op, ast.Pow):\n                # Pow chains right-to-left\n                src = self.visit(op).join((self.visit(node.left,\n                                                      dfltChaining=False),\n                                           self.visit(node.right)))\n            else:\n                src = self.visit(op).join((self.visit(node.left),\n                                           self.visit(node.right,\n                                                      dfltChaining=False)))\n            return self.wrap_expr(src, dfltChaining)","return_type":"str","function_name":"SourceGenerator.visit_BinOp","stripped_code":"def visit_BinOp(self, node: AST, dfltChaining: bool = True):\n        \"\"\"Return `node`s operator and operands as inlined expression.\"\"\"\n        op = node.op\n        with self.op_man(op):\n            if isinstance(op, ast.Pow):\n                # Pow chains right-to-left\n                src = self.visit(op).join((self.visit(node.left,\n                                                      dfltChaining=False),\n                                           self.visit(node.right)))\n            else:\n                src = self.visit(op).join((self.visit(node.left),\n                                           self.visit(node.right,\n                                                      dfltChaining=False)))\n            return self.wrap_expr(src, dfltChaining)"}
{"code":"def do_signal(self, signame: str) -> None:\n        \"\"\"Send a Unix signal\"\"\"\n        if hasattr(signal, signame):\n            os.kill(os.getpid(), getattr(signal, signame))\n        else:\n            self._sout.write('Unknown signal %s\\n' % signame)","return_type":"None","function_name":"Monitor.do_signal","stripped_code":"def do_signal(self, signame: str):\n        \"\"\"Send a Unix signal\"\"\"\n        if hasattr(signal, signame):\n            os.kill(os.getpid(), getattr(signal, signame))\n        else:\n            self._sout.write('Unknown signal %s\\n' % signame)"}
{"code":"def metar(trans: MetarTrans) -> str:\n    \"\"\"\n    Condense the translation strings into a single report summary string\n    \"\"\"\n    summary = []\n    if trans.wind:\n        summary.append('Winds ' + trans.wind)\n    if trans.visibility:\n        summary.append('Vis ' + trans.visibility[:trans.visibility.find(' (')].lower())\n    if trans.temperature:\n        summary.append('Temp ' + trans.temperature[:trans.temperature.find(' (')])\n    if trans.dewpoint:\n        summary.append('Dew ' + trans.dewpoint[:trans.dewpoint.find(' (')])\n    if trans.altimeter:\n        summary.append('Alt ' + trans.altimeter[:trans.altimeter.find(' (')])\n    if trans.other:\n        summary.append(trans.other)\n    if trans.clouds:\n        summary.append(trans.clouds.replace(' - Reported AGL', ''))\n    return ', '.join(summary)","return_type":"str","function_name":"metar","stripped_code":"def metar(trans: MetarTrans):\n    \"\"\"\n    Condense the translation strings into a single report summary string\n    \"\"\"\n    summary = []\n    if trans.wind:\n        summary.append('Winds ' + trans.wind)\n    if trans.visibility:\n        summary.append('Vis ' + trans.visibility[:trans.visibility.find(' (')].lower())\n    if trans.temperature:\n        summary.append('Temp ' + trans.temperature[:trans.temperature.find(' (')])\n    if trans.dewpoint:\n        summary.append('Dew ' + trans.dewpoint[:trans.dewpoint.find(' (')])\n    if trans.altimeter:\n        summary.append('Alt ' + trans.altimeter[:trans.altimeter.find(' (')])\n    if trans.other:\n        summary.append(trans.other)\n    if trans.clouds:\n        summary.append(trans.clouds.replace(' - Reported AGL', ''))\n    return ', '.join(summary)"}
{"code":"def image_url(self) -> Optional[str]:\n        r\"\"\"(:class:`~typing.Optional`\\ [:class:`str`]) The image url.\n        It may be :const:`None` if it's not an image.\n\n        \"\"\"\n        images = self.attributes.get('imageinfo', [])\n        if images and isinstance(images, collections.abc.Sequence):\n            return images[0]['url']\n        return None","return_type":"Optional[str]","function_name":"File.image_url","stripped_code":"def image_url(self):\n        r\"\"\"(:class:`~typing.Optional`\\ [:class:`str`]) The image url.\n        It may be :const:`None` if it's not an image.\n\n        \"\"\"\n        images = self.attributes.get('imageinfo', [])\n        if images and isinstance(images, collections.abc.Sequence):\n            return images[0]['url']\n        return None"}
{"code":"def sanitize_args(cmd: List[str]) -> List[str]:\n    \"\"\" Filter the command so that it no longer contains passwords\n    \"\"\"\n    sanitized = []\n    for idx, fieldname in enumerate(cmd):\n        def _is_password(cmdstr):\n            return 'wifi-sec.psk' in cmdstr\\\n                or 'password' in cmdstr.lower()\n        if idx > 0 and _is_password(cmd[idx-1]):\n            sanitized.append('****')\n        else:\n            sanitized.append(fieldname)\n    return sanitized","return_type":"List[str]","function_name":"sanitize_args","stripped_code":"def sanitize_args(cmd: List[str]):\n    \"\"\" Filter the command so that it no longer contains passwords\n    \"\"\"\n    sanitized = []\n    for idx, fieldname in enumerate(cmd):\n        def _is_password(cmdstr):\n            return 'wifi-sec.psk' in cmdstr\\\n                or 'password' in cmdstr.lower()\n        if idx > 0 and _is_password(cmd[idx-1]):\n            sanitized.append('****')\n        else:\n            sanitized.append(fieldname)\n    return sanitized"}
{"code":"def from_file(cls, file_path: Path, w3: Web3) -> \"Package\":\n        \"\"\"\n        Returns a ``Package`` instantiated by a manifest located at the provided Path.\n        ``file_path`` arg must be a ``pathlib.Path`` instance.\n        A valid ``Web3`` instance is required to instantiate a ``Package``.\n        \"\"\"\n        if isinstance(file_path, Path):\n            raw_manifest = file_path.read_text()\n            validate_raw_manifest_format(raw_manifest)\n            manifest = json.loads(raw_manifest)\n        else:\n            raise TypeError(\n                \"The Package.from_file method expects a pathlib.Path instance.\"\n                f\"Got {type(file_path)} instead.\"\n            )\n        return cls(manifest, w3, file_path.as_uri())","return_type":"\"Package\"","function_name":"Package.from_file","stripped_code":"def from_file(cls, file_path: Path, w3: Web3):\n        \"\"\"\n        Returns a ``Package`` instantiated by a manifest located at the provided Path.\n        ``file_path`` arg must be a ``pathlib.Path`` instance.\n        A valid ``Web3`` instance is required to instantiate a ``Package``.\n        \"\"\"\n        if isinstance(file_path, Path):\n            raw_manifest = file_path.read_text()\n            validate_raw_manifest_format(raw_manifest)\n            manifest = json.loads(raw_manifest)\n        else:\n            raise TypeError(\n                \"The Package.from_file method expects a pathlib.Path instance.\"\n                f\"Got {type(file_path)} instead.\"\n            )\n        return cls(manifest, w3, file_path.as_uri())"}
{"code":"def get_config_path(self) -> str:\n        \"\"\"\n        Returns the path where the active config file is expected.\n        This is the user's profile folder.\n        \"\"\"\n        dst_dir = self.__get_user_path()\n        dst = dst_dir + \"/\" + config_filename\n        return dst","return_type":"str","function_name":"Config.get_config_path","stripped_code":"def get_config_path(self):\n        \"\"\"\n        Returns the path where the active config file is expected.\n        This is the user's profile folder.\n        \"\"\"\n        dst_dir = self.__get_user_path()\n        dst = dst_dir + \"/\" + config_filename\n        return dst"}
{"code":"def all_near_zero(a: Union[float, complex, Iterable[float], np.ndarray],\n                  *,\n                  atol: float = 1e-8) -> bool:\n    \"\"\"Checks if the tensor's elements are all near zero.\n\n    Args:\n        a: Tensor of elements that could all be near zero.\n        atol: Absolute tolerance.\n    \"\"\"\n    return np.all(np.less_equal(np.abs(a), atol))","return_type":"bool","function_name":"all_near_zero","stripped_code":"def all_near_zero(a: Union[float, complex, Iterable[float], np.ndarray],\n                  *,\n                  atol: float = 1e-8):\n    \"\"\"Checks if the tensor's elements are all near zero.\n\n    Args:\n        a: Tensor of elements that could all be near zero.\n        atol: Absolute tolerance.\n    \"\"\"\n    return np.all(np.less_equal(np.abs(a), atol))"}
{"code":"def split_bel_stmt(stmt: str, line_num) -> tuple:\n    \"\"\"Split bel statement into subject, relation, object tuple\"\"\"\n\n    m = re.match(f\"^(.*?\\))\\s+([a-zA-Z=\\->\\|:]+)\\s+([\\w(]+.*?)$\", stmt, flags=0)\n    if m:\n        return (m.group(1), m.group(2), m.group(3))\n    else:\n        log.info(\n            f\"Could not parse bel statement into components at line number: {line_num} assertion: {stmt}\"\n        )\n        return (stmt, None, None)","return_type":"tuple","function_name":"split_bel_stmt","stripped_code":"def split_bel_stmt(stmt: str, line_num):\n    \"\"\"Split bel statement into subject, relation, object tuple\"\"\"\n\n    m = re.match(f\"^(.*?\\))\\s+([a-zA-Z=\\->\\|:]+)\\s+([\\w(]+.*?)$\", stmt, flags=0)\n    if m:\n        return (m.group(1), m.group(2), m.group(3))\n    else:\n        log.info(\n            f\"Could not parse bel statement into components at line number: {line_num} assertion: {stmt}\"\n        )\n        return (stmt, None, None)"}
{"code":"def evolve(self, rho: Density) -> Density:\n        \"\"\"Apply the action of this channel upon a density\"\"\"\n        N = rho.qubit_nb\n        qubits = rho.qubits\n\n        indices = list([qubits.index(q) for q in self.qubits]) + \\\n            list([qubits.index(q) + N for q in self.qubits])\n\n        tensor = bk.tensormul(self.tensor, rho.tensor, indices)\n        return Density(tensor, qubits, rho.memory)","return_type":"Density","function_name":"Channel.evolve","stripped_code":"def evolve(self, rho: Density):\n        \"\"\"Apply the action of this channel upon a density\"\"\"\n        N = rho.qubit_nb\n        qubits = rho.qubits\n\n        indices = list([qubits.index(q) for q in self.qubits]) + \\\n            list([qubits.index(q) + N for q in self.qubits])\n\n        tensor = bk.tensormul(self.tensor, rho.tensor, indices)\n        return Density(tensor, qubits, rho.memory)"}
{"code":"def inv(z: int) -> int:\n    \"\"\"$= z^{-1} mod q$, for z != 0\"\"\"\n    # Adapted from curve25519_athlon.c in djb's Curve25519.\n    z2 = z * z % q  # 2\n    z9 = pow2(z2, 2) * z % q  # 9\n    z11 = z9 * z2 % q  # 11\n    z2_5_0 = (z11 * z11) % q * z9 % q  # 31 == 2^5 - 2^0\n    z2_10_0 = pow2(z2_5_0, 5) * z2_5_0 % q  # 2^10 - 2^0\n    z2_20_0 = pow2(z2_10_0, 10) * z2_10_0 % q  # ...\n    z2_40_0 = pow2(z2_20_0, 20) * z2_20_0 % q\n    z2_50_0 = pow2(z2_40_0, 10) * z2_10_0 % q\n    z2_100_0 = pow2(z2_50_0, 50) * z2_50_0 % q\n    z2_200_0 = pow2(z2_100_0, 100) * z2_100_0 % q\n    z2_250_0 = pow2(z2_200_0, 50) * z2_50_0 % q  # 2^250 - 2^0\n    return pow2(z2_250_0, 5) * z11 % q","return_type":"int","function_name":"inv","stripped_code":"def inv(z: int):\n    \"\"\"$= z^{-1} mod q$, for z != 0\"\"\"\n    # Adapted from curve25519_athlon.c in djb's Curve25519.\n    z2 = z * z % q  # 2\n    z9 = pow2(z2, 2) * z % q  # 9\n    z11 = z9 * z2 % q  # 11\n    z2_5_0 = (z11 * z11) % q * z9 % q  # 31 == 2^5 - 2^0\n    z2_10_0 = pow2(z2_5_0, 5) * z2_5_0 % q  # 2^10 - 2^0\n    z2_20_0 = pow2(z2_10_0, 10) * z2_10_0 % q  # ...\n    z2_40_0 = pow2(z2_20_0, 20) * z2_20_0 % q\n    z2_50_0 = pow2(z2_40_0, 10) * z2_10_0 % q\n    z2_100_0 = pow2(z2_50_0, 50) * z2_50_0 % q\n    z2_200_0 = pow2(z2_100_0, 100) * z2_100_0 % q\n    z2_250_0 = pow2(z2_200_0, 50) * z2_50_0 % q  # 2^250 - 2^0\n    return pow2(z2_250_0, 5) * z11 % q"}
{"code":"def _import_to_py_ast(ctx: GeneratorContext, node: Import) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a Basilisp `import*` expression.\"\"\"\n    assert node.op == NodeOp.IMPORT\n\n    last = None\n    deps: List[ast.AST] = []\n    for alias in node.aliases:\n        safe_name = munge(alias.name)\n\n        try:\n            module = importlib.import_module(safe_name)\n            if alias.alias is not None:\n                ctx.add_import(sym.symbol(alias.name), module, sym.symbol(alias.alias))\n            else:\n                ctx.add_import(sym.symbol(alias.name), module)\n        except ModuleNotFoundError as e:\n            raise ImportError(\n                f\"Python module '{alias.name}' not found\", node.form, node\n            ) from e\n\n        py_import_alias = (\n            munge(alias.alias)\n            if alias.alias is not None\n            else safe_name.split(\".\", maxsplit=1)[0]\n        )\n        deps.append(\n            ast.Assign(\n                targets=[ast.Name(id=py_import_alias, ctx=ast.Store())],\n                value=ast.Call(\n                    func=_load_attr(\"builtins.__import__\"),\n                    args=[ast.Str(safe_name)],\n                    keywords=[],\n                ),\n            )\n        )\n        last = ast.Name(id=py_import_alias, ctx=ast.Load())\n\n        # Note that we add this import to the live running system in the above\n        # calls to `ctx.add_import`, however, since we compile and cache Python\n        # bytecode, we need to generate calls to `add_import` for the running\n        # namespace so when this code is reloaded from the cache, the runtime\n        # is correctly configured.\n        deps.append(\n            ast.Call(\n                func=_load_attr(f\"{_NS_VAR_VALUE}.add_import\"),\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME, args=[ast.Str(safe_name)], keywords=[]\n                    ),\n                    last,\n                ],\n                keywords=[],\n            )\n        )\n\n    assert last is not None, \"import* node must have at least one import\"\n    return GeneratedPyAST(node=last, dependencies=deps)","return_type":"GeneratedPyAST","function_name":"_import_to_py_ast","stripped_code":"def _import_to_py_ast(ctx: GeneratorContext, node: Import):\n    \"\"\"Return a Python AST node for a Basilisp `import*` expression.\"\"\"\n    assert node.op == NodeOp.IMPORT\n\n    last = None\n    deps: List[ast.AST] = []\n    for alias in node.aliases:\n        safe_name = munge(alias.name)\n\n        try:\n            module = importlib.import_module(safe_name)\n            if alias.alias is not None:\n                ctx.add_import(sym.symbol(alias.name), module, sym.symbol(alias.alias))\n            else:\n                ctx.add_import(sym.symbol(alias.name), module)\n        except ModuleNotFoundError as e:\n            raise ImportError(\n                f\"Python module '{alias.name}' not found\", node.form, node\n            ) from e\n\n        py_import_alias = (\n            munge(alias.alias)\n            if alias.alias is not None\n            else safe_name.split(\".\", maxsplit=1)[0]\n        )\n        deps.append(\n            ast.Assign(\n                targets=[ast.Name(id=py_import_alias, ctx=ast.Store())],\n                value=ast.Call(\n                    func=_load_attr(\"builtins.__import__\"),\n                    args=[ast.Str(safe_name)],\n                    keywords=[],\n                ),\n            )\n        )\n        last = ast.Name(id=py_import_alias, ctx=ast.Load())\n\n        # Note that we add this import to the live running system in the above\n        # calls to `ctx.add_import`, however, since we compile and cache Python\n        # bytecode, we need to generate calls to `add_import` for the running\n        # namespace so when this code is reloaded from the cache, the runtime\n        # is correctly configured.\n        deps.append(\n            ast.Call(\n                func=_load_attr(f\"{_NS_VAR_VALUE}.add_import\"),\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME, args=[ast.Str(safe_name)], keywords=[]\n                    ),\n                    last,\n                ],\n                keywords=[],\n            )\n        )\n\n    assert last is not None, \"import* node must have at least one import\"\n    return GeneratedPyAST(node=last, dependencies=deps)"}
{"code":"def update(self, uid: str, data={}) -> str:\n        \"\"\"\n        Specifies new values for the customizable messages in a form (specified by form_id).\n        You can format messages with bold (*bold*) and italic (_italic_) text. HTML tags are forbidden.\n        Return a `str` based on success of change, `OK` on success, otherwise an error message.\n        \"\"\"\n        return self.__client.request('put', '/forms/%s/messages' % uid, data=data)","return_type":"str","function_name":"FormMessages.update","stripped_code":"def update(self, uid: str, data={}):\n        \"\"\"\n        Specifies new values for the customizable messages in a form (specified by form_id).\n        You can format messages with bold (*bold*) and italic (_italic_) text. HTML tags are forbidden.\n        Return a `str` based on success of change, `OK` on success, otherwise an error message.\n        \"\"\"\n        return self.__client.request('put', '/forms/%s/messages' % uid, data=data)"}
{"code":"def fetch(reload: bool = False) -> dict:\n    \"\"\"\n    Returns a dictionary containing all of the available Cauldron commands\n    currently registered. This data is cached for performance. Unless the\n    reload argument is set to True, the command list will only be generated\n    the first time this function is called.\n\n    :param reload:\n        Whether or not to disregard any cached command data and generate a\n        new dictionary of available commands.\n\n    :return:\n        A dictionary where the keys are the name of the commands and the\n        values are the modules for the command .\n    \"\"\"\n\n    if len(list(COMMANDS.keys())) > 0 and not reload:\n        return COMMANDS\n\n    COMMANDS.clear()\n\n    for key in dir(commands):\n        e = getattr(commands, key)\n        if e and hasattr(e, 'NAME') and hasattr(e, 'DESCRIPTION'):\n            COMMANDS[e.NAME] = e\n\n    return dict(COMMANDS.items())","return_type":"dict","function_name":"fetch","stripped_code":"def fetch(reload: bool = False):\n    \"\"\"\n    Returns a dictionary containing all of the available Cauldron commands\n    currently registered. This data is cached for performance. Unless the\n    reload argument is set to True, the command list will only be generated\n    the first time this function is called.\n\n    :param reload:\n        Whether or not to disregard any cached command data and generate a\n        new dictionary of available commands.\n\n    :return:\n        A dictionary where the keys are the name of the commands and the\n        values are the modules for the command .\n    \"\"\"\n\n    if len(list(COMMANDS.keys())) > 0 and not reload:\n        return COMMANDS\n\n    COMMANDS.clear()\n\n    for key in dir(commands):\n        e = getattr(commands, key)\n        if e and hasattr(e, 'NAME') and hasattr(e, 'DESCRIPTION'):\n            COMMANDS[e.NAME] = e\n\n    return dict(COMMANDS.items())"}
{"code":"def commit(self, changeset_id: uuid.UUID) -> None:\n        \"\"\"\n        Commits a given changeset. This merges the given changeset and all\n        subsequent changesets into the previous changeset giving precidence\n        to later changesets in case of any conflicting keys.\n\n        If this is the base changeset then all changes will be written to\n        the underlying database and the Journal starts a new recording.\n        Typically, callers won't have access to the base changeset, because\n        it is dropped during .reset() which is called in JournalDB().\n        \"\"\"\n        self._validate_changeset(changeset_id)\n        journal_data = self.journal.commit_changeset(changeset_id)\n\n        if self.journal.is_empty():\n            # Ensure the journal automatically restarts recording after\n            # it has been persisted to the underlying db\n            self.reset()\n\n            for key, value in journal_data.items():\n                try:\n                    if value is DELETED_ENTRY:\n                        del self.wrapped_db[key]\n                    elif value is ERASE_CREATED_ENTRY:\n                        pass\n                    else:\n                        self.wrapped_db[key] = cast(bytes, value)\n                except Exception:\n                    self._reapply_changeset_to_journal(changeset_id, journal_data)\n                    raise","return_type":"None","function_name":"JournalDB.commit","stripped_code":"def commit(self, changeset_id: uuid.UUID):\n        \"\"\"\n        Commits a given changeset. This merges the given changeset and all\n        subsequent changesets into the previous changeset giving precidence\n        to later changesets in case of any conflicting keys.\n\n        If this is the base changeset then all changes will be written to\n        the underlying database and the Journal starts a new recording.\n        Typically, callers won't have access to the base changeset, because\n        it is dropped during .reset() which is called in JournalDB().\n        \"\"\"\n        self._validate_changeset(changeset_id)\n        journal_data = self.journal.commit_changeset(changeset_id)\n\n        if self.journal.is_empty():\n            # Ensure the journal automatically restarts recording after\n            # it has been persisted to the underlying db\n            self.reset()\n\n            for key, value in journal_data.items():\n                try:\n                    if value is DELETED_ENTRY:\n                        del self.wrapped_db[key]\n                    elif value is ERASE_CREATED_ENTRY:\n                        pass\n                    else:\n                        self.wrapped_db[key] = cast(bytes, value)\n                except Exception:\n                    self._reapply_changeset_to_journal(changeset_id, journal_data)\n                    raise"}
{"code":"def get_last_modified_date(\n            self,\n            bucket: str,\n            key: str,\n    ) -> datetime.datetime:\n        \"\"\"\n        Retrieves last modified date for a given key in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which the last modified date is being retrieved.\n        :return: the last modified date\n        \"\"\"\n        blob_obj = self._get_blob_obj(bucket, key)\n        return blob_obj.updated","return_type":"datetime.datetime","function_name":"GSBlobStore.get_last_modified_date","stripped_code":"def get_last_modified_date(\n            self,\n            bucket: str,\n            key: str,\n    ):\n        \"\"\"\n        Retrieves last modified date for a given key in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which the last modified date is being retrieved.\n        :return: the last modified date\n        \"\"\"\n        blob_obj = self._get_blob_obj(bucket, key)\n        return blob_obj.updated"}
{"code":"def handel_default(self) -> None:\n        \"\"\"\n        \u5904\u7406\u8bbe\u7f6e\u5230body\u4e0a\u7684\u6570\u636e\u9ed8\u8ba4 headers\n        \"\"\"\n        raw_body = self._body\n        body = cast(Optional[bytes], None)\n        default_type = 2\n        charset = self._charset or self._default_charset\n        if raw_body is None:\n            pass\n        elif isinstance(raw_body, bytes):\n            # body\u4e3abytes\n            default_type = 2\n            body = raw_body\n        elif isinstance(raw_body, str):\n            # body \u4e3a\u5b57\u7b26\u4e32\n            default_type = 2\n            body = encode_str(raw_body, charset)\n        elif isinstance(raw_body, (list, dict)):\n            # body \u4e3ajson\n            default_type = 3\n            body = encode_str(json.dumps(raw_body, ensure_ascii=False), charset)\n        elif isinstance(raw_body, RawIOBase):\n            # body \u4e3a\u6587\u4ef6\n            default_type = 1\n            body = raw_body.read()\n            raw_body.close()\n        if \"Content-Length\" not in self._headers and \\\n                \"Transfer-Encoding\" not in self._headers \\\n                or self._headers[\"Transfer-Encoding\"] != \"chunked\":\n            if self.length is None:\n                if body is not None:\n                    self.length = len(body)\n                else:\n                    self.length = 0\n            # \u8bbe\u7f6e\u9ed8\u8ba4 Content-Length\n            self.set(\"Content-Length\", str(self.length))\n        # print(body[0], body[1])\n        if body is not None and body.startswith(encode_str(\"<\", charset)):\n            default_type = 4\n        if \"Content-Type\" not in self._headers.keys():\n            type_str = self.type\n            if type_str is None:\n                temp = DEFAULT_TYPE.get(default_type)\n                if temp is not None:\n                    if default_type != 1:\n                        temp += \"; charset=%s\" % charset\n                    type_str = temp\n            if type_str is not None:\n                # \u8bbe\u7f6e\u9ed8\u8ba4 Content-Type\n                self.set(\"Content-Type\", type_str)\n        self._body = body","return_type":"None","function_name":"Response.handel_default","stripped_code":"def handel_default(self):\n        \"\"\"\n        \u5904\u7406\u8bbe\u7f6e\u5230body\u4e0a\u7684\u6570\u636e\u9ed8\u8ba4 headers\n        \"\"\"\n        raw_body = self._body\n        body = cast(Optional[bytes], None)\n        default_type = 2\n        charset = self._charset or self._default_charset\n        if raw_body is None:\n            pass\n        elif isinstance(raw_body, bytes):\n            # body\u4e3abytes\n            default_type = 2\n            body = raw_body\n        elif isinstance(raw_body, str):\n            # body \u4e3a\u5b57\u7b26\u4e32\n            default_type = 2\n            body = encode_str(raw_body, charset)\n        elif isinstance(raw_body, (list, dict)):\n            # body \u4e3ajson\n            default_type = 3\n            body = encode_str(json.dumps(raw_body, ensure_ascii=False), charset)\n        elif isinstance(raw_body, RawIOBase):\n            # body \u4e3a\u6587\u4ef6\n            default_type = 1\n            body = raw_body.read()\n            raw_body.close()\n        if \"Content-Length\" not in self._headers and \\\n                \"Transfer-Encoding\" not in self._headers \\\n                or self._headers[\"Transfer-Encoding\"] != \"chunked\":\n            if self.length is None:\n                if body is not None:\n                    self.length = len(body)\n                else:\n                    self.length = 0\n            # \u8bbe\u7f6e\u9ed8\u8ba4 Content-Length\n            self.set(\"Content-Length\", str(self.length))\n        # print(body[0], body[1])\n        if body is not None and body.startswith(encode_str(\"<\", charset)):\n            default_type = 4\n        if \"Content-Type\" not in self._headers.keys():\n            type_str = self.type\n            if type_str is None:\n                temp = DEFAULT_TYPE.get(default_type)\n                if temp is not None:\n                    if default_type != 1:\n                        temp += \"; charset=%s\" % charset\n                    type_str = temp\n            if type_str is not None:\n                # \u8bbe\u7f6e\u9ed8\u8ba4 Content-Type\n                self.set(\"Content-Type\", type_str)\n        self._body = body"}
{"code":"def convert(self, pattern: str) -> str:\n        \"\"\"Convert the goat step string to CFParse String\"\"\"\n        parameters = OrderedDict()\n        for parameter in self.signature.parameters.values():\n            annotation = self.convert_type_to_parse_type(parameter)\n            parameters[parameter.name] = \"{%s:%s}\" % (parameter.name, annotation)\n\n        formatter = GoatFormatter()\n\n        # We have to use vformat here to ensure that kwargs will be OrderedDict\n        values = parameters.values()\n        parameter_list = list(values)\n        converted_pattern = formatter.vformat(pattern, parameter_list, parameters)\n\n        self.context_params = formatter.unused_args\n        return converted_pattern","return_type":"str","function_name":"GoatMatcher.convert","stripped_code":"def convert(self, pattern: str):\n        \"\"\"Convert the goat step string to CFParse String\"\"\"\n        parameters = OrderedDict()\n        for parameter in self.signature.parameters.values():\n            annotation = self.convert_type_to_parse_type(parameter)\n            parameters[parameter.name] = \"{%s:%s}\" % (parameter.name, annotation)\n\n        formatter = GoatFormatter()\n\n        # We have to use vformat here to ensure that kwargs will be OrderedDict\n        values = parameters.values()\n        parameter_list = list(values)\n        converted_pattern = formatter.vformat(pattern, parameter_list, parameters)\n\n        self.context_params = formatter.unused_args\n        return converted_pattern"}
{"code":"def top_k_accuracy(input:Tensor, targs:Tensor, k:int=5)->Rank0Tensor:\n    \"Computes the Top-k accuracy (target is in the top k predictions).\"\n    input = input.topk(k=k, dim=-1)[1]\n    targs = targs.unsqueeze(dim=-1).expand_as(input)\n    return (input == targs).max(dim=-1)[0].float().mean()","return_type":"Rank0Tensor","function_name":"top_k_accuracy","stripped_code":"def top_k_accuracy(input:Tensor, targs:Tensor, k:int=5):\n    \"Computes the Top-k accuracy (target is in the top k predictions).\"\n    input = input.topk(k=k, dim=-1)[1]\n    targs = targs.unsqueeze(dim=-1).expand_as(input)\n    return (input == targs).max(dim=-1)[0].float().mean()"}
{"code":"def check_keystore_json(jsondata: Dict) -> bool:\n    \"\"\" Check if ``jsondata`` has the structure of a keystore file version 3.\n\n    Note that this test is not complete, e.g. it doesn't check key derivation or cipher parameters.\n    Copied from https://github.com/vbuterin/pybitcointools\n\n    Args:\n        jsondata: Dictionary containing the data from the json file\n\n    Returns:\n        `True` if the data appears to be valid, otherwise `False`\n    \"\"\"\n    if 'crypto' not in jsondata and 'Crypto' not in jsondata:\n        return False\n    if 'version' not in jsondata:\n        return False\n    if jsondata['version'] != 3:\n        return False\n\n    crypto = jsondata.get('crypto', jsondata.get('Crypto'))\n    if 'cipher' not in crypto:\n        return False\n    if 'ciphertext' not in crypto:\n        return False\n    if 'kdf' not in crypto:\n        return False\n    if 'mac' not in crypto:\n        return False\n    return True","return_type":"bool","function_name":"check_keystore_json","stripped_code":"def check_keystore_json(jsondata: Dict):\n    \"\"\" Check if ``jsondata`` has the structure of a keystore file version 3.\n\n    Note that this test is not complete, e.g. it doesn't check key derivation or cipher parameters.\n    Copied from https://github.com/vbuterin/pybitcointools\n\n    Args:\n        jsondata: Dictionary containing the data from the json file\n\n    Returns:\n        `True` if the data appears to be valid, otherwise `False`\n    \"\"\"\n    if 'crypto' not in jsondata and 'Crypto' not in jsondata:\n        return False\n    if 'version' not in jsondata:\n        return False\n    if jsondata['version'] != 3:\n        return False\n\n    crypto = jsondata.get('crypto', jsondata.get('Crypto'))\n    if 'cipher' not in crypto:\n        return False\n    if 'ciphertext' not in crypto:\n        return False\n    if 'kdf' not in crypto:\n        return False\n    if 'mac' not in crypto:\n        return False\n    return True"}
{"code":"def get_private_endpoint(id: str, guid: str) -> str:\n    \"\"\"Get remote endpoint for delivering private payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/users/%s\" % (domain, guid)","return_type":"str","function_name":"get_private_endpoint","stripped_code":"def get_private_endpoint(id: str, guid: str):\n    \"\"\"Get remote endpoint for delivering private payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/users/%s\" % (domain, guid)"}
{"code":"def add_mappings(self, defn: Definition, target: Dict) -> None:\n        \"\"\" Process any mappings in defn, adding all of the mappings prefixes to the namespace map and\n        add a link to the first mapping to the target\n\n        @param defn: Class or Slot definition\n        @param target: context target\n        \"\"\"\n        self.add_id_prefixes(defn)\n        for mapping in defn.mappings:\n            if '://' in mapping:\n                target['@id'] = mapping\n            else:\n                if ':' not in mapping or len(mapping.split(':')) != 2:\n                    raise ValueError(f\"Definition {defn.name} = unrecognized mapping: {mapping}\")\n                ns = mapping.split(':')[0]\n                self.add_prefix(ns)\n                target['@id'] = defn.mappings[0]","return_type":"None","function_name":"ContextGenerator.add_mappings","stripped_code":"def add_mappings(self, defn: Definition, target: Dict):\n        \"\"\" Process any mappings in defn, adding all of the mappings prefixes to the namespace map and\n        add a link to the first mapping to the target\n\n        @param defn: Class or Slot definition\n        @param target: context target\n        \"\"\"\n        self.add_id_prefixes(defn)\n        for mapping in defn.mappings:\n            if '://' in mapping:\n                target['@id'] = mapping\n            else:\n                if ':' not in mapping or len(mapping.split(':')) != 2:\n                    raise ValueError(f\"Definition {defn.name} = unrecognized mapping: {mapping}\")\n                ns = mapping.split(':')[0]\n                self.add_prefix(ns)\n                target['@id'] = defn.mappings[0]"}
{"code":"def addEventListener(self, event: str, listener: _EventListenerType\n                         ) -> None:\n        \"\"\"Add event listener to this node.\n\n        ``event`` is a string which determines the event type when the new\n        listener called. Acceptable events are same as JavaScript, without\n        ``on``. For example, to add a listener which is called when this node\n        is clicked, event is ``'click``.\n        \"\"\"\n        self._add_event_listener(event, listener)","return_type":"None","function_name":"EventTarget.addEventListener","stripped_code":"def addEventListener(self, event: str, listener: _EventListenerType\n                         ):\n        \"\"\"Add event listener to this node.\n\n        ``event`` is a string which determines the event type when the new\n        listener called. Acceptable events are same as JavaScript, without\n        ``on``. For example, to add a listener which is called when this node\n        is clicked, event is ``'click``.\n        \"\"\"\n        self._add_event_listener(event, listener)"}
{"code":"def _input_likelihood(self, logits: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Computes the (batch_size,) denominator term for the log-likelihood, which is the\n        sum of the likelihoods across all possible state sequences.\n        \"\"\"\n        batch_size, sequence_length, num_tags = logits.size()\n\n        # Transpose batch size and sequence dimensions\n        mask = mask.float().transpose(0, 1).contiguous()\n        logits = logits.transpose(0, 1).contiguous()\n\n        # Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the\n        # transitions to the initial states and the logits for the first timestep.\n        if self.include_start_end_transitions:\n            alpha = self.start_transitions.view(1, num_tags) + logits[0]\n        else:\n            alpha = logits[0]\n\n        # For each i we compute logits for the transitions from timestep i-1 to timestep i.\n        # We do so in a (batch_size, num_tags, num_tags) tensor where the axes are\n        # (instance, current_tag, next_tag)\n        for i in range(1, sequence_length):\n            # The emit scores are for time i (\"next_tag\") so we broadcast along the current_tag axis.\n            emit_scores = logits[i].view(batch_size, 1, num_tags)\n            # Transition scores are (current_tag, next_tag) so we broadcast along the instance axis.\n            transition_scores = self.transitions.view(1, num_tags, num_tags)\n            # Alpha is for the current_tag, so we broadcast along the next_tag axis.\n            broadcast_alpha = alpha.view(batch_size, num_tags, 1)\n\n            # Add all the scores together and logexp over the current_tag axis\n            inner = broadcast_alpha + emit_scores + transition_scores\n\n            # In valid positions (mask == 1) we want to take the logsumexp over the current_tag dimension\n            # of ``inner``. Otherwise (mask == 0) we want to retain the previous alpha.\n            alpha = (util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) +\n                     alpha * (1 - mask[i]).view(batch_size, 1))\n\n        # Every sequence needs to end with a transition to the stop_tag.\n        if self.include_start_end_transitions:\n            stops = alpha + self.end_transitions.view(1, num_tags)\n        else:\n            stops = alpha\n\n        # Finally we log_sum_exp along the num_tags dim, result is (batch_size,)\n        return util.logsumexp(stops)","return_type":"torch.Tensor","function_name":"ConditionalRandomField._input_likelihood","stripped_code":"def _input_likelihood(self, logits: torch.Tensor, mask: torch.Tensor):\n        \"\"\"\n        Computes the (batch_size,) denominator term for the log-likelihood, which is the\n        sum of the likelihoods across all possible state sequences.\n        \"\"\"\n        batch_size, sequence_length, num_tags = logits.size()\n\n        # Transpose batch size and sequence dimensions\n        mask = mask.float().transpose(0, 1).contiguous()\n        logits = logits.transpose(0, 1).contiguous()\n\n        # Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the\n        # transitions to the initial states and the logits for the first timestep.\n        if self.include_start_end_transitions:\n            alpha = self.start_transitions.view(1, num_tags) + logits[0]\n        else:\n            alpha = logits[0]\n\n        # For each i we compute logits for the transitions from timestep i-1 to timestep i.\n        # We do so in a (batch_size, num_tags, num_tags) tensor where the axes are\n        # (instance, current_tag, next_tag)\n        for i in range(1, sequence_length):\n            # The emit scores are for time i (\"next_tag\") so we broadcast along the current_tag axis.\n            emit_scores = logits[i].view(batch_size, 1, num_tags)\n            # Transition scores are (current_tag, next_tag) so we broadcast along the instance axis.\n            transition_scores = self.transitions.view(1, num_tags, num_tags)\n            # Alpha is for the current_tag, so we broadcast along the next_tag axis.\n            broadcast_alpha = alpha.view(batch_size, num_tags, 1)\n\n            # Add all the scores together and logexp over the current_tag axis\n            inner = broadcast_alpha + emit_scores + transition_scores\n\n            # In valid positions (mask == 1) we want to take the logsumexp over the current_tag dimension\n            # of ``inner``. Otherwise (mask == 0) we want to retain the previous alpha.\n            alpha = (util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) +\n                     alpha * (1 - mask[i]).view(batch_size, 1))\n\n        # Every sequence needs to end with a transition to the stop_tag.\n        if self.include_start_end_transitions:\n            stops = alpha + self.end_transitions.view(1, num_tags)\n        else:\n            stops = alpha\n\n        # Finally we log_sum_exp along the num_tags dim, result is (batch_size,)\n        return util.logsumexp(stops)"}
{"code":"def stack_pop(self, num_items: int=1, type_hint: str=None) -> Any:\n        # TODO: Needs to be replaced with\n        # `Union[int, bytes, Tuple[Union[int, bytes], ...]]` if done properly\n        \"\"\"\n        Pop and return a number of items equal to ``num_items`` from the stack.\n        ``type_hint`` can be either ``'uint256'`` or ``'bytes'``.  The return value\n        will be an ``int`` or ``bytes`` type depending on the value provided for\n        the ``type_hint``.\n\n        Raise `eth.exceptions.InsufficientStack` if there are not enough items on\n        the stack.\n        \"\"\"\n        return self._stack.pop(num_items, type_hint)","return_type":"Any","function_name":"BaseComputation.stack_pop","stripped_code":"def stack_pop(self, num_items: int=1, type_hint: str=None):\n        # TODO: Needs to be replaced with\n        # `Union[int, bytes, Tuple[Union[int, bytes], ...]]` if done properly\n        \"\"\"\n        Pop and return a number of items equal to ``num_items`` from the stack.\n        ``type_hint`` can be either ``'uint256'`` or ``'bytes'``.  The return value\n        will be an ``int`` or ``bytes`` type depending on the value provided for\n        the ``type_hint``.\n\n        Raise `eth.exceptions.InsufficientStack` if there are not enough items on\n        the stack.\n        \"\"\"\n        return self._stack.pop(num_items, type_hint)"}
{"code":"def parse_datetime(record: str) -> Optional[datetime]:\n    \"\"\" Parse a datetime string into a python datetime object \"\"\"\n    # NEM defines Date8, DateTime12 and DateTime14\n    format_strings = {8: '%Y%m%d', 12: '%Y%m%d%H%M', 14: '%Y%m%d%H%M%S'}\n    if record == '':\n        return None\n    return datetime.strptime(record.strip(),\n                                          format_strings[len(record.strip())])","return_type":"Optional[datetime]","function_name":"parse_datetime","stripped_code":"def parse_datetime(record: str):\n    \"\"\" Parse a datetime string into a python datetime object \"\"\"\n    # NEM defines Date8, DateTime12 and DateTime14\n    format_strings = {8: '%Y%m%d', 12: '%Y%m%d%H%M', 14: '%Y%m%d%H%M%S'}\n    if record == '':\n        return None\n    return datetime.strptime(record.strip(),\n                                          format_strings[len(record.strip())])"}
{"code":"def pause_tube(self, tube: str, delay: int) -> None:\n        \"\"\"Prevents jobs from being reserved from a tube for a period of time.\n\n        :param tube: The tube to pause.\n        :param delay: The number of seconds to pause the tube for.\n        \"\"\"\n        self._send_cmd(b'pause-tube %b %d' % (tube.encode('ascii'), delay), b'PAUSED')","return_type":"None","function_name":"Client.pause_tube","stripped_code":"def pause_tube(self, tube: str, delay: int):\n        \"\"\"Prevents jobs from being reserved from a tube for a period of time.\n\n        :param tube: The tube to pause.\n        :param delay: The number of seconds to pause the tube for.\n        \"\"\"\n        self._send_cmd(b'pause-tube %b %d' % (tube.encode('ascii'), delay), b'PAUSED')"}
{"code":"def _state_after_transition(self, current_state: int, target_state: int) -> int:\n        \"\"\"\n        Return the state reachable after a transition.\n        Since the state for a gene can only change by 1, if the absolute value of the\n        difference current_state - target_state is greater than 1, we lower it to 1 or -1.\n        \n        Examples\n        --------\n\n        >>> model._state_after_transition(0, 2)\n        1  # Because 2 is too far from 0, the gene can only reach 1.\n        >>> model._state_after_transition(1, 5)\n        2  # 5 is still is too far from 1, so the gene can only reach 2.\n        >>> model._state_after_transition(2, 1)\n        1  # No problem here, 1 is at distance 1 from 2\n        >>> model._state_after_transition(1, 1)\n        1  # The state does not change here\n\n        \"\"\"\n        return current_state + (current_state < target_state) - (current_state > target_state)","return_type":"int","function_name":"DiscreteModel._state_after_transition","stripped_code":"def _state_after_transition(self, current_state: int, target_state: int):\n        \"\"\"\n        Return the state reachable after a transition.\n        Since the state for a gene can only change by 1, if the absolute value of the\n        difference current_state - target_state is greater than 1, we lower it to 1 or -1.\n        \n        Examples\n        --------\n\n        >>> model._state_after_transition(0, 2)\n        1  # Because 2 is too far from 0, the gene can only reach 1.\n        >>> model._state_after_transition(1, 5)\n        2  # 5 is still is too far from 1, so the gene can only reach 2.\n        >>> model._state_after_transition(2, 1)\n        1  # No problem here, 1 is at distance 1 from 2\n        >>> model._state_after_transition(1, 1)\n        1  # The state does not change here\n\n        \"\"\"\n        return current_state + (current_state < target_state) - (current_state > target_state)"}
{"code":"def fetch_all_objects_from_db_where(self,\n                                        cls: Type[T],\n                                        table: str,\n                                        fieldlist: Sequence[str],\n                                        construct_with_pk: bool,\n                                        wheredict: Optional[Dict[str, Any]],\n                                        *args) -> List[T]:\n        \"\"\"\n        Fetches all objects from a table, given a set of WHERE criteria\n        (ANDed), returning an array of objects of class cls.\n        As usual here, the first field in the fieldlist must be the PK.\n        \"\"\"\n        sql = (\n            \"SELECT \" + \",\".join([self.delimit(x) for x in fieldlist]) +\n            \" FROM \" + self.delimit(table)\n        )\n        whereargs = []\n        if wheredict is not None:\n            sql += \" WHERE \" + \" AND \".join([\n                self.delimit(k) + \"=?\"\n                for k in wheredict.keys()\n            ])\n            whereargs = wheredict.values()\n        rows = self.fetchall(sql, *whereargs)\n        objects = []\n        for row in rows:\n            objects.append(\n                create_object_from_list(cls, fieldlist, row, *args,\n                                        construct_with_pk=construct_with_pk))\n        return objects","return_type":"List[T]","function_name":"DatabaseSupporter.fetch_all_objects_from_db_where","stripped_code":"def fetch_all_objects_from_db_where(self,\n                                        cls: Type[T],\n                                        table: str,\n                                        fieldlist: Sequence[str],\n                                        construct_with_pk: bool,\n                                        wheredict: Optional[Dict[str, Any]],\n                                        *args):\n        \"\"\"\n        Fetches all objects from a table, given a set of WHERE criteria\n        (ANDed), returning an array of objects of class cls.\n        As usual here, the first field in the fieldlist must be the PK.\n        \"\"\"\n        sql = (\n            \"SELECT \" + \",\".join([self.delimit(x) for x in fieldlist]) +\n            \" FROM \" + self.delimit(table)\n        )\n        whereargs = []\n        if wheredict is not None:\n            sql += \" WHERE \" + \" AND \".join([\n                self.delimit(k) + \"=?\"\n                for k in wheredict.keys()\n            ])\n            whereargs = wheredict.values()\n        rows = self.fetchall(sql, *whereargs)\n        objects = []\n        for row in rows:\n            objects.append(\n                create_object_from_list(cls, fieldlist, row, *args,\n                                        construct_with_pk=construct_with_pk))\n        return objects"}
{"code":"def filtered_search(self,\n               id_list: Iterable,\n               negated_classes: Iterable,\n               limit: Optional[int],\n               taxon_filter: Optional,\n               category_filter: Optional,\n               method: Optional) -> SimResult:\n        \"\"\"\n        Given an input iterable of classes or individuals,\n        provides a ranking of similar profiles\n        \"\"\"\n        pass","return_type":"SimResult","function_name":"FilteredSearchable.filtered_search","stripped_code":"def filtered_search(self,\n               id_list: Iterable,\n               negated_classes: Iterable,\n               limit: Optional[int],\n               taxon_filter: Optional,\n               category_filter: Optional,\n               method: Optional):\n        \"\"\"\n        Given an input iterable of classes or individuals,\n        provides a ranking of similar profiles\n        \"\"\"\n        pass"}
{"code":"def update_properties(self) -> None:\n        \"\"\"Update properties group of parameters.\"\"\"\n        self.update(path=URL_GET + GROUP.format(group=PROPERTIES))","return_type":"None","function_name":"Properties.update_properties","stripped_code":"def update_properties(self):\n        \"\"\"Update properties group of parameters.\"\"\"\n        self.update(path=URL_GET + GROUP.format(group=PROPERTIES))"}
{"code":"def register(\n        self,\n        pattern: str,\n        handler: Any,\n        flags: int = 0,\n        channel: str = \"*\",\n        subtype: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Register a new handler for a specific :class:`slack.events.Message`.\n\n        The routing is based on regex pattern matching the message text and the incoming slack channel.\n\n        Args:\n            pattern: Regex pattern matching the message text.\n            handler: Callback\n            flags: Regex flags.\n            channel: Slack channel ID. Use * for any.\n            subtype: Message subtype\n        \"\"\"\n        LOG.debug('Registering message endpoint \"%s: %s\"', pattern, handler)\n        match = re.compile(pattern, flags)\n\n        if subtype not in self._routes[channel]:\n            self._routes[channel][subtype] = dict()\n\n        if match in self._routes[channel][subtype]:\n            self._routes[channel][subtype][match].append(handler)\n        else:\n            self._routes[channel][subtype][match] = [handler]","return_type":"None","function_name":"MessageRouter.register","stripped_code":"def register(\n        self,\n        pattern: str,\n        handler: Any,\n        flags: int = 0,\n        channel: str = \"*\",\n        subtype: Optional[str] = None,\n    ):\n        \"\"\"\n        Register a new handler for a specific :class:`slack.events.Message`.\n\n        The routing is based on regex pattern matching the message text and the incoming slack channel.\n\n        Args:\n            pattern: Regex pattern matching the message text.\n            handler: Callback\n            flags: Regex flags.\n            channel: Slack channel ID. Use * for any.\n            subtype: Message subtype\n        \"\"\"\n        LOG.debug('Registering message endpoint \"%s: %s\"', pattern, handler)\n        match = re.compile(pattern, flags)\n\n        if subtype not in self._routes[channel]:\n            self._routes[channel][subtype] = dict()\n\n        if match in self._routes[channel][subtype]:\n            self._routes[channel][subtype][match].append(handler)\n        else:\n            self._routes[channel][subtype][match] = [handler]"}
{"code":"def read_var_bytes(self, max_size=sys.maxsize) -> bytes:\n        \"\"\"\n        Read a variable length of bytes from the stream.\n\n        Args:\n            max_size (int): (Optional) maximum number of bytes to read.\n\n        Returns:\n            bytes:\n        \"\"\"\n        length = self.read_var_int(max_size)\n        return self.read_bytes(length)","return_type":"bytes","function_name":"BinaryReader.read_var_bytes","stripped_code":"def read_var_bytes(self, max_size=sys.maxsize):\n        \"\"\"\n        Read a variable length of bytes from the stream.\n\n        Args:\n            max_size (int): (Optional) maximum number of bytes to read.\n\n        Returns:\n            bytes:\n        \"\"\"\n        length = self.read_var_int(max_size)\n        return self.read_bytes(length)"}
{"code":"def read_response(self, delegate: httputil.HTTPMessageDelegate) -> Awaitable[bool]:\n        \"\"\"Read a single HTTP response.\n\n        Typical client-mode usage is to write a request using `write_headers`,\n        `write`, and `finish`, and then call ``read_response``.\n\n        :arg delegate: a `.HTTPMessageDelegate`\n\n        Returns a `.Future` that resolves to a bool after the full response has\n        been read. The result is true if the stream is still open.\n        \"\"\"\n        if self.params.decompress:\n            delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)\n        return self._read_message(delegate)","return_type":"Awaitable[bool]","function_name":"HTTP1Connection.read_response","stripped_code":"def read_response(self, delegate: httputil.HTTPMessageDelegate):\n        \"\"\"Read a single HTTP response.\n\n        Typical client-mode usage is to write a request using `write_headers`,\n        `write`, and `finish`, and then call ``read_response``.\n\n        :arg delegate: a `.HTTPMessageDelegate`\n\n        Returns a `.Future` that resolves to a bool after the full response has\n        been read. The result is true if the stream is still open.\n        \"\"\"\n        if self.params.decompress:\n            delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)\n        return self._read_message(delegate)"}
{"code":"def to_tag(self) -> str:\n        \"\"\"\n        Convert a Language back to a standard language tag, as a string.\n        This is also the str() representation of a Language object.\n\n        >>> Language.make(language='en', region='GB').to_tag()\n        'en-GB'\n\n        >>> Language.make(language='yue', script='Hant', region='HK').to_tag()\n        'yue-Hant-HK'\n\n        >>> Language.make(script='Arab').to_tag()\n        'und-Arab'\n\n        >>> str(Language.make(region='IN'))\n        'und-IN'\n        \"\"\"\n        if self._str_tag is not None:\n            return self._str_tag\n        subtags = ['und']\n        if self.language:\n            subtags[0] = self.language\n        if self.extlangs:\n            for extlang in sorted(self.extlangs):\n                subtags.append(extlang)\n        if self.script:\n            subtags.append(self.script)\n        if self.region:\n            subtags.append(self.region)\n        if self.variants:\n            for variant in sorted(self.variants):\n                subtags.append(variant)\n        if self.extensions:\n            for ext in self.extensions:\n                subtags.append(ext)\n        if self.private:\n            subtags.append(self.private)\n        self._str_tag = '-'.join(subtags)\n        return self._str_tag","return_type":"str","function_name":"Language.to_tag","stripped_code":"def to_tag(self):\n        \"\"\"\n        Convert a Language back to a standard language tag, as a string.\n        This is also the str() representation of a Language object.\n\n        >>> Language.make(language='en', region='GB').to_tag()\n        'en-GB'\n\n        >>> Language.make(language='yue', script='Hant', region='HK').to_tag()\n        'yue-Hant-HK'\n\n        >>> Language.make(script='Arab').to_tag()\n        'und-Arab'\n\n        >>> str(Language.make(region='IN'))\n        'und-IN'\n        \"\"\"\n        if self._str_tag is not None:\n            return self._str_tag\n        subtags = ['und']\n        if self.language:\n            subtags[0] = self.language\n        if self.extlangs:\n            for extlang in sorted(self.extlangs):\n                subtags.append(extlang)\n        if self.script:\n            subtags.append(self.script)\n        if self.region:\n            subtags.append(self.region)\n        if self.variants:\n            for variant in sorted(self.variants):\n                subtags.append(variant)\n        if self.extensions:\n            for ext in self.extensions:\n                subtags.append(ext)\n        if self.private:\n            subtags.append(self.private)\n        self._str_tag = '-'.join(subtags)\n        return self._str_tag"}
{"code":"def lastElementChild(self) -> Optional[AbstractNode]:\n        \"\"\"Last Element child node.\n\n        If this node has no element child, return None.\n        \"\"\"\n        for child in reversed(self.childNodes):  # type: ignore\n            if child.nodeType == Node.ELEMENT_NODE:\n                return child\n        return None","return_type":"Optional[AbstractNode]","function_name":"ParentNode.lastElementChild","stripped_code":"def lastElementChild(self):\n        \"\"\"Last Element child node.\n\n        If this node has no element child, return None.\n        \"\"\"\n        for child in reversed(self.childNodes):  # type: ignore\n            if child.nodeType == Node.ELEMENT_NODE:\n                return child\n        return None"}
{"code":"def __uncompress_public_key(public_key: bytes) -> bytes:\n        \"\"\"\n        Uncompress the compressed public key.\n        :param public_key: compressed public key\n        :return: uncompressed public key\n        \"\"\"\n        is_even = public_key.startswith(b'\\x02')\n        x = string_to_number(public_key[1:])\n\n        curve = NIST256p.curve\n        order = NIST256p.order\n        p = curve.p()\n        alpha = (pow(x, 3, p) + (curve.a() * x) + curve.b()) % p\n        beta = square_root_mod_prime(alpha, p)\n        if is_even == bool(beta & 1):\n            y = p - beta\n        else:\n            y = beta\n        point = Point(curve, x, y, order)\n        return b''.join([number_to_string(point.x(), order), number_to_string(point.y(), order)])","return_type":"bytes","function_name":"ECIES.__uncompress_public_key","stripped_code":"def __uncompress_public_key(public_key: bytes):\n        \"\"\"\n        Uncompress the compressed public key.\n        :param public_key: compressed public key\n        :return: uncompressed public key\n        \"\"\"\n        is_even = public_key.startswith(b'\\x02')\n        x = string_to_number(public_key[1:])\n\n        curve = NIST256p.curve\n        order = NIST256p.order\n        p = curve.p()\n        alpha = (pow(x, 3, p) + (curve.a() * x) + curve.b()) % p\n        beta = square_root_mod_prime(alpha, p)\n        if is_even == bool(beta & 1):\n            y = p - beta\n        else:\n            y = beta\n        point = Point(curve, x, y, order)\n        return b''.join([number_to_string(point.x(), order), number_to_string(point.y(), order)])"}
{"code":"def unsigned_hex_to_signed_int(hex_string: str) -> int:\n    \"\"\"Converts a 64-bit hex string to a signed int value.\n\n    This is due to the fact that Apache Thrift only has signed values.\n\n    Examples:\n        '17133d482ba4f605' => 1662740067609015813\n        'b6dbb1c2b362bf51' => -5270423489115668655\n\n    :param hex_string: the string representation of a zipkin ID\n    :returns: signed int representation\n    \"\"\"\n    v = struct.unpack(\n        'q', struct.pack('Q', int(hex_string, 16)))[0]  # type: int\n    return v","return_type":"int","function_name":"unsigned_hex_to_signed_int","stripped_code":"def unsigned_hex_to_signed_int(hex_string: str):\n    \"\"\"Converts a 64-bit hex string to a signed int value.\n\n    This is due to the fact that Apache Thrift only has signed values.\n\n    Examples:\n        '17133d482ba4f605' => 1662740067609015813\n        'b6dbb1c2b362bf51' => -5270423489115668655\n\n    :param hex_string: the string representation of a zipkin ID\n    :returns: signed int representation\n    \"\"\"\n    v = struct.unpack(\n        'q', struct.pack('Q', int(hex_string, 16)))[0]  # type: int\n    return v"}
{"code":"def replace_masked_values(tensor: torch.Tensor, mask: torch.Tensor, replace_with: float) -> torch.Tensor:\n    \"\"\"\n    Replaces all masked values in ``tensor`` with ``replace_with``.  ``mask`` must be broadcastable\n    to the same shape as ``tensor``. We require that ``tensor.dim() == mask.dim()``, as otherwise we\n    won't know which dimensions of the mask to unsqueeze.\n\n    This just does ``tensor.masked_fill()``, except the pytorch method fills in things with a mask\n    value of 1, where we want the opposite.  You can do this in your own code with\n    ``tensor.masked_fill((1 - mask).byte(), replace_with)``.\n    \"\"\"\n    if tensor.dim() != mask.dim():\n        raise ConfigurationError(\"tensor.dim() (%d) != mask.dim() (%d)\" % (tensor.dim(), mask.dim()))\n    return tensor.masked_fill((1 - mask).byte(), replace_with)","return_type":"torch.Tensor","function_name":"replace_masked_values","stripped_code":"def replace_masked_values(tensor: torch.Tensor, mask: torch.Tensor, replace_with: float):\n    \"\"\"\n    Replaces all masked values in ``tensor`` with ``replace_with``.  ``mask`` must be broadcastable\n    to the same shape as ``tensor``. We require that ``tensor.dim() == mask.dim()``, as otherwise we\n    won't know which dimensions of the mask to unsqueeze.\n\n    This just does ``tensor.masked_fill()``, except the pytorch method fills in things with a mask\n    value of 1, where we want the opposite.  You can do this in your own code with\n    ``tensor.masked_fill((1 - mask).byte(), replace_with)``.\n    \"\"\"\n    if tensor.dim() != mask.dim():\n        raise ConfigurationError(\"tensor.dim() (%d) != mask.dim() (%d)\" % (tensor.dim(), mask.dim()))\n    return tensor.masked_fill((1 - mask).byte(), replace_with)"}
{"code":"def already_pending(self, unit_type: Union[UpgradeId, UnitTypeId], all_units: bool=False) -> int:\n        \"\"\"\n        Returns a number of buildings or units already in progress, or if a\n        worker is en route to build it. This also includes queued orders for\n        workers and build queues of buildings.\n\n        If all_units==True, then build queues of other units (such as Carriers\n        (Interceptors) or Oracles (Stasis Ward)) are also included.\n        \"\"\"\n\n        # TODO / FIXME: SCV building a structure might be counted as two units\n\n        if isinstance(unit_type, UpgradeId):\n            return self.already_pending_upgrade(unit_type)\n            \n        ability = self._game_data.units[unit_type.value].creation_ability\n\n        amount = len(self.units(unit_type).not_ready)\n\n        if all_units:\n            amount += sum([o.ability == ability for u in self.units for o in u.orders])\n        else:\n            amount += sum([o.ability == ability for w in self.workers for o in w.orders])\n            amount += sum([egg.orders[0].ability == ability for egg in self.units(UnitTypeId.EGG)])\n\n        return amount","return_type":"int","function_name":"BotAI.already_pending","stripped_code":"def already_pending(self, unit_type: Union[UpgradeId, UnitTypeId], all_units: bool=False):\n        \"\"\"\n        Returns a number of buildings or units already in progress, or if a\n        worker is en route to build it. This also includes queued orders for\n        workers and build queues of buildings.\n\n        If all_units==True, then build queues of other units (such as Carriers\n        (Interceptors) or Oracles (Stasis Ward)) are also included.\n        \"\"\"\n\n        # TODO / FIXME: SCV building a structure might be counted as two units\n\n        if isinstance(unit_type, UpgradeId):\n            return self.already_pending_upgrade(unit_type)\n            \n        ability = self._game_data.units[unit_type.value].creation_ability\n\n        amount = len(self.units(unit_type).not_ready)\n\n        if all_units:\n            amount += sum([o.ability == ability for u in self.units for o in u.orders])\n        else:\n            amount += sum([o.ability == ability for w in self.workers for o in w.orders])\n            amount += sum([egg.orders[0].ability == ability for egg in self.units(UnitTypeId.EGG)])\n\n        return amount"}
{"code":"def deconstruct(self, including_private: bool=False) -> bytes:\n        \"\"\"Return state of this FinTSClient instance as an opaque datablob. You should not\n        use this object after calling this method.\n\n        Information about the connection is implicitly retrieved from the bank and\n        cached in the FinTSClient. This includes: system identifier, bank parameter\n        data, user parameter data. It's not strictly required to retain this information\n        across sessions, but beneficial. If possible, an API user SHOULD use this method\n        to serialize the client instance before destroying it, and provide the serialized\n        data next time an instance is constructed.\n\n        Parameter `including_private` should be set to True, if the storage is sufficiently\n        secure (with regards to confidentiality) to include private data, specifically,\n        account numbers and names. Most often this is the case.\n\n        Note: No connection information is stored in the datablob, neither is the PIN.\n        \"\"\"\n        data = self._deconstruct_v1(including_private=including_private)\n        return compress_datablob(DATA_BLOB_MAGIC, 1, data)","return_type":"bytes","function_name":"FinTS3Client.deconstruct","stripped_code":"def deconstruct(self, including_private: bool=False):\n        \"\"\"Return state of this FinTSClient instance as an opaque datablob. You should not\n        use this object after calling this method.\n\n        Information about the connection is implicitly retrieved from the bank and\n        cached in the FinTSClient. This includes: system identifier, bank parameter\n        data, user parameter data. It's not strictly required to retain this information\n        across sessions, but beneficial. If possible, an API user SHOULD use this method\n        to serialize the client instance before destroying it, and provide the serialized\n        data next time an instance is constructed.\n\n        Parameter `including_private` should be set to True, if the storage is sufficiently\n        secure (with regards to confidentiality) to include private data, specifically,\n        account numbers and names. Most often this is the case.\n\n        Note: No connection information is stored in the datablob, neither is the PIN.\n        \"\"\"\n        data = self._deconstruct_v1(including_private=including_private)\n        return compress_datablob(DATA_BLOB_MAGIC, 1, data)"}
{"code":"def max_lemma_count(ambiguous_word: str) -> \"wn.Synset\":\n    \"\"\"\n    Returns the sense with the highest lemma_name count.\n    The max_lemma_count() can be treated as a rough gauge for the\n    Most Frequent Sense (MFS), if no other sense annotated corpus is available.\n    NOTE: The lemma counts are from the Brown Corpus\n\n    :param ambiguous_word: String, a single word.\n    :return: The estimated most common Synset.\n    \"\"\"\n    sense2lemmacounts = {}\n    for i in wn.synsets(ambiguous_word, pos=None):\n        sense2lemmacounts[i] = sum(j.count() for j in i.lemmas())\n    return max(sense2lemmacounts, key=sense2lemmacounts.get)","return_type":"\"wn.Synset\"","function_name":"max_lemma_count","stripped_code":"def max_lemma_count(ambiguous_word: str):\n    \"\"\"\n    Returns the sense with the highest lemma_name count.\n    The max_lemma_count() can be treated as a rough gauge for the\n    Most Frequent Sense (MFS), if no other sense annotated corpus is available.\n    NOTE: The lemma counts are from the Brown Corpus\n\n    :param ambiguous_word: String, a single word.\n    :return: The estimated most common Synset.\n    \"\"\"\n    sense2lemmacounts = {}\n    for i in wn.synsets(ambiguous_word, pos=None):\n        sense2lemmacounts[i] = sum(j.count() for j in i.lemmas())\n    return max(sense2lemmacounts, key=sense2lemmacounts.get)"}
{"code":"def visit_Call(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s representation as function call.\"\"\"\n        args = node.args\n        try:\n            kwds = node.keywords\n        except AttributeError:\n            kwds = []\n        self.compact = True\n        args_src = (self.visit(arg) for arg in args)\n        kwds_src = (self.visit(kwd) for kwd in kwds)\n        param_src = ', '.join(chain(args_src, kwds_src))\n        src = f\"{self.visit(node.func)}({param_src})\"\n        self.compact = False\n        return src","return_type":"str","function_name":"SourceGenerator.visit_Call","stripped_code":"def visit_Call(self, node: AST, dfltChaining: bool = True):\n        \"\"\"Return `node`s representation as function call.\"\"\"\n        args = node.args\n        try:\n            kwds = node.keywords\n        except AttributeError:\n            kwds = []\n        self.compact = True\n        args_src = (self.visit(arg) for arg in args)\n        kwds_src = (self.visit(kwd) for kwd in kwds)\n        param_src = ', '.join(chain(args_src, kwds_src))\n        src = f\"{self.visit(node.func)}({param_src})\"\n        self.compact = False\n        return src"}
{"code":"def make_copy_paste_env(env: Dict[str, str]) -> str:\n    \"\"\"\n    Convert an environment into a set of commands that can be copied/pasted, on\n    the build platform, to recreate that environment.\n    \"\"\"\n    windows = platform.system() == \"Windows\"\n    cmd = \"set\" if windows else \"export\"\n    return (\n        \"\\n\".join(\n            \"{cmd} {k}={v}\".format(\n                cmd=cmd,\n                k=k,\n                v=env[k] if windows else subprocess.list2cmdline([env[k]])\n            ) for k in sorted(env.keys())\n        )\n    )","return_type":"str","function_name":"make_copy_paste_env","stripped_code":"def make_copy_paste_env(env: Dict[str, str]):\n    \"\"\"\n    Convert an environment into a set of commands that can be copied/pasted, on\n    the build platform, to recreate that environment.\n    \"\"\"\n    windows = platform.system() == \"Windows\"\n    cmd = \"set\" if windows else \"export\"\n    return (\n        \"\\n\".join(\n            \"{cmd} {k}={v}\".format(\n                cmd=cmd,\n                k=k,\n                v=env[k] if windows else subprocess.list2cmdline([env[k]])\n            ) for k in sorted(env.keys())\n        )\n    )"}
{"code":"def after_request(self, func: Callable, name: AppOrBlueprintKey=None) -> Callable:\n        \"\"\"Add an after request function.\n\n        This is designed to be used as a decorator. An example usage,\n\n        .. code-block:: python\n\n            @app.after_request\n            def func(response):\n                return response\n\n        Arguments:\n            func: The after request function itself.\n            name: Optional blueprint key name.\n        \"\"\"\n        handler = ensure_coroutine(func)\n        self.after_request_funcs[name].append(handler)\n        return func","return_type":"Callable","function_name":"Quart.after_request","stripped_code":"def after_request(self, func: Callable, name: AppOrBlueprintKey=None):\n        \"\"\"Add an after request function.\n\n        This is designed to be used as a decorator. An example usage,\n\n        .. code-block:: python\n\n            @app.after_request\n            def func(response):\n                return response\n\n        Arguments:\n            func: The after request function itself.\n            name: Optional blueprint key name.\n        \"\"\"\n        handler = ensure_coroutine(func)\n        self.after_request_funcs[name].append(handler)\n        return func"}
{"code":"def string(self, units: typing.Optional[str] = None) -> str:\n        \"\"\"Return a string representation of the pressure, using the given units.\"\"\"\n        if not units:\n            _units: str = self._units\n        else:\n            if not units.upper() in CustomPressure.legal_units:\n                raise UnitsError(\"unrecognized pressure unit: '\" + units + \"'\")\n            _units = units.upper()\n        val = self.value(units)\n        if _units == \"MB\":\n            return \"%.0f mb\" % val\n        if _units == \"HPA\":\n            return \"%.0f hPa\" % val\n        if _units == \"IN\":\n            return \"%.2f inches\" % val\n        if _units == \"MM\":\n            return \"%.0f mmHg\" % val\n\n        raise ValueError(_units)","return_type":"str","function_name":"CustomPressure.string","stripped_code":"def string(self, units: typing.Optional[str] = None):\n        \"\"\"Return a string representation of the pressure, using the given units.\"\"\"\n        if not units:\n            _units: str = self._units\n        else:\n            if not units.upper() in CustomPressure.legal_units:\n                raise UnitsError(\"unrecognized pressure unit: '\" + units + \"'\")\n            _units = units.upper()\n        val = self.value(units)\n        if _units == \"MB\":\n            return \"%.0f mb\" % val\n        if _units == \"HPA\":\n            return \"%.0f hPa\" % val\n        if _units == \"IN\":\n            return \"%.2f inches\" % val\n        if _units == \"MM\":\n            return \"%.0f mmHg\" % val\n\n        raise ValueError(_units)"}
{"code":"def _collapse_edge_passing_predicates(graph: BELGraph, edge_predicates: EdgePredicates = None) -> None:\n    \"\"\"Collapse all edges passing the given edge predicates.\"\"\"\n    for u, v, _ in filter_edges(graph, edge_predicates=edge_predicates):\n        collapse_pair(graph, survivor=u, victim=v)","return_type":"None","function_name":"_collapse_edge_passing_predicates","stripped_code":"def _collapse_edge_passing_predicates(graph: BELGraph, edge_predicates: EdgePredicates = None):\n    \"\"\"Collapse all edges passing the given edge predicates.\"\"\"\n    for u, v, _ in filter_edges(graph, edge_predicates=edge_predicates):\n        collapse_pair(graph, survivor=u, victim=v)"}
{"code":"def _collection_function_inclusion_builder(funcs: Iterable[str]) -> NodePredicate:\n    \"\"\"Build a function inclusion filter for a collection of functions.\"\"\"\n    funcs = set(funcs)\n\n    if not funcs:\n        raise ValueError('can not build function inclusion filter with empty list of functions')\n\n    def functions_inclusion_filter(_: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a node that is one of the enclosed functions.\"\"\"\n        return node.function in funcs\n\n    return functions_inclusion_filter","return_type":"NodePredicate","function_name":"_collection_function_inclusion_builder","stripped_code":"def _collection_function_inclusion_builder(funcs: Iterable[str]):\n    \"\"\"Build a function inclusion filter for a collection of functions.\"\"\"\n    funcs = set(funcs)\n\n    if not funcs:\n        raise ValueError('can not build function inclusion filter with empty list of functions')\n\n    def functions_inclusion_filter(_: BELGraph, node: BaseEntity) -> bool:\n        \"\"\"Pass only for a node that is one of the enclosed functions.\"\"\"\n        return node.function in funcs\n\n    return functions_inclusion_filter"}
{"code":"def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path","return_type":"Optional[str]","function_name":"StaticFileHandler.validate_absolute_path","stripped_code":"def validate_absolute_path(self, root: str, absolute_path: str):\n        \"\"\"Validate and return the absolute path.\n\n        ``root`` is the configured path for the `StaticFileHandler`,\n        and ``path`` is the result of `get_absolute_path`\n\n        This is an instance method called during request processing,\n        so it may raise `HTTPError` or use methods like\n        `RequestHandler.redirect` (return None after redirecting to\n        halt further processing).  This is where 404 errors for missing files\n        are generated.\n\n        This method may modify the path before returning it, but note that\n        any such modifications will not be understood by `make_static_url`.\n\n        In instance methods, this method's result is available as\n        ``self.absolute_path``.\n\n        .. versionadded:: 3.1\n        \"\"\"\n        # os.path.abspath strips a trailing /.\n        # We must add it back to `root` so that we only match files\n        # in a directory named `root` instead of files starting with\n        # that prefix.\n        root = os.path.abspath(root)\n        if not root.endswith(os.path.sep):\n            # abspath always removes a trailing slash, except when\n            # root is '/'. This is an unusual case, but several projects\n            # have independently discovered this technique to disable\n            # Tornado's path validation and (hopefully) do their own,\n            # so we need to support it.\n            root += os.path.sep\n        # The trailing slash also needs to be temporarily added back\n        # the requested path so a request to root/ will match.\n        if not (absolute_path + os.path.sep).startswith(root):\n            raise HTTPError(403, \"%s is not in root static directory\", self.path)\n        if os.path.isdir(absolute_path) and self.default_filename is not None:\n            # need to look at the request.path here for when path is empty\n            # but there is some prefix to the path that was already\n            # trimmed by the routing\n            if not self.request.path.endswith(\"/\"):\n                self.redirect(self.request.path + \"/\", permanent=True)\n                return None\n            absolute_path = os.path.join(absolute_path, self.default_filename)\n        if not os.path.exists(absolute_path):\n            raise HTTPError(404)\n        if not os.path.isfile(absolute_path):\n            raise HTTPError(403, \"%s is not a file\", self.path)\n        return absolute_path"}
{"code":"def stream_with_context(func: Callable) -> Callable:\n    \"\"\"Share the current request context with a generator.\n\n    This allows the request context to be accessed within a streaming\n    generator, for example,\n\n    .. code-block:: python\n\n        @app.route('/')\n        def index() -> AsyncGenerator[bytes, None]:\n            @stream_with_context\n            async def generator() -> bytes:\n                yield request.method.encode()\n                yield b' '\n                yield request.path.encode()\n\n            return generator()\n\n    \"\"\"\n    request_context = _request_ctx_stack.top.copy()\n\n    @wraps(func)\n    async def generator(*args: Any, **kwargs: Any) -> Any:\n        async with request_context:\n            async for data in func(*args, **kwargs):\n                yield data\n    return generator","return_type":"Callable","function_name":"stream_with_context","stripped_code":"def stream_with_context(func: Callable):\n    \"\"\"Share the current request context with a generator.\n\n    This allows the request context to be accessed within a streaming\n    generator, for example,\n\n    .. code-block:: python\n\n        @app.route('/')\n        def index() -> AsyncGenerator[bytes, None]:\n            @stream_with_context\n            async def generator() -> bytes:\n                yield request.method.encode()\n                yield b' '\n                yield request.path.encode()\n\n            return generator()\n\n    \"\"\"\n    request_context = _request_ctx_stack.top.copy()\n\n    @wraps(func)\n    async def generator(*args: Any, **kwargs: Any) -> Any:\n        async with request_context:\n            async for data in func(*args, **kwargs):\n                yield data\n    return generator"}
{"code":"def _upload_image(registry, docker_tag, image_id) -> None:\n    \"\"\"\n    Upload the passed image by id, tag it with docker tag and upload to S3 bucket\n    :param registry: Docker registry name\n    :param docker_tag: Docker tag\n    :param image_id: Image id\n    :return: None\n    \"\"\"\n    # We don't have to retag the image since it is already in the right format\n    logging.info('Uploading %s (%s) to %s', docker_tag, image_id, registry)\n    push_cmd = ['docker', 'push', docker_tag]\n    subprocess.check_call(push_cmd)","return_type":"None","function_name":"_upload_image","stripped_code":"def _upload_image(registry, docker_tag, image_id):\n    \"\"\"\n    Upload the passed image by id, tag it with docker tag and upload to S3 bucket\n    :param registry: Docker registry name\n    :param docker_tag: Docker tag\n    :param image_id: Image id\n    :return: None\n    \"\"\"\n    # We don't have to retag the image since it is already in the right format\n    logging.info('Uploading %s (%s) to %s', docker_tag, image_id, registry)\n    push_cmd = ['docker', 'push', docker_tag]\n    subprocess.check_call(push_cmd)"}
{"code":"def get_container_list(self) -> list:\n        \"\"\"Get list of containers.\n\n        Returns:\n            list, all the ids of containers\n\n        \"\"\"\n        # Initialising empty list\n        containers = []\n\n        containers_list = self._client.containers.list()\n        for c_list in containers_list:\n            containers.append(c_list.short_id)\n        return containers","return_type":"list","function_name":"DockerSwarmClient.get_container_list","stripped_code":"def get_container_list(self):\n        \"\"\"Get list of containers.\n\n        Returns:\n            list, all the ids of containers\n\n        \"\"\"\n        # Initialising empty list\n        containers = []\n\n        containers_list = self._client.containers.list()\n        for c_list in containers_list:\n            containers.append(c_list.short_id)\n        return containers"}
{"code":"def replace_in_file(filename: str, text_from: str, text_to: str) -> None:\n    \"\"\"\n    Replaces text in a file.\n\n    Args:\n        filename: filename to process (modifying it in place)\n        text_from: original text to replace\n        text_to: replacement text\n    \"\"\"\n    log.info(\"Amending {}: {} -> {}\",\n             filename, repr(text_from), repr(text_to))\n    with open(filename) as infile:\n        contents = infile.read()\n    contents = contents.replace(text_from, text_to)\n    with open(filename, 'w') as outfile:\n        outfile.write(contents)","return_type":"None","function_name":"replace_in_file","stripped_code":"def replace_in_file(filename: str, text_from: str, text_to: str):\n    \"\"\"\n    Replaces text in a file.\n\n    Args:\n        filename: filename to process (modifying it in place)\n        text_from: original text to replace\n        text_to: replacement text\n    \"\"\"\n    log.info(\"Amending {}: {} -> {}\",\n             filename, repr(text_from), repr(text_to))\n    with open(filename) as infile:\n        contents = infile.read()\n    contents = contents.replace(text_from, text_to)\n    with open(filename, 'w') as outfile:\n        outfile.write(contents)"}
{"code":"def parse_uri(self, text: str) -> URIRef:\n        \"\"\"\n        Parse input text into URI\n\n        :param text: can be one of\n              1. URI, directly return\n              2. prefix:name, query namespace for prefix, return expanded URI\n              3. name, use default namespace to expand it and return it\n        :return: URIRef\n        \"\"\"\n        if self.check_uriref(text):\n            return self.check_uriref(text)\n        elif isinstance(text, str):\n            text = text.strip()\n            m = URI_ABBR_PATTERN.match(text)\n            if m:\n                prefix, name = m.groups()\n                base = self.store.namespace(prefix if prefix else '')\n                if not base:\n                    raise PrefixNotFoundException(\"Prefix: %s\", prefix)\n                return URIRef(base + name)\n        raise WrongFormatURIException()","return_type":"URIRef","function_name":"OntologyNamespaceManager.parse_uri","stripped_code":"def parse_uri(self, text: str):\n        \"\"\"\n        Parse input text into URI\n\n        :param text: can be one of\n              1. URI, directly return\n              2. prefix:name, query namespace for prefix, return expanded URI\n              3. name, use default namespace to expand it and return it\n        :return: URIRef\n        \"\"\"\n        if self.check_uriref(text):\n            return self.check_uriref(text)\n        elif isinstance(text, str):\n            text = text.strip()\n            m = URI_ABBR_PATTERN.match(text)\n            if m:\n                prefix, name = m.groups()\n                base = self.store.namespace(prefix if prefix else '')\n                if not base:\n                    raise PrefixNotFoundException(\"Prefix: %s\", prefix)\n                return URIRef(base + name)\n        raise WrongFormatURIException()"}
{"code":"def inherit_from_std_ex(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"\n    Return true if the given class node is subclass of\n    exceptions.Exception.\n    \"\"\"\n    ancestors = node.ancestors() if hasattr(node, \"ancestors\") else []\n    for ancestor in itertools.chain([node], ancestors):\n        if (\n            ancestor.name in (\"Exception\", \"BaseException\")\n            and ancestor.root().name == EXCEPTIONS_MODULE\n        ):\n            return True\n    return False","return_type":"bool","function_name":"inherit_from_std_ex","stripped_code":"def inherit_from_std_ex(node: astroid.node_classes.NodeNG):\n    \"\"\"\n    Return true if the given class node is subclass of\n    exceptions.Exception.\n    \"\"\"\n    ancestors = node.ancestors() if hasattr(node, \"ancestors\") else []\n    for ancestor in itertools.chain([node], ancestors):\n        if (\n            ancestor.name in (\"Exception\", \"BaseException\")\n            and ancestor.root().name == EXCEPTIONS_MODULE\n        ):\n            return True\n    return False"}
{"code":"def parse_tibia_date(date_str) -> Optional[datetime.date]:\n    \"\"\"Parses a date from the format used in Tibia.com\n\n    Accepted format:\n\n    - ``MMM DD YYYY``, e.g. ``Jul 23 2015``\n\n    Parameters\n    -----------\n    date_str: :class:`str`\n        The date as represented in Tibia.com\n\n    Returns\n    -----------\n    :class:`datetime.date`, optional\n        The represented date.\"\"\"\n    try:\n        t = datetime.datetime.strptime(date_str.strip(), \"%b %d %Y\")\n        return t.date()\n    except (ValueError, AttributeError):\n        return None","return_type":"Optional[datetime.date]","function_name":"parse_tibia_date","stripped_code":"def parse_tibia_date(date_str):\n    \"\"\"Parses a date from the format used in Tibia.com\n\n    Accepted format:\n\n    - ``MMM DD YYYY``, e.g. ``Jul 23 2015``\n\n    Parameters\n    -----------\n    date_str: :class:`str`\n        The date as represented in Tibia.com\n\n    Returns\n    -----------\n    :class:`datetime.date`, optional\n        The represented date.\"\"\"\n    try:\n        t = datetime.datetime.strptime(date_str.strip(), \"%b %d %Y\")\n        return t.date()\n    except (ValueError, AttributeError):\n        return None"}
{"code":"def _get_result(self) -> float:\n        \"\"\"Return current measurement result in lx.\"\"\"\n        try:\n            data = self._bus.read_word_data(self._i2c_add, self._mode)\n            self._ok = True\n        except OSError as exc:\n            self.log_error(\"Bad reading in bus: %s\", exc)\n            self._ok = False\n            return -1\n\n        count = data >> 8 | (data & 0xff) << 8\n        mode2coeff = 2 if self._high_res else 1\n        ratio = 1 / (1.2 * (self._mtreg / 69.0) * mode2coeff)\n        return ratio * count","return_type":"float","function_name":"BH1750._get_result","stripped_code":"def _get_result(self):\n        \"\"\"Return current measurement result in lx.\"\"\"\n        try:\n            data = self._bus.read_word_data(self._i2c_add, self._mode)\n            self._ok = True\n        except OSError as exc:\n            self.log_error(\"Bad reading in bus: %s\", exc)\n            self._ok = False\n            return -1\n\n        count = data >> 8 | (data & 0xff) << 8\n        mode2coeff = 2 if self._high_res else 1\n        ratio = 1 / (1.2 * (self._mtreg / 69.0) * mode2coeff)\n        return ratio * count"}
{"code":"def create(message: str, pubkey: Optional[str] = None, signing_keys: Optional[List[SigningKey]] = None,\n               message_comment: Optional[str] = None, signatures_comment: Optional[str] = None) -> str:\n        \"\"\"\n        Encrypt a message in ascii armor format, optionally signing it\n\n        :param message: Utf-8 message\n        :param pubkey: Public key of recipient for encryption\n        :param signing_keys: Optional list of SigningKey instances\n        :param message_comment: Optional message comment field\n        :param signatures_comment: Optional signatures comment field\n        :return:\n        \"\"\"\n        # if no public key and no signing key...\n        if not pubkey and not signing_keys:\n            # We can not create an Ascii Armor Message\n            raise MISSING_PUBLIC_KEY_AND_SIGNING_KEY_EXCEPTION\n\n        # keep only one newline at the end of the message\n        message = message.rstrip(\"\\n\\r\") + \"\\n\"\n\n        # create block with headers\n        ascii_armor_block = \"\"\"{begin_message_header}\n\"\"\".format(begin_message_header=BEGIN_MESSAGE_HEADER)\n\n        # if encrypted message...\n        if pubkey:\n            # add encrypted message fields\n            ascii_armor_block += \"\"\"{version_field}\n\"\"\".format(version_field=AsciiArmor._get_version_field())\n\n        # add message comment if specified\n        if message_comment:\n            ascii_armor_block += \"\"\"{comment_field}\n\"\"\".format(comment_field=AsciiArmor._get_comment_field(message_comment))\n\n        # blank line separator\n        ascii_armor_block += '\\n'\n\n        if pubkey:\n            # add encrypted message\n            pubkey_instance = PublicKey(pubkey)\n            base64_encrypted_message = base64.b64encode(pubkey_instance.encrypt_seal(message))  # type: bytes\n            ascii_armor_block += \"\"\"{base64_encrypted_message}\n\"\"\".format(base64_encrypted_message=base64_encrypted_message.decode('utf-8'))\n        else:\n            # remove trailing spaces\n            message = AsciiArmor._remove_trailing_spaces(message)\n\n            # add dash escaped message to ascii armor content\n            ascii_armor_block += AsciiArmor._dash_escape_text(message)\n\n        # if no signature...\n        if signing_keys is None:\n            # add message tail\n            ascii_armor_block += END_MESSAGE_HEADER\n        else:\n            # add signature blocks and close block on last signature\n            count = 1\n            for signing_key in signing_keys:\n                ascii_armor_block += AsciiArmor._get_signature_block(message, signing_key, count == len(signing_keys),\n                                                                     signatures_comment)\n                count += 1\n\n        return ascii_armor_block","return_type":"str","function_name":"AsciiArmor.create","stripped_code":"def create(message: str, pubkey: Optional[str] = None, signing_keys: Optional[List[SigningKey]] = None,\n               message_comment: Optional[str] = None, signatures_comment: Optional[str] = None):\n        \"\"\"\n        Encrypt a message in ascii armor format, optionally signing it\n\n        :param message: Utf-8 message\n        :param pubkey: Public key of recipient for encryption\n        :param signing_keys: Optional list of SigningKey instances\n        :param message_comment: Optional message comment field\n        :param signatures_comment: Optional signatures comment field\n        :return:\n        \"\"\"\n        # if no public key and no signing key...\n        if not pubkey and not signing_keys:\n            # We can not create an Ascii Armor Message\n            raise MISSING_PUBLIC_KEY_AND_SIGNING_KEY_EXCEPTION\n\n        # keep only one newline at the end of the message\n        message = message.rstrip(\"\\n\\r\") + \"\\n\"\n\n        # create block with headers\n        ascii_armor_block = \"\"\"{begin_message_header}\n\"\"\".format(begin_message_header=BEGIN_MESSAGE_HEADER)\n\n        # if encrypted message...\n        if pubkey:\n            # add encrypted message fields\n            ascii_armor_block += \"\"\"{version_field}\n\"\"\".format(version_field=AsciiArmor._get_version_field())\n\n        # add message comment if specified\n        if message_comment:\n            ascii_armor_block += \"\"\"{comment_field}\n\"\"\".format(comment_field=AsciiArmor._get_comment_field(message_comment))\n\n        # blank line separator\n        ascii_armor_block += '\\n'\n\n        if pubkey:\n            # add encrypted message\n            pubkey_instance = PublicKey(pubkey)\n            base64_encrypted_message = base64.b64encode(pubkey_instance.encrypt_seal(message))  # type: bytes\n            ascii_armor_block += \"\"\"{base64_encrypted_message}\n\"\"\".format(base64_encrypted_message=base64_encrypted_message.decode('utf-8'))\n        else:\n            # remove trailing spaces\n            message = AsciiArmor._remove_trailing_spaces(message)\n\n            # add dash escaped message to ascii armor content\n            ascii_armor_block += AsciiArmor._dash_escape_text(message)\n\n        # if no signature...\n        if signing_keys is None:\n            # add message tail\n            ascii_armor_block += END_MESSAGE_HEADER\n        else:\n            # add signature blocks and close block on last signature\n            count = 1\n            for signing_key in signing_keys:\n                ascii_armor_block += AsciiArmor._get_signature_block(message, signing_key, count == len(signing_keys),\n                                                                     signatures_comment)\n                count += 1\n\n        return ascii_armor_block"}
{"code":"def save_files(self, selections) -> None:\n        \"\"\"Save the |Selection| objects contained in the given |Selections|\n        instance to separate network files.\"\"\"\n        try:\n            currentpath = self.currentpath\n            selections = selectiontools.Selections(selections)\n            for selection in selections:\n                if selection.name == 'complete':\n                    continue\n                path = os.path.join(currentpath, selection.name+'.py')\n                selection.save_networkfile(filepath=path)\n        except BaseException:\n            objecttools.augment_excmessage(\n                'While trying to save selections `%s` into network files'\n                % selections)","return_type":"None","function_name":"NetworkManager.save_files","stripped_code":"def save_files(self, selections):\n        \"\"\"Save the |Selection| objects contained in the given |Selections|\n        instance to separate network files.\"\"\"\n        try:\n            currentpath = self.currentpath\n            selections = selectiontools.Selections(selections)\n            for selection in selections:\n                if selection.name == 'complete':\n                    continue\n                path = os.path.join(currentpath, selection.name+'.py')\n                selection.save_networkfile(filepath=path)\n        except BaseException:\n            objecttools.augment_excmessage(\n                'While trying to save selections `%s` into network files'\n                % selections)"}
{"code":"def cancel_orders(self, order_ids: List[str]) -> List[str]:\n        \"\"\"Cancel multiple orders by a list of IDs.\"\"\"\n        orders_to_cancel = order_ids\n        self.log.debug(f'Canceling orders on {self.name}: ids={orders_to_cancel}')\n        cancelled_orders = []\n\n        if self.dry_run:  # Don't cancel if dry run\n            self.log.warning(f'DRY RUN: Orders cancelled on {self.name}: {orders_to_cancel}')\n            return orders_to_cancel\n\n        try:  # Iterate and cancel orders\n            if self.has_batch_cancel:\n                self._cancel_orders(orders_to_cancel)\n                cancelled_orders.append(orders_to_cancel)\n                orders_to_cancel.clear()\n            else:\n                for i, order_id in enumerate(orders_to_cancel):\n                    self._cancel_order(order_id)\n                    cancelled_orders.append(order_id)\n                    orders_to_cancel.pop(i)\n        except Exception as e:\n            msg = f'Failed to cancel {len(orders_to_cancel)} orders on {self.name}: ids={orders_to_cancel}'\n            raise self.exception(OrderNotFound, msg, e) from e\n\n        self.log.info(f'Orders cancelled on {self.name}: ids={cancelled_orders}')\n        return cancelled_orders","return_type":"List[str]","function_name":"TradingClient.cancel_orders","stripped_code":"def cancel_orders(self, order_ids: List[str]):\n        \"\"\"Cancel multiple orders by a list of IDs.\"\"\"\n        orders_to_cancel = order_ids\n        self.log.debug(f'Canceling orders on {self.name}: ids={orders_to_cancel}')\n        cancelled_orders = []\n\n        if self.dry_run:  # Don't cancel if dry run\n            self.log.warning(f'DRY RUN: Orders cancelled on {self.name}: {orders_to_cancel}')\n            return orders_to_cancel\n\n        try:  # Iterate and cancel orders\n            if self.has_batch_cancel:\n                self._cancel_orders(orders_to_cancel)\n                cancelled_orders.append(orders_to_cancel)\n                orders_to_cancel.clear()\n            else:\n                for i, order_id in enumerate(orders_to_cancel):\n                    self._cancel_order(order_id)\n                    cancelled_orders.append(order_id)\n                    orders_to_cancel.pop(i)\n        except Exception as e:\n            msg = f'Failed to cancel {len(orders_to_cancel)} orders on {self.name}: ids={orders_to_cancel}'\n            raise self.exception(OrderNotFound, msg, e) from e\n\n        self.log.info(f'Orders cancelled on {self.name}: ids={cancelled_orders}')\n        return cancelled_orders"}
{"code":"def names(self) -> [str]:\n        \"\"\"\n        The snames of all axis objects\n        \"\"\"\n        return sorted([name for name in self.axes_by_sname.keys() if name is not ''])","return_type":"[str]","function_name":"Axes.names","stripped_code":"def names(self):\n        \"\"\"\n        The snames of all axis objects\n        \"\"\"\n        return sorted([name for name in self.axes_by_sname.keys() if name is not ''])"}
{"code":"def convert_sent_dict_to_conll(sent_dic, domain) -> str:\n    \"\"\"\n    Given a dictionary from sentence -> extractions,\n    return a corresponding CoNLL representation.\n    \"\"\"\n    return '\\n\\n'.join(['\\n'.join(['\\t'.join(map(str, pad_line_to_ontonotes(line, domain)))\n                                   for line in convert_sent_to_conll(sent_ls)])\n                        for sent_ls\n                        in sent_dic.iteritems()])","return_type":"str","function_name":"convert_sent_dict_to_conll","stripped_code":"def convert_sent_dict_to_conll(sent_dic, domain):\n    \"\"\"\n    Given a dictionary from sentence -> extractions,\n    return a corresponding CoNLL representation.\n    \"\"\"\n    return '\\n\\n'.join(['\\n'.join(['\\t'.join(map(str, pad_line_to_ontonotes(line, domain)))\n                                   for line in convert_sent_to_conll(sent_ls)])\n                        for sent_ls\n                        in sent_dic.iteritems()])"}
{"code":"def add_glossary(self, glossary: List[str], attr_name: str) -> None:\n        \"\"\"\n        Adds a glossary for the given attribute name\n        :param glossary: a list of possible mentions of the attribute name\n        :param attr_name: the attribute name (field name)\n        \"\"\"\n        self.glossaries[attr_name] = glossary","return_type":"None","function_name":"EntityTableDataExtraction.add_glossary","stripped_code":"def add_glossary(self, glossary: List[str], attr_name: str):\n        \"\"\"\n        Adds a glossary for the given attribute name\n        :param glossary: a list of possible mentions of the attribute name\n        :param attr_name: the attribute name (field name)\n        \"\"\"\n        self.glossaries[attr_name] = glossary"}
{"code":"def compare_3PC_keys(key1, key2) -> int:\n    \"\"\"\n    Return >0 if key2 is greater than key1, <0 if lesser, 0 otherwise\n    \"\"\"\n    if key1[0] == key2[0]:\n        return key2[1] - key1[1]\n    else:\n        return key2[0] - key1[0]","return_type":"int","function_name":"compare_3PC_keys","stripped_code":"def compare_3PC_keys(key1, key2):\n    \"\"\"\n    Return >0 if key2 is greater than key1, <0 if lesser, 0 otherwise\n    \"\"\"\n    if key1[0] == key2[0]:\n        return key2[1] - key1[1]\n    else:\n        return key2[0] - key1[0]"}
{"code":"def files_sharedPublicURL(self, *, id: str, **kwargs) -> SlackResponse:\n        \"\"\"Enables a file for public/external sharing.\n\n        Args:\n            id (str): The file id. e.g. 'F1234467890'\n        \"\"\"\n        self._validate_xoxp_token()\n        kwargs.update({\"id\": id})\n        return self.api_call(\"files.sharedPublicURL\", json=kwargs)","return_type":"SlackResponse","function_name":"WebClient.files_sharedPublicURL","stripped_code":"def files_sharedPublicURL(self, *, id: str, **kwargs):\n        \"\"\"Enables a file for public/external sharing.\n\n        Args:\n            id (str): The file id. e.g. 'F1234467890'\n        \"\"\"\n        self._validate_xoxp_token()\n        kwargs.update({\"id\": id})\n        return self.api_call(\"files.sharedPublicURL\", json=kwargs)"}
{"code":"def get_canonical_block_by_number(self, block_number: BlockNumber) -> BaseBlock:\n        \"\"\"\n        Returns the block with the given number in the canonical chain.\n\n        Raises BlockNotFound if there's no block with the given number in the\n        canonical chain.\n        \"\"\"\n        validate_uint256(block_number, title=\"Block Number\")\n        return self.get_block_by_hash(self.chaindb.get_canonical_block_hash(block_number))","return_type":"BaseBlock","function_name":"Chain.get_canonical_block_by_number","stripped_code":"def get_canonical_block_by_number(self, block_number: BlockNumber):\n        \"\"\"\n        Returns the block with the given number in the canonical chain.\n\n        Raises BlockNotFound if there's no block with the given number in the\n        canonical chain.\n        \"\"\"\n        validate_uint256(block_number, title=\"Block Number\")\n        return self.get_block_by_hash(self.chaindb.get_canonical_block_hash(block_number))"}
{"code":"def validate_xml(self) -> None:\n        \"\"\"Raise an error if the actual XML does not agree with one of the\n        available schema files.\n\n        # ToDo: should it be accompanied by a script function?\n\n        The first example relies on a distorted version of the configuration\n        file `single_run.xml`:\n\n        >>> from hydpy.core.examples import prepare_full_example_1\n        >>> prepare_full_example_1()\n        >>> from hydpy import TestIO, xml_replace\n        >>> from hydpy.auxs.xmltools import XMLInterface\n        >>> import os\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run',\n        ...                 firstdate='1996-01-32T00:00:00')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <...HydPyConfigBase.xsd\"\n                      ...HydPyConfigSingleRun.xsd\"> (default argument)\n          firstdate --> 1996-01-32T00:00:00 (given argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </hpcsr:config> (default argument)\n        >>> with TestIO():\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        hydpy.core.objecttools.xmlschema.validators.exceptions.\\\nXMLSchemaDecodeError: While trying to validate XML file `...single_run.xml`, \\\nthe following error occurred: failed validating '1996-01-32T00:00:00' with \\\nXsdAtomicBuiltin(name='xs:dateTime').\n        ...\n        Reason: day is out of range for month\n        ...\n        Schema:\n        ...\n        Instance:\n        ...\n          <firstdate xmlns=\"https://github.com/hydpy-dev/hydpy/releases/\\\ndownload/your-hydpy-version/HydPyConfigBase.xsd\">1996-01-32T00:00:00</firstdate>\n        ...\n        Path: /hpcsr:config/timegrid/firstdate\n        ...\n\n        In the second example, we examine a correct configuration file:\n\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run')\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <...HydPyConfigBase.xsd\"\n                      ...HydPyConfigSingleRun.xsd\"> (default argument)\n          firstdate --> 1996-01-01T00:00:00 (default argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </hpcsr:config> (default argument)\n        >>> interface.validate_xml()\n\n        The XML configuration file must correctly refer to the corresponding\n        schema file:\n\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run',\n        ...                 config_start='<config>',\n        ...                 config_end='</config>')\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <config> (given argument)\n          firstdate --> 1996-01-01T00:00:00 (default argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </config> (given argument)\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        RuntimeError: While trying to validate XML file `...single_run.xml`, \\\nthe following error occurred: Configuration file `single_run.xml` does not \\\ncorrectly refer to one of the available XML schema files \\\n(HydPyConfigSingleRun.xsd and HydPyConfigMultipleRuns.xsd).\n\n        XML files based on `HydPyConfigMultipleRuns.xsd` can be validated\n        as well:\n\n        >>> with TestIO():\n        ...     interface = XMLInterface('multiple_runs.xml', 'LahnH')\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        \"\"\"\n        try:\n            filenames = ('HydPyConfigSingleRun.xsd',\n                         'HydPyConfigMultipleRuns.xsd')\n            for name in filenames:\n                if name in self.root.tag:\n                    schemafile = name\n                    break\n            else:\n                raise RuntimeError(\n                    f'Configuration file `{os.path.split(self.filepath)[-1]}` '\n                    f'does not correctly refer to one of the available XML '\n                    f'schema files ({objecttools.enumeration(filenames)}).')\n            schemapath = os.path.join(conf.__path__[0], schemafile)\n            schema = xmlschema.XMLSchema(schemapath)\n            schema.validate(self.filepath)\n        except BaseException:\n            objecttools.augment_excmessage(\n                f'While trying to validate XML file `{self.filepath}`')","return_type":"None","function_name":"XMLInterface.validate_xml","stripped_code":"def validate_xml(self):\n        \"\"\"Raise an error if the actual XML does not agree with one of the\n        available schema files.\n\n        # ToDo: should it be accompanied by a script function?\n\n        The first example relies on a distorted version of the configuration\n        file `single_run.xml`:\n\n        >>> from hydpy.core.examples import prepare_full_example_1\n        >>> prepare_full_example_1()\n        >>> from hydpy import TestIO, xml_replace\n        >>> from hydpy.auxs.xmltools import XMLInterface\n        >>> import os\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run',\n        ...                 firstdate='1996-01-32T00:00:00')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <...HydPyConfigBase.xsd\"\n                      ...HydPyConfigSingleRun.xsd\"> (default argument)\n          firstdate --> 1996-01-32T00:00:00 (given argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </hpcsr:config> (default argument)\n        >>> with TestIO():\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        hydpy.core.objecttools.xmlschema.validators.exceptions.\\\nXMLSchemaDecodeError: While trying to validate XML file `...single_run.xml`, \\\nthe following error occurred: failed validating '1996-01-32T00:00:00' with \\\nXsdAtomicBuiltin(name='xs:dateTime').\n        ...\n        Reason: day is out of range for month\n        ...\n        Schema:\n        ...\n        Instance:\n        ...\n          <firstdate xmlns=\"https://github.com/hydpy-dev/hydpy/releases/\\\ndownload/your-hydpy-version/HydPyConfigBase.xsd\">1996-01-32T00:00:00</firstdate>\n        ...\n        Path: /hpcsr:config/timegrid/firstdate\n        ...\n\n        In the second example, we examine a correct configuration file:\n\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run')\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <...HydPyConfigBase.xsd\"\n                      ...HydPyConfigSingleRun.xsd\"> (default argument)\n          firstdate --> 1996-01-01T00:00:00 (default argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </hpcsr:config> (default argument)\n        >>> interface.validate_xml()\n\n        The XML configuration file must correctly refer to the corresponding\n        schema file:\n\n        >>> with TestIO():    # doctest: +ELLIPSIS\n        ...     xml_replace('LahnH/single_run',\n        ...                 config_start='<config>',\n        ...                 config_end='</config>')\n        ...     interface = XMLInterface('single_run.xml', 'LahnH')\n        template file: LahnH/single_run.xmlt\n        target file: LahnH/single_run.xml\n        replacements:\n          config_start --> <config> (given argument)\n          firstdate --> 1996-01-01T00:00:00 (default argument)\n          zip_ --> false (default argument)\n          zip_ --> false (default argument)\n          config_end --> </config> (given argument)\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        RuntimeError: While trying to validate XML file `...single_run.xml`, \\\nthe following error occurred: Configuration file `single_run.xml` does not \\\ncorrectly refer to one of the available XML schema files \\\n(HydPyConfigSingleRun.xsd and HydPyConfigMultipleRuns.xsd).\n\n        XML files based on `HydPyConfigMultipleRuns.xsd` can be validated\n        as well:\n\n        >>> with TestIO():\n        ...     interface = XMLInterface('multiple_runs.xml', 'LahnH')\n        >>> interface.validate_xml()    # doctest: +ELLIPSIS\n        \"\"\"\n        try:\n            filenames = ('HydPyConfigSingleRun.xsd',\n                         'HydPyConfigMultipleRuns.xsd')\n            for name in filenames:\n                if name in self.root.tag:\n                    schemafile = name\n                    break\n            else:\n                raise RuntimeError(\n                    f'Configuration file `{os.path.split(self.filepath)[-1]}` '\n                    f'does not correctly refer to one of the available XML '\n                    f'schema files ({objecttools.enumeration(filenames)}).')\n            schemapath = os.path.join(conf.__path__[0], schemafile)\n            schema = xmlschema.XMLSchema(schemapath)\n            schema.validate(self.filepath)\n        except BaseException:\n            objecttools.augment_excmessage(\n                f'While trying to validate XML file `{self.filepath}`')"}
{"code":"def decodes(self, s: str) -> BioCCollection:\n        \"\"\"\n        Deserialize ``s`` to a BioC collection object.\n\n        Args:\n            s: a \"str\" instance containing a BioC collection\n\n        Returns:\n            an object of BioCollection\n        \"\"\"\n        tree = etree.parse(io.BytesIO(bytes(s, encoding='UTF-8')))\n        collection = self.__parse_collection(tree.getroot())\n        collection.encoding = tree.docinfo.encoding\n        collection.standalone = tree.docinfo.standalone\n        collection.version = tree.docinfo.xml_version\n        return collection","return_type":"BioCCollection","function_name":"BioCXMLDecoder.decodes","stripped_code":"def decodes(self, s: str):\n        \"\"\"\n        Deserialize ``s`` to a BioC collection object.\n\n        Args:\n            s: a \"str\" instance containing a BioC collection\n\n        Returns:\n            an object of BioCollection\n        \"\"\"\n        tree = etree.parse(io.BytesIO(bytes(s, encoding='UTF-8')))\n        collection = self.__parse_collection(tree.getroot())\n        collection.encoding = tree.docinfo.encoding\n        collection.standalone = tree.docinfo.standalone\n        collection.version = tree.docinfo.xml_version\n        return collection"}
{"code":"def select_inputs(self, address: str, amount: int) -> dict:\n        '''finds apropriate utxo's to include in rawtx, while being careful\n        to never spend old transactions with a lot of coin age.\n        Argument is intiger, returns list of apropriate UTXO's'''\n\n        utxos = []\n        utxo_sum = Decimal(0)\n        for tx in sorted(self.listunspent(address=address), key=itemgetter('confirmations')):\n\n            if tx[\"address\"] not in (self.pa_parameters.P2TH_addr,\n                                     self.pa_parameters.test_P2TH_addr):\n\n                utxos.append(\n                        MutableTxIn(txid=tx['txid'],\n                                    txout=tx['vout'],\n                                    sequence=Sequence.max(),\n                                    script_sig=ScriptSig.empty())\n                         )\n\n                utxo_sum += Decimal(tx[\"amount\"])\n                if utxo_sum >= amount:\n                    return {'utxos': utxos, 'total': utxo_sum}\n\n        if utxo_sum < amount:\n            raise InsufficientFunds(\"Insufficient funds.\")\n\n        raise Exception(\"undefined behavior :.(\")","return_type":"dict","function_name":"RpcNode.select_inputs","stripped_code":"def select_inputs(self, address: str, amount: int):\n        '''finds apropriate utxo's to include in rawtx, while being careful\n        to never spend old transactions with a lot of coin age.\n        Argument is intiger, returns list of apropriate UTXO's'''\n\n        utxos = []\n        utxo_sum = Decimal(0)\n        for tx in sorted(self.listunspent(address=address), key=itemgetter('confirmations')):\n\n            if tx[\"address\"] not in (self.pa_parameters.P2TH_addr,\n                                     self.pa_parameters.test_P2TH_addr):\n\n                utxos.append(\n                        MutableTxIn(txid=tx['txid'],\n                                    txout=tx['vout'],\n                                    sequence=Sequence.max(),\n                                    script_sig=ScriptSig.empty())\n                         )\n\n                utxo_sum += Decimal(tx[\"amount\"])\n                if utxo_sum >= amount:\n                    return {'utxos': utxos, 'total': utxo_sum}\n\n        if utxo_sum < amount:\n            raise InsufficientFunds(\"Insufficient funds.\")\n\n        raise Exception(\"undefined behavior :.(\")"}
{"code":"def fatal(self, i: int=None) -> str:\n        \"\"\"\n        Returns a fatal error message\n        \"\"\"\n        head = \"[\" + colors.red(\"\\033[1mfatal error\") + \"]\"\n        if i is not None:\n            head = str(i) + \" \" + head\n        return head","return_type":"str","function_name":"Msg.fatal","stripped_code":"def fatal(self, i: int=None):\n        \"\"\"\n        Returns a fatal error message\n        \"\"\"\n        head = \"[\" + colors.red(\"\\033[1mfatal error\") + \"]\"\n        if i is not None:\n            head = str(i) + \" \" + head\n        return head"}
{"code":"def market_normal(self, session, after_open, before_close) -> Session:\n        \"\"\"\n        Time intervals between market\n\n        Args:\n            session: [allday, day, am, pm, night]\n            after_open: mins after open\n            before_close: mins before close\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        logger = logs.get_logger(self.market_normal)\n\n        if session not in self.exch: return SessNA\n        ss = self.exch[session]\n\n        s_time = shift_time(ss[0], int(after_open) + 1)\n        e_time = shift_time(ss[-1], -int(before_close))\n\n        request_cross = pd.Timestamp(s_time) >= pd.Timestamp(e_time)\n        session_cross = pd.Timestamp(ss[0]) >= pd.Timestamp(ss[1])\n        if request_cross and (not session_cross):\n            logger.warning(f'end time {e_time} is earlier than {s_time} ...')\n            return SessNA\n\n        return Session(s_time, e_time)","return_type":"Session","function_name":"Intervals.market_normal","stripped_code":"def market_normal(self, session, after_open, before_close):\n        \"\"\"\n        Time intervals between market\n\n        Args:\n            session: [allday, day, am, pm, night]\n            after_open: mins after open\n            before_close: mins before close\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        logger = logs.get_logger(self.market_normal)\n\n        if session not in self.exch: return SessNA\n        ss = self.exch[session]\n\n        s_time = shift_time(ss[0], int(after_open) + 1)\n        e_time = shift_time(ss[-1], -int(before_close))\n\n        request_cross = pd.Timestamp(s_time) >= pd.Timestamp(e_time)\n        session_cross = pd.Timestamp(ss[0]) >= pd.Timestamp(ss[1])\n        if request_cross and (not session_cross):\n            logger.warning(f'end time {e_time} is earlier than {s_time} ...')\n            return SessNA\n\n        return Session(s_time, e_time)"}
{"code":"def console_set_alignment(con: tcod.console.Console, alignment: int) -> None:\n    \"\"\"Change this consoles current alignment mode.\n\n    * tcod.LEFT\n    * tcod.CENTER\n    * tcod.RIGHT\n\n    Args:\n        con (Console): Any Console instance.\n        alignment (int):\n\n    .. deprecated:: 8.5\n        Set :any:`Console.default_alignment` instead.\n    \"\"\"\n    lib.TCOD_console_set_alignment(_console(con), alignment)","return_type":"None","function_name":"console_set_alignment","stripped_code":"def console_set_alignment(con: tcod.console.Console, alignment: int):\n    \"\"\"Change this consoles current alignment mode.\n\n    * tcod.LEFT\n    * tcod.CENTER\n    * tcod.RIGHT\n\n    Args:\n        con (Console): Any Console instance.\n        alignment (int):\n\n    .. deprecated:: 8.5\n        Set :any:`Console.default_alignment` instead.\n    \"\"\"\n    lib.TCOD_console_set_alignment(_console(con), alignment)"}
{"code":"def output_best_scores(self, best_epoch_str: str) -> None:\n        \"\"\"Output best scores to the filesystem\"\"\"\n        BEST_SCORES_FILENAME = \"best_scores.txt\"\n        with open(os.path.join(self.exp_dir, BEST_SCORES_FILENAME),\n                  \"w\", encoding=ENCODING) as best_f:\n            print(best_epoch_str, file=best_f, flush=True)","return_type":"None","function_name":"Model.output_best_scores","stripped_code":"def output_best_scores(self, best_epoch_str: str):\n        \"\"\"Output best scores to the filesystem\"\"\"\n        BEST_SCORES_FILENAME = \"best_scores.txt\"\n        with open(os.path.join(self.exp_dir, BEST_SCORES_FILENAME),\n                  \"w\", encoding=ENCODING) as best_f:\n            print(best_epoch_str, file=best_f, flush=True)"}
{"code":"def parse_config(data: dict) -> dict:\n    \"\"\"Parse MIP config file.\n\n    Args:\n        data (dict): raw YAML input from MIP analysis config file\n\n    Returns:\n        dict: parsed data\n    \"\"\"\n    return {\n        'email': data.get('email'),\n        'family': data['family_id'],\n        'samples': [{\n            'id': sample_id,\n            'type': analysis_type,\n        } for sample_id, analysis_type in data['analysis_type'].items()],\n        'config_path': data['config_file_analysis'],\n        'is_dryrun': True if 'dry_run_all' in data else False,\n        'log_path': data['log_file'],\n        'out_dir': data['outdata_dir'],\n        'priority': data['slurm_quality_of_service'],\n        'sampleinfo_path': data['sample_info_file'],\n    }","return_type":"dict","function_name":"parse_config","stripped_code":"def parse_config(data: dict):\n    \"\"\"Parse MIP config file.\n\n    Args:\n        data (dict): raw YAML input from MIP analysis config file\n\n    Returns:\n        dict: parsed data\n    \"\"\"\n    return {\n        'email': data.get('email'),\n        'family': data['family_id'],\n        'samples': [{\n            'id': sample_id,\n            'type': analysis_type,\n        } for sample_id, analysis_type in data['analysis_type'].items()],\n        'config_path': data['config_file_analysis'],\n        'is_dryrun': True if 'dry_run_all' in data else False,\n        'log_path': data['log_file'],\n        'out_dir': data['outdata_dir'],\n        'priority': data['slurm_quality_of_service'],\n        'sampleinfo_path': data['sample_info_file'],\n    }"}
{"code":"def convert_data_element_to_data_and_metadata_1(data_element) -> DataAndMetadata.DataAndMetadata:\n    \"\"\"Convert a data element to xdata. No data copying occurs.\n\n    The data element can have the following keys:\n        data (required)\n        is_sequence, collection_dimension_count, datum_dimension_count (optional description of the data)\n        spatial_calibrations (optional list of spatial calibration dicts, scale, offset, units)\n        intensity_calibration (optional intensity calibration dict, scale, offset, units)\n        metadata (optional)\n        properties (get stored into metadata.hardware_source)\n        one of either timestamp or datetime_modified\n        if datetime_modified (dst, tz) it is converted and used as timestamp\n            then timezone gets stored into metadata.description.timezone.\n    \"\"\"\n    # data. takes ownership.\n    data = data_element[\"data\"]\n    dimensional_shape = Image.dimensional_shape_from_data(data)\n    is_sequence = data_element.get(\"is_sequence\", False)\n    dimension_count = len(Image.dimensional_shape_from_data(data))\n    adjusted_dimension_count = dimension_count - (1 if is_sequence else 0)\n    collection_dimension_count = data_element.get(\"collection_dimension_count\", 2 if adjusted_dimension_count in (3, 4) else 0)\n    datum_dimension_count = data_element.get(\"datum_dimension_count\", adjusted_dimension_count - collection_dimension_count)\n    data_descriptor = DataAndMetadata.DataDescriptor(is_sequence, collection_dimension_count, datum_dimension_count)\n\n    # dimensional calibrations\n    dimensional_calibrations = None\n    if \"spatial_calibrations\" in data_element:\n        dimensional_calibrations_list = data_element.get(\"spatial_calibrations\")\n        if len(dimensional_calibrations_list) == len(dimensional_shape):\n            dimensional_calibrations = list()\n            for dimension_calibration in dimensional_calibrations_list:\n                offset = float(dimension_calibration.get(\"offset\", 0.0))\n                scale = float(dimension_calibration.get(\"scale\", 1.0))\n                units = dimension_calibration.get(\"units\", \"\")\n                units = str(units) if units is not None else str()\n                if scale != 0.0:\n                    dimensional_calibrations.append(Calibration.Calibration(offset, scale, units))\n                else:\n                    dimensional_calibrations.append(Calibration.Calibration())\n\n    # intensity calibration\n    intensity_calibration = None\n    if \"intensity_calibration\" in data_element:\n        intensity_calibration_dict = data_element.get(\"intensity_calibration\")\n        offset = float(intensity_calibration_dict.get(\"offset\", 0.0))\n        scale = float(intensity_calibration_dict.get(\"scale\", 1.0))\n        units = intensity_calibration_dict.get(\"units\", \"\")\n        units = str(units) if units is not None else str()\n        if scale != 0.0:\n            intensity_calibration = Calibration.Calibration(offset, scale, units)\n\n    # properties (general tags)\n    metadata = dict()\n    if \"metadata\" in data_element:\n        metadata.update(Utility.clean_dict(data_element.get(\"metadata\")))\n    if \"properties\" in data_element and data_element[\"properties\"]:\n        hardware_source_metadata = metadata.setdefault(\"hardware_source\", dict())\n        hardware_source_metadata.update(Utility.clean_dict(data_element.get(\"properties\")))\n\n    # dates are _local_ time and must use this specific ISO 8601 format. 2013-11-17T08:43:21.389391\n    # time zones are offsets (east of UTC) in the following format \"+HHMM\" or \"-HHMM\"\n    # daylight savings times are time offset (east of UTC) in format \"+MM\" or \"-MM\"\n    # timezone is for conversion and is the Olson timezone string.\n    # datetime.datetime.strptime(datetime.datetime.isoformat(datetime.datetime.now()), \"%Y-%m-%dT%H:%M:%S.%f\" )\n    # datetime_modified, datetime_modified_tz, datetime_modified_dst, datetime_modified_tzname is the time at which this image was modified.\n    # datetime_original, datetime_original_tz, datetime_original_dst, datetime_original_tzname is the time at which this image was created.\n    timestamp = data_element.get(\"timestamp\", datetime.datetime.utcnow())\n    datetime_item = data_element.get(\"datetime_modified\", Utility.get_datetime_item_from_utc_datetime(timestamp))\n\n    local_datetime = Utility.get_datetime_from_datetime_item(datetime_item)\n    dst_value = datetime_item.get(\"dst\", \"+00\")\n    tz_value = datetime_item.get(\"tz\", \"+0000\")\n    timezone = datetime_item.get(\"timezone\")\n    time_zone = { \"dst\": dst_value, \"tz\": tz_value}\n    if timezone is not None:\n        time_zone[\"timezone\"] = timezone\n    # note: dst is informational only; tz already include dst\n    tz_adjust = (int(tz_value[1:3]) * 60 + int(tz_value[3:5])) * (-1 if tz_value[0] == '-' else 1)\n    utc_datetime = local_datetime - datetime.timedelta(minutes=tz_adjust)  # tz_adjust already contains dst_adjust\n    timestamp = utc_datetime\n\n    return DataAndMetadata.new_data_and_metadata(data,\n                                                 intensity_calibration=intensity_calibration,\n                                                 dimensional_calibrations=dimensional_calibrations,\n                                                 metadata=metadata,\n                                                 timestamp=timestamp,\n                                                 data_descriptor=data_descriptor,\n                                                 timezone=timezone,\n                                                 timezone_offset=tz_value)","return_type":"DataAndMetadata.DataAndMetadata","function_name":"convert_data_element_to_data_and_metadata_1","stripped_code":"def convert_data_element_to_data_and_metadata_1(data_element):\n    \"\"\"Convert a data element to xdata. No data copying occurs.\n\n    The data element can have the following keys:\n        data (required)\n        is_sequence, collection_dimension_count, datum_dimension_count (optional description of the data)\n        spatial_calibrations (optional list of spatial calibration dicts, scale, offset, units)\n        intensity_calibration (optional intensity calibration dict, scale, offset, units)\n        metadata (optional)\n        properties (get stored into metadata.hardware_source)\n        one of either timestamp or datetime_modified\n        if datetime_modified (dst, tz) it is converted and used as timestamp\n            then timezone gets stored into metadata.description.timezone.\n    \"\"\"\n    # data. takes ownership.\n    data = data_element[\"data\"]\n    dimensional_shape = Image.dimensional_shape_from_data(data)\n    is_sequence = data_element.get(\"is_sequence\", False)\n    dimension_count = len(Image.dimensional_shape_from_data(data))\n    adjusted_dimension_count = dimension_count - (1 if is_sequence else 0)\n    collection_dimension_count = data_element.get(\"collection_dimension_count\", 2 if adjusted_dimension_count in (3, 4) else 0)\n    datum_dimension_count = data_element.get(\"datum_dimension_count\", adjusted_dimension_count - collection_dimension_count)\n    data_descriptor = DataAndMetadata.DataDescriptor(is_sequence, collection_dimension_count, datum_dimension_count)\n\n    # dimensional calibrations\n    dimensional_calibrations = None\n    if \"spatial_calibrations\" in data_element:\n        dimensional_calibrations_list = data_element.get(\"spatial_calibrations\")\n        if len(dimensional_calibrations_list) == len(dimensional_shape):\n            dimensional_calibrations = list()\n            for dimension_calibration in dimensional_calibrations_list:\n                offset = float(dimension_calibration.get(\"offset\", 0.0))\n                scale = float(dimension_calibration.get(\"scale\", 1.0))\n                units = dimension_calibration.get(\"units\", \"\")\n                units = str(units) if units is not None else str()\n                if scale != 0.0:\n                    dimensional_calibrations.append(Calibration.Calibration(offset, scale, units))\n                else:\n                    dimensional_calibrations.append(Calibration.Calibration())\n\n    # intensity calibration\n    intensity_calibration = None\n    if \"intensity_calibration\" in data_element:\n        intensity_calibration_dict = data_element.get(\"intensity_calibration\")\n        offset = float(intensity_calibration_dict.get(\"offset\", 0.0))\n        scale = float(intensity_calibration_dict.get(\"scale\", 1.0))\n        units = intensity_calibration_dict.get(\"units\", \"\")\n        units = str(units) if units is not None else str()\n        if scale != 0.0:\n            intensity_calibration = Calibration.Calibration(offset, scale, units)\n\n    # properties (general tags)\n    metadata = dict()\n    if \"metadata\" in data_element:\n        metadata.update(Utility.clean_dict(data_element.get(\"metadata\")))\n    if \"properties\" in data_element and data_element[\"properties\"]:\n        hardware_source_metadata = metadata.setdefault(\"hardware_source\", dict())\n        hardware_source_metadata.update(Utility.clean_dict(data_element.get(\"properties\")))\n\n    # dates are _local_ time and must use this specific ISO 8601 format. 2013-11-17T08:43:21.389391\n    # time zones are offsets (east of UTC) in the following format \"+HHMM\" or \"-HHMM\"\n    # daylight savings times are time offset (east of UTC) in format \"+MM\" or \"-MM\"\n    # timezone is for conversion and is the Olson timezone string.\n    # datetime.datetime.strptime(datetime.datetime.isoformat(datetime.datetime.now()), \"%Y-%m-%dT%H:%M:%S.%f\" )\n    # datetime_modified, datetime_modified_tz, datetime_modified_dst, datetime_modified_tzname is the time at which this image was modified.\n    # datetime_original, datetime_original_tz, datetime_original_dst, datetime_original_tzname is the time at which this image was created.\n    timestamp = data_element.get(\"timestamp\", datetime.datetime.utcnow())\n    datetime_item = data_element.get(\"datetime_modified\", Utility.get_datetime_item_from_utc_datetime(timestamp))\n\n    local_datetime = Utility.get_datetime_from_datetime_item(datetime_item)\n    dst_value = datetime_item.get(\"dst\", \"+00\")\n    tz_value = datetime_item.get(\"tz\", \"+0000\")\n    timezone = datetime_item.get(\"timezone\")\n    time_zone = { \"dst\": dst_value, \"tz\": tz_value}\n    if timezone is not None:\n        time_zone[\"timezone\"] = timezone\n    # note: dst is informational only; tz already include dst\n    tz_adjust = (int(tz_value[1:3]) * 60 + int(tz_value[3:5])) * (-1 if tz_value[0] == '-' else 1)\n    utc_datetime = local_datetime - datetime.timedelta(minutes=tz_adjust)  # tz_adjust already contains dst_adjust\n    timestamp = utc_datetime\n\n    return DataAndMetadata.new_data_and_metadata(data,\n                                                 intensity_calibration=intensity_calibration,\n                                                 dimensional_calibrations=dimensional_calibrations,\n                                                 metadata=metadata,\n                                                 timestamp=timestamp,\n                                                 data_descriptor=data_descriptor,\n                                                 timezone=timezone,\n                                                 timezone_offset=tz_value)"}
{"code":"def _tdec(code: str, unit: str = 'C') -> str:\n    \"\"\"\n    Translates a 4-digit decimal temperature representation\n\n    Ex: 1045 -> -4.5\u00b0C    0237 -> 23.7\u00b0C\n    \"\"\"\n    ret = f\"{'-' if code[0] == '1' else ''}{int(code[1:3])}.{code[3]}\"\n    if unit:\n        ret += f'\u00b0{unit}'\n    return ret","return_type":"str","function_name":"_tdec","stripped_code":"def _tdec(code: str, unit: str = 'C'):\n    \"\"\"\n    Translates a 4-digit decimal temperature representation\n\n    Ex: 1045 -> -4.5\u00b0C    0237 -> 23.7\u00b0C\n    \"\"\"\n    ret = f\"{'-' if code[0] == '1' else ''}{int(code[1:3])}.{code[3]}\"\n    if unit:\n        ret += f'\u00b0{unit}'\n    return ret"}
{"code":"def data_item(self) -> DataItem:\n        \"\"\"Return the data item associated with this display panel.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_panel = self.__display_panel\n        if not display_panel:\n            return None\n        data_item = display_panel.data_item\n        return DataItem(data_item) if data_item else None","return_type":"DataItem","function_name":"DisplayPanel.data_item","stripped_code":"def data_item(self):\n        \"\"\"Return the data item associated with this display panel.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        display_panel = self.__display_panel\n        if not display_panel:\n            return None\n        data_item = display_panel.data_item\n        return DataItem(data_item) if data_item else None"}
{"code":"def _check_ip(ip: str) -> bool:\n    \"\"\"\n    Check IP in range\n\n    :param ip:\n    :return:\n    \"\"\"\n    address = ipaddress.IPv4Address(ip)\n    return address in allowed_ips","return_type":"bool","function_name":"_check_ip","stripped_code":"def _check_ip(ip: str):\n    \"\"\"\n    Check IP in range\n\n    :param ip:\n    :return:\n    \"\"\"\n    address = ipaddress.IPv4Address(ip)\n    return address in allowed_ips"}
{"code":"def decomposed_diffusion_program(qubits: List[int]) -> Program:\n    \"\"\"\n    Constructs the diffusion operator used in Grover's Algorithm, acted on both sides by an\n    a Hadamard gate on each qubit. Note that this means that the matrix representation of this\n    operator is diag(1, -1, ..., -1). In particular, this decomposes the diffusion operator, which\n    is a :math:`2**{len(qubits)}\\times2**{len(qubits)}` sparse matrix, into\n     :math:`\\mathcal{O}(len(qubits)**2) single and two qubit gates.\n\n    See C. Lavor, L.R.U. Manssur, and R. Portugal (2003) `Grover's Algorithm: Quantum Database\n    Search`_ for more information.\n\n    .. _`Grover's Algorithm: Quantum Database Search`: https://arxiv.org/abs/quant-ph/0301079\n\n    :param qubits: A list of ints corresponding to the qubits to operate on.\n                   The operator operates on bistrings of the form\n                   ``|qubits[0], ..., qubits[-1]>``.\n    \"\"\"\n    program = Program()\n    if len(qubits) == 1:\n        program.inst(Z(qubits[0]))\n    else:\n        program.inst([X(q) for q in qubits])\n        program.inst(H(qubits[-1]))\n        program.inst(RZ(-np.pi, qubits[0]))\n        program += (ControlledProgramBuilder()\n                              .with_controls(qubits[:-1])\n                              .with_target(qubits[-1])\n                              .with_operation(X_GATE)\n                              .with_gate_name(X_GATE_LABEL).build())\n        program.inst(RZ(-np.pi, qubits[0]))\n        program.inst(H(qubits[-1]))\n        program.inst([X(q) for q in qubits])\n    return program","return_type":"Program","function_name":"decomposed_diffusion_program","stripped_code":"def decomposed_diffusion_program(qubits: List[int]):\n    \"\"\"\n    Constructs the diffusion operator used in Grover's Algorithm, acted on both sides by an\n    a Hadamard gate on each qubit. Note that this means that the matrix representation of this\n    operator is diag(1, -1, ..., -1). In particular, this decomposes the diffusion operator, which\n    is a :math:`2**{len(qubits)}\\times2**{len(qubits)}` sparse matrix, into\n     :math:`\\mathcal{O}(len(qubits)**2) single and two qubit gates.\n\n    See C. Lavor, L.R.U. Manssur, and R. Portugal (2003) `Grover's Algorithm: Quantum Database\n    Search`_ for more information.\n\n    .. _`Grover's Algorithm: Quantum Database Search`: https://arxiv.org/abs/quant-ph/0301079\n\n    :param qubits: A list of ints corresponding to the qubits to operate on.\n                   The operator operates on bistrings of the form\n                   ``|qubits[0], ..., qubits[-1]>``.\n    \"\"\"\n    program = Program()\n    if len(qubits) == 1:\n        program.inst(Z(qubits[0]))\n    else:\n        program.inst([X(q) for q in qubits])\n        program.inst(H(qubits[-1]))\n        program.inst(RZ(-np.pi, qubits[0]))\n        program += (ControlledProgramBuilder()\n                              .with_controls(qubits[:-1])\n                              .with_target(qubits[-1])\n                              .with_operation(X_GATE)\n                              .with_gate_name(X_GATE_LABEL).build())\n        program.inst(RZ(-np.pi, qubits[0]))\n        program.inst(H(qubits[-1]))\n        program.inst([X(q) for q in qubits])\n    return program"}
{"code":"def teardown_request(self, func: Callable) -> Callable:\n        \"\"\"Add a teardown request function to the Blueprint.\n\n        This is designed to be used as a decorator, and has the same arguments\n        as :meth:`~quart.Quart.teardown_request`. It applies only to requests that\n        are routed to an endpoint in this blueprint. An example usage,\n\n        .. code-block:: python\n\n            blueprint = Blueprint(__name__)\n            @blueprint.teardown_request\n            def teardown():\n                ...\n        \"\"\"\n        self.record_once(lambda state: state.app.teardown_request(func, self.name))\n        return func","return_type":"Callable","function_name":"Blueprint.teardown_request","stripped_code":"def teardown_request(self, func: Callable):\n        \"\"\"Add a teardown request function to the Blueprint.\n\n        This is designed to be used as a decorator, and has the same arguments\n        as :meth:`~quart.Quart.teardown_request`. It applies only to requests that\n        are routed to an endpoint in this blueprint. An example usage,\n\n        .. code-block:: python\n\n            blueprint = Blueprint(__name__)\n            @blueprint.teardown_request\n            def teardown():\n                ...\n        \"\"\"\n        self.record_once(lambda state: state.app.teardown_request(func, self.name))\n        return func"}
{"code":"def get_release_count(self, package_name: str) -> int:\n        \"\"\"\n        Returns the number of releases of the given package name available on the current registry.\n        \"\"\"\n        validate_package_name(package_name)\n        self._validate_set_registry()\n        return self.registry._num_release_ids(package_name)","return_type":"int","function_name":"PM.get_release_count","stripped_code":"def get_release_count(self, package_name: str):\n        \"\"\"\n        Returns the number of releases of the given package name available on the current registry.\n        \"\"\"\n        validate_package_name(package_name)\n        self._validate_set_registry()\n        return self.registry._num_release_ids(package_name)"}
{"code":"def url(self) -> str:\n        \"\"\"\n        Get URL for the message\n\n        :return: str\n        \"\"\"\n        if self.chat.type not in [ChatType.SUPER_GROUP, ChatType.CHANNEL]:\n            raise TypeError('Invalid chat type!')\n        elif not self.chat.username:\n            raise TypeError('This chat does not have @username')\n\n        return f\"https://t.me/{self.chat.username}/{self.message_id}\"","return_type":"str","function_name":"Message.url","stripped_code":"def url(self):\n        \"\"\"\n        Get URL for the message\n\n        :return: str\n        \"\"\"\n        if self.chat.type not in [ChatType.SUPER_GROUP, ChatType.CHANNEL]:\n            raise TypeError('Invalid chat type!')\n        elif not self.chat.username:\n            raise TypeError('This chat does not have @username')\n\n        return f\"https://t.me/{self.chat.username}/{self.message_id}\""}
{"code":"def converted_gate_set(circuit: circuits.Circuit,\n                       no_clifford_gates: bool = False,\n                       atol: float = 1e-8,\n                       ) -> circuits.Circuit:\n    \"\"\"Returns a new, equivalent circuit using the gate set\n    {SingleQubitCliffordGate,\n    CZ/PauliInteractionGate, PauliStringPhasor}.\n    \"\"\"\n    conv_circuit = circuits.Circuit(circuit)\n    optimizers.ConvertToCzAndSingleGates().optimize_circuit(conv_circuit)\n    optimizers.MergeSingleQubitGates().optimize_circuit(conv_circuit)\n    ConvertToPauliStringPhasors(ignore_failures=True,\n                                keep_clifford=not no_clifford_gates,\n                                atol=atol,\n                                ).optimize_circuit(conv_circuit)\n    optimizers.DropEmptyMoments().optimize_circuit(conv_circuit)\n    return conv_circuit","return_type":"circuits.Circuit","function_name":"converted_gate_set","stripped_code":"def converted_gate_set(circuit: circuits.Circuit,\n                       no_clifford_gates: bool = False,\n                       atol: float = 1e-8,\n                       ):\n    \"\"\"Returns a new, equivalent circuit using the gate set\n    {SingleQubitCliffordGate,\n    CZ/PauliInteractionGate, PauliStringPhasor}.\n    \"\"\"\n    conv_circuit = circuits.Circuit(circuit)\n    optimizers.ConvertToCzAndSingleGates().optimize_circuit(conv_circuit)\n    optimizers.MergeSingleQubitGates().optimize_circuit(conv_circuit)\n    ConvertToPauliStringPhasors(ignore_failures=True,\n                                keep_clifford=not no_clifford_gates,\n                                atol=atol,\n                                ).optimize_circuit(conv_circuit)\n    optimizers.DropEmptyMoments().optimize_circuit(conv_circuit)\n    return conv_circuit"}
{"code":"def on_batch_end(self, train, **kwargs:Any)->None:\n        \"Take one step forward on the annealing schedule for the optim params.\"\n        if train:\n            if self.idx_s >= len(self.lr_scheds): return {'stop_training': True, 'stop_epoch': True}\n            self.opt.lr = self.lr_scheds[self.idx_s].step()\n            self.opt.mom = self.mom_scheds[self.idx_s].step()\n            # when the current schedule is complete we move onto the next\n            # schedule. (in 1-cycle there are two schedules)\n            if self.lr_scheds[self.idx_s].is_done:\n                self.idx_s += 1","return_type":"None","function_name":"OneCycleScheduler.on_batch_end","stripped_code":"def on_batch_end(self, train, **kwargs:Any):\n        \"Take one step forward on the annealing schedule for the optim params.\"\n        if train:\n            if self.idx_s >= len(self.lr_scheds): return {'stop_training': True, 'stop_epoch': True}\n            self.opt.lr = self.lr_scheds[self.idx_s].step()\n            self.opt.mom = self.mom_scheds[self.idx_s].step()\n            # when the current schedule is complete we move onto the next\n            # schedule. (in 1-cycle there are two schedules)\n            if self.lr_scheds[self.idx_s].is_done:\n                self.idx_s += 1"}
{"code":"def _norm_date(datestr: str, date_fmt: str) -> date:\n    \"\"\"normalize symbolic date values (e.g. 'TODAY')\n\n    Convert a symbolic value in a valid date.\n    Currenlty known symbolic values are 'TODAY', 'YESTERDAY' and 'TOMORROW'.\n\n    NOTE: This function will return `date` (not `datetime`) instances.\n\n    Parameters:\n        `datestr`: the date to parse, formatted as `date_fmt`\n        `date_fmt`: expected output date format\n\n    Returns:\n        The interpreted date as a datetime.datetime object.\n        If `datestr` doesn't match any of the known symbolic names, it just parses it.\n\n    \"\"\"\n    try:\n        days = {'TODAY': 0, 'YESTERDAY': -1, 'TOMORROW': 1}[datestr.upper()]\n        return date.today() + pd.Timedelta(days=days)\n    except KeyError:\n        return datetime.strptime(datestr, date_fmt).date()","return_type":"date","function_name":"_norm_date","stripped_code":"def _norm_date(datestr: str, date_fmt: str):\n    \"\"\"normalize symbolic date values (e.g. 'TODAY')\n\n    Convert a symbolic value in a valid date.\n    Currenlty known symbolic values are 'TODAY', 'YESTERDAY' and 'TOMORROW'.\n\n    NOTE: This function will return `date` (not `datetime`) instances.\n\n    Parameters:\n        `datestr`: the date to parse, formatted as `date_fmt`\n        `date_fmt`: expected output date format\n\n    Returns:\n        The interpreted date as a datetime.datetime object.\n        If `datestr` doesn't match any of the known symbolic names, it just parses it.\n\n    \"\"\"\n    try:\n        days = {'TODAY': 0, 'YESTERDAY': -1, 'TOMORROW': 1}[datestr.upper()]\n        return date.today() + pd.Timedelta(days=days)\n    except KeyError:\n        return datetime.strptime(datestr, date_fmt).date()"}
{"code":"def debug(self, on: bool = True) -> None:\n        \"\"\"\n        Switches debug mode of the engine on or off. This does not interrupt\n        other ongoing operations.\n        \"\"\"\n        if on:\n            self.send_line(\"debug on\")\n        else:\n            self.send_line(\"debug off\")","return_type":"None","function_name":"UciProtocol.debug","stripped_code":"def debug(self, on: bool = True):\n        \"\"\"\n        Switches debug mode of the engine on or off. This does not interrupt\n        other ongoing operations.\n        \"\"\"\n        if on:\n            self.send_line(\"debug on\")\n        else:\n            self.send_line(\"debug off\")"}
{"code":"def set_user_profile_photo(\n        self,\n        photo: str\n    ) -> bool:\n        \"\"\"Use this method to set a new profile photo.\n\n        This method only works for Users.\n        Bots profile photos must be set using BotFather.\n\n        Args:\n            photo (``str``):\n                Profile photo to set.\n                Pass a file path as string to upload a new photo that exists on your local machine.\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n\n        return bool(\n            self.send(\n                functions.photos.UploadProfilePhoto(\n                    file=self.save_file(photo)\n                )\n            )\n        )","return_type":"bool","function_name":"SetUserProfilePhoto.set_user_profile_photo","stripped_code":"def set_user_profile_photo(\n        self,\n        photo: str\n    ):\n        \"\"\"Use this method to set a new profile photo.\n\n        This method only works for Users.\n        Bots profile photos must be set using BotFather.\n\n        Args:\n            photo (``str``):\n                Profile photo to set.\n                Pass a file path as string to upload a new photo that exists on your local machine.\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n\n        return bool(\n            self.send(\n                functions.photos.UploadProfilePhoto(\n                    file=self.save_file(photo)\n                )\n            )\n        )"}
{"code":"def logs_handle_build_job(job_uuid: str,\n                          job_name: str,\n                          log_lines: Optional[Union[str, Iterable[str]]],\n                          temp: bool = True) -> None:\n    \"\"\"Task handling for sidecars logs.\"\"\"\n    handle_build_job_logs(job_uuid=job_uuid,\n                          job_name=job_name,\n                          log_lines=log_lines,\n                          temp=temp)","return_type":"None","function_name":"logs_handle_build_job","stripped_code":"def logs_handle_build_job(job_uuid: str,\n                          job_name: str,\n                          log_lines: Optional[Union[str, Iterable[str]]],\n                          temp: bool = True):\n    \"\"\"Task handling for sidecars logs.\"\"\"\n    handle_build_job_logs(job_uuid=job_uuid,\n                          job_name=job_name,\n                          log_lines=log_lines,\n                          temp=temp)"}
{"code":"def load_weight(weight_file: str,\n                weight_name: str,\n                weight_file_cache: Dict[str, Dict]) -> mx.nd.NDArray:\n    \"\"\"\n    Load wight fron a file or the cache if it was loaded before.\n\n    :param weight_file: Weight file.\n    :param weight_name: Weight name.\n    :param weight_file_cache: Cache of loaded files.\n    :return: Loaded weight.\n    \"\"\"\n    logger.info('Loading input weight file: %s', weight_file)\n    if weight_file.endswith(\".npy\"):\n        return np.load(weight_file)\n    elif weight_file.endswith(\".npz\"):\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = np.load(weight_file)\n        return weight_file_cache[weight_file][weight_name]\n    else:\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = mx.nd.load(weight_file)\n        return weight_file_cache[weight_file]['arg:%s' % weight_name].asnumpy()","return_type":"mx.nd.NDArray","function_name":"load_weight","stripped_code":"def load_weight(weight_file: str,\n                weight_name: str,\n                weight_file_cache: Dict[str, Dict]):\n    \"\"\"\n    Load wight fron a file or the cache if it was loaded before.\n\n    :param weight_file: Weight file.\n    :param weight_name: Weight name.\n    :param weight_file_cache: Cache of loaded files.\n    :return: Loaded weight.\n    \"\"\"\n    logger.info('Loading input weight file: %s', weight_file)\n    if weight_file.endswith(\".npy\"):\n        return np.load(weight_file)\n    elif weight_file.endswith(\".npz\"):\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = np.load(weight_file)\n        return weight_file_cache[weight_file][weight_name]\n    else:\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = mx.nd.load(weight_file)\n        return weight_file_cache[weight_file]['arg:%s' % weight_name].asnumpy()"}
{"code":"def clean_ids(feed: \"Feed\") -> \"Feed\":\n    \"\"\"\n    In the given \"Feed\", strip whitespace from all string IDs and\n    then replace every remaining whitespace chunk with an underscore.\n    Return the resulting \"Feed\".\n    \"\"\"\n    # Alter feed inputs only, and build a new feed from them.\n    # The derived feed attributes, such as feed.trips_i,\n    # will be automatically handled when creating the new feed.\n    feed = feed.copy()\n\n    for table in cs.GTFS_REF[\"table\"].unique():\n        f = getattr(feed, table)\n        if f is None:\n            continue\n        for column in cs.GTFS_REF.loc[cs.GTFS_REF[\"table\"] == table, \"column\"]:\n            if column in f.columns and column.endswith(\"_id\"):\n                try:\n                    f[column] = f[column].str.strip().str.replace(r\"\\s+\", \"_\")\n                    setattr(feed, table, f)\n                except AttributeError:\n                    # Column is not of string type\n                    continue\n\n    return feed","return_type":"\"Feed\"","function_name":"clean_ids","stripped_code":"def clean_ids(feed: \"Feed\"):\n    \"\"\"\n    In the given \"Feed\", strip whitespace from all string IDs and\n    then replace every remaining whitespace chunk with an underscore.\n    Return the resulting \"Feed\".\n    \"\"\"\n    # Alter feed inputs only, and build a new feed from them.\n    # The derived feed attributes, such as feed.trips_i,\n    # will be automatically handled when creating the new feed.\n    feed = feed.copy()\n\n    for table in cs.GTFS_REF[\"table\"].unique():\n        f = getattr(feed, table)\n        if f is None:\n            continue\n        for column in cs.GTFS_REF.loc[cs.GTFS_REF[\"table\"] == table, \"column\"]:\n            if column in f.columns and column.endswith(\"_id\"):\n                try:\n                    f[column] = f[column].str.strip().str.replace(r\"\\s+\", \"_\")\n                    setattr(feed, table, f)\n                except AttributeError:\n                    # Column is not of string type\n                    continue\n\n    return feed"}
{"code":"def validate_supported_quil(program: Program) -> None:\n    \"\"\"\n    Ensure that a program is supported Quil which can run on any QPU, otherwise raise a ValueError.\n    We support a global RESET before any gates, and MEASUREs on each qubit after any gates\n    on that qubit. PRAGMAs and DECLAREs are always allowed, and a final HALT instruction is allowed.\n\n    :param program: The Quil program to validate.\n    \"\"\"\n    gates_seen = False\n    measured_qubits: Set[int] = set()\n    for i, instr in enumerate(program.instructions):\n        if isinstance(instr, Pragma) or isinstance(instr, Declare):\n            continue\n        elif isinstance(instr, Halt):\n            if i != len(program.instructions) - 1:\n                raise ValueError(f\"Cannot have instructions after HALT\")\n        elif isinstance(instr, Gate):\n            gates_seen = True\n            if any(q.index in measured_qubits for q in instr.qubits):\n                raise ValueError(\"Cannot apply gates to qubits that were already measured.\")\n        elif isinstance(instr, Reset):\n            if gates_seen:\n                raise ValueError(\"RESET can only be applied before any gate applications.\")\n        elif isinstance(instr, ResetQubit):\n            raise ValueError(\"Only global RESETs are currently supported.\")\n        elif isinstance(instr, Measurement):\n            if instr.qubit.index in measured_qubits:\n                raise ValueError(\"Multiple measurements per qubit is not supported.\")\n            measured_qubits.add(instr.qubit.index)\n        else:\n            raise ValueError(f\"Unhandled instruction type in supported Quil validation: {instr}\")","return_type":"None","function_name":"validate_supported_quil","stripped_code":"def validate_supported_quil(program: Program):\n    \"\"\"\n    Ensure that a program is supported Quil which can run on any QPU, otherwise raise a ValueError.\n    We support a global RESET before any gates, and MEASUREs on each qubit after any gates\n    on that qubit. PRAGMAs and DECLAREs are always allowed, and a final HALT instruction is allowed.\n\n    :param program: The Quil program to validate.\n    \"\"\"\n    gates_seen = False\n    measured_qubits: Set[int] = set()\n    for i, instr in enumerate(program.instructions):\n        if isinstance(instr, Pragma) or isinstance(instr, Declare):\n            continue\n        elif isinstance(instr, Halt):\n            if i != len(program.instructions) - 1:\n                raise ValueError(f\"Cannot have instructions after HALT\")\n        elif isinstance(instr, Gate):\n            gates_seen = True\n            if any(q.index in measured_qubits for q in instr.qubits):\n                raise ValueError(\"Cannot apply gates to qubits that were already measured.\")\n        elif isinstance(instr, Reset):\n            if gates_seen:\n                raise ValueError(\"RESET can only be applied before any gate applications.\")\n        elif isinstance(instr, ResetQubit):\n            raise ValueError(\"Only global RESETs are currently supported.\")\n        elif isinstance(instr, Measurement):\n            if instr.qubit.index in measured_qubits:\n                raise ValueError(\"Multiple measurements per qubit is not supported.\")\n            measured_qubits.add(instr.qubit.index)\n        else:\n            raise ValueError(f\"Unhandled instruction type in supported Quil validation: {instr}\")"}
{"code":"def total_duration(self) -> datetime.timedelta:\n        \"\"\"\n        Returns a ``datetime.timedelta`` object with the total sum of\n        durations. If there is overlap, time will be double-counted, so beware!\n        \"\"\"\n        total = datetime.timedelta()\n        for interval in self.intervals:\n            total += interval.duration()\n        return total","return_type":"datetime.timedelta","function_name":"IntervalList.total_duration","stripped_code":"def total_duration(self):\n        \"\"\"\n        Returns a ``datetime.timedelta`` object with the total sum of\n        durations. If there is overlap, time will be double-counted, so beware!\n        \"\"\"\n        total = datetime.timedelta()\n        for interval in self.intervals:\n            total += interval.duration()\n        return total"}
{"code":"def FromHandle(self, hwnd: int, left: int = 0, top: int = 0, right: int = 0, bottom: int = 0) -> bool:\n        \"\"\"\n        Capture a native window to Bitmap by its handle.\n        hwnd: int, the handle of a native window.\n        left: int.\n        top: int.\n        right: int.\n        bottom: int.\n        left, top, right and bottom are control's internal postion(from 0,0).\n        Return bool, True if succeed otherwise False.\n        \"\"\"\n        self.Release()\n        root = GetRootControl()\n        rect = ctypes.wintypes.RECT()\n        ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect))\n        left, top, right, bottom = left + rect.left, top + rect.top, right + rect.left, bottom + rect.top\n        self._bitmap = _DllClient.instance().dll.BitmapFromWindow(root.NativeWindowHandle, left, top, right, bottom)\n        self._getsize()\n        return self._bitmap > 0","return_type":"bool","function_name":"Bitmap.FromHandle","stripped_code":"def FromHandle(self, hwnd: int, left: int = 0, top: int = 0, right: int = 0, bottom: int = 0):\n        \"\"\"\n        Capture a native window to Bitmap by its handle.\n        hwnd: int, the handle of a native window.\n        left: int.\n        top: int.\n        right: int.\n        bottom: int.\n        left, top, right and bottom are control's internal postion(from 0,0).\n        Return bool, True if succeed otherwise False.\n        \"\"\"\n        self.Release()\n        root = GetRootControl()\n        rect = ctypes.wintypes.RECT()\n        ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect))\n        left, top, right, bottom = left + rect.left, top + rect.top, right + rect.left, bottom + rect.top\n        self._bitmap = _DllClient.instance().dll.BitmapFromWindow(root.NativeWindowHandle, left, top, right, bottom)\n        self._getsize()\n        return self._bitmap > 0"}
{"code":"def enable_chunked_encoding(self, chunk_size: Optional[int]=None) -> None:\n        \"\"\"Enables automatic chunked transfer encoding.\"\"\"\n        self._chunked = True\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\"You can't enable chunked encoding when \"\n                               \"a content length is set\")\n        if chunk_size is not None:\n            warnings.warn('Chunk size is deprecated #1615', DeprecationWarning)","return_type":"None","function_name":"StreamResponse.enable_chunked_encoding","stripped_code":"def enable_chunked_encoding(self, chunk_size: Optional[int]=None):\n        \"\"\"Enables automatic chunked transfer encoding.\"\"\"\n        self._chunked = True\n\n        if hdrs.CONTENT_LENGTH in self._headers:\n            raise RuntimeError(\"You can't enable chunked encoding when \"\n                               \"a content length is set\")\n        if chunk_size is not None:\n            warnings.warn('Chunk size is deprecated #1615', DeprecationWarning)"}
{"code":"def allready(self, obj) -> bool:\n        \"\"\"Return |True| or |False| to indicate whether all protected\n        properties are ready or not.\"\"\"\n        for prop in self.__properties:\n            if not prop.isready(obj):\n                return False\n        return True","return_type":"bool","function_name":"ProtectedProperties.allready","stripped_code":"def allready(self, obj):\n        \"\"\"Return |True| or |False| to indicate whether all protected\n        properties are ready or not.\"\"\"\n        for prop in self.__properties:\n            if not prop.isready(obj):\n                return False\n        return True"}
{"code":"def fluent_shape(self) -> Sequence[int]:\n        '''Returns a copy of the fluent shape, ignoring batch size if in batch mode.'''\n        return tuple(self._shape.as_list()[1:] if self._batch else self._shape.as_list()[:])","return_type":"Sequence[int]","function_name":"TensorFluentShape.fluent_shape","stripped_code":"def fluent_shape(self):\n        '''Returns a copy of the fluent shape, ignoring batch size if in batch mode.'''\n        return tuple(self._shape.as_list()[1:] if self._batch else self._shape.as_list()[:])"}
{"code":"def ch(self) -> np.array:\n        \"\"\"An integer array with the shape (height, width).\n\n        You can change the consoles character codes by using this array.\n\n        Index this array with ``console.ch[i, j]  # order='C'`` or\n        ``console.ch[x, y]  # order='F'``.\n        \"\"\"\n        return self._tiles[\"ch\"].T if self._order == \"F\" else self._tiles[\"ch\"]","return_type":"np.array","function_name":"Console.ch","stripped_code":"def ch(self):\n        \"\"\"An integer array with the shape (height, width).\n\n        You can change the consoles character codes by using this array.\n\n        Index this array with ``console.ch[i, j]  # order='C'`` or\n        ``console.ch[x, y]  # order='F'``.\n        \"\"\"\n        return self._tiles[\"ch\"].T if self._order == \"F\" else self._tiles[\"ch\"]"}
{"code":"def visit_ExceptHandler(self, node: ast.ExceptHandler) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from except handler bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.ExceptHandler)\n        return ast.copy_location(\n            ast.ExceptHandler(\n                type=new_node.type,\n                name=new_node.name,\n                body=_filter_dead_code(new_node.body),\n            ),\n            new_node,\n        )","return_type":"Optional[ast.AST]","function_name":"PythonASTOptimizer.visit_ExceptHandler","stripped_code":"def visit_ExceptHandler(self, node: ast.ExceptHandler):\n        \"\"\"Eliminate dead code from except handler bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.ExceptHandler)\n        return ast.copy_location(\n            ast.ExceptHandler(\n                type=new_node.type,\n                name=new_node.name,\n                body=_filter_dead_code(new_node.body),\n            ),\n            new_node,\n        )"}
{"code":"def exist(self, table: str, libref: str =\"\") -> bool:\n      \"\"\"\n      table  - the name of the SAS Data Set\n      libref - the libref for the Data Set, defaults to WORK, or USER if assigned\n\n      Returns True it the Data Set exists and False if it does not\n      \"\"\"\n      code  = \"data _null_; e = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"');\\n\"\n      code += \"v = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"', 'VIEW');\\n if e or v then e = 1;\\n\"\n      code += \"te='TABLE_EXISTS='; put te e;run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"TABLE_EXISTS= \")\n      l2 = l2[2].partition(\"\\n\")\n      exists = int(l2[0])\n\n      return bool(exists)","return_type":"bool","function_name":"SASsessionSTDIO.exist","stripped_code":"def exist(self, table: str, libref: str =\"\"):\n      \"\"\"\n      table  - the name of the SAS Data Set\n      libref - the libref for the Data Set, defaults to WORK, or USER if assigned\n\n      Returns True it the Data Set exists and False if it does not\n      \"\"\"\n      code  = \"data _null_; e = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"');\\n\"\n      code += \"v = exist('\"\n      if len(libref):\n         code += libref+\".\"\n      code += table+\"', 'VIEW');\\n if e or v then e = 1;\\n\"\n      code += \"te='TABLE_EXISTS='; put te e;run;\"\n\n      ll = self.submit(code, \"text\")\n\n      l2 = ll['LOG'].rpartition(\"TABLE_EXISTS= \")\n      l2 = l2[2].partition(\"\\n\")\n      exists = int(l2[0])\n\n      return bool(exists)"}
{"code":"def k8s_events_handle_experiment_job_statuses(self: 'celery_app.task', payload: Dict) -> None:\n    \"\"\"Experiment jobs statuses\"\"\"\n    details = payload['details']\n    job_uuid = details['labels']['job_uuid']\n    logger.debug('handling events status for job_uuid: %s, status: %s',\n                 job_uuid, payload['status'])\n\n    try:\n        job = ExperimentJob.objects.get(uuid=job_uuid)\n    except ExperimentJob.DoesNotExist:\n        logger.debug('Job uuid`%s` does not exist', job_uuid)\n        return\n\n    try:\n        job.experiment\n    except Experiment.DoesNotExist:\n        logger.debug('Experiment for job `%s` does not exist anymore', job_uuid)\n        return\n\n    if job.last_status is None and self.request.retries < 2:\n        self.retry(countdown=1)\n\n    # Set the new status\n    try:\n        set_node_scheduling(job, details['node_name'])\n        job.set_status(status=payload['status'],\n                       message=payload['message'],\n                       created_at=payload.get('created_at'),\n                       traceback=payload.get('traceback'),\n                       details=details)\n        logger.debug('status %s is set for job %s %s', payload['status'], job_uuid, job.id)\n    except IntegrityError:\n        # Due to concurrency this could happen, we just retry it\n        logger.info('Retry job status %s handling %s', payload['status'], job_uuid)\n        self.retry(countdown=Intervals.EXPERIMENTS_SCHEDULER)","return_type":"None","function_name":"k8s_events_handle_experiment_job_statuses","stripped_code":"def k8s_events_handle_experiment_job_statuses(self: 'celery_app.task', payload: Dict):\n    \"\"\"Experiment jobs statuses\"\"\"\n    details = payload['details']\n    job_uuid = details['labels']['job_uuid']\n    logger.debug('handling events status for job_uuid: %s, status: %s',\n                 job_uuid, payload['status'])\n\n    try:\n        job = ExperimentJob.objects.get(uuid=job_uuid)\n    except ExperimentJob.DoesNotExist:\n        logger.debug('Job uuid`%s` does not exist', job_uuid)\n        return\n\n    try:\n        job.experiment\n    except Experiment.DoesNotExist:\n        logger.debug('Experiment for job `%s` does not exist anymore', job_uuid)\n        return\n\n    if job.last_status is None and self.request.retries < 2:\n        self.retry(countdown=1)\n\n    # Set the new status\n    try:\n        set_node_scheduling(job, details['node_name'])\n        job.set_status(status=payload['status'],\n                       message=payload['message'],\n                       created_at=payload.get('created_at'),\n                       traceback=payload.get('traceback'),\n                       details=details)\n        logger.debug('status %s is set for job %s %s', payload['status'], job_uuid, job.id)\n    except IntegrityError:\n        # Due to concurrency this could happen, we just retry it\n        logger.info('Retry job status %s handling %s', payload['status'], job_uuid)\n        self.retry(countdown=Intervals.EXPERIMENTS_SCHEDULER)"}
{"code":"def update_headers(self, headers: Optional[LooseHeaders]) -> None:\n        \"\"\"Update request headers.\"\"\"\n        self.headers = CIMultiDict()  # type: CIMultiDict[str]\n\n        # add host\n        netloc = cast(str, self.url.raw_host)\n        if helpers.is_ipv6_address(netloc):\n            netloc = '[{}]'.format(netloc)\n        if not self.url.is_default_port():\n            netloc += ':' + str(self.url.port)\n        self.headers[hdrs.HOST] = netloc\n\n        if headers:\n            if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\n                headers = headers.items()  # type: ignore\n\n            for key, value in headers:\n                # A special case for Host header\n                if key.lower() == 'host':\n                    self.headers[key] = value\n                else:\n                    self.headers.add(key, value)","return_type":"None","function_name":"ClientRequest.update_headers","stripped_code":"def update_headers(self, headers: Optional[LooseHeaders]):\n        \"\"\"Update request headers.\"\"\"\n        self.headers = CIMultiDict()  # type: CIMultiDict[str]\n\n        # add host\n        netloc = cast(str, self.url.raw_host)\n        if helpers.is_ipv6_address(netloc):\n            netloc = '[{}]'.format(netloc)\n        if not self.url.is_default_port():\n            netloc += ':' + str(self.url.port)\n        self.headers[hdrs.HOST] = netloc\n\n        if headers:\n            if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\n                headers = headers.items()  # type: ignore\n\n            for key, value in headers:\n                # A special case for Host header\n                if key.lower() == 'host':\n                    self.headers[key] = value\n                else:\n                    self.headers.add(key, value)"}
{"code":"def refresh(self) -> None:\n        \"\"\"Update the actual simulation values based on the toy-value pairs.\n\n        Usually, one does not need to call refresh explicitly.  The\n        \"magic\" methods __call__, __setattr__, and __delattr__ invoke\n        it automatically, when required.\n\n        Instantiate a 1-dimensional |SeasonalParameter| object:\n\n        >>> from hydpy.core.parametertools import SeasonalParameter\n        >>> class Par(SeasonalParameter):\n        ...     NDIM = 1\n        ...     TYPE = float\n        ...     TIME = None\n        >>> par = Par(None)\n        >>> par.simulationstep = '1d'\n        >>> par.shape = (None,)\n\n        When a |SeasonalParameter| object does not contain any toy-value\n        pairs yet, the method |SeasonalParameter.refresh| sets all actual\n        simulation values to zero:\n\n        >>> par.values = 1.\n        >>> par.refresh()\n        >>> par.values[0]\n        0.0\n\n        When there is only one toy-value pair, its values are relevant\n        for all actual simulation values:\n\n        >>> par.toy_1 = 2. # calls refresh automatically\n        >>> par.values[0]\n        2.0\n\n        Method |SeasonalParameter.refresh| performs a linear interpolation\n        for the central time points of each simulation time step.  Hence,\n        in the following example, the original values of the toy-value\n        pairs do not show up:\n\n        >>> par.toy_12_31 = 4.\n        >>> from hydpy import round_\n        >>> round_(par.values[0])\n        2.00274\n        >>> round_(par.values[-2])\n        3.99726\n        >>> par.values[-1]\n        3.0\n\n        If one wants to preserve the original values in this example, one\n        would have to set the corresponding toy instances in the middle of\n        some simulation step intervals:\n\n        >>> del par.toy_1\n        >>> del par.toy_12_31\n        >>> par.toy_1_1_12 = 2\n        >>> par.toy_12_31_12 = 4.\n        >>> par.values[0]\n        2.0\n        >>> round_(par.values[1])\n        2.005479\n        >>> round_(par.values[-2])\n        3.994521\n        >>> par.values[-1]\n        4.0\n        \"\"\"\n        if not self:\n            self.values[:] = 0.\n        elif len(self) == 1:\n            values = list(self._toy2values.values())[0]\n            self.values[:] = self.apply_timefactor(values)\n        else:\n            for idx, date in enumerate(\n                    timetools.TOY.centred_timegrid(self.simulationstep)):\n                values = self.interp(date)\n                self.values[idx] = self.apply_timefactor(values)","return_type":"None","function_name":"SeasonalParameter.refresh","stripped_code":"def refresh(self):\n        \"\"\"Update the actual simulation values based on the toy-value pairs.\n\n        Usually, one does not need to call refresh explicitly.  The\n        \"magic\" methods __call__, __setattr__, and __delattr__ invoke\n        it automatically, when required.\n\n        Instantiate a 1-dimensional |SeasonalParameter| object:\n\n        >>> from hydpy.core.parametertools import SeasonalParameter\n        >>> class Par(SeasonalParameter):\n        ...     NDIM = 1\n        ...     TYPE = float\n        ...     TIME = None\n        >>> par = Par(None)\n        >>> par.simulationstep = '1d'\n        >>> par.shape = (None,)\n\n        When a |SeasonalParameter| object does not contain any toy-value\n        pairs yet, the method |SeasonalParameter.refresh| sets all actual\n        simulation values to zero:\n\n        >>> par.values = 1.\n        >>> par.refresh()\n        >>> par.values[0]\n        0.0\n\n        When there is only one toy-value pair, its values are relevant\n        for all actual simulation values:\n\n        >>> par.toy_1 = 2. # calls refresh automatically\n        >>> par.values[0]\n        2.0\n\n        Method |SeasonalParameter.refresh| performs a linear interpolation\n        for the central time points of each simulation time step.  Hence,\n        in the following example, the original values of the toy-value\n        pairs do not show up:\n\n        >>> par.toy_12_31 = 4.\n        >>> from hydpy import round_\n        >>> round_(par.values[0])\n        2.00274\n        >>> round_(par.values[-2])\n        3.99726\n        >>> par.values[-1]\n        3.0\n\n        If one wants to preserve the original values in this example, one\n        would have to set the corresponding toy instances in the middle of\n        some simulation step intervals:\n\n        >>> del par.toy_1\n        >>> del par.toy_12_31\n        >>> par.toy_1_1_12 = 2\n        >>> par.toy_12_31_12 = 4.\n        >>> par.values[0]\n        2.0\n        >>> round_(par.values[1])\n        2.005479\n        >>> round_(par.values[-2])\n        3.994521\n        >>> par.values[-1]\n        4.0\n        \"\"\"\n        if not self:\n            self.values[:] = 0.\n        elif len(self) == 1:\n            values = list(self._toy2values.values())[0]\n            self.values[:] = self.apply_timefactor(values)\n        else:\n            for idx, date in enumerate(\n                    timetools.TOY.centred_timegrid(self.simulationstep)):\n                values = self.interp(date)\n                self.values[idx] = self.apply_timefactor(values)"}
{"code":"def trigger(self, event: str, *args: T.Any, **kw: T.Any) -> bool:\n        \"\"\"Triggers all handlers which are subscribed to an event.\n        Returns True when there were callbacks to execute, False otherwise.\"\"\"\n\n        callbacks = list(self._events.get(event, []))\n        if not callbacks:\n            return False\n\n        for callback in callbacks:\n            callback(*args, **kw)\n        return True","return_type":"bool","function_name":"Observable.trigger","stripped_code":"def trigger(self, event: str, *args: T.Any, **kw: T.Any):\n        \"\"\"Triggers all handlers which are subscribed to an event.\n        Returns True when there were callbacks to execute, False otherwise.\"\"\"\n\n        callbacks = list(self._events.get(event, []))\n        if not callbacks:\n            return False\n\n        for callback in callbacks:\n            callback(*args, **kw)\n        return True"}
{"code":"def requires_git(func: Callable) -> Callable:\n    \"\"\"\n    Decorator to ensure `git` is accessible before calling a function.\n    :param func: the function to wrap\n    :return: the wrapped function\n    \"\"\"\n    def decorated(*args, **kwargs):\n        try:\n            run([GIT_COMMAND, \"--version\"])\n        except RunException as e:\n            raise RuntimeError(\"`git` does not appear to be working\") from e\n        return func(*args, **kwargs)\n\n    return decorated","return_type":"Callable","function_name":"requires_git","stripped_code":"def requires_git(func: Callable):\n    \"\"\"\n    Decorator to ensure `git` is accessible before calling a function.\n    :param func: the function to wrap\n    :return: the wrapped function\n    \"\"\"\n    def decorated(*args, **kwargs):\n        try:\n            run([GIT_COMMAND, \"--version\"])\n        except RunException as e:\n            raise RuntimeError(\"`git` does not appear to be working\") from e\n        return func(*args, **kwargs)\n\n    return decorated"}
{"code":"def nodeInLiteralStem(_: Context, n: Node, s: ShExJ.LiteralStem) -> bool:\n    \"\"\" http://shex.io/shex-semantics/#values\n\n        **nodeIn**: asserts that an RDF node n is equal to an RDF term s or is in a set defined by a\n        :py:class:`ShExJ.IriStem`, :py:class:`LiteralStem` or :py:class:`LanguageStem`.\n\n        The expression `nodeInLiteralStem(n, s)` is satisfied iff:\n         #) `s` is a :py:class:`ShExJ.WildCard` or\n         #) `n` is an :py:class:`rdflib.Literal` and fn:starts-with(`n`, `s`)\n     \"\"\"\n    return isinstance(s, ShExJ.Wildcard) or \\\n        (isinstance(n, Literal) and str(n.value).startswith(str(s)))","return_type":"bool","function_name":"nodeInLiteralStem","stripped_code":"def nodeInLiteralStem(_: Context, n: Node, s: ShExJ.LiteralStem):\n    \"\"\" http://shex.io/shex-semantics/#values\n\n        **nodeIn**: asserts that an RDF node n is equal to an RDF term s or is in a set defined by a\n        :py:class:`ShExJ.IriStem`, :py:class:`LiteralStem` or :py:class:`LanguageStem`.\n\n        The expression `nodeInLiteralStem(n, s)` is satisfied iff:\n         #) `s` is a :py:class:`ShExJ.WildCard` or\n         #) `n` is an :py:class:`rdflib.Literal` and fn:starts-with(`n`, `s`)\n     \"\"\"\n    return isinstance(s, ShExJ.Wildcard) or \\\n        (isinstance(n, Literal) and str(n.value).startswith(str(s)))"}
{"code":"def _deconstruct_url(self, url: str) -> List[str]:\n        \"\"\"\n        Split a regular URL into parts\n\n        :param url: A normalized URL\n        :return: Parts of the URL\n        :raises kua.routes.RouteError: \\\n        If the depth of the URL exceeds\\\n        the max depth of the deepest\\\n        registered pattern\n\n        :private:\n        \"\"\"\n        parts = url.split('/', self._max_depth + 1)\n\n        if depth_of(parts) > self._max_depth:\n            raise RouteError('No match')\n\n        return parts","return_type":"List[str]","function_name":"Routes._deconstruct_url","stripped_code":"def _deconstruct_url(self, url: str):\n        \"\"\"\n        Split a regular URL into parts\n\n        :param url: A normalized URL\n        :return: Parts of the URL\n        :raises kua.routes.RouteError: \\\n        If the depth of the URL exceeds\\\n        the max depth of the deepest\\\n        registered pattern\n\n        :private:\n        \"\"\"\n        parts = url.split('/', self._max_depth + 1)\n\n        if depth_of(parts) > self._max_depth:\n            raise RouteError('No match')\n\n        return parts"}
{"code":"def cusum(self, data: ['SASdata', str] = None,\n              by: str = None,\n              inset: str = None,\n              xchart: str = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the CUSUM procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_cusum_toc.htm&locale=en\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm inset: The inset variable can only be a string type.\n        :parm xchart: The xchart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"","return_type":"'SASresults'","function_name":"SASqc.cusum","stripped_code":"def cusum(self, data: ['SASdata', str] = None,\n              by: str = None,\n              inset: str = None,\n              xchart: str = None,\n              procopts: str = None,\n              stmtpassthrough: str = None,\n              **kwargs: dict):\n        \"\"\"\n        Python method to call the CUSUM procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_cusum_toc.htm&locale=en\n        :param data: SASdata object or string. This parameter is required.\n        :parm by: The by variable can only be a string type.\n        :parm inset: The inset variable can only be a string type.\n        :parm xchart: The xchart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\""}
{"code":"def raw(self) -> str:\n        \"\"\"\n        Return signed raw format string of the Membership instance\n\n        :return:\n        \"\"\"\n        return \"\"\"Version: {0}\nType: Membership\nCurrency: {1}\nIssuer: {2}\nBlock: {3}\nMembership: {4}\nUserID: {5}\nCertTS: {6}\n\"\"\".format(self.version,\n           self.currency,\n           self.issuer,\n           self.membership_ts,\n           self.membership_type,\n           self.uid,\n           self.identity_ts)","return_type":"str","function_name":"Membership.raw","stripped_code":"def raw(self):\n        \"\"\"\n        Return signed raw format string of the Membership instance\n\n        :return:\n        \"\"\"\n        return \"\"\"Version: {0}\nType: Membership\nCurrency: {1}\nIssuer: {2}\nBlock: {3}\nMembership: {4}\nUserID: {5}\nCertTS: {6}\n\"\"\".format(self.version,\n           self.currency,\n           self.issuer,\n           self.membership_ts,\n           self.membership_type,\n           self.uid,\n           self.identity_ts)"}
{"code":"def handle_error(self, error: Exception) -> None:\n        \"\"\"\n        Populates :data:`chess.pgn.Game.errors` with encountered errors and\n        logs them.\n        \"\"\"\n        LOGGER.exception(\"error during pgn parsing\")\n        self.game.errors.append(error)","return_type":"None","function_name":"GameBuilder.handle_error","stripped_code":"def handle_error(self, error: Exception):\n        \"\"\"\n        Populates :data:`chess.pgn.Game.errors` with encountered errors and\n        logs them.\n        \"\"\"\n        LOGGER.exception(\"error during pgn parsing\")\n        self.game.errors.append(error)"}
{"code":"def do_wait(coro: Callable) -> Any:\n    \"\"\"\n    Perform aynchronous operation; await then return the result.\n\n    :param coro: coroutine to await\n    :return: coroutine result\n    \"\"\"\n\n    event_loop = None\n    try:\n        event_loop = asyncio.get_event_loop()\n    except RuntimeError:\n        event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(event_loop)\n    return event_loop.run_until_complete(coro)","return_type":"Any","function_name":"do_wait","stripped_code":"def do_wait(coro: Callable):\n    \"\"\"\n    Perform aynchronous operation; await then return the result.\n\n    :param coro: coroutine to await\n    :return: coroutine result\n    \"\"\"\n\n    event_loop = None\n    try:\n        event_loop = asyncio.get_event_loop()\n    except RuntimeError:\n        event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(event_loop)\n    return event_loop.run_until_complete(coro)"}
{"code":"def resolve(\n    requirements: typing.List[str],\n    index_urls: list = None,\n    python_version: int = 3,\n    exclude_packages: set = None,\n    transitive: bool = True,\n    subgraph_check_api: str = None,\n) -> dict:\n    \"\"\"Resolve given requirements for the given Python version.\"\"\"\n    assert python_version in (2, 3), \"Unknown Python version\"\n\n    if subgraph_check_api and not transitive:\n        _LOGGER.error(\"The check against subgraph API cannot be done if no transitive dependencies are resolved\")\n        sys.exit(2)\n\n    python_bin = \"python3\" if python_version == 3 else \"python2\"\n    run_command(\"virtualenv -p python3 venv\")\n    python_bin = \"venv/bin/\" + python_bin\n\n    run_command(\"{} -m pip install pipdeptree\".format(python_bin))\n    environment_details = _get_environment_details(python_bin)\n\n    result = {\"tree\": [], \"errors\": [], \"unparsed\": [], \"unresolved\": [], \"environment\": environment_details}\n\n    all_solvers = []\n    for index_url in index_urls:\n        source = Source(index_url)\n        all_solvers.append(PythonSolver(fetcher_kwargs={\"source\": source}))\n\n    for solver in all_solvers:\n        solver_result = _do_resolve_index(\n            python_bin=python_bin,\n            solver=solver,\n            all_solvers=all_solvers,\n            requirements=requirements,\n            exclude_packages=exclude_packages,\n            transitive=transitive,\n            subgraph_check_api=subgraph_check_api,\n        )\n\n        result[\"tree\"].extend(solver_result[\"tree\"])\n        result[\"errors\"].extend(solver_result[\"errors\"])\n        result[\"unparsed\"].extend(solver_result[\"unparsed\"])\n        result[\"unresolved\"].extend(solver_result[\"unresolved\"])\n\n    return result","return_type":"dict","function_name":"resolve","stripped_code":"def resolve(\n    requirements: typing.List[str],\n    index_urls: list = None,\n    python_version: int = 3,\n    exclude_packages: set = None,\n    transitive: bool = True,\n    subgraph_check_api: str = None,\n):\n    \"\"\"Resolve given requirements for the given Python version.\"\"\"\n    assert python_version in (2, 3), \"Unknown Python version\"\n\n    if subgraph_check_api and not transitive:\n        _LOGGER.error(\"The check against subgraph API cannot be done if no transitive dependencies are resolved\")\n        sys.exit(2)\n\n    python_bin = \"python3\" if python_version == 3 else \"python2\"\n    run_command(\"virtualenv -p python3 venv\")\n    python_bin = \"venv/bin/\" + python_bin\n\n    run_command(\"{} -m pip install pipdeptree\".format(python_bin))\n    environment_details = _get_environment_details(python_bin)\n\n    result = {\"tree\": [], \"errors\": [], \"unparsed\": [], \"unresolved\": [], \"environment\": environment_details}\n\n    all_solvers = []\n    for index_url in index_urls:\n        source = Source(index_url)\n        all_solvers.append(PythonSolver(fetcher_kwargs={\"source\": source}))\n\n    for solver in all_solvers:\n        solver_result = _do_resolve_index(\n            python_bin=python_bin,\n            solver=solver,\n            all_solvers=all_solvers,\n            requirements=requirements,\n            exclude_packages=exclude_packages,\n            transitive=transitive,\n            subgraph_check_api=subgraph_check_api,\n        )\n\n        result[\"tree\"].extend(solver_result[\"tree\"])\n        result[\"errors\"].extend(solver_result[\"errors\"])\n        result[\"unparsed\"].extend(solver_result[\"unparsed\"])\n        result[\"unresolved\"].extend(solver_result[\"unresolved\"])\n\n    return result"}
{"code":"def verify_consistency(self, expected_leaf_count) -> bool:\n        \"\"\"\n        Check that the tree has same leaf count as expected and the\n        number of nodes are also as expected\n        \"\"\"\n        if expected_leaf_count != self.leafCount:\n            raise ConsistencyVerificationFailed()\n        if self.get_expected_node_count(self.leafCount) != self.nodeCount:\n            raise ConsistencyVerificationFailed()\n        return True","return_type":"bool","function_name":"CompactMerkleTree.verify_consistency","stripped_code":"def verify_consistency(self, expected_leaf_count):\n        \"\"\"\n        Check that the tree has same leaf count as expected and the\n        number of nodes are also as expected\n        \"\"\"\n        if expected_leaf_count != self.leafCount:\n            raise ConsistencyVerificationFailed()\n        if self.get_expected_node_count(self.leafCount) != self.nodeCount:\n            raise ConsistencyVerificationFailed()\n        return True"}
{"code":"def after_epoch(self, epoch_id: int, epoch_data: EpochData) -> None:\n        \"\"\"\n        Call :py:meth:`_on_plateau_action` if the ``long_term``\n        variable mean is lower/greater than the ``short_term`` mean.\n        \"\"\"\n\n        super().after_epoch(epoch_id=epoch_id, epoch_data=epoch_data)\n\n        self._saved_loss.append(epoch_data[self._stream][self._variable][OnPlateau._AGGREGATION])\n\n        long_mean = np.mean(self._saved_loss[-self._long_term:])\n        short_mean = np.mean(self._saved_loss[-self._short_term:])\n        if self._objective == 'min' and long_mean < short_mean:\n            self._on_plateau_action(epoch_id=epoch_id, epoch_data=epoch_data)\n        elif self._objective == 'max' and long_mean > short_mean:\n            self._on_plateau_action(epoch_id=epoch_id, epoch_data=epoch_data)","return_type":"None","function_name":"OnPlateau.after_epoch","stripped_code":"def after_epoch(self, epoch_id: int, epoch_data: EpochData):\n        \"\"\"\n        Call :py:meth:`_on_plateau_action` if the ``long_term``\n        variable mean is lower/greater than the ``short_term`` mean.\n        \"\"\"\n\n        super().after_epoch(epoch_id=epoch_id, epoch_data=epoch_data)\n\n        self._saved_loss.append(epoch_data[self._stream][self._variable][OnPlateau._AGGREGATION])\n\n        long_mean = np.mean(self._saved_loss[-self._long_term:])\n        short_mean = np.mean(self._saved_loss[-self._short_term:])\n        if self._objective == 'min' and long_mean < short_mean:\n            self._on_plateau_action(epoch_id=epoch_id, epoch_data=epoch_data)\n        elif self._objective == 'max' and long_mean > short_mean:\n            self._on_plateau_action(epoch_id=epoch_id, epoch_data=epoch_data)"}
{"code":"def pseudo_raw_input(self, prompt: str) -> str:\n        \"\"\"Began life as a copy of cmd's cmdloop; like raw_input but\n\n        - accounts for changed stdin, stdout\n        - if input is a pipe (instead of a tty), look at self.echo\n          to decide whether to print the prompt and the input\n        \"\"\"\n        if self.use_rawinput:\n            try:\n                if sys.stdin.isatty():\n                    # Wrap in try since terminal_lock may not be locked when this function is called from unit tests\n                    try:\n                        # A prompt is about to be drawn. Allow asynchronous changes to the terminal.\n                        self.terminal_lock.release()\n                    except RuntimeError:\n                        pass\n\n                    # Deal with the vagaries of readline and ANSI escape codes\n                    safe_prompt = rl_make_safe_prompt(prompt)\n                    line = input(safe_prompt)\n                else:\n                    line = input()\n                    if self.echo:\n                        sys.stdout.write('{}{}\\n'.format(prompt, line))\n            except EOFError:\n                line = 'eof'\n            finally:\n                if sys.stdin.isatty():\n                    # The prompt is gone. Do not allow asynchronous changes to the terminal.\n                    self.terminal_lock.acquire()\n        else:\n            if self.stdin.isatty():\n                # on a tty, print the prompt first, then read the line\n                self.poutput(prompt, end='')\n                self.stdout.flush()\n                line = self.stdin.readline()\n                if len(line) == 0:\n                    line = 'eof'\n            else:\n                # we are reading from a pipe, read the line to see if there is\n                # anything there, if so, then decide whether to print the\n                # prompt or not\n                line = self.stdin.readline()\n                if len(line):\n                    # we read something, output the prompt and the something\n                    if self.echo:\n                        self.poutput('{}{}'.format(prompt, line))\n                else:\n                    line = 'eof'\n\n        return line.strip()","return_type":"str","function_name":"Cmd.pseudo_raw_input","stripped_code":"def pseudo_raw_input(self, prompt: str):\n        \"\"\"Began life as a copy of cmd's cmdloop; like raw_input but\n\n        - accounts for changed stdin, stdout\n        - if input is a pipe (instead of a tty), look at self.echo\n          to decide whether to print the prompt and the input\n        \"\"\"\n        if self.use_rawinput:\n            try:\n                if sys.stdin.isatty():\n                    # Wrap in try since terminal_lock may not be locked when this function is called from unit tests\n                    try:\n                        # A prompt is about to be drawn. Allow asynchronous changes to the terminal.\n                        self.terminal_lock.release()\n                    except RuntimeError:\n                        pass\n\n                    # Deal with the vagaries of readline and ANSI escape codes\n                    safe_prompt = rl_make_safe_prompt(prompt)\n                    line = input(safe_prompt)\n                else:\n                    line = input()\n                    if self.echo:\n                        sys.stdout.write('{}{}\\n'.format(prompt, line))\n            except EOFError:\n                line = 'eof'\n            finally:\n                if sys.stdin.isatty():\n                    # The prompt is gone. Do not allow asynchronous changes to the terminal.\n                    self.terminal_lock.acquire()\n        else:\n            if self.stdin.isatty():\n                # on a tty, print the prompt first, then read the line\n                self.poutput(prompt, end='')\n                self.stdout.flush()\n                line = self.stdin.readline()\n                if len(line) == 0:\n                    line = 'eof'\n            else:\n                # we are reading from a pipe, read the line to see if there is\n                # anything there, if so, then decide whether to print the\n                # prompt or not\n                line = self.stdin.readline()\n                if len(line):\n                    # we read something, output the prompt and the something\n                    if self.echo:\n                        self.poutput('{}{}'.format(prompt, line))\n                else:\n                    line = 'eof'\n\n        return line.strip()"}
{"code":"def align_lines(lines: list, column_separator: str='|') -> list:\n    \"\"\"\n    Pads lines so that all rows in single column match. Columns separated by '|' in every line.\n    :param lines: list of lines\n    :param column_separator: column separator. default is '|'\n    :return: list of lines\n    \"\"\"\n    rows = []\n    col_len = []\n    for line in lines:\n        line = str(line)\n        cols = []\n        for col_index, col in enumerate(line.split(column_separator)):\n            col = str(col).strip()\n            cols.append(col)\n            if col_index >= len(col_len):\n                col_len.append(0)\n            col_len[col_index] = max(col_len[col_index], len(col))\n        rows.append(cols)\n\n    lines_out = []\n    for row in rows:\n        cols_out = []\n        for col_index, col in enumerate(row):\n            if col_index == 0:\n                col = col.ljust(col_len[col_index])\n            else:\n                col = col.rjust(col_len[col_index])\n            cols_out.append(col)\n        lines_out.append(' '.join(cols_out))\n    return lines_out","return_type":"list","function_name":"align_lines","stripped_code":"def align_lines(lines: list, column_separator: str='|'):\n    \"\"\"\n    Pads lines so that all rows in single column match. Columns separated by '|' in every line.\n    :param lines: list of lines\n    :param column_separator: column separator. default is '|'\n    :return: list of lines\n    \"\"\"\n    rows = []\n    col_len = []\n    for line in lines:\n        line = str(line)\n        cols = []\n        for col_index, col in enumerate(line.split(column_separator)):\n            col = str(col).strip()\n            cols.append(col)\n            if col_index >= len(col_len):\n                col_len.append(0)\n            col_len[col_index] = max(col_len[col_index], len(col))\n        rows.append(cols)\n\n    lines_out = []\n    for row in rows:\n        cols_out = []\n        for col_index, col in enumerate(row):\n            if col_index == 0:\n                col = col.ljust(col_len[col_index])\n            else:\n                col = col.rjust(col_len[col_index])\n            cols_out.append(col)\n        lines_out.append(' '.join(cols_out))\n    return lines_out"}
{"code":"def clear_database(self) -> None:\n        \"\"\"Remove all Entities and Components from the World.\"\"\"\n        self._next_entity_id = 0\n        self._dead_entities.clear()\n        self._components.clear()\n        self._entities.clear()\n        self.clear_cache()","return_type":"None","function_name":"World.clear_database","stripped_code":"def clear_database(self):\n        \"\"\"Remove all Entities and Components from the World.\"\"\"\n        self._next_entity_id = 0\n        self._dead_entities.clear()\n        self._components.clear()\n        self._entities.clear()\n        self.clear_cache()"}
{"code":"def checksum(source: bytes) -> int:\n    \"\"\"Calculates the checksum of the input bytes.\n\n    RFC1071: https://tools.ietf.org/html/rfc1071\n    RFC792: https://tools.ietf.org/html/rfc792\n\n    Args:\n        source: The input to be calculated.\n\n    Returns:\n        Calculated checksum.\n    \"\"\"\n    if len(source) % 2:  # if the total length is odd, padding with one octet of zeros for computing the checksum\n        source += b'\\x00'\n    sum = 0\n    for i in range(0, len(source), 2):\n        sum = ones_comp_sum16(sum, (source[i + 1] << 8) + source[i])\n    return ~sum & 0xffff","return_type":"int","function_name":"checksum","stripped_code":"def checksum(source: bytes):\n    \"\"\"Calculates the checksum of the input bytes.\n\n    RFC1071: https://tools.ietf.org/html/rfc1071\n    RFC792: https://tools.ietf.org/html/rfc792\n\n    Args:\n        source: The input to be calculated.\n\n    Returns:\n        Calculated checksum.\n    \"\"\"\n    if len(source) % 2:  # if the total length is odd, padding with one octet of zeros for computing the checksum\n        source += b'\\x00'\n    sum = 0\n    for i in range(0, len(source), 2):\n        sum = ones_comp_sum16(sum, (source[i + 1] << 8) + source[i])\n    return ~sum & 0xffff"}
{"code":"def clear() -> None:\n        \"\"\"\n        Clear all archivable caches in memory.\n        \"\"\"\n\n        LOGGER.debug('clear >>>')\n\n        with SCHEMA_CACHE.lock:\n            SCHEMA_CACHE.clear()\n        with CRED_DEF_CACHE.lock:\n            CRED_DEF_CACHE.clear()\n        with REVO_CACHE.lock:\n            REVO_CACHE.clear()\n\n        LOGGER.debug('clear <<<')","return_type":"None","function_name":"ArchivableCaches.clear","stripped_code":"def clear():\n        \"\"\"\n        Clear all archivable caches in memory.\n        \"\"\"\n\n        LOGGER.debug('clear >>>')\n\n        with SCHEMA_CACHE.lock:\n            SCHEMA_CACHE.clear()\n        with CRED_DEF_CACHE.lock:\n            CRED_DEF_CACHE.clear()\n        with REVO_CACHE.lock:\n            REVO_CACHE.clear()\n\n        LOGGER.debug('clear <<<')"}
{"code":"def wait_for_close(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        channel_ids: List[ChannelID],\n        retry_timeout: float,\n) -> None:\n    \"\"\"Wait until all channels are closed.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    return wait_for_channel_in_states(\n        raiden=raiden,\n        payment_network_id=payment_network_id,\n        token_address=token_address,\n        channel_ids=channel_ids,\n        retry_timeout=retry_timeout,\n        target_states=CHANNEL_AFTER_CLOSE_STATES,\n    )","return_type":"None","function_name":"wait_for_close","stripped_code":"def wait_for_close(\n        raiden: 'RaidenService',\n        payment_network_id: PaymentNetworkID,\n        token_address: TokenAddress,\n        channel_ids: List[ChannelID],\n        retry_timeout: float,\n):\n    \"\"\"Wait until all channels are closed.\n\n    Note:\n        This does not time out, use gevent.Timeout.\n    \"\"\"\n    return wait_for_channel_in_states(\n        raiden=raiden,\n        payment_network_id=payment_network_id,\n        token_address=token_address,\n        channel_ids=channel_ids,\n        retry_timeout=retry_timeout,\n        target_states=CHANNEL_AFTER_CLOSE_STATES,\n    )"}
{"code":"def RightPressMouse(x: int, y: int, waitTime: float = OPERATION_WAIT_TIME) -> None:\n    \"\"\"\n    Press right mouse.\n    x: int.\n    y: int.\n    waitTime: float.\n    \"\"\"\n    SetCursorPos(x, y)\n    screenWidth, screenHeight = GetScreenSize()\n    mouse_event(MouseEventFlag.RightDown | MouseEventFlag.Absolute, x * 65535 // screenWidth, y * 65535 // screenHeight, 0, 0)\n    time.sleep(waitTime)","return_type":"None","function_name":"RightPressMouse","stripped_code":"def RightPressMouse(x: int, y: int, waitTime: float = OPERATION_WAIT_TIME):\n    \"\"\"\n    Press right mouse.\n    x: int.\n    y: int.\n    waitTime: float.\n    \"\"\"\n    SetCursorPos(x, y)\n    screenWidth, screenHeight = GetScreenSize()\n    mouse_event(MouseEventFlag.RightDown | MouseEventFlag.Absolute, x * 65535 // screenWidth, y * 65535 // screenHeight, 0, 0)\n    time.sleep(waitTime)"}
{"code":"def opened(self, block_identifier: BlockSpecification) -> bool:\n        \"\"\" Returns if the channel is opened. \"\"\"\n        return self.token_network.channel_is_opened(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )","return_type":"bool","function_name":"PaymentChannel.opened","stripped_code":"def opened(self, block_identifier: BlockSpecification):\n        \"\"\" Returns if the channel is opened. \"\"\"\n        return self.token_network.channel_is_opened(\n            participant1=self.participant1,\n            participant2=self.participant2,\n            block_identifier=block_identifier,\n            channel_identifier=self.channel_identifier,\n        )"}
{"code":"def with_category(category: str) -> Callable:\n    \"\"\"A decorator to apply a category to a command function.\"\"\"\n    def cat_decorator(func):\n        categorize(func, category)\n        return func\n    return cat_decorator","return_type":"Callable","function_name":"with_category","stripped_code":"def with_category(category: str):\n    \"\"\"A decorator to apply a category to a command function.\"\"\"\n    def cat_decorator(func):\n        categorize(func, category)\n        return func\n    return cat_decorator"}
{"code":"def loadmask(self, filename: str) -> np.ndarray:\n        \"\"\"Load a mask file.\"\"\"\n        mask = scipy.io.loadmat(self.find_file(filename, what='mask'))\n        maskkey = [k for k in mask.keys() if not (k.startswith('_') or k.endswith('_'))][0]\n        return mask[maskkey].astype(np.bool)","return_type":"np.ndarray","function_name":"Loader.loadmask","stripped_code":"def loadmask(self, filename: str):\n        \"\"\"Load a mask file.\"\"\"\n        mask = scipy.io.loadmat(self.find_file(filename, what='mask'))\n        maskkey = [k for k in mask.keys() if not (k.startswith('_') or k.endswith('_'))][0]\n        return mask[maskkey].astype(np.bool)"}
{"code":"def reg(self, data: ['SASdata', str] = None,\n            add: str = None,\n            by: str = None,\n            code: str = None,\n            id: str = None,\n            lsmeans: str = None,\n            model: str = None,\n            out: [str, bool, 'SASdata'] = None,\n            random: str = None,\n            repeated: str = None,\n            slice: str = None,\n            test: str = None,\n            var: str = None,\n            weight: str = None,\n            procopts: str = None,\n            stmtpassthrough: str = None,\n            **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the REG procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_reg_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm add: The add variable can only be a string type.\n        :parm by: The by variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm lsmeans: The lsmeans variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm random: The random variable can only be a string type.\n        :parm repeated: The repeated variable can only be a string type.\n        :parm slice: The slice variable can only be a string type.\n        :parm test: The test variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"","return_type":"'SASresults'","function_name":"SASstat.reg","stripped_code":"def reg(self, data: ['SASdata', str] = None,\n            add: str = None,\n            by: str = None,\n            code: str = None,\n            id: str = None,\n            lsmeans: str = None,\n            model: str = None,\n            out: [str, bool, 'SASdata'] = None,\n            random: str = None,\n            repeated: str = None,\n            slice: str = None,\n            test: str = None,\n            var: str = None,\n            weight: str = None,\n            procopts: str = None,\n            stmtpassthrough: str = None,\n            **kwargs: dict):\n        \"\"\"\n        Python method to call the REG procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=statug&docsetTarget=statug_reg_syntax.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm add: The add variable can only be a string type.\n        :parm by: The by variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm lsmeans: The lsmeans variable can only be a string type.\n        :parm model: The model variable can only be a string type.\n        :parm out: The out variable can be a string, boolean or SASdata type. The member name for a boolean is \"_output\".\n        :parm random: The random variable can only be a string type.\n        :parm repeated: The repeated variable can only be a string type.\n        :parm slice: The slice variable can only be a string type.\n        :parm test: The test variable can only be a string type.\n        :parm var: The var variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\""}
{"code":"def _execute(self) -> None:\n        '''Run Fixture actions (setup, run, check).\n\n        Core test loop for Fixture.  Executes setup, run, and check in order.\n\n        '''\n\n        if hasattr(self, '_last_resolver_exception'):\n            logger.warning('last exception from %s.%s:', self.__class__.__name__, self._last_resolver_exception[0], exc_info = self._last_resolver_exception[1])\n\n        self.setup()\n        self.run()\n        self.check()","return_type":"None","function_name":"Fixture._execute","stripped_code":"def _execute(self):\n        '''Run Fixture actions (setup, run, check).\n\n        Core test loop for Fixture.  Executes setup, run, and check in order.\n\n        '''\n\n        if hasattr(self, '_last_resolver_exception'):\n            logger.warning('last exception from %s.%s:', self.__class__.__name__, self._last_resolver_exception[0], exc_info = self._last_resolver_exception[1])\n\n        self.setup()\n        self.run()\n        self.check()"}
{"code":"def postloop(self) -> None:\n        \"\"\"Hook method executed once when the cmdloop() method is about to return.\"\"\"\n        code = self.exit_code if self.exit_code is not None else 0\n        self.poutput('{!r} exiting with code: {}'.format(sys.argv[0], code))","return_type":"None","function_name":"ReplWithExitCode.postloop","stripped_code":"def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to return.\"\"\"\n        code = self.exit_code if self.exit_code is not None else 0\n        self.poutput('{!r} exiting with code: {}'.format(sys.argv[0], code))"}
{"code":"def is_causal_sink(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true if the node is a causal sink.\n\n    - Does have causal in edge(s)\n    - Doesn't have any causal out edge(s)\n    \"\"\"\n    return has_causal_in_edges(graph, node) and not has_causal_out_edges(graph, node)","return_type":"bool","function_name":"is_causal_sink","stripped_code":"def is_causal_sink(graph: BELGraph, node: BaseEntity):\n    \"\"\"Return true if the node is a causal sink.\n\n    - Does have causal in edge(s)\n    - Doesn't have any causal out edge(s)\n    \"\"\"\n    return has_causal_in_edges(graph, node) and not has_causal_out_edges(graph, node)"}
{"code":"def create_data_item(self, title: str=None) -> DataItem:\n        \"\"\"Create an empty data item in the library.\n\n        :param title: The title of the data item (optional).\n        :return: The new :py:class:`nion.swift.Facade.DataItem` object.\n        :rtype: :py:class:`nion.swift.Facade.DataItem`\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        data_item = DataItemModule.DataItem()\n        data_item.ensure_data_source()\n        if title is not None:\n            data_item.title = title\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)","return_type":"DataItem","function_name":"Library.create_data_item","stripped_code":"def create_data_item(self, title: str=None):\n        \"\"\"Create an empty data item in the library.\n\n        :param title: The title of the data item (optional).\n        :return: The new :py:class:`nion.swift.Facade.DataItem` object.\n        :rtype: :py:class:`nion.swift.Facade.DataItem`\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        data_item = DataItemModule.DataItem()\n        data_item.ensure_data_source()\n        if title is not None:\n            data_item.title = title\n        self.__document_model.append_data_item(data_item)\n        return DataItem(data_item)"}
{"code":"def function_signature_for_name_and_inputs(name: str, inputs: Sequence[Mapping[str, Any]]) -> str:\n        \"\"\"Returns the function signature for the specified name and Solidity JSON metadata inputs array.\n\n        The ABI specification defines the function signature as the function name followed by the parenthesised list of\n        parameter types separated by single commas and no spaces.\n        See https://solidity.readthedocs.io/en/latest/abi-spec.html#function-selector\n        \"\"\"\n        return name + SolidityMetadata.tuple_signature_for_components(inputs)","return_type":"str","function_name":"SolidityMetadata.function_signature_for_name_and_inputs","stripped_code":"def function_signature_for_name_and_inputs(name: str, inputs: Sequence[Mapping[str, Any]]):\n        \"\"\"Returns the function signature for the specified name and Solidity JSON metadata inputs array.\n\n        The ABI specification defines the function signature as the function name followed by the parenthesised list of\n        parameter types separated by single commas and no spaces.\n        See https://solidity.readthedocs.io/en/latest/abi-spec.html#function-selector\n        \"\"\"\n        return name + SolidityMetadata.tuple_signature_for_components(inputs)"}
{"code":"def write(self)->None:\n        \"Writes single model graph to Tensorboard.\"\n        self.tbwriter.add_graph(model=self.model, input_to_model=self.input_to_model)","return_type":"None","function_name":"GraphTBRequest.write","stripped_code":"def write(self):\n        \"Writes single model graph to Tensorboard.\"\n        self.tbwriter.add_graph(model=self.model, input_to_model=self.input_to_model)"}
{"code":"def refresh_index(meta, index) -> None:\n    \"\"\"Recalculate the projection, hash_key, and range_key for the given index.\n\n    :param meta: model.Meta to find columns by name\n    :param index: The index to refresh\n    \"\"\"\n    # All projections include model + index keys\n    projection_keys = set.union(meta.keys, index.keys)\n\n    proj = index.projection\n    mode = proj[\"mode\"]\n\n    if mode == \"keys\":\n        proj[\"included\"] = projection_keys\n    elif mode == \"all\":\n        proj[\"included\"] = meta.columns\n    elif mode == \"include\":  # pragma: no branch\n        if all(isinstance(p, str) for p in proj[\"included\"]):\n            proj[\"included\"] = set(meta.columns_by_name[n] for n in proj[\"included\"])\n        else:\n            proj[\"included\"] = set(proj[\"included\"])\n        proj[\"included\"].update(projection_keys)\n\n    if proj[\"strict\"]:\n        proj[\"available\"] = proj[\"included\"]\n    else:\n        proj[\"available\"] = meta.columns","return_type":"None","function_name":"refresh_index","stripped_code":"def refresh_index(meta, index):\n    \"\"\"Recalculate the projection, hash_key, and range_key for the given index.\n\n    :param meta: model.Meta to find columns by name\n    :param index: The index to refresh\n    \"\"\"\n    # All projections include model + index keys\n    projection_keys = set.union(meta.keys, index.keys)\n\n    proj = index.projection\n    mode = proj[\"mode\"]\n\n    if mode == \"keys\":\n        proj[\"included\"] = projection_keys\n    elif mode == \"all\":\n        proj[\"included\"] = meta.columns\n    elif mode == \"include\":  # pragma: no branch\n        if all(isinstance(p, str) for p in proj[\"included\"]):\n            proj[\"included\"] = set(meta.columns_by_name[n] for n in proj[\"included\"])\n        else:\n            proj[\"included\"] = set(proj[\"included\"])\n        proj[\"included\"].update(projection_keys)\n\n    if proj[\"strict\"]:\n        proj[\"available\"] = proj[\"included\"]\n    else:\n        proj[\"available\"] = meta.columns"}
{"code":"def is_terminal(self) -> bool:\n        \"\"\"True if the response contained an untagged ``BYE`` response\n        indicating that the session should be terminated.\n\n        \"\"\"\n        for resp in self._untagged:\n            if resp.is_terminal:\n                return True\n        return False","return_type":"bool","function_name":"Response.is_terminal","stripped_code":"def is_terminal(self):\n        \"\"\"True if the response contained an untagged ``BYE`` response\n        indicating that the session should be terminated.\n\n        \"\"\"\n        for resp in self._untagged:\n            if resp.is_terminal:\n                return True\n        return False"}
{"code":"def gen_df_save(df_grid_group: pd.DataFrame)->pd.DataFrame:\n    '''generate a dataframe for saving\n\n    Parameters\n    ----------\n    df_output_grid_group : pd.DataFrame\n        an output dataframe of a single group and grid\n\n    Returns\n    -------\n    pd.DataFrame\n        a dataframe with date time info prepended for saving\n    '''\n    # generate df_datetime for prepending\n    idx_dt = df_grid_group.index\n    ser_year = pd.Series(idx_dt.year, index=idx_dt, name='Year')\n    ser_DOY = pd.Series(idx_dt.dayofyear, index=idx_dt, name='DOY')\n    ser_hour = pd.Series(idx_dt.hour, index=idx_dt, name='Hour')\n    ser_min = pd.Series(idx_dt.minute, index=idx_dt, name='Min')\n    df_datetime = pd.concat([\n        ser_year,\n        ser_DOY,\n        ser_hour,\n        ser_min,\n    ], axis=1)\n    df_datetime['Dectime'] = ser_DOY-1+idx_dt.to_perioddelta(\n        'd').total_seconds()/(24*60*60)\n    df_save = pd.concat([df_datetime, df_grid_group], axis=1)\n    return df_save","return_type":"pd.DataFrame","function_name":"gen_df_save","stripped_code":"def gen_df_save(df_grid_group: pd.DataFrame):\n    '''generate a dataframe for saving\n\n    Parameters\n    ----------\n    df_output_grid_group : pd.DataFrame\n        an output dataframe of a single group and grid\n\n    Returns\n    -------\n    pd.DataFrame\n        a dataframe with date time info prepended for saving\n    '''\n    # generate df_datetime for prepending\n    idx_dt = df_grid_group.index\n    ser_year = pd.Series(idx_dt.year, index=idx_dt, name='Year')\n    ser_DOY = pd.Series(idx_dt.dayofyear, index=idx_dt, name='DOY')\n    ser_hour = pd.Series(idx_dt.hour, index=idx_dt, name='Hour')\n    ser_min = pd.Series(idx_dt.minute, index=idx_dt, name='Min')\n    df_datetime = pd.concat([\n        ser_year,\n        ser_DOY,\n        ser_hour,\n        ser_min,\n    ], axis=1)\n    df_datetime['Dectime'] = ser_DOY-1+idx_dt.to_perioddelta(\n        'd').total_seconds()/(24*60*60)\n    df_save = pd.concat([df_datetime, df_grid_group], axis=1)\n    return df_save"}
{"code":"def last_modified(self, name: str = None) -> str:\n\t\t\"\"\"\n\t\tReturn a compact ISO8601 timestamp (UTC timezone) indicating when the layer was last modified\n\n\t\tNote: if name is None, the modification time of the most recently modified layer is returned\n\t\t\"\"\"\n\t\tif name is not None:\n\t\t\treturn self[name].last_modified()\n\t\tts = \"\"\n\t\tfor name in self.keys():\n\t\t\tif ts is None:\n\t\t\t\tts = self[name].last_modified()\n\t\t\telse:\n\t\t\t\tif self[name].last_modified() > ts:\n\t\t\t\t\tts = self[name].last_modified()\n\t\treturn ts","return_type":"str","function_name":"LayerManager.last_modified","stripped_code":"def last_modified(self, name: str = None):\n\t\t\"\"\"\n\t\tReturn a compact ISO8601 timestamp (UTC timezone) indicating when the layer was last modified\n\n\t\tNote: if name is None, the modification time of the most recently modified layer is returned\n\t\t\"\"\"\n\t\tif name is not None:\n\t\t\treturn self[name].last_modified()\n\t\tts = \"\"\n\t\tfor name in self.keys():\n\t\t\tif ts is None:\n\t\t\t\tts = self[name].last_modified()\n\t\t\telse:\n\t\t\t\tif self[name].last_modified() > ts:\n\t\t\t\t\tts = self[name].last_modified()\n\t\treturn ts"}
{"code":"def metainfo_to_dict(self) -> dict:\n        '''encode deck into dictionary'''\n\n        r = {\n            \"version\": self.version,\n            \"name\": self.name,\n            \"number_of_decimals\": self.number_of_decimals,\n            \"issue_mode\": self.issue_mode\n        }\n\n        if self.asset_specific_data:\n            r.update({'asset_specific_data': self.asset_specific_data})\n\n        return r","return_type":"dict","function_name":"Deck.metainfo_to_dict","stripped_code":"def metainfo_to_dict(self):\n        '''encode deck into dictionary'''\n\n        r = {\n            \"version\": self.version,\n            \"name\": self.name,\n            \"number_of_decimals\": self.number_of_decimals,\n            \"issue_mode\": self.issue_mode\n        }\n\n        if self.asset_specific_data:\n            r.update({'asset_specific_data': self.asset_specific_data})\n\n        return r"}
{"code":"def handle_dataframe(\n        df: pd.DataFrame,\n        entrez_id_name,\n        log2_fold_change_name,\n        adjusted_p_value_name,\n        entrez_delimiter,\n        base_mean=None,\n) -> List[Gene]:\n    \"\"\"Convert data frame on differential expression values as Gene objects.\n\n    :param df: Data frame with columns showing values on differential\n    expression.\n    :param cfp: An object that includes paths, cutoffs and other information.\n    :return list: A list of Gene objects.\n    \"\"\"\n    logger.info(\"In _handle_df()\")\n\n    if base_mean is not None and base_mean in df.columns:\n        df = df[pd.notnull(df[base_mean])]\n\n    df = df[pd.notnull(df[entrez_id_name])]\n    df = df[pd.notnull(df[log2_fold_change_name])]\n    df = df[pd.notnull(df[adjusted_p_value_name])]\n\n    # try:\n    #     import bio2bel_hgnc\n    # except ImportError:\n    #     logger.debug('skipping mapping')\n    # else:\n    #     manager = bio2bel_hgnc.Manager()\n    #     # TODO @cthoyt\n\n    return [\n        Gene(\n            entrez_id=entrez_id,\n            log2_fold_change=data[log2_fold_change_name],\n            padj=data[adjusted_p_value_name]\n        )\n        for _, data in df.iterrows()\n        for entrez_id in str(data[entrez_id_name]).split(entrez_delimiter)\n    ]","return_type":"List[Gene]","function_name":"handle_dataframe","stripped_code":"def handle_dataframe(\n        df: pd.DataFrame,\n        entrez_id_name,\n        log2_fold_change_name,\n        adjusted_p_value_name,\n        entrez_delimiter,\n        base_mean=None,\n):\n    \"\"\"Convert data frame on differential expression values as Gene objects.\n\n    :param df: Data frame with columns showing values on differential\n    expression.\n    :param cfp: An object that includes paths, cutoffs and other information.\n    :return list: A list of Gene objects.\n    \"\"\"\n    logger.info(\"In _handle_df()\")\n\n    if base_mean is not None and base_mean in df.columns:\n        df = df[pd.notnull(df[base_mean])]\n\n    df = df[pd.notnull(df[entrez_id_name])]\n    df = df[pd.notnull(df[log2_fold_change_name])]\n    df = df[pd.notnull(df[adjusted_p_value_name])]\n\n    # try:\n    #     import bio2bel_hgnc\n    # except ImportError:\n    #     logger.debug('skipping mapping')\n    # else:\n    #     manager = bio2bel_hgnc.Manager()\n    #     # TODO @cthoyt\n\n    return [\n        Gene(\n            entrez_id=entrez_id,\n            log2_fold_change=data[log2_fold_change_name],\n            padj=data[adjusted_p_value_name]\n        )\n        for _, data in df.iterrows()\n        for entrez_id in str(data[entrez_id_name]).split(entrez_delimiter)\n    ]"}
{"code":"def variant_names(self, language=DEFAULT_LANGUAGE, min_score: int=75) -> list:\n        \"\"\"\n        Describe each of the variant parts of the language tag in a natural\n        language.\n        \"\"\"\n        names = []\n        for variant in self.variants:\n            var_names = code_to_names('variant', variant)\n            names.append(self._best_name(var_names, language, min_score))\n        return names","return_type":"list","function_name":"Language.variant_names","stripped_code":"def variant_names(self, language=DEFAULT_LANGUAGE, min_score: int=75):\n        \"\"\"\n        Describe each of the variant parts of the language tag in a natural\n        language.\n        \"\"\"\n        names = []\n        for variant in self.variants:\n            var_names = code_to_names('variant', variant)\n            names.append(self._best_name(var_names, language, min_score))\n        return names"}
{"code":"def strnumlist(prefix: str, numbers: List[int], suffix: str = \"\") -> List[str]:\n    \"\"\"\n    Makes a string of the format ``<prefix><number><suffix>`` for every number\n    in ``numbers``, and returns them as a list.\n    \"\"\"\n    return [\"{}{}{}\".format(prefix, num, suffix) for num in numbers]","return_type":"List[str]","function_name":"strnumlist","stripped_code":"def strnumlist(prefix: str, numbers: List[int], suffix: str = \"\"):\n    \"\"\"\n    Makes a string of the format ``<prefix><number><suffix>`` for every number\n    in ``numbers``, and returns them as a list.\n    \"\"\"\n    return [\"{}{}{}\".format(prefix, num, suffix) for num in numbers]"}
{"code":"def plot_fitspace(self, name: str, X: np.ndarray, y: np.ndarray, clf: object) -> None:\n        \"\"\" Plot 2dplane of fitspace \"\"\"\n        cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n        cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, m_max]x[y_min, y_max].\n        h = 0.01  # Mesh step size\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        plt.figure()\n        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n        # Plot also the training points\n        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.xlabel(r'$\\log_{10}$ Noise')\n        plt.ylabel(r'$\\log_{10}$ Curvature')\n        plt.savefig(name)","return_type":"None","function_name":"ML.plot_fitspace","stripped_code":"def plot_fitspace(self, name: str, X: np.ndarray, y: np.ndarray, clf: object):\n        \"\"\" Plot 2dplane of fitspace \"\"\"\n        cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n        cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, m_max]x[y_min, y_max].\n        h = 0.01  # Mesh step size\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        plt.figure()\n        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n        # Plot also the training points\n        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.xlabel(r'$\\log_{10}$ Noise')\n        plt.ylabel(r'$\\log_{10}$ Curvature')\n        plt.savefig(name)"}
{"code":"def insert_empty_columns(self, x: int, amount: int = 1) -> None:\n        \"\"\"Insert a number of columns after the given column.\"\"\"\n        def transform_columns(\n                column: Union[int, float],\n                row: Union[int, float]\n        ) -> Tuple[Union[int, float], Union[int, float]]:\n            return column + (amount if column >= x else 0), row\n        self._transform_coordinates(transform_columns)","return_type":"None","function_name":"TextDiagramDrawer.insert_empty_columns","stripped_code":"def insert_empty_columns(self, x: int, amount: int = 1):\n        \"\"\"Insert a number of columns after the given column.\"\"\"\n        def transform_columns(\n                column: Union[int, float],\n                row: Union[int, float]\n        ) -> Tuple[Union[int, float], Union[int, float]]:\n            return column + (amount if column >= x else 0), row\n        self._transform_coordinates(transform_columns)"}
{"code":"def contains_relevant_concept(\n    s: Influence, relevant_concepts: List[str], cutoff=0.7\n) -> bool:\n    \"\"\" Returns true if a given Influence statement has a relevant concept, and\n    false otherwise. \"\"\"\n\n    return any(\n        map(lambda c: contains_concept(s, c, cutoff=cutoff), relevant_concepts)\n    )","return_type":"bool","function_name":"contains_relevant_concept","stripped_code":"def contains_relevant_concept(\n    s: Influence, relevant_concepts: List[str], cutoff=0.7\n):\n    \"\"\" Returns true if a given Influence statement has a relevant concept, and\n    false otherwise. \"\"\"\n\n    return any(\n        map(lambda c: contains_concept(s, c, cutoff=cutoff), relevant_concepts)\n    )"}
{"code":"def is_valid_python(tkn: str) -> bool:\n    \"\"\"Determine whether tkn is a valid python identifier\n\n    :param tkn:\n    :return:\n    \"\"\"\n    try:\n        root = ast.parse(tkn)\n    except SyntaxError:\n        return False\n    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)","return_type":"bool","function_name":"is_valid_python","stripped_code":"def is_valid_python(tkn: str):\n    \"\"\"Determine whether tkn is a valid python identifier\n\n    :param tkn:\n    :return:\n    \"\"\"\n    try:\n        root = ast.parse(tkn)\n    except SyntaxError:\n        return False\n    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)"}
{"code":"def team_billableInfo(self, **kwargs) -> SlackResponse:\n        \"\"\"Gets billable users information for the current team.\"\"\"\n        self._validate_xoxp_token()\n        return self.api_call(\"team.billableInfo\", http_verb=\"GET\", params=kwargs)","return_type":"SlackResponse","function_name":"WebClient.team_billableInfo","stripped_code":"def team_billableInfo(self, **kwargs):\n        \"\"\"Gets billable users information for the current team.\"\"\"\n        self._validate_xoxp_token()\n        return self.api_call(\"team.billableInfo\", http_verb=\"GET\", params=kwargs)"}
{"code":"def logger(self, logger: typing.Union[logging.Logger, str, None]) -> None:\n        \"\"\"Logger instance to use as override.\"\"\"\n        if logger is None or isinstance(logger, logging.Logger):\n            self.__logger = logger\n        else:\n            self.__logger = logging.getLogger(logger)","return_type":"None","function_name":"LogOnAccess.logger","stripped_code":"def logger(self, logger: typing.Union[logging.Logger, str, None]):\n        \"\"\"Logger instance to use as override.\"\"\"\n        if logger is None or isinstance(logger, logging.Logger):\n            self.__logger = logger\n        else:\n            self.__logger = logging.getLogger(logger)"}
{"code":"def console_set_default_foreground(\n    con: tcod.console.Console, col: Tuple[int, int, int]\n) -> None:\n    \"\"\"Change the default foreground color for a console.\n\n    Args:\n        con (Console): Any Console instance.\n        col (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.default_fg` instead.\n    \"\"\"\n    lib.TCOD_console_set_default_foreground(_console(con), col)","return_type":"None","function_name":"console_set_default_foreground","stripped_code":"def console_set_default_foreground(\n    con: tcod.console.Console, col: Tuple[int, int, int]\n):\n    \"\"\"Change the default foreground color for a console.\n\n    Args:\n        con (Console): Any Console instance.\n        col (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.default_fg` instead.\n    \"\"\"\n    lib.TCOD_console_set_default_foreground(_console(con), col)"}
{"code":"def orthographic_u_umlaut(sound: str) -> str:\n        \"\"\"\n        >>> OldNorsePhonology.orthographic_u_umlaut(\"a\")\n        '\u00f6'\n        >>> OldNorsePhonology.orthographic_u_umlaut(\"e\")\n        'e'\n\n        :param sound:\n        :return:\n        \"\"\"\n        if sound in OldNorsePhonology.U_UMLAUT:\n            return OldNorsePhonology.U_UMLAUT[sound]\n        else:\n            return sound","return_type":"str","function_name":"OldNorsePhonology.orthographic_u_umlaut","stripped_code":"def orthographic_u_umlaut(sound: str):\n        \"\"\"\n        >>> OldNorsePhonology.orthographic_u_umlaut(\"a\")\n        '\u00f6'\n        >>> OldNorsePhonology.orthographic_u_umlaut(\"e\")\n        'e'\n\n        :param sound:\n        :return:\n        \"\"\"\n        if sound in OldNorsePhonology.U_UMLAUT:\n            return OldNorsePhonology.U_UMLAUT[sound]\n        else:\n            return sound"}
{"code":"def academic_degree(self) -> str:\n        \"\"\"Get a random academic degree.\n\n        :return: Degree.\n\n        :Example:\n            Bachelor.\n        \"\"\"\n        degrees = self._data['academic_degree']\n        return self.random.choice(degrees)","return_type":"str","function_name":"Person.academic_degree","stripped_code":"def academic_degree(self):\n        \"\"\"Get a random academic degree.\n\n        :return: Degree.\n\n        :Example:\n            Bachelor.\n        \"\"\"\n        degrees = self._data['academic_degree']\n        return self.random.choice(degrees)"}
{"code":"def from_randomness(randomness: RandomnessPrimitive) -> ulid.ULID:\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance using the given randomness value of a supported type.\n\n    The following types are supported for randomness values:\n\n    * :class:`~int`\n    * :class:`~float`\n    * :class:`~str`\n    * :class:`~memoryview`\n    * :class:`~ulid.ulid.Randomness`\n    * :class:`~ulid.ulid.ULID`\n    * :class:`~bytes`\n    * :class:`~bytearray`\n\n    :param randomness: Random bytes\n    :type randomness: See docstring for types\n    :return: ULID using new timestamp and given randomness\n    :rtype: :class:`~ulid.ulid.ULID`\n    :raises ValueError: when the value is an unsupported type\n    :raises ValueError: when the value is a string and cannot be Base32 decoded\n    :raises ValueError: when the value is or was converted to something 80 bits\n    \"\"\"\n    if isinstance(randomness, (int, float)):\n        randomness = int(randomness).to_bytes(10, byteorder='big')\n    elif isinstance(randomness, str):\n        randomness = base32.decode_randomness(randomness)\n    elif isinstance(randomness, memoryview):\n        randomness = randomness.tobytes()\n    elif isinstance(randomness, ulid.Randomness):\n        randomness = randomness.bytes\n    elif isinstance(randomness, ulid.ULID):\n        randomness = randomness.randomness().bytes\n\n    if not isinstance(randomness, (bytes, bytearray)):\n        raise ValueError('Expected int, float, str, memoryview, Randomness, ULID, '\n                         'bytes, or bytearray; got {}'.format(type(randomness).__name__))\n\n    length = len(randomness)\n    if length != 10:\n        raise ValueError('Expects randomness to be 80 bits; got {} bytes'.format(length))\n\n    timestamp = int(time.time() * 1000).to_bytes(6, byteorder='big')\n    return ulid.ULID(timestamp + randomness)","return_type":"ulid.ULID","function_name":"from_randomness","stripped_code":"def from_randomness(randomness: RandomnessPrimitive):\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance using the given randomness value of a supported type.\n\n    The following types are supported for randomness values:\n\n    * :class:`~int`\n    * :class:`~float`\n    * :class:`~str`\n    * :class:`~memoryview`\n    * :class:`~ulid.ulid.Randomness`\n    * :class:`~ulid.ulid.ULID`\n    * :class:`~bytes`\n    * :class:`~bytearray`\n\n    :param randomness: Random bytes\n    :type randomness: See docstring for types\n    :return: ULID using new timestamp and given randomness\n    :rtype: :class:`~ulid.ulid.ULID`\n    :raises ValueError: when the value is an unsupported type\n    :raises ValueError: when the value is a string and cannot be Base32 decoded\n    :raises ValueError: when the value is or was converted to something 80 bits\n    \"\"\"\n    if isinstance(randomness, (int, float)):\n        randomness = int(randomness).to_bytes(10, byteorder='big')\n    elif isinstance(randomness, str):\n        randomness = base32.decode_randomness(randomness)\n    elif isinstance(randomness, memoryview):\n        randomness = randomness.tobytes()\n    elif isinstance(randomness, ulid.Randomness):\n        randomness = randomness.bytes\n    elif isinstance(randomness, ulid.ULID):\n        randomness = randomness.randomness().bytes\n\n    if not isinstance(randomness, (bytes, bytearray)):\n        raise ValueError('Expected int, float, str, memoryview, Randomness, ULID, '\n                         'bytes, or bytearray; got {}'.format(type(randomness).__name__))\n\n    length = len(randomness)\n    if length != 10:\n        raise ValueError('Expects randomness to be 80 bits; got {} bytes'.format(length))\n\n    timestamp = int(time.time() * 1000).to_bytes(6, byteorder='big')\n    return ulid.ULID(timestamp + randomness)"}
{"code":"def load_features(paths: List[str],\n                  expected_shape: Optional[tuple] = None) -> List[np.ndarray]:\n    \"\"\"\n    Load features specified with absolute paths.\n\n    :param paths: List of files specified with paths.\n    :param expected_shape: Optional expected shape.\n    :return: A list of loaded images (numpy arrays).\n    \"\"\"\n    data = []  # type: List[np.ndarray]\n    for path in paths:\n        data.append(load_feature(path, expected_shape))\n    return data","return_type":"List[np.ndarray]","function_name":"load_features","stripped_code":"def load_features(paths: List[str],\n                  expected_shape: Optional[tuple] = None):\n    \"\"\"\n    Load features specified with absolute paths.\n\n    :param paths: List of files specified with paths.\n    :param expected_shape: Optional expected shape.\n    :return: A list of loaded images (numpy arrays).\n    \"\"\"\n    data = []  # type: List[np.ndarray]\n    for path in paths:\n        data.append(load_feature(path, expected_shape))\n    return data"}
{"code":"def _raw_read(self, where: int, size=1) -> bytes:\n        \"\"\"\n        Selects bytes from memory. Attempts to do so faster than via read_bytes.\n\n        :param where: address to read from\n        :param size: number of bytes to read\n        :return: the bytes in memory\n        \"\"\"\n        map = self.memory.map_containing(where)\n        start = map._get_offset(where)\n        mapType = type(map)\n        if mapType is FileMap:\n            end = map._get_offset(where + size)\n\n            if end > map._mapped_size:\n                logger.warning(f\"Missing {end - map._mapped_size} bytes at the end of {map._filename}\")\n\n            raw_data = map._data[map._get_offset(where): min(end, map._mapped_size)]\n            if len(raw_data) < end:\n                raw_data += b'\\x00' * (end - len(raw_data))\n\n            data = b''\n            for offset in sorted(map._overlay.keys()):\n                data += raw_data[len(data):offset]\n                data += map._overlay[offset]\n            data += raw_data[len(data):]\n\n        elif mapType is AnonMap:\n            data = bytes(map._data[start:start + size])\n        else:\n            data = b''.join(self.memory[where:where + size])\n        assert len(data) == size, 'Raw read resulted in wrong data read which should never happen'\n        return data","return_type":"bytes","function_name":"Cpu._raw_read","stripped_code":"def _raw_read(self, where: int, size=1):\n        \"\"\"\n        Selects bytes from memory. Attempts to do so faster than via read_bytes.\n\n        :param where: address to read from\n        :param size: number of bytes to read\n        :return: the bytes in memory\n        \"\"\"\n        map = self.memory.map_containing(where)\n        start = map._get_offset(where)\n        mapType = type(map)\n        if mapType is FileMap:\n            end = map._get_offset(where + size)\n\n            if end > map._mapped_size:\n                logger.warning(f\"Missing {end - map._mapped_size} bytes at the end of {map._filename}\")\n\n            raw_data = map._data[map._get_offset(where): min(end, map._mapped_size)]\n            if len(raw_data) < end:\n                raw_data += b'\\x00' * (end - len(raw_data))\n\n            data = b''\n            for offset in sorted(map._overlay.keys()):\n                data += raw_data[len(data):offset]\n                data += map._overlay[offset]\n            data += raw_data[len(data):]\n\n        elif mapType is AnonMap:\n            data = bytes(map._data[start:start + size])\n        else:\n            data = b''.join(self.memory[where:where + size])\n        assert len(data) == size, 'Raw read resulted in wrong data read which should never happen'\n        return data"}
{"code":"def noise_get(\n    n: tcod.noise.Noise, f: Sequence[float], typ: int = NOISE_DEFAULT\n) -> float:\n    \"\"\"Return the noise value sampled from the ``f`` coordinate.\n\n    ``f`` should be a tuple or list with a length matching\n    :any:`Noise.dimensions`.\n    If ``f`` is shoerter than :any:`Noise.dimensions` the missing coordinates\n    will be filled with zeros.\n\n    Args:\n        n (Noise): A Noise instance.\n        f (Sequence[float]): The point to sample the noise from.\n        typ (int): The noise algorithm to use.\n\n    Returns:\n        float: The sampled noise value.\n    \"\"\"\n    return float(lib.TCOD_noise_get_ex(n.noise_c, ffi.new(\"float[4]\", f), typ))","return_type":"float","function_name":"noise_get","stripped_code":"def noise_get(\n    n: tcod.noise.Noise, f: Sequence[float], typ: int = NOISE_DEFAULT\n):\n    \"\"\"Return the noise value sampled from the ``f`` coordinate.\n\n    ``f`` should be a tuple or list with a length matching\n    :any:`Noise.dimensions`.\n    If ``f`` is shoerter than :any:`Noise.dimensions` the missing coordinates\n    will be filled with zeros.\n\n    Args:\n        n (Noise): A Noise instance.\n        f (Sequence[float]): The point to sample the noise from.\n        typ (int): The noise algorithm to use.\n\n    Returns:\n        float: The sampled noise value.\n    \"\"\"\n    return float(lib.TCOD_noise_get_ex(n.noise_c, ffi.new(\"float[4]\", f), typ))"}
{"code":"def asgate(self) -> Gate:\n        \"\"\"Return one of the composite Kraus operators at random with\n        the appropriate weights\"\"\"\n        return np.random.choice(self.operators, p=self.weights)","return_type":"Gate","function_name":"UnitaryMixture.asgate","stripped_code":"def asgate(self):\n        \"\"\"Return one of the composite Kraus operators at random with\n        the appropriate weights\"\"\"\n        return np.random.choice(self.operators, p=self.weights)"}
{"code":"def rollback(self) -> None:\n        \"\"\"\n        Roll back to previous database state. However stay inside transaction\n        management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"rollback called outside transaction\")\n\n        _debug(\"rollback:\", self._transactions[-1])\n        # if something goes wrong here, nothing we can do about it, leave\n        # database as is.\n        try:\n            # for every rollback action ...\n            for on_rollback in self._transactions[-1]:\n                # execute it\n                _debug(\"--> rolling back\", on_rollback)\n                self._do_with_retry(on_rollback)\n        except:  # noqa: E722\n            _debug(\"--> rollback failed\")\n            exc_class, exc, tb = sys.exc_info()\n            raise tldap.exceptions.RollbackError(\n                \"FATAL Unrecoverable rollback error: %r\" % exc)\n        finally:\n            # reset everything to clean state\n            _debug(\"--> rollback success\")\n            self.reset()","return_type":"None","function_name":"LDAPwrapper.rollback","stripped_code":"def rollback(self):\n        \"\"\"\n        Roll back to previous database state. However stay inside transaction\n        management.\n        \"\"\"\n        if len(self._transactions) == 0:\n            raise RuntimeError(\"rollback called outside transaction\")\n\n        _debug(\"rollback:\", self._transactions[-1])\n        # if something goes wrong here, nothing we can do about it, leave\n        # database as is.\n        try:\n            # for every rollback action ...\n            for on_rollback in self._transactions[-1]:\n                # execute it\n                _debug(\"--> rolling back\", on_rollback)\n                self._do_with_retry(on_rollback)\n        except:  # noqa: E722\n            _debug(\"--> rollback failed\")\n            exc_class, exc, tb = sys.exc_info()\n            raise tldap.exceptions.RollbackError(\n                \"FATAL Unrecoverable rollback error: %r\" % exc)\n        finally:\n            # reset everything to clean state\n            _debug(\"--> rollback success\")\n            self.reset()"}
{"code":"def addToProcessedTxns(self,\n                           identifier: str,\n                           txnId: str,\n                           reply: Reply) -> None:\n        \"\"\"\n        Add a client request to the transaction store's list of processed\n        requests.\n        \"\"\"\n        self.transactions[txnId] = reply\n        if identifier not in self.processedRequests:\n            self.processedRequests[identifier] = {}\n        self.processedRequests[identifier][reply.reqId] = txnId","return_type":"None","function_name":"TransactionStore.addToProcessedTxns","stripped_code":"def addToProcessedTxns(self,\n                           identifier: str,\n                           txnId: str,\n                           reply: Reply):\n        \"\"\"\n        Add a client request to the transaction store's list of processed\n        requests.\n        \"\"\"\n        self.transactions[txnId] = reply\n        if identifier not in self.processedRequests:\n            self.processedRequests[identifier] = {}\n        self.processedRequests[identifier][reply.reqId] = txnId"}
{"code":"def _get_covered_instructions(self) -> int:\n        \"\"\"Gets the total number of covered instructions for all accounts in\n        the svm.\n        :return:\n        \"\"\"\n        total_covered_instructions = 0\n        for _, cv in self.coverage.items():\n            total_covered_instructions += sum(cv[1])\n        return total_covered_instructions","return_type":"int","function_name":"InstructionCoveragePlugin._get_covered_instructions","stripped_code":"def _get_covered_instructions(self):\n        \"\"\"Gets the total number of covered instructions for all accounts in\n        the svm.\n        :return:\n        \"\"\"\n        total_covered_instructions = 0\n        for _, cv in self.coverage.items():\n            total_covered_instructions += sum(cv[1])\n        return total_covered_instructions"}
{"code":"def getblock(self, blockhash: str) -> dict:\n        '''query block using <blockhash> as key.'''\n\n        query = 'block.raw.dws?coin={net}&hash={blockhash}'.format(\n            net=self.format_name(self.net),\n            blockhash=blockhash,\n        )\n        return cast(dict, self.get_url(self.explorer_url + query))","return_type":"dict","function_name":"Cryptoid.getblock","stripped_code":"def getblock(self, blockhash: str):\n        '''query block using <blockhash> as key.'''\n\n        query = 'block.raw.dws?coin={net}&hash={blockhash}'.format(\n            net=self.format_name(self.net),\n            blockhash=blockhash,\n        )\n        return cast(dict, self.get_url(self.explorer_url + query))"}
{"code":"def dirac_notation(state: Sequence, decimals: int=2) -> str:\n    \"\"\"Returns the wavefunction as a string in Dirac notation.\n\n    For example:\n\n        state = np.array([1/np.sqrt(2), 1/np.sqrt(2)], dtype=np.complex64)\n        print(dirac_notation(state)) -> 0.71|0\u27e9 + 0.71|1\u27e9\n\n    Args:\n        state: A sequence representing a wave function in which the ordering\n            mapping to qubits follows the standard Kronecker convention of\n            numpy.kron.\n        decimals: How many decimals to include in the pretty print.\n\n    Returns:\n        A pretty string consisting of a sum of computational basis kets\n        and non-zero floats of the specified accuracy.\n    \"\"\"\n    perm_list = [\"\".join(seq) for seq in itertools.product(\n        \"01\", repeat=int(len(state)).bit_length() - 1)]\n    components = []\n    ket = \"|{}\u27e9\"\n    for x in range(len(perm_list)):\n        format_str = \"({:.\" + str(decimals) + \"g})\"\n        # Python 2 rounds imaginary numbers to 0, so need to round separately.\n        val = (round(state[x].real, decimals)\n               + 1j * round(state[x].imag, decimals))\n        if round(val.real, decimals) == 0 and round(val.imag, decimals) != 0:\n            val = val.imag\n            format_str = \"{:.\" + str(decimals) + \"g}j\"\n        elif round(val.imag, decimals) == 0 and round(val.real, decimals) != 0:\n            val = val.real\n            format_str = \"{:.\" + str(decimals) + \"g}\"\n        if val != 0:\n            if round(state[x], decimals) == 1:\n                components.append(ket.format(perm_list[x]))\n            else:\n                components.append((format_str + ket).format(val, perm_list[x]))\n    if not components:\n        return '0'\n\n    return ' + '.join(components).replace(' + -', ' - ')","return_type":"str","function_name":"dirac_notation","stripped_code":"def dirac_notation(state: Sequence, decimals: int=2):\n    \"\"\"Returns the wavefunction as a string in Dirac notation.\n\n    For example:\n\n        state = np.array([1/np.sqrt(2), 1/np.sqrt(2)], dtype=np.complex64)\n        print(dirac_notation(state)) -> 0.71|0\u27e9 + 0.71|1\u27e9\n\n    Args:\n        state: A sequence representing a wave function in which the ordering\n            mapping to qubits follows the standard Kronecker convention of\n            numpy.kron.\n        decimals: How many decimals to include in the pretty print.\n\n    Returns:\n        A pretty string consisting of a sum of computational basis kets\n        and non-zero floats of the specified accuracy.\n    \"\"\"\n    perm_list = [\"\".join(seq) for seq in itertools.product(\n        \"01\", repeat=int(len(state)).bit_length() - 1)]\n    components = []\n    ket = \"|{}\u27e9\"\n    for x in range(len(perm_list)):\n        format_str = \"({:.\" + str(decimals) + \"g})\"\n        # Python 2 rounds imaginary numbers to 0, so need to round separately.\n        val = (round(state[x].real, decimals)\n               + 1j * round(state[x].imag, decimals))\n        if round(val.real, decimals) == 0 and round(val.imag, decimals) != 0:\n            val = val.imag\n            format_str = \"{:.\" + str(decimals) + \"g}j\"\n        elif round(val.imag, decimals) == 0 and round(val.real, decimals) != 0:\n            val = val.real\n            format_str = \"{:.\" + str(decimals) + \"g}\"\n        if val != 0:\n            if round(state[x], decimals) == 1:\n                components.append(ket.format(perm_list[x]))\n            else:\n                components.append((format_str + ket).format(val, perm_list[x]))\n    if not components:\n        return '0'\n\n    return ' + '.join(components).replace(' + -', ' - ')"}
{"code":"def sftp(\n    task: Task, src: str, dst: str, action: str, dry_run: Optional[bool] = None\n) -> Result:\n    \"\"\"\n    Transfer files from/to the device using sftp protocol\n\n    Example::\n\n        nornir.run(files.sftp,\n                    action=\"put\",\n                    src=\"README.md\",\n                    dst=\"/tmp/README.md\")\n\n    Arguments:\n        dry_run: Whether to apply changes or not\n        src: source file\n        dst: destination\n        action: ``put``, ``get``.\n\n    Returns:\n        Result object with the following attributes set:\n          * changed (``bool``):\n          * files_changed (``list``): list of files that changed\n    \"\"\"\n    dry_run = task.is_dry_run(dry_run)\n    actions = {\"put\": put, \"get\": get}\n    client = task.host.get_connection(\"paramiko\", task.nornir.config)\n    scp_client = SCPClient(client.get_transport())\n    sftp_client = paramiko.SFTPClient.from_transport(client.get_transport())\n    files_changed = actions[action](task, scp_client, sftp_client, src, dst, dry_run)\n    return Result(\n        host=task.host, changed=bool(files_changed), files_changed=files_changed\n    )","return_type":"Result","function_name":"sftp","stripped_code":"def sftp(\n    task: Task, src: str, dst: str, action: str, dry_run: Optional[bool] = None\n):\n    \"\"\"\n    Transfer files from/to the device using sftp protocol\n\n    Example::\n\n        nornir.run(files.sftp,\n                    action=\"put\",\n                    src=\"README.md\",\n                    dst=\"/tmp/README.md\")\n\n    Arguments:\n        dry_run: Whether to apply changes or not\n        src: source file\n        dst: destination\n        action: ``put``, ``get``.\n\n    Returns:\n        Result object with the following attributes set:\n          * changed (``bool``):\n          * files_changed (``list``): list of files that changed\n    \"\"\"\n    dry_run = task.is_dry_run(dry_run)\n    actions = {\"put\": put, \"get\": get}\n    client = task.host.get_connection(\"paramiko\", task.nornir.config)\n    scp_client = SCPClient(client.get_transport())\n    sftp_client = paramiko.SFTPClient.from_transport(client.get_transport())\n    files_changed = actions[action](task, scp_client, sftp_client, src, dst, dry_run)\n    return Result(\n        host=task.host, changed=bool(files_changed), files_changed=files_changed\n    )"}
{"code":"def _concat_translations(translations: List[Translation],\n                         stop_ids: Set[int],\n                         length_penalty: LengthPenalty,\n                         brevity_penalty: Optional[BrevityPenalty] = None) -> Translation:\n    \"\"\"\n    Combines translations through concatenation.\n\n    :param translations: A list of translations (sequence starting with BOS symbol, attention_matrix), score and length.\n    :param stop_ids: The EOS symbols.\n    :param length_penalty: Instance of the LengthPenalty class initialized with alpha and beta.\n    :param brevity_penalty: Optional Instance of the BrevityPenalty class initialized with a brevity weight.\n    :return: A concatenation of the translations with a score.\n    \"\"\"\n    # Concatenation of all target ids without BOS and EOS\n    target_ids = []\n    attention_matrices = []\n    beam_histories = []  # type: List[BeamHistory]\n    estimated_reference_length = None  # type: float\n\n    for idx, translation in enumerate(translations):\n        if idx == len(translations) - 1:\n            target_ids.extend(translation.target_ids)\n            attention_matrices.append(translation.attention_matrix)\n        else:\n            if translation.target_ids[-1] in stop_ids:\n                target_ids.extend(translation.target_ids[:-1])\n                attention_matrices.append(translation.attention_matrix[:-1, :])\n            else:\n                target_ids.extend(translation.target_ids)\n                attention_matrices.append(translation.attention_matrix)\n        beam_histories.extend(translation.beam_histories)\n        if translation.estimated_reference_length is not None:\n            if estimated_reference_length is None:\n                estimated_reference_length = translation.estimated_reference_length\n            else:\n                estimated_reference_length += translation.estimated_reference_length\n    # Combine attention matrices:\n    attention_shapes = [attention_matrix.shape for attention_matrix in attention_matrices]\n    attention_matrix_combined = np.zeros(np.sum(np.asarray(attention_shapes), axis=0))\n    pos_t, pos_s = 0, 0\n    for attention_matrix, (len_t, len_s) in zip(attention_matrices, attention_shapes):\n        attention_matrix_combined[pos_t:pos_t + len_t, pos_s:pos_s + len_s] = attention_matrix\n        pos_t += len_t\n        pos_s += len_s\n\n    def _brevity_penalty(hypothesis_length, reference_length):\n        return 0.0 if brevity_penalty is None else brevity_penalty.get(hypothesis_length, reference_length)\n\n    # Unnormalize + sum and renormalize the score:\n    score = sum((translation.score + _brevity_penalty(len(translation.target_ids), translation.estimated_reference_length)) \\\n                    * length_penalty.get(len(translation.target_ids))\n                 for translation in translations)\n    score = score / length_penalty.get(len(target_ids)) - _brevity_penalty(len(target_ids), estimated_reference_length)\n    return Translation(target_ids, attention_matrix_combined, score, beam_histories,\n                       estimated_reference_length=estimated_reference_length)","return_type":"Translation","function_name":"_concat_translations","stripped_code":"def _concat_translations(translations: List[Translation],\n                         stop_ids: Set[int],\n                         length_penalty: LengthPenalty,\n                         brevity_penalty: Optional[BrevityPenalty] = None):\n    \"\"\"\n    Combines translations through concatenation.\n\n    :param translations: A list of translations (sequence starting with BOS symbol, attention_matrix), score and length.\n    :param stop_ids: The EOS symbols.\n    :param length_penalty: Instance of the LengthPenalty class initialized with alpha and beta.\n    :param brevity_penalty: Optional Instance of the BrevityPenalty class initialized with a brevity weight.\n    :return: A concatenation of the translations with a score.\n    \"\"\"\n    # Concatenation of all target ids without BOS and EOS\n    target_ids = []\n    attention_matrices = []\n    beam_histories = []  # type: List[BeamHistory]\n    estimated_reference_length = None  # type: float\n\n    for idx, translation in enumerate(translations):\n        if idx == len(translations) - 1:\n            target_ids.extend(translation.target_ids)\n            attention_matrices.append(translation.attention_matrix)\n        else:\n            if translation.target_ids[-1] in stop_ids:\n                target_ids.extend(translation.target_ids[:-1])\n                attention_matrices.append(translation.attention_matrix[:-1, :])\n            else:\n                target_ids.extend(translation.target_ids)\n                attention_matrices.append(translation.attention_matrix)\n        beam_histories.extend(translation.beam_histories)\n        if translation.estimated_reference_length is not None:\n            if estimated_reference_length is None:\n                estimated_reference_length = translation.estimated_reference_length\n            else:\n                estimated_reference_length += translation.estimated_reference_length\n    # Combine attention matrices:\n    attention_shapes = [attention_matrix.shape for attention_matrix in attention_matrices]\n    attention_matrix_combined = np.zeros(np.sum(np.asarray(attention_shapes), axis=0))\n    pos_t, pos_s = 0, 0\n    for attention_matrix, (len_t, len_s) in zip(attention_matrices, attention_shapes):\n        attention_matrix_combined[pos_t:pos_t + len_t, pos_s:pos_s + len_s] = attention_matrix\n        pos_t += len_t\n        pos_s += len_s\n\n    def _brevity_penalty(hypothesis_length, reference_length):\n        return 0.0 if brevity_penalty is None else brevity_penalty.get(hypothesis_length, reference_length)\n\n    # Unnormalize + sum and renormalize the score:\n    score = sum((translation.score + _brevity_penalty(len(translation.target_ids), translation.estimated_reference_length)) \\\n                    * length_penalty.get(len(translation.target_ids))\n                 for translation in translations)\n    score = score / length_penalty.get(len(target_ids)) - _brevity_penalty(len(target_ids), estimated_reference_length)\n    return Translation(target_ids, attention_matrix_combined, score, beam_histories,\n                       estimated_reference_length=estimated_reference_length)"}
{"code":"def setup_timezone(timezone: str) -> None:\n    \"\"\"Shortcut helper to configure timezone for backend application.\n\n    :param timezone: Timezone to use, e.g. \"UTC\", \"Europe/Kiev\".\n    \"\"\"\n    if timezone and hasattr(time, 'tzset'):\n        tz_root = '/usr/share/zoneinfo'\n        tz_filename = os.path.join(tz_root, *(timezone.split('/')))\n\n        if os.path.exists(tz_root) and not os.path.exists(tz_filename):\n            raise ValueError('Incorrect timezone value: {0}'.format(timezone))\n\n        os.environ['TZ'] = timezone\n        time.tzset()","return_type":"None","function_name":"setup_timezone","stripped_code":"def setup_timezone(timezone: str):\n    \"\"\"Shortcut helper to configure timezone for backend application.\n\n    :param timezone: Timezone to use, e.g. \"UTC\", \"Europe/Kiev\".\n    \"\"\"\n    if timezone and hasattr(time, 'tzset'):\n        tz_root = '/usr/share/zoneinfo'\n        tz_filename = os.path.join(tz_root, *(timezone.split('/')))\n\n        if os.path.exists(tz_root) and not os.path.exists(tz_filename):\n            raise ValueError('Incorrect timezone value: {0}'.format(timezone))\n\n        os.environ['TZ'] = timezone\n        time.tzset()"}
{"code":"def reorder(self, indices: mx.nd.NDArray) -> None:\n        \"\"\"\n        Reorders the avoid list according to the selected row indices.\n        This can produce duplicates, but this is fixed if state changes occur in consume().\n\n        :param indices: An mx.nd.NDArray containing indices of hypotheses to select.\n        \"\"\"\n        if self.global_avoid_states:\n            self.global_avoid_states = [self.global_avoid_states[x] for x in indices.asnumpy()]\n\n        if self.local_avoid_states:\n            self.local_avoid_states = [self.local_avoid_states[x] for x in indices.asnumpy()]","return_type":"None","function_name":"AvoidBatch.reorder","stripped_code":"def reorder(self, indices: mx.nd.NDArray):\n        \"\"\"\n        Reorders the avoid list according to the selected row indices.\n        This can produce duplicates, but this is fixed if state changes occur in consume().\n\n        :param indices: An mx.nd.NDArray containing indices of hypotheses to select.\n        \"\"\"\n        if self.global_avoid_states:\n            self.global_avoid_states = [self.global_avoid_states[x] for x in indices.asnumpy()]\n\n        if self.local_avoid_states:\n            self.local_avoid_states = [self.local_avoid_states[x] for x in indices.asnumpy()]"}
{"code":"def load_image(file) -> DataAndMetadata.DataAndMetadata:\n    \"\"\"\n    Loads the image from the file-like object or string file.\n    If file is a string, the file is opened and then read.\n    Returns a numpy ndarray of our best guess for the most important image\n    in the file.\n    \"\"\"\n    if isinstance(file, str) or isinstance(file, str):\n        with open(file, \"rb\") as f:\n            return load_image(f)\n    dmtag = parse_dm3.parse_dm_header(file)\n    dmtag = fix_strings(dmtag)\n    # display_keys(dmtag)\n    img_index = -1\n    image_tags = dmtag['ImageList'][img_index]\n    data = imagedatadict_to_ndarray(image_tags['ImageData'])\n    calibrations = []\n    calibration_tags = image_tags['ImageData'].get('Calibrations', dict())\n    for dimension in calibration_tags.get('Dimension', list()):\n        origin, scale, units = dimension.get('Origin', 0.0), dimension.get('Scale', 1.0), dimension.get('Units', str())\n        calibrations.append((-origin * scale, scale, units))\n    calibrations = tuple(reversed(calibrations))\n    if len(data.shape) == 3 and data.dtype != numpy.uint8:\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"Format\", str()).lower() in (\"spectrum\", \"spectrum image\"):\n            if data.shape[1] == 1:\n                data = numpy.squeeze(data, 1)\n                data = numpy.moveaxis(data, 0, 1)\n                data_descriptor = DataAndMetadata.DataDescriptor(False, 1, 1)\n                calibrations = (calibrations[2], calibrations[0])\n            else:\n                data = numpy.moveaxis(data, 0, 2)\n                data_descriptor = DataAndMetadata.DataDescriptor(False, 2, 1)\n                calibrations = tuple(calibrations[1:]) + (calibrations[0],)\n        else:\n            data_descriptor = DataAndMetadata.DataDescriptor(False, 1, 2)\n    elif len(data.shape) == 4 and data.dtype != numpy.uint8:\n        # data = numpy.moveaxis(data, 0, 2)\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 2, 2)\n    elif data.dtype == numpy.uint8:\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 0, len(data.shape[:-1]))\n    else:\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 0, len(data.shape))\n    brightness = calibration_tags.get('Brightness', dict())\n    origin, scale, units = brightness.get('Origin', 0.0), brightness.get('Scale', 1.0), brightness.get('Units', str())\n    intensity = -origin * scale, scale, units\n    timestamp = None\n    timezone = None\n    timezone_offset = None\n    title = image_tags.get('Name')\n    properties = dict()\n    if 'ImageTags' in image_tags:\n        voltage = image_tags['ImageTags'].get('ImageScanned', dict()).get('EHT', dict())\n        if voltage:\n            properties.setdefault(\"hardware_source\", dict())[\"autostem\"] = { \"high_tension_v\": float(voltage) }\n        dm_metadata_signal = image_tags['ImageTags'].get('Meta Data', dict()).get('Signal')\n        if dm_metadata_signal and dm_metadata_signal.lower() == \"eels\":\n            properties.setdefault(\"hardware_source\", dict())[\"signal_type\"] = dm_metadata_signal\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"Format\", str()).lower() in (\"spectrum\", \"spectrum image\"):\n            data_descriptor.collection_dimension_count += data_descriptor.datum_dimension_count - 1\n            data_descriptor.datum_dimension_count = 1\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"IsSequence\", False) and data_descriptor.collection_dimension_count > 0:\n            data_descriptor.is_sequence = True\n            data_descriptor.collection_dimension_count -= 1\n        timestamp_str = image_tags['ImageTags'].get(\"Timestamp\")\n        if timestamp_str:\n            timestamp = get_datetime_from_timestamp_str(timestamp_str)\n        timezone = image_tags['ImageTags'].get(\"Timezone\")\n        timezone_offset = image_tags['ImageTags'].get(\"TimezoneOffset\")\n        # to avoid having duplicate copies in Swift, get rid of these tags\n        image_tags['ImageTags'].pop(\"Timestamp\", None)\n        image_tags['ImageTags'].pop(\"Timezone\", None)\n        image_tags['ImageTags'].pop(\"TimezoneOffset\", None)\n        # put the image tags into properties\n        properties.update(image_tags['ImageTags'])\n    dimensional_calibrations = [Calibration.Calibration(c[0], c[1], c[2]) for c in calibrations]\n    while len(dimensional_calibrations) < data_descriptor.expected_dimension_count:\n        dimensional_calibrations.append(Calibration.Calibration())\n    intensity_calibration = Calibration.Calibration(intensity[0], intensity[1], intensity[2])\n    return DataAndMetadata.new_data_and_metadata(data,\n                                                 data_descriptor=data_descriptor,\n                                                 dimensional_calibrations=dimensional_calibrations,\n                                                 intensity_calibration=intensity_calibration,\n                                                 metadata=properties,\n                                                 timestamp=timestamp,\n                                                 timezone=timezone,\n                                                 timezone_offset=timezone_offset)","return_type":"DataAndMetadata.DataAndMetadata","function_name":"load_image","stripped_code":"def load_image(file):\n    \"\"\"\n    Loads the image from the file-like object or string file.\n    If file is a string, the file is opened and then read.\n    Returns a numpy ndarray of our best guess for the most important image\n    in the file.\n    \"\"\"\n    if isinstance(file, str) or isinstance(file, str):\n        with open(file, \"rb\") as f:\n            return load_image(f)\n    dmtag = parse_dm3.parse_dm_header(file)\n    dmtag = fix_strings(dmtag)\n    # display_keys(dmtag)\n    img_index = -1\n    image_tags = dmtag['ImageList'][img_index]\n    data = imagedatadict_to_ndarray(image_tags['ImageData'])\n    calibrations = []\n    calibration_tags = image_tags['ImageData'].get('Calibrations', dict())\n    for dimension in calibration_tags.get('Dimension', list()):\n        origin, scale, units = dimension.get('Origin', 0.0), dimension.get('Scale', 1.0), dimension.get('Units', str())\n        calibrations.append((-origin * scale, scale, units))\n    calibrations = tuple(reversed(calibrations))\n    if len(data.shape) == 3 and data.dtype != numpy.uint8:\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"Format\", str()).lower() in (\"spectrum\", \"spectrum image\"):\n            if data.shape[1] == 1:\n                data = numpy.squeeze(data, 1)\n                data = numpy.moveaxis(data, 0, 1)\n                data_descriptor = DataAndMetadata.DataDescriptor(False, 1, 1)\n                calibrations = (calibrations[2], calibrations[0])\n            else:\n                data = numpy.moveaxis(data, 0, 2)\n                data_descriptor = DataAndMetadata.DataDescriptor(False, 2, 1)\n                calibrations = tuple(calibrations[1:]) + (calibrations[0],)\n        else:\n            data_descriptor = DataAndMetadata.DataDescriptor(False, 1, 2)\n    elif len(data.shape) == 4 and data.dtype != numpy.uint8:\n        # data = numpy.moveaxis(data, 0, 2)\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 2, 2)\n    elif data.dtype == numpy.uint8:\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 0, len(data.shape[:-1]))\n    else:\n        data_descriptor = DataAndMetadata.DataDescriptor(False, 0, len(data.shape))\n    brightness = calibration_tags.get('Brightness', dict())\n    origin, scale, units = brightness.get('Origin', 0.0), brightness.get('Scale', 1.0), brightness.get('Units', str())\n    intensity = -origin * scale, scale, units\n    timestamp = None\n    timezone = None\n    timezone_offset = None\n    title = image_tags.get('Name')\n    properties = dict()\n    if 'ImageTags' in image_tags:\n        voltage = image_tags['ImageTags'].get('ImageScanned', dict()).get('EHT', dict())\n        if voltage:\n            properties.setdefault(\"hardware_source\", dict())[\"autostem\"] = { \"high_tension_v\": float(voltage) }\n        dm_metadata_signal = image_tags['ImageTags'].get('Meta Data', dict()).get('Signal')\n        if dm_metadata_signal and dm_metadata_signal.lower() == \"eels\":\n            properties.setdefault(\"hardware_source\", dict())[\"signal_type\"] = dm_metadata_signal\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"Format\", str()).lower() in (\"spectrum\", \"spectrum image\"):\n            data_descriptor.collection_dimension_count += data_descriptor.datum_dimension_count - 1\n            data_descriptor.datum_dimension_count = 1\n        if image_tags['ImageTags'].get('Meta Data', dict()).get(\"IsSequence\", False) and data_descriptor.collection_dimension_count > 0:\n            data_descriptor.is_sequence = True\n            data_descriptor.collection_dimension_count -= 1\n        timestamp_str = image_tags['ImageTags'].get(\"Timestamp\")\n        if timestamp_str:\n            timestamp = get_datetime_from_timestamp_str(timestamp_str)\n        timezone = image_tags['ImageTags'].get(\"Timezone\")\n        timezone_offset = image_tags['ImageTags'].get(\"TimezoneOffset\")\n        # to avoid having duplicate copies in Swift, get rid of these tags\n        image_tags['ImageTags'].pop(\"Timestamp\", None)\n        image_tags['ImageTags'].pop(\"Timezone\", None)\n        image_tags['ImageTags'].pop(\"TimezoneOffset\", None)\n        # put the image tags into properties\n        properties.update(image_tags['ImageTags'])\n    dimensional_calibrations = [Calibration.Calibration(c[0], c[1], c[2]) for c in calibrations]\n    while len(dimensional_calibrations) < data_descriptor.expected_dimension_count:\n        dimensional_calibrations.append(Calibration.Calibration())\n    intensity_calibration = Calibration.Calibration(intensity[0], intensity[1], intensity[2])\n    return DataAndMetadata.new_data_and_metadata(data,\n                                                 data_descriptor=data_descriptor,\n                                                 dimensional_calibrations=dimensional_calibrations,\n                                                 intensity_calibration=intensity_calibration,\n                                                 metadata=properties,\n                                                 timestamp=timestamp,\n                                                 timezone=timezone,\n                                                 timezone_offset=timezone_offset)"}
{"code":"def trim(self: 'Variable', lower=None, upper=None) -> None:\n    \"\"\"Trim the value(s) of a |Variable| instance.\n\n    Usually, users do not need to apply function |trim| directly.\n    Instead, some |Variable| subclasses implement their own `trim`\n    methods relying on function |trim|.  Model developers should\n    implement individual `trim` methods for their |Parameter| or\n    |Sequence| subclasses when their boundary values depend on the\n    actual project configuration (one example is soil moisture;\n    its lowest possible value should possibly be zero in all cases,\n    but its highest possible value could depend on another parameter\n    defining the maximum storage capacity).\n\n    For the following examples, we prepare a simple (not fully\n    functional) |Variable| subclass, making use of function |trim|\n    without any modifications.  Function |trim| works slightly\n    different for variables handling |float|, |int|, and |bool|\n    values.  We start with the most common content type |float|:\n\n    >>> from hydpy.core.variabletools import trim, Variable\n    >>> class Var(Variable):\n    ...     NDIM = 0\n    ...     TYPE = float\n    ...     SPAN = 1.0, 3.0\n    ...     trim = trim\n    ...     initinfo = 2.0, False\n    ...     __hydpy__connect_variable2subgroup__ = None\n\n    First, we enable the printing of warning messages raised by function\n    |trim|:\n\n    >>> from hydpy import pub\n    >>> pub.options.warntrim = True\n\n    When not passing boundary values, function |trim| extracts them from\n    class attribute `SPAN` of the given |Variable| instance, if available:\n\n    >>> var = Var(None)\n    >>> var.value = 2.0\n    >>> var.trim()\n    >>> var\n    var(2.0)\n\n    >>> var.value = 0.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `0.0` and `1.0`, respectively.\n    >>> var\n    var(1.0)\n\n    >>> var.value = 4.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `4.0` and `3.0`, respectively.\n    >>> var\n    var(3.0)\n\n    In the examples above, outlier values are set to the respective\n    boundary value, accompanied by suitable warning messages.  For very\n    tiny deviations, which might be due to precision problems only,\n    outliers are trimmed but not reported:\n\n    >>> var.value = 1.0 - 1e-15\n    >>> var == 1.0\n    False\n    >>> trim(var)\n    >>> var == 1.0\n    True\n\n    >>> var.value = 3.0 + 1e-15\n    >>> var == 3.0\n    False\n    >>> var.trim()\n    >>> var == 3.0\n    True\n\n    Use arguments `lower` and `upper` to override the (eventually)\n    available `SPAN` entries:\n\n    >>> var.trim(lower=4.0)\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `3.0` and `4.0`, respectively.\n\n    >>> var.trim(upper=3.0)\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `4.0` and `3.0`, respectively.\n\n    Function |trim| interprets both |None| and |numpy.nan| values as if\n    no boundary value exists:\n\n    >>> import numpy\n    >>> var.value = 0.0\n    >>> var.trim(lower=numpy.nan)\n    >>> var.value = 5.0\n    >>> var.trim(upper=numpy.nan)\n\n    You can disable function |trim| via option |Options.trimvariables|:\n\n    >>> with pub.options.trimvariables(False):\n    ...     var.value = 5.0\n    ...     var.trim()\n    >>> var\n    var(5.0)\n\n    Alternatively, you can omit the warning messages only:\n\n    >>> with pub.options.warntrim(False):\n    ...     var.value = 5.0\n    ...     var.trim()\n    >>> var\n    var(3.0)\n\n    If a |Variable| subclass does not have (fixed) boundaries, give it\n    either no `SPAN` attribute or a |tuple| containing |None| values:\n\n    >>> del Var.SPAN\n    >>> var.value = 5.0\n    >>> var.trim()\n    >>> var\n    var(5.0)\n\n    >>> Var.SPAN = (None, None)\n    >>> var.trim()\n    >>> var\n    var(5.0)\n\n    The above examples deal with a 0-dimensional |Variable| subclass.\n    The following examples repeat the most relevant examples for a\n    2-dimensional subclass:\n\n    >>> Var.SPAN = 1.0, 3.0\n    >>> Var.NDIM = 2\n    >>> var.shape = 1, 3\n    >>> var.values = 2.0\n    >>> var.trim()\n\n    >>> var.values = 0.0, 1.0, 2.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  1.  2.]]` and `[[ 1.  1.  2.]]`, \\\nrespectively.\n    >>> var\n    var([[1.0, 1.0, 2.0]])\n\n    >>> var.values = 2.0, 3.0, 4.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 2.  3.  4.]]` and `[[ 2.  3.  3.]]`, \\\nrespectively.\n    >>> var\n    var([[2.0, 3.0, 3.0]])\n\n    >>> var.values = 1.0-1e-15, 2.0, 3.0+1e-15\n    >>> var.values == (1.0, 2.0, 3.0)\n    array([[False,  True, False]], dtype=bool)\n    >>> var.trim()\n    >>> var.values == (1.0, 2.0, 3.0)\n    array([[ True,  True,  True]], dtype=bool)\n\n    >>> var.values = 0.0, 2.0, 4.0\n    >>> var.trim(lower=numpy.nan, upper=numpy.nan)\n    >>> var\n    var([[0.0, 2.0, 4.0]])\n\n    >>> var.trim(lower=[numpy.nan, 3.0, 3.0])\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  2.  4.]]` and `[[ 0.  3.  3.]]`, \\\nrespectively.\n\n    >>> var.values = 0.0, 2.0, 4.0\n    >>> var.trim(upper=[numpy.nan, 1.0, numpy.nan])\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  2.  4.]]` and `[[ 1.  1.  4.]]`, \\\nrespectively.\n\n    For |Variable| subclasses handling |float| values, setting outliers\n    to the respective boundary value might often be an acceptable approach.\n    However, this is often not the case for subclasses handling |int|\n    values, which often serve as option flags (e.g. to enable/disable\n    a certain hydrological process for different land-use types). Hence,\n    function |trim| raises an exception instead of a warning and does\n    not modify the wrong |int| value:\n\n    >>> Var.TYPE = int\n    >>> Var.NDIM = 0\n    >>> Var.SPAN = 1, 3\n\n    >>> var.value = 2\n    >>> var.trim()\n    >>> var\n    var(2)\n\n    >>> var.value = 0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `0` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(0)\n    >>> var.value = 4\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `4` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(4)\n\n    >>> from hydpy import INT_NAN\n    >>> var.value = 0\n    >>> var.trim(lower=0)\n    >>> var.trim(lower=INT_NAN)\n\n    >>> var.value = 4\n    >>> var.trim(upper=4)\n    >>> var.trim(upper=INT_NAN)\n\n    >>> Var.SPAN = 1, None\n    >>> var.value = 0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `0` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(0)\n\n    >>> Var.SPAN = None, 3\n    >>> var.value = 0\n    >>> var.trim()\n    >>> var.value = 4\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `4` of parameter `var` of element `?` is not valid.\n\n    >>> del Var.SPAN\n    >>> var.value = 0\n    >>> var.trim()\n    >>> var.value = 4\n    >>> var.trim()\n\n    >>> Var.SPAN = 1, 3\n    >>> Var.NDIM = 2\n    >>> var.shape = (1, 3)\n    >>> var.values = 2\n    >>> var.trim()\n\n    >>> var.values = 0, 1, 2\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: At least one value of parameter `var` of element `?` \\\nis not valid.\n    >>> var\n    var([[0, 1, 2]])\n    >>> var.values = 2, 3, 4\n    >>> var.trim()\n    Traceback (most recent call last):\n     ...\n    ValueError: At least one value of parameter `var` of element `?` \\\nis not valid.\n    >>> var\n    var([[2, 3, 4]])\n\n\n    >>> var.values = 0, 0, 2\n    >>> var.trim(lower=[0, INT_NAN, 2])\n\n    >>> var.values = 2, 4, 4\n    >>> var.trim(upper=[2, INT_NAN, 4])\n\n    For |bool| values, defining outliers does not make much sense,\n    which is why function |trim| does nothing when applied on\n    variables handling |bool| values:\n\n    >>> Var.TYPE = bool\n    >>> var.trim()\n\n    If function |trim| encounters an unmanageable type, it raises an\n    exception like the following:\n\n    >>> Var.TYPE = str\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    NotImplementedError: Method `trim` can only be applied on parameters \\\nhandling floating point, integer, or boolean values, but the \"value type\" \\\nof parameter `var` is `str`.\n\n    >>> pub.options.warntrim = False\n    \"\"\"\n    if hydpy.pub.options.trimvariables:\n        if lower is None:\n            lower = self.SPAN[0]\n        if upper is None:\n            upper = self.SPAN[1]\n        type_ = getattr(self, 'TYPE', float)\n        if type_ is float:\n            if self.NDIM == 0:\n                _trim_float_0d(self, lower, upper)\n            else:\n                _trim_float_nd(self, lower, upper)\n        elif type_ is int:\n            if self.NDIM == 0:\n                _trim_int_0d(self, lower, upper)\n            else:\n                _trim_int_nd(self, lower, upper)\n        elif type_ is bool:\n            pass\n        else:\n            raise NotImplementedError(\n                f'Method `trim` can only be applied on parameters '\n                f'handling floating point, integer, or boolean values, '\n                f'but the \"value type\" of parameter `{self.name}` is '\n                f'`{objecttools.classname(self.TYPE)}`.')","return_type":"None","function_name":"trim","stripped_code":"def trim(self: 'Variable', lower=None, upper=None):\n    \"\"\"Trim the value(s) of a |Variable| instance.\n\n    Usually, users do not need to apply function |trim| directly.\n    Instead, some |Variable| subclasses implement their own `trim`\n    methods relying on function |trim|.  Model developers should\n    implement individual `trim` methods for their |Parameter| or\n    |Sequence| subclasses when their boundary values depend on the\n    actual project configuration (one example is soil moisture;\n    its lowest possible value should possibly be zero in all cases,\n    but its highest possible value could depend on another parameter\n    defining the maximum storage capacity).\n\n    For the following examples, we prepare a simple (not fully\n    functional) |Variable| subclass, making use of function |trim|\n    without any modifications.  Function |trim| works slightly\n    different for variables handling |float|, |int|, and |bool|\n    values.  We start with the most common content type |float|:\n\n    >>> from hydpy.core.variabletools import trim, Variable\n    >>> class Var(Variable):\n    ...     NDIM = 0\n    ...     TYPE = float\n    ...     SPAN = 1.0, 3.0\n    ...     trim = trim\n    ...     initinfo = 2.0, False\n    ...     __hydpy__connect_variable2subgroup__ = None\n\n    First, we enable the printing of warning messages raised by function\n    |trim|:\n\n    >>> from hydpy import pub\n    >>> pub.options.warntrim = True\n\n    When not passing boundary values, function |trim| extracts them from\n    class attribute `SPAN` of the given |Variable| instance, if available:\n\n    >>> var = Var(None)\n    >>> var.value = 2.0\n    >>> var.trim()\n    >>> var\n    var(2.0)\n\n    >>> var.value = 0.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `0.0` and `1.0`, respectively.\n    >>> var\n    var(1.0)\n\n    >>> var.value = 4.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `4.0` and `3.0`, respectively.\n    >>> var\n    var(3.0)\n\n    In the examples above, outlier values are set to the respective\n    boundary value, accompanied by suitable warning messages.  For very\n    tiny deviations, which might be due to precision problems only,\n    outliers are trimmed but not reported:\n\n    >>> var.value = 1.0 - 1e-15\n    >>> var == 1.0\n    False\n    >>> trim(var)\n    >>> var == 1.0\n    True\n\n    >>> var.value = 3.0 + 1e-15\n    >>> var == 3.0\n    False\n    >>> var.trim()\n    >>> var == 3.0\n    True\n\n    Use arguments `lower` and `upper` to override the (eventually)\n    available `SPAN` entries:\n\n    >>> var.trim(lower=4.0)\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `3.0` and `4.0`, respectively.\n\n    >>> var.trim(upper=3.0)\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `4.0` and `3.0`, respectively.\n\n    Function |trim| interprets both |None| and |numpy.nan| values as if\n    no boundary value exists:\n\n    >>> import numpy\n    >>> var.value = 0.0\n    >>> var.trim(lower=numpy.nan)\n    >>> var.value = 5.0\n    >>> var.trim(upper=numpy.nan)\n\n    You can disable function |trim| via option |Options.trimvariables|:\n\n    >>> with pub.options.trimvariables(False):\n    ...     var.value = 5.0\n    ...     var.trim()\n    >>> var\n    var(5.0)\n\n    Alternatively, you can omit the warning messages only:\n\n    >>> with pub.options.warntrim(False):\n    ...     var.value = 5.0\n    ...     var.trim()\n    >>> var\n    var(3.0)\n\n    If a |Variable| subclass does not have (fixed) boundaries, give it\n    either no `SPAN` attribute or a |tuple| containing |None| values:\n\n    >>> del Var.SPAN\n    >>> var.value = 5.0\n    >>> var.trim()\n    >>> var\n    var(5.0)\n\n    >>> Var.SPAN = (None, None)\n    >>> var.trim()\n    >>> var\n    var(5.0)\n\n    The above examples deal with a 0-dimensional |Variable| subclass.\n    The following examples repeat the most relevant examples for a\n    2-dimensional subclass:\n\n    >>> Var.SPAN = 1.0, 3.0\n    >>> Var.NDIM = 2\n    >>> var.shape = 1, 3\n    >>> var.values = 2.0\n    >>> var.trim()\n\n    >>> var.values = 0.0, 1.0, 2.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  1.  2.]]` and `[[ 1.  1.  2.]]`, \\\nrespectively.\n    >>> var\n    var([[1.0, 1.0, 2.0]])\n\n    >>> var.values = 2.0, 3.0, 4.0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 2.  3.  4.]]` and `[[ 2.  3.  3.]]`, \\\nrespectively.\n    >>> var\n    var([[2.0, 3.0, 3.0]])\n\n    >>> var.values = 1.0-1e-15, 2.0, 3.0+1e-15\n    >>> var.values == (1.0, 2.0, 3.0)\n    array([[False,  True, False]], dtype=bool)\n    >>> var.trim()\n    >>> var.values == (1.0, 2.0, 3.0)\n    array([[ True,  True,  True]], dtype=bool)\n\n    >>> var.values = 0.0, 2.0, 4.0\n    >>> var.trim(lower=numpy.nan, upper=numpy.nan)\n    >>> var\n    var([[0.0, 2.0, 4.0]])\n\n    >>> var.trim(lower=[numpy.nan, 3.0, 3.0])\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  2.  4.]]` and `[[ 0.  3.  3.]]`, \\\nrespectively.\n\n    >>> var.values = 0.0, 2.0, 4.0\n    >>> var.trim(upper=[numpy.nan, 1.0, numpy.nan])\n    Traceback (most recent call last):\n    ...\n    UserWarning: For variable `var` at least one value needed to be trimmed.  \\\nThe old and the new value(s) are `[[ 0.  2.  4.]]` and `[[ 1.  1.  4.]]`, \\\nrespectively.\n\n    For |Variable| subclasses handling |float| values, setting outliers\n    to the respective boundary value might often be an acceptable approach.\n    However, this is often not the case for subclasses handling |int|\n    values, which often serve as option flags (e.g. to enable/disable\n    a certain hydrological process for different land-use types). Hence,\n    function |trim| raises an exception instead of a warning and does\n    not modify the wrong |int| value:\n\n    >>> Var.TYPE = int\n    >>> Var.NDIM = 0\n    >>> Var.SPAN = 1, 3\n\n    >>> var.value = 2\n    >>> var.trim()\n    >>> var\n    var(2)\n\n    >>> var.value = 0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `0` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(0)\n    >>> var.value = 4\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `4` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(4)\n\n    >>> from hydpy import INT_NAN\n    >>> var.value = 0\n    >>> var.trim(lower=0)\n    >>> var.trim(lower=INT_NAN)\n\n    >>> var.value = 4\n    >>> var.trim(upper=4)\n    >>> var.trim(upper=INT_NAN)\n\n    >>> Var.SPAN = 1, None\n    >>> var.value = 0\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `0` of parameter `var` of element `?` is not valid.\n    >>> var\n    var(0)\n\n    >>> Var.SPAN = None, 3\n    >>> var.value = 0\n    >>> var.trim()\n    >>> var.value = 4\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: The value `4` of parameter `var` of element `?` is not valid.\n\n    >>> del Var.SPAN\n    >>> var.value = 0\n    >>> var.trim()\n    >>> var.value = 4\n    >>> var.trim()\n\n    >>> Var.SPAN = 1, 3\n    >>> Var.NDIM = 2\n    >>> var.shape = (1, 3)\n    >>> var.values = 2\n    >>> var.trim()\n\n    >>> var.values = 0, 1, 2\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    ValueError: At least one value of parameter `var` of element `?` \\\nis not valid.\n    >>> var\n    var([[0, 1, 2]])\n    >>> var.values = 2, 3, 4\n    >>> var.trim()\n    Traceback (most recent call last):\n     ...\n    ValueError: At least one value of parameter `var` of element `?` \\\nis not valid.\n    >>> var\n    var([[2, 3, 4]])\n\n\n    >>> var.values = 0, 0, 2\n    >>> var.trim(lower=[0, INT_NAN, 2])\n\n    >>> var.values = 2, 4, 4\n    >>> var.trim(upper=[2, INT_NAN, 4])\n\n    For |bool| values, defining outliers does not make much sense,\n    which is why function |trim| does nothing when applied on\n    variables handling |bool| values:\n\n    >>> Var.TYPE = bool\n    >>> var.trim()\n\n    If function |trim| encounters an unmanageable type, it raises an\n    exception like the following:\n\n    >>> Var.TYPE = str\n    >>> var.trim()\n    Traceback (most recent call last):\n    ...\n    NotImplementedError: Method `trim` can only be applied on parameters \\\nhandling floating point, integer, or boolean values, but the \"value type\" \\\nof parameter `var` is `str`.\n\n    >>> pub.options.warntrim = False\n    \"\"\"\n    if hydpy.pub.options.trimvariables:\n        if lower is None:\n            lower = self.SPAN[0]\n        if upper is None:\n            upper = self.SPAN[1]\n        type_ = getattr(self, 'TYPE', float)\n        if type_ is float:\n            if self.NDIM == 0:\n                _trim_float_0d(self, lower, upper)\n            else:\n                _trim_float_nd(self, lower, upper)\n        elif type_ is int:\n            if self.NDIM == 0:\n                _trim_int_0d(self, lower, upper)\n            else:\n                _trim_int_nd(self, lower, upper)\n        elif type_ is bool:\n            pass\n        else:\n            raise NotImplementedError(\n                f'Method `trim` can only be applied on parameters '\n                f'handling floating point, integer, or boolean values, '\n                f'but the \"value type\" of parameter `{self.name}` is '\n                f'`{objecttools.classname(self.TYPE)}`.')"}
{"code":"def get_metadata_value(metadata_source, key: str) -> typing.Any:\n    \"\"\"Get the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"session_metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    desc = key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    raise KeyError()","return_type":"typing.Any","function_name":"get_metadata_value","stripped_code":"def get_metadata_value(metadata_source, key: str):\n    \"\"\"Get the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"\n    desc = session_key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"session_metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    desc = key_map.get(key)\n    if desc is not None:\n        v = getattr(metadata_source, \"metadata\", dict())\n        for k in desc['path']:\n            v =  v.get(k) if v is not None else None\n        return v\n    raise KeyError()"}
{"code":"def rev_regs(self) -> list:\n        \"\"\"\n        Return list of revocation registry identifiers for which HolderProver has tails files.\n\n        :return: list of revocation registry identifiers for which HolderProver has tails files\n        \"\"\"\n\n        LOGGER.debug('HolderProver.rev_regs >>>')\n\n        rv = [basename(f) for f in Tails.links(self._dir_tails)]\n        LOGGER.debug('HolderProver.rev_regs <<< %s', rv)\n        return rv","return_type":"list","function_name":"HolderProver.rev_regs","stripped_code":"def rev_regs(self):\n        \"\"\"\n        Return list of revocation registry identifiers for which HolderProver has tails files.\n\n        :return: list of revocation registry identifiers for which HolderProver has tails files\n        \"\"\"\n\n        LOGGER.debug('HolderProver.rev_regs >>>')\n\n        rv = [basename(f) for f in Tails.links(self._dir_tails)]\n        LOGGER.debug('HolderProver.rev_regs <<< %s', rv)\n        return rv"}
{"code":"def template_filter(self,\n                        arg: Optional[Callable] = None,\n                        *,\n                        name: Optional[str] = None,\n                        pass_context: bool = False,\n                        inject: Optional[Union[bool, Iterable[str]]] = None,\n                        safe: bool = False,\n                        ) -> Callable:\n        \"\"\"\n        Decorator to mark a function as a Jinja template filter.\n\n        :param name: The name of the filter, if different from the function name.\n        :param pass_context: Whether or not to pass the template context into the filter.\n            If ``True``, the first argument must be the context.\n        :param inject: Whether or not this filter needs any dependencies injected.\n        :param safe: Whether or not to mark the output of this filter as html-safe.\n        \"\"\"\n        def wrapper(fn):\n            fn = _inject(fn, inject)\n            if safe:\n                fn = _make_safe(fn)\n            if pass_context:\n                fn = jinja2.contextfilter(fn)\n            self._defer(lambda app: app.add_template_filter(fn, name=name))\n            return fn\n\n        if callable(arg):\n            return wrapper(arg)\n        return wrapper","return_type":"Callable","function_name":"Unchained.template_filter","stripped_code":"def template_filter(self,\n                        arg: Optional[Callable] = None,\n                        *,\n                        name: Optional[str] = None,\n                        pass_context: bool = False,\n                        inject: Optional[Union[bool, Iterable[str]]] = None,\n                        safe: bool = False,\n                        ):\n        \"\"\"\n        Decorator to mark a function as a Jinja template filter.\n\n        :param name: The name of the filter, if different from the function name.\n        :param pass_context: Whether or not to pass the template context into the filter.\n            If ``True``, the first argument must be the context.\n        :param inject: Whether or not this filter needs any dependencies injected.\n        :param safe: Whether or not to mark the output of this filter as html-safe.\n        \"\"\"\n        def wrapper(fn):\n            fn = _inject(fn, inject)\n            if safe:\n                fn = _make_safe(fn)\n            if pass_context:\n                fn = jinja2.contextfilter(fn)\n            self._defer(lambda app: app.add_template_filter(fn, name=name))\n            return fn\n\n        if callable(arg):\n            return wrapper(arg)\n        return wrapper"}
{"code":"def parse_010c(v) -> int:\n    \"\"\"\n    Parses Engine RPM and returns it in [RPM] as a float from 0 - 16383.75\n    :param str v:\n    :return int:\n    \"\"\"\n    try:\n        val = int(trim_obd_value(v), 16)\n        return int(val / 4)\n    except ValueError:\n        return None","return_type":"int","function_name":"parse_010c","stripped_code":"def parse_010c(v):\n    \"\"\"\n    Parses Engine RPM and returns it in [RPM] as a float from 0 - 16383.75\n    :param str v:\n    :return int:\n    \"\"\"\n    try:\n        val = int(trim_obd_value(v), 16)\n        return int(val / 4)\n    except ValueError:\n        return None"}
{"code":"def set_fore(\n        self, x: int, y: int, r: int, g: int, b: int, char: str\n    ) -> None:\n        \"\"\"Set the character and foreground color of one cell.\n\n        Args:\n            x (int): X position to change.\n            y (int): Y position to change.\n            r (int): Red foreground color, from 0 to 255.\n            g (int): Green foreground color, from 0 to 255.\n            b (int): Blue foreground color, from 0 to 255.\n            char (AnyStr): A single character str or bytes object.\n        \"\"\"\n        i = self.width * y + x\n        self.fore_r[i] = r\n        self.fore_g[i] = g\n        self.fore_b[i] = b\n        self.char[i] = ord(char)","return_type":"None","function_name":"ConsoleBuffer.set_fore","stripped_code":"def set_fore(\n        self, x: int, y: int, r: int, g: int, b: int, char: str\n    ):\n        \"\"\"Set the character and foreground color of one cell.\n\n        Args:\n            x (int): X position to change.\n            y (int): Y position to change.\n            r (int): Red foreground color, from 0 to 255.\n            g (int): Green foreground color, from 0 to 255.\n            b (int): Blue foreground color, from 0 to 255.\n            char (AnyStr): A single character str or bytes object.\n        \"\"\"\n        i = self.width * y + x\n        self.fore_r[i] = r\n        self.fore_g[i] = g\n        self.fore_b[i] = b\n        self.char[i] = ord(char)"}
{"code":"def cast_conditionally(data: mx.sym.Symbol, dtype: str) -> mx.sym.Symbol:\n    \"\"\"\n    Workaround until no-op cast will be fixed in MXNet codebase.\n    Creates cast symbol only if dtype is different from default one, i.e. float32.\n\n    :param data: Input symbol.\n    :param dtype: Target dtype.\n    :return: Cast symbol or just data symbol.\n    \"\"\"\n    if dtype != C.DTYPE_FP32:\n        return mx.sym.cast(data=data, dtype=dtype)\n    return data","return_type":"mx.sym.Symbol","function_name":"cast_conditionally","stripped_code":"def cast_conditionally(data: mx.sym.Symbol, dtype: str):\n    \"\"\"\n    Workaround until no-op cast will be fixed in MXNet codebase.\n    Creates cast symbol only if dtype is different from default one, i.e. float32.\n\n    :param data: Input symbol.\n    :param dtype: Target dtype.\n    :return: Cast symbol or just data symbol.\n    \"\"\"\n    if dtype != C.DTYPE_FP32:\n        return mx.sym.cast(data=data, dtype=dtype)\n    return data"}
{"code":"def import_block(self, block: BaseBlock) -> BaseBlock:\n        \"\"\"\n        Import the given block to the chain.\n        \"\"\"\n        if self.block.number != block.number:\n            raise ValidationError(\n                \"This VM can only import blocks at number #{}, the attempted block was #{}\".format(\n                    self.block.number,\n                    block.number,\n                )\n            )\n\n        self.block = self.block.copy(\n            header=self.configure_header(\n                coinbase=block.header.coinbase,\n                gas_limit=block.header.gas_limit,\n                timestamp=block.header.timestamp,\n                extra_data=block.header.extra_data,\n                mix_hash=block.header.mix_hash,\n                nonce=block.header.nonce,\n                uncles_hash=keccak(rlp.encode(block.uncles)),\n            ),\n            uncles=block.uncles,\n        )\n        # we need to re-initialize the `state` to update the execution context.\n        self._state = self.build_state(self.chaindb.db, self.header, self.previous_hashes)\n\n        # run all of the transactions.\n        new_header, receipts, _ = self.apply_all_transactions(block.transactions, self.header)\n\n        self.block = self.set_block_transactions(\n            self.block,\n            new_header,\n            block.transactions,\n            receipts,\n        )\n\n        return self.mine_block()","return_type":"BaseBlock","function_name":"VM.import_block","stripped_code":"def import_block(self, block: BaseBlock):\n        \"\"\"\n        Import the given block to the chain.\n        \"\"\"\n        if self.block.number != block.number:\n            raise ValidationError(\n                \"This VM can only import blocks at number #{}, the attempted block was #{}\".format(\n                    self.block.number,\n                    block.number,\n                )\n            )\n\n        self.block = self.block.copy(\n            header=self.configure_header(\n                coinbase=block.header.coinbase,\n                gas_limit=block.header.gas_limit,\n                timestamp=block.header.timestamp,\n                extra_data=block.header.extra_data,\n                mix_hash=block.header.mix_hash,\n                nonce=block.header.nonce,\n                uncles_hash=keccak(rlp.encode(block.uncles)),\n            ),\n            uncles=block.uncles,\n        )\n        # we need to re-initialize the `state` to update the execution context.\n        self._state = self.build_state(self.chaindb.db, self.header, self.previous_hashes)\n\n        # run all of the transactions.\n        new_header, receipts, _ = self.apply_all_transactions(block.transactions, self.header)\n\n        self.block = self.set_block_transactions(\n            self.block,\n            new_header,\n            block.transactions,\n            receipts,\n        )\n\n        return self.mine_block()"}
{"code":"def if_sqlserver_disable_constraints(session: SqlASession,\n                                     tablename: str) -> None:\n    \"\"\"\n    If we're running under SQL Server, disable constraint checking for the\n    specified table while the resource is held.\n\n    Args:\n        session: SQLAlchemy :class:`Session`\n        tablename: table name\n\n    See\n    https://stackoverflow.com/questions/123558/sql-server-2005-t-sql-to-temporarily-disable-a-trigger\n    \"\"\"  # noqa\n    engine = get_engine_from_session(session)\n    if is_sqlserver(engine):\n        quoted_tablename = quote_identifier(tablename, engine)\n        session.execute(\n            \"ALTER TABLE {} NOCHECK CONSTRAINT all\".format(\n                quoted_tablename))\n        yield\n        session.execute(\n            \"ALTER TABLE {} WITH CHECK CHECK CONSTRAINT all\".format(\n                quoted_tablename))\n    else:\n        yield","return_type":"None","function_name":"if_sqlserver_disable_constraints","stripped_code":"def if_sqlserver_disable_constraints(session: SqlASession,\n                                     tablename: str):\n    \"\"\"\n    If we're running under SQL Server, disable constraint checking for the\n    specified table while the resource is held.\n\n    Args:\n        session: SQLAlchemy :class:`Session`\n        tablename: table name\n\n    See\n    https://stackoverflow.com/questions/123558/sql-server-2005-t-sql-to-temporarily-disable-a-trigger\n    \"\"\"  # noqa\n    engine = get_engine_from_session(session)\n    if is_sqlserver(engine):\n        quoted_tablename = quote_identifier(tablename, engine)\n        session.execute(\n            \"ALTER TABLE {} NOCHECK CONSTRAINT all\".format(\n                quoted_tablename))\n        yield\n        session.execute(\n            \"ALTER TABLE {} WITH CHECK CHECK CONSTRAINT all\".format(\n                quoted_tablename))\n    else:\n        yield"}
{"code":"def cmdline_split(s: str, platform: Union[int, str] = 'this') -> List[str]:\n    \"\"\"\n    As per\n    https://stackoverflow.com/questions/33560364/python-windows-parsing-command-lines-with-shlex.\n\n    Multi-platform variant of ``shlex.split()`` for command-line splitting.\n    For use with ``subprocess``, for ``argv`` injection etc. Using fast REGEX.\n    \n    Args:\n        s:\n            string to split\n        platform:\n            - ``'this'`` = auto from current platform;\n            - ``1`` = POSIX;\n            - ``0`` = Windows/CMD\n            - (other values reserved)\n    \"\"\"  # noqa\n    if platform == 'this':\n        platform = (sys.platform != 'win32')  # RNC: includes 64-bit Windows\n    if platform == 1:  # POSIX\n        re_cmd_lex = r'''\"((?:\\\\[\"\\\\]|[^\"])*)\"|'([^']*)'|(\\\\.)|(&&?|\\|\\|?|\\d?\\>|[<])|([^\\s'\"\\\\&|<>]+)|(\\s+)|(.)'''  # noqa\n    elif platform == 0:  # Windows/CMD\n        re_cmd_lex = r'''\"((?:\"\"|\\\\[\"\\\\]|[^\"])*)\"?()|(\\\\\\\\(?=\\\\*\")|\\\\\")|(&&?|\\|\\|?|\\d?>|[<])|([^\\s\"&|<>]+)|(\\s+)|(.)'''  # noqa\n    else:\n        raise AssertionError('unknown platform %r' % platform)\n\n    args = []\n    accu = None   # collects pieces of one arg\n    for qs, qss, esc, pipe, word, white, fail in re.findall(re_cmd_lex, s):\n        if word:\n            pass   # most frequent\n        elif esc:\n            word = esc[1]\n        elif white or pipe:\n            if accu is not None:\n                args.append(accu)\n            if pipe:\n                args.append(pipe)\n            accu = None\n            continue\n        elif fail:\n            raise ValueError(\"invalid or incomplete shell string\")\n        elif qs:\n            word = qs.replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n            if platform == 0:\n                word = word.replace('\"\"', '\"')\n        else:\n            word = qss   # may be even empty; must be last\n\n        accu = (accu or '') + word\n\n    if accu is not None:\n        args.append(accu)\n\n    return args","return_type":"List[str]","function_name":"cmdline_split","stripped_code":"def cmdline_split(s: str, platform: Union[int, str] = 'this'):\n    \"\"\"\n    As per\n    https://stackoverflow.com/questions/33560364/python-windows-parsing-command-lines-with-shlex.\n\n    Multi-platform variant of ``shlex.split()`` for command-line splitting.\n    For use with ``subprocess``, for ``argv`` injection etc. Using fast REGEX.\n    \n    Args:\n        s:\n            string to split\n        platform:\n            - ``'this'`` = auto from current platform;\n            - ``1`` = POSIX;\n            - ``0`` = Windows/CMD\n            - (other values reserved)\n    \"\"\"  # noqa\n    if platform == 'this':\n        platform = (sys.platform != 'win32')  # RNC: includes 64-bit Windows\n    if platform == 1:  # POSIX\n        re_cmd_lex = r'''\"((?:\\\\[\"\\\\]|[^\"])*)\"|'([^']*)'|(\\\\.)|(&&?|\\|\\|?|\\d?\\>|[<])|([^\\s'\"\\\\&|<>]+)|(\\s+)|(.)'''  # noqa\n    elif platform == 0:  # Windows/CMD\n        re_cmd_lex = r'''\"((?:\"\"|\\\\[\"\\\\]|[^\"])*)\"?()|(\\\\\\\\(?=\\\\*\")|\\\\\")|(&&?|\\|\\|?|\\d?>|[<])|([^\\s\"&|<>]+)|(\\s+)|(.)'''  # noqa\n    else:\n        raise AssertionError('unknown platform %r' % platform)\n\n    args = []\n    accu = None   # collects pieces of one arg\n    for qs, qss, esc, pipe, word, white, fail in re.findall(re_cmd_lex, s):\n        if word:\n            pass   # most frequent\n        elif esc:\n            word = esc[1]\n        elif white or pipe:\n            if accu is not None:\n                args.append(accu)\n            if pipe:\n                args.append(pipe)\n            accu = None\n            continue\n        elif fail:\n            raise ValueError(\"invalid or incomplete shell string\")\n        elif qs:\n            word = qs.replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n            if platform == 0:\n                word = word.replace('\"\"', '\"')\n        else:\n            word = qss   # may be even empty; must be last\n\n        accu = (accu or '') + word\n\n    if accu is not None:\n        args.append(accu)\n\n    return args"}
{"code":"def preprocess_bel_stmt(stmt: str) -> str:\n    \"\"\"Clean up basic formatting of BEL statement\n\n    Args:\n        stmt: BEL statement as single string\n\n    Returns:\n        cleaned BEL statement\n    \"\"\"\n\n    stmt = stmt.strip()  # remove newline at end of stmt\n    stmt = re.sub(r\",+\", \",\", stmt)  # remove multiple commas\n    stmt = re.sub(r\",\", \", \", stmt)  # add space after each comma\n    stmt = re.sub(r\" +\", \" \", stmt)  # remove multiple spaces\n\n    return stmt","return_type":"str","function_name":"preprocess_bel_stmt","stripped_code":"def preprocess_bel_stmt(stmt: str):\n    \"\"\"Clean up basic formatting of BEL statement\n\n    Args:\n        stmt: BEL statement as single string\n\n    Returns:\n        cleaned BEL statement\n    \"\"\"\n\n    stmt = stmt.strip()  # remove newline at end of stmt\n    stmt = re.sub(r\",+\", \",\", stmt)  # remove multiple commas\n    stmt = re.sub(r\",\", \", \", stmt)  # add space after each comma\n    stmt = re.sub(r\" +\", \" \", stmt)  # remove multiple spaces\n\n    return stmt"}
{"code":"def run_until(self, end_time: Time, with_logging=True) -> int:\n        \"\"\"Run the simulation until the provided end time.\n\n        Parameters\n        ----------\n        end_time\n            The time to run the simulation until. The simulation will run until\n            its clock is greater than or equal to the provided end time.\n        with_logging\n            Whether or not to log the simulation steps. Only works in an ipython\n            environment.\n\n        Returns\n        -------\n            The number of steps the simulation took.\n        \"\"\"\n        if not isinstance(end_time, type(self.clock.time)):\n            raise ValueError(f\"Provided time must be an instance of {type(self.clock.time)}\")\n\n        iterations = int(ceil((end_time - self.clock.time)/self.clock.step_size))\n        self.take_steps(number_of_steps=iterations, with_logging=with_logging)\n        assert self.clock.time - self.clock.step_size < end_time <= self.clock.time\n        return iterations","return_type":"int","function_name":"InteractiveContext.run_until","stripped_code":"def run_until(self, end_time: Time, with_logging=True):\n        \"\"\"Run the simulation until the provided end time.\n\n        Parameters\n        ----------\n        end_time\n            The time to run the simulation until. The simulation will run until\n            its clock is greater than or equal to the provided end time.\n        with_logging\n            Whether or not to log the simulation steps. Only works in an ipython\n            environment.\n\n        Returns\n        -------\n            The number of steps the simulation took.\n        \"\"\"\n        if not isinstance(end_time, type(self.clock.time)):\n            raise ValueError(f\"Provided time must be an instance of {type(self.clock.time)}\")\n\n        iterations = int(ceil((end_time - self.clock.time)/self.clock.step_size))\n        self.take_steps(number_of_steps=iterations, with_logging=with_logging)\n        assert self.clock.time - self.clock.step_size < end_time <= self.clock.time\n        return iterations"}
{"code":"def get_account_by_ont_id(self, ont_id: str, password: str) -> Account:\n        \"\"\"\n        :param ont_id: OntId.\n        :param password: a password which is used to decrypt the encrypted private key.\n        :return:\n        \"\"\"\n        WalletManager.__check_ont_id(ont_id)\n        for identity in self.wallet_in_mem.identities:\n            if identity.ont_id == ont_id:\n                addr = identity.ont_id.replace(DID_ONT, \"\")\n                key = identity.controls[0].key\n                salt = base64.b64decode(identity.controls[0].salt)\n                n = self.wallet_in_mem.scrypt.n\n                private_key = Account.get_gcm_decoded_private_key(key, password, addr, salt, n, self.scheme)\n                return Account(private_key, self.scheme)\n        raise SDKException(ErrorCode.other_error(f'Get account {ont_id} failed.'))","return_type":"Account","function_name":"WalletManager.get_account_by_ont_id","stripped_code":"def get_account_by_ont_id(self, ont_id: str, password: str):\n        \"\"\"\n        :param ont_id: OntId.\n        :param password: a password which is used to decrypt the encrypted private key.\n        :return:\n        \"\"\"\n        WalletManager.__check_ont_id(ont_id)\n        for identity in self.wallet_in_mem.identities:\n            if identity.ont_id == ont_id:\n                addr = identity.ont_id.replace(DID_ONT, \"\")\n                key = identity.controls[0].key\n                salt = base64.b64decode(identity.controls[0].salt)\n                n = self.wallet_in_mem.scrypt.n\n                private_key = Account.get_gcm_decoded_private_key(key, password, addr, salt, n, self.scheme)\n                return Account(private_key, self.scheme)\n        raise SDKException(ErrorCode.other_error(f'Get account {ont_id} failed.'))"}
{"code":"def add(self, interval: Interval) -> None:\n        \"\"\"\n        Adds an interval to the list. If ``self.no_overlap`` is True, as is the\n        default, it will merge any overlapping intervals thus created.\n        \"\"\"\n        if interval is None:\n            return\n        if not isinstance(interval, Interval):\n            raise TypeError(\n                \"Attempt to insert non-Interval into IntervalList\")\n        self.intervals.append(interval)\n        self._tidy()","return_type":"None","function_name":"IntervalList.add","stripped_code":"def add(self, interval: Interval):\n        \"\"\"\n        Adds an interval to the list. If ``self.no_overlap`` is True, as is the\n        default, it will merge any overlapping intervals thus created.\n        \"\"\"\n        if interval is None:\n            return\n        if not isinstance(interval, Interval):\n            raise TypeError(\n                \"Attempt to insert non-Interval into IntervalList\")\n        self.intervals.append(interval)\n        self._tidy()"}
{"code":"def rev_before(self, rev: int) -> int:\n        \"\"\"Return the latest past rev on which the value changed.\"\"\"\n        self.seek(rev)\n        if self._past:\n            return self._past[-1][0]","return_type":"int","function_name":"WindowDict.rev_before","stripped_code":"def rev_before(self, rev: int):\n        \"\"\"Return the latest past rev on which the value changed.\"\"\"\n        self.seek(rev)\n        if self._past:\n            return self._past[-1][0]"}
{"code":"def check_backend() -> bool:\n        \"\"\"Check if gatttool is available on the system.\"\"\"\n        try:\n            call('gatttool', stdout=PIPE, stderr=PIPE)\n            return True\n        except OSError as os_err:\n            msg = 'gatttool not found: {}'.format(str(os_err))\n            _LOGGER.error(msg)\n        return False","return_type":"bool","function_name":"GatttoolBackend.check_backend","stripped_code":"def check_backend():\n        \"\"\"Check if gatttool is available on the system.\"\"\"\n        try:\n            call('gatttool', stdout=PIPE, stderr=PIPE)\n            return True\n        except OSError as os_err:\n            msg = 'gatttool not found: {}'.format(str(os_err))\n            _LOGGER.error(msg)\n        return False"}
{"code":"def close(self) -> None:\n        \"\"\"\n        Close the gRPC channel and free the acquired resources. Using a closed client is\n        not supported.\n        \"\"\"\n        self._channel.close()\n        self._channel = self._stub_v1 = self._stub_v2 = None","return_type":"None","function_name":"BblfshClient.close","stripped_code":"def close(self):\n        \"\"\"\n        Close the gRPC channel and free the acquired resources. Using a closed client is\n        not supported.\n        \"\"\"\n        self._channel.close()\n        self._channel = self._stub_v1 = self._stub_v2 = None"}
{"code":"def __get_or_create_database(self, doc_client, id) -> str:\n        \"\"\"Return the database link.\n\n        Check if the database exists or create the db.\n\n        :param doc_client:\n        :param id:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a database with that name/id\n        dbs = list(doc_client.QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": id}\n            ]\n        }))\n        # if there are results, return the first (db names are unique)\n        if len(dbs) > 0:\n            return dbs[0]['id']\n        else:\n            # create the database if it didn't exist\n            res = doc_client.CreateDatabase({'id': id})\n            return res['id']","return_type":"str","function_name":"CosmosDbStorage.__get_or_create_database","stripped_code":"def __get_or_create_database(self, doc_client, id):\n        \"\"\"Return the database link.\n\n        Check if the database exists or create the db.\n\n        :param doc_client:\n        :param id:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a database with that name/id\n        dbs = list(doc_client.QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": id}\n            ]\n        }))\n        # if there are results, return the first (db names are unique)\n        if len(dbs) > 0:\n            return dbs[0]['id']\n        else:\n            # create the database if it didn't exist\n            res = doc_client.CreateDatabase({'id': id})\n            return res['id']"}
{"code":"def raw_corpus_rouge2(hypotheses: Iterable[str], references: Iterable[str]) -> float:\n    \"\"\"\n    Simple wrapper around ROUGE-2 implementation.\n\n    :param hypotheses: Hypotheses stream.\n    :param references: Reference stream.\n    :return: ROUGE-2 score as float between 0 and 1.\n    \"\"\"\n    return rouge.rouge_2(hypotheses, references)","return_type":"float","function_name":"raw_corpus_rouge2","stripped_code":"def raw_corpus_rouge2(hypotheses: Iterable[str], references: Iterable[str]):\n    \"\"\"\n    Simple wrapper around ROUGE-2 implementation.\n\n    :param hypotheses: Hypotheses stream.\n    :param references: Reference stream.\n    :return: ROUGE-2 score as float between 0 and 1.\n    \"\"\"\n    return rouge.rouge_2(hypotheses, references)"}
{"code":"def next_future_periodic_delta(self) -> Optional[float]:\n        \"\"\"Give the amount of seconds before the next periodic task is due.\"\"\"\n        try:\n            next_event = self._scheduler.queue[0]\n        except IndexError:\n            return None\n\n        now = time.monotonic()\n        next_event_time = next_event[0]\n        if next_event_time < now:\n            return 0\n\n        return next_event_time - now","return_type":"Optional[float]","function_name":"MemoryBroker.next_future_periodic_delta","stripped_code":"def next_future_periodic_delta(self):\n        \"\"\"Give the amount of seconds before the next periodic task is due.\"\"\"\n        try:\n            next_event = self._scheduler.queue[0]\n        except IndexError:\n            return None\n\n        now = time.monotonic()\n        next_event_time = next_event[0]\n        if next_event_time < now:\n            return 0\n\n        return next_event_time - now"}
{"code":"def read_text(self, encoding='utf-8') -> str:\n        ''' read all text into memory. '''\n        with self.open('r', encoding=encoding) as fp:\n            return fp.read()","return_type":"str","function_name":"FileInfo.read_text","stripped_code":"def read_text(self, encoding='utf-8'):\n        ''' read all text into memory. '''\n        with self.open('r', encoding=encoding) as fp:\n            return fp.read()"}
{"code":"def _round_multiple(x:int, mult:int=None)->int:\n    \"Calc `x` to nearest multiple of `mult`.\"\n    return (int(x/mult+0.5)*mult) if mult is not None else x","return_type":"int","function_name":"_round_multiple","stripped_code":"def _round_multiple(x:int, mult:int=None):\n    \"Calc `x` to nearest multiple of `mult`.\"\n    return (int(x/mult+0.5)*mult) if mult is not None else x"}
{"code":"def b58decode_check(v: str) -> bytes:\n    '''Decode and verify the checksum of a Base58 encoded string'''\n\n    result = b58decode(v)\n    result, check = result[:-4], result[-4:]\n    digest = sha256(sha256(result).digest()).digest()\n\n    if check != digest[:4]:\n        raise ValueError(\"Invalid checksum\")\n\n    return result","return_type":"bytes","function_name":"b58decode_check","stripped_code":"def b58decode_check(v: str):\n    '''Decode and verify the checksum of a Base58 encoded string'''\n\n    result = b58decode(v)\n    result, check = result[:-4], result[-4:]\n    digest = sha256(sha256(result).digest()).digest()\n\n    if check != digest[:4]:\n        raise ValueError(\"Invalid checksum\")\n\n    return result"}
{"code":"def decode_packet_id(reader) -> int:\n    \"\"\"\n    Read a packet ID as 2-bytes int from stream according to MQTT specification (2.3.1)\n    :param reader: Stream reader\n    :return: Packet ID\n    \"\"\"\n    packet_id_bytes = yield from read_or_raise(reader, 2)\n    packet_id = unpack(\"!H\", packet_id_bytes)\n    return packet_id[0]","return_type":"int","function_name":"decode_packet_id","stripped_code":"def decode_packet_id(reader):\n    \"\"\"\n    Read a packet ID as 2-bytes int from stream according to MQTT specification (2.3.1)\n    :param reader: Stream reader\n    :return: Packet ID\n    \"\"\"\n    packet_id_bytes = yield from read_or_raise(reader, 2)\n    packet_id = unpack(\"!H\", packet_id_bytes)\n    return packet_id[0]"}
{"code":"def get_dependencies(self) -> Optional[List[str]]:\n        \"\"\" Returns the ``apt_dependencies`` setting to a standardized format.\n\n        :raise ArcaMisconfigured: if the dependencies can't be converted into a list of strings\n        :return: List of dependencies, ``None`` if there are none.\n        \"\"\"\n\n        if not self.apt_dependencies:\n            return None\n\n        try:\n            dependencies = list([str(x).strip() for x in self.apt_dependencies])\n        except (TypeError, ValueError):\n            raise ArcaMisconfigured(\"Apk dependencies can't be converted into a list of strings\")\n\n        if not len(dependencies):\n            return None\n\n        dependencies.sort()\n\n        return dependencies","return_type":"Optional[List[str]]","function_name":"DockerBackend.get_dependencies","stripped_code":"def get_dependencies(self):\n        \"\"\" Returns the ``apt_dependencies`` setting to a standardized format.\n\n        :raise ArcaMisconfigured: if the dependencies can't be converted into a list of strings\n        :return: List of dependencies, ``None`` if there are none.\n        \"\"\"\n\n        if not self.apt_dependencies:\n            return None\n\n        try:\n            dependencies = list([str(x).strip() for x in self.apt_dependencies])\n        except (TypeError, ValueError):\n            raise ArcaMisconfigured(\"Apk dependencies can't be converted into a list of strings\")\n\n        if not len(dependencies):\n            return None\n\n        dependencies.sort()\n\n        return dependencies"}
{"code":"def time_to_str(timestamp: int) -> str:\n    \"\"\"\n    Convert seconds past Epoch to human readable string.\n    \"\"\"\n    datetimestamp = datetime.datetime.fromtimestamp(timestamp)\n    return '{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}'.format(\n            datetimestamp.year, datetimestamp.month, datetimestamp.day,\n            datetimestamp.hour, datetimestamp.minute, datetimestamp.second\n    )","return_type":"str","function_name":"time_to_str","stripped_code":"def time_to_str(timestamp: int):\n    \"\"\"\n    Convert seconds past Epoch to human readable string.\n    \"\"\"\n    datetimestamp = datetime.datetime.fromtimestamp(timestamp)\n    return '{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}'.format(\n            datetimestamp.year, datetimestamp.month, datetimestamp.day,\n            datetimestamp.hour, datetimestamp.minute, datetimestamp.second\n    )"}
{"code":"def gen_slot_variables(self, cls: ClassDefinition) -> str:\n        \"\"\" Generate python definition for class cls, generating primary keys first followed by the rest of the slots\n        \"\"\"\n        return '\\n\\t'.join([self.gen_slot_variable(cls, pk) for pk in self.primary_keys_for(cls)] +\n                           [self.gen_slot_variable(cls, slot)\n                            for slot in cls.slots\n                            if not self.schema.slots[slot].primary_key and not self.schema.slots[slot].identifier])","return_type":"str","function_name":"PythonGenerator.gen_slot_variables","stripped_code":"def gen_slot_variables(self, cls: ClassDefinition):\n        \"\"\" Generate python definition for class cls, generating primary keys first followed by the rest of the slots\n        \"\"\"\n        return '\\n\\t'.join([self.gen_slot_variable(cls, pk) for pk in self.primary_keys_for(cls)] +\n                           [self.gen_slot_variable(cls, slot)\n                            for slot in cls.slots\n                            if not self.schema.slots[slot].primary_key and not self.schema.slots[slot].identifier])"}
{"code":"def load(cls,\n             config: Params,\n             serialization_dir: str,\n             weights_file: str = None,\n             cuda_device: int = -1) -> 'Model':\n        \"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n\n        Parameters\n        ----------\n        config: Params\n            The configuration that was used to train the model. It should definitely\n            have a `model` section, and should probably have a `trainer` section\n            as well.\n        serialization_dir: str = None\n            The directory containing the serialized weights, parameters, and vocabulary\n            of the model.\n        weights_file: str = None\n            By default we load the weights from `best.th` in the serialization\n            directory, but you can override that value here.\n        cuda_device: int = -1\n            By default we load the model on the CPU, but if you want to load it\n            for GPU usage you can specify the id of your GPU here\n\n\n        Returns\n        -------\n        model: Model\n            The model specified in the configuration, loaded with the serialized\n            vocabulary and the trained weights.\n        \"\"\"\n\n        # Peak at the class of the model.\n        model_type = config[\"model\"][\"type\"]\n\n        # Load using an overridable _load method.\n        # This allows subclasses of Model to override _load.\n        # pylint: disable=protected-access\n        return cls.by_name(model_type)._load(config, serialization_dir, weights_file, cuda_device)","return_type":"'Model'","function_name":"Model.load","stripped_code":"def load(cls,\n             config: Params,\n             serialization_dir: str,\n             weights_file: str = None,\n             cuda_device: int = -1):\n        \"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n\n        Parameters\n        ----------\n        config: Params\n            The configuration that was used to train the model. It should definitely\n            have a `model` section, and should probably have a `trainer` section\n            as well.\n        serialization_dir: str = None\n            The directory containing the serialized weights, parameters, and vocabulary\n            of the model.\n        weights_file: str = None\n            By default we load the weights from `best.th` in the serialization\n            directory, but you can override that value here.\n        cuda_device: int = -1\n            By default we load the model on the CPU, but if you want to load it\n            for GPU usage you can specify the id of your GPU here\n\n\n        Returns\n        -------\n        model: Model\n            The model specified in the configuration, loaded with the serialized\n            vocabulary and the trained weights.\n        \"\"\"\n\n        # Peak at the class of the model.\n        model_type = config[\"model\"][\"type\"]\n\n        # Load using an overridable _load method.\n        # This allows subclasses of Model to override _load.\n        # pylint: disable=protected-access\n        return cls.by_name(model_type)._load(config, serialization_dir, weights_file, cuda_device)"}
{"code":"def entropy_bits(\n            lst: Union[\n                List[Union[int, str, float, complex]],\n                Tuple[Union[int, str, float, complex]]\n            ]\n    ) -> float:\n        \"\"\"Calculate the entropy of a wordlist or a numerical range.\n\n        Keyword arguments:\n        lst -- A wordlist as list or tuple, or a numerical range as a list:\n        (minimum, maximum)\n\n        \"\"\"\n        if not isinstance(lst, (tuple, list)):\n            raise TypeError('lst must be a list or a tuple')\n\n        size = len(lst)\n        if (\n                size == 2\n                and isinstance(lst[0], (int, float))\n                and isinstance(lst[1], (int, float))\n        ):\n            return calc_entropy_bits_nrange(lst[0], lst[1])\n\n        return calc_entropy_bits(lst)","return_type":"float","function_name":"Passphrase.entropy_bits","stripped_code":"def entropy_bits(\n            lst: Union[\n                List[Union[int, str, float, complex]],\n                Tuple[Union[int, str, float, complex]]\n            ]\n    ):\n        \"\"\"Calculate the entropy of a wordlist or a numerical range.\n\n        Keyword arguments:\n        lst -- A wordlist as list or tuple, or a numerical range as a list:\n        (minimum, maximum)\n\n        \"\"\"\n        if not isinstance(lst, (tuple, list)):\n            raise TypeError('lst must be a list or a tuple')\n\n        size = len(lst)\n        if (\n                size == 2\n                and isinstance(lst[0], (int, float))\n                and isinstance(lst[1], (int, float))\n        ):\n            return calc_entropy_bits_nrange(lst[0], lst[1])\n\n        return calc_entropy_bits(lst)"}
{"code":"def get_name_as_short_text(cls, name_field: cryptography.x509.Name) -> str:\n        \"\"\"Convert a name field returned by the cryptography module to a string suitable for displaying it to the user.\n        \"\"\"\n        # Name_field is supposed to be a Subject or an Issuer; print the CN if there is one\n        common_names = cls.get_common_names(name_field)\n        if common_names:\n            # We don't support certs with multiple CNs\n            return common_names[0]\n        else:\n            # Otherwise show the whole field\n            return cls.get_name_as_text(name_field)","return_type":"str","function_name":"CertificateUtils.get_name_as_short_text","stripped_code":"def get_name_as_short_text(cls, name_field: cryptography.x509.Name):\n        \"\"\"Convert a name field returned by the cryptography module to a string suitable for displaying it to the user.\n        \"\"\"\n        # Name_field is supposed to be a Subject or an Issuer; print the CN if there is one\n        common_names = cls.get_common_names(name_field)\n        if common_names:\n            # We don't support certs with multiple CNs\n            return common_names[0]\n        else:\n            # Otherwise show the whole field\n            return cls.get_name_as_text(name_field)"}
{"code":"def traceplot(trace: sample_types, labels: List[Union[str, Tuple[str, str]]] = None, ax: Any = None,\n              x0: int = 0) -> Any:\n    \"\"\"\n    Plot samples values.\n\n    :param trace:  result of MCMC run\n    :param labels: labels of vertices to be plotted. if None, all vertices are plotted.\n    :param ax: Matplotlib axes\n    :param x0: index of first data point, used for sample stream plots\n    \"\"\"\n\n    if labels is None:\n        labels = list(trace.keys())\n\n    if ax is None:\n        _, ax = plt.subplots(len(labels), 1, squeeze=False)\n\n    for index, label in enumerate(labels):\n        data = [sample for sample in trace[label]]\n\n        ax[index][0].set_title(label)\n        ax[index][0].plot(__integer_xaxis(ax[index][0], x0, len(data)), data)\n\n    __pause_for_crude_animation()\n\n    return ax","return_type":"Any","function_name":"traceplot","stripped_code":"def traceplot(trace: sample_types, labels: List[Union[str, Tuple[str, str]]] = None, ax: Any = None,\n              x0: int = 0):\n    \"\"\"\n    Plot samples values.\n\n    :param trace:  result of MCMC run\n    :param labels: labels of vertices to be plotted. if None, all vertices are plotted.\n    :param ax: Matplotlib axes\n    :param x0: index of first data point, used for sample stream plots\n    \"\"\"\n\n    if labels is None:\n        labels = list(trace.keys())\n\n    if ax is None:\n        _, ax = plt.subplots(len(labels), 1, squeeze=False)\n\n    for index, label in enumerate(labels):\n        data = [sample for sample in trace[label]]\n\n        ax[index][0].set_title(label)\n        ax[index][0].plot(__integer_xaxis(ax[index][0], x0, len(data)), data)\n\n    __pause_for_crude_animation()\n\n    return ax"}
{"code":"def _write_max_norm(self, norms:[])->None:\n        \"Writes the maximum norm of the gradients to Tensorboard.\"\n        max_norm = max(norms)\n        self._add_gradient_scalar('max_norm', scalar_value=max_norm)","return_type":"None","function_name":"ModelStatsTBRequest._write_max_norm","stripped_code":"def _write_max_norm(self, norms:[]):\n        \"Writes the maximum norm of the gradients to Tensorboard.\"\n        max_norm = max(norms)\n        self._add_gradient_scalar('max_norm', scalar_value=max_norm)"}
{"code":"def overwrite(char_list: List[str], regexp: str, quality: str, offset: int = 0) -> List[str]:\n    \"\"\"\n    Given a list of characters and spaces, a matching regular expression, and a quality or\n    character, replace the matching character with a space, overwriting with an offset and\n    a multiplier if provided.\n\n    :param char_list:\n    :param regexp:\n    :param quality:\n    :param offset:\n    :return:\n\n    >>> overwrite(list('multe igne'), r'e\\s[aeiou]', ' ')\n    ['m', 'u', 'l', 't', ' ', ' ', 'i', 'g', 'n', 'e']\n    \"\"\"\n    long_matcher = re.compile(regexp)\n    line = \"\".join(char_list)\n    long_positions = long_matcher.finditer(line)\n    for match in long_positions:\n        (start, end) = match.span()  # pylint: disable=unused-variable\n        char_list[start + offset] = quality\n    return char_list","return_type":"List[str]","function_name":"overwrite","stripped_code":"def overwrite(char_list: List[str], regexp: str, quality: str, offset: int = 0):\n    \"\"\"\n    Given a list of characters and spaces, a matching regular expression, and a quality or\n    character, replace the matching character with a space, overwriting with an offset and\n    a multiplier if provided.\n\n    :param char_list:\n    :param regexp:\n    :param quality:\n    :param offset:\n    :return:\n\n    >>> overwrite(list('multe igne'), r'e\\s[aeiou]', ' ')\n    ['m', 'u', 'l', 't', ' ', ' ', 'i', 'g', 'n', 'e']\n    \"\"\"\n    long_matcher = re.compile(regexp)\n    line = \"\".join(char_list)\n    long_positions = long_matcher.finditer(line)\n    for match in long_positions:\n        (start, end) = match.span()  # pylint: disable=unused-variable\n        char_list[start + offset] = quality\n    return char_list"}
{"code":"def squad_v2_exact_match(y_true: List[List[str]], y_predicted: List[str]) -> float:\n    \"\"\" Calculates Exact Match score between y_true and y_predicted\n        EM score uses the best matching y_true answer:\n            if y_pred equal at least to one answer in y_true then EM = 1, else EM = 0\n\n    The same as in SQuAD-v2.0\n\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n\n    Returns:\n        exact match score : float\n    \"\"\"\n    EM_total = sum(normalize_answer(prediction) in map(normalize_answer, ground_truth)\n                   for ground_truth, prediction in zip(y_true, y_predicted))\n    return 100 * EM_total / len(y_true) if len(y_true) > 0 else 0","return_type":"float","function_name":"squad_v2_exact_match","stripped_code":"def squad_v2_exact_match(y_true: List[List[str]], y_predicted: List[str]):\n    \"\"\" Calculates Exact Match score between y_true and y_predicted\n        EM score uses the best matching y_true answer:\n            if y_pred equal at least to one answer in y_true then EM = 1, else EM = 0\n\n    The same as in SQuAD-v2.0\n\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n\n    Returns:\n        exact match score : float\n    \"\"\"\n    EM_total = sum(normalize_answer(prediction) in map(normalize_answer, ground_truth)\n                   for ground_truth, prediction in zip(y_true, y_predicted))\n    return 100 * EM_total / len(y_true) if len(y_true) > 0 else 0"}
{"code":"def validate_conventions(self, ds: loompy.LoomConnection) -> bool:\n\t\t\"\"\"\n\t\tValidate the LoomConnection object against the attribute name/dtype conventions.\n\n\t\tArgs:\n\t\t\tds:\t\t\tLoomConnection object\n\t\t\n\t\tReturns:\n\t\t\tTrue if the file conforms to the conventions, else False\n\t\t\n\t\tRemarks:\n\t\t\tUpon return, the instance attributes 'self.errors' and 'self.warnings' contain\n\t\t\tlists of errors and warnings.\n\t\t\"\"\"\n\t\t(n_genes, n_cells) = ds.shape\n\n\t\tself._warn(\"Description\" in ds.attrs, \"Optional global attribute 'Description' is missing\")\n\t\tself._warn(\"Journal\" in ds.attrs, \"Optional global attribute 'Journal' is missing\")\n\t\tself._warn(\"Authors\" in ds.attrs, \"Optional global attribute 'Authors' is missing\")\n\t\tself._warn(\"Title\" in ds.attrs, \"Optional global attribute 'Title' is missing\")\n\t\tself._warn(\"Year\" in ds.attrs, \"Optional global attribute 'Year' is missing\")\n\t\tself._warn(\"CreationDate\" in ds.attrs, \"Optional global attribute 'CreationDate' is missing\")\n\n\t\tif self._check(\"ClusterID\" in ds.ca, \"Column attribute 'ClusterID' is missing\"):\n\t\t\tself._check(np.issubdtype(ds.ca.ClusterID.dtype, np.int_), \"Column attribute 'ClusterID' must be integer dtype\")\n\t\t\tself._check(len(np.unique(ds.ca.ClusterID)) == np.max(ds.ca.ClusterID) and np.min(ds.ca.ClusterID) == 0, \"Column attribute 'ClusterID' must be integers 0, 1, 2, ... with no missing values\")\n\t\t\tself._check(ds.ca.ClusterID.shape == (n_cells,), f\"Column attribute 'ClusterID' must be 1-dimensional array of {n_cells} elements\")\n\n\t\tif \"ClusterName\" in ds.ca:\n\t\t\tself._check(ds.ca.ClusterName.dtype == object and np.issubdtype(ds.ca.ClusterName[0].dtype, np.str_), \"Column attribute 'ClusterName' must be an array of strings\")\n\t\t\tself._check(ds.ca.ClusterName.shape == (n_cells,), f\"Column attribute 'ClusterName' must be 1-dimensional array of {n_cells} elements\")\n\t\t\tone_to_one = True\n\t\t\tfor cid in np.unique(ds.ca.ClusterID):\n\t\t\t\tif len(np.unique(ds.ca.ClusterName[ds.ca.ClusterID == cid])) != 1:\n\t\t\t\t\tone_to_one = False\n\t\t\t\t\tbreak\n\t\t\tfor cn in np.unique(ds.ca.ClusterName):\n\t\t\t\tif len(np.unique(ds.ca.ClusterID[ds.ca.ClusterName == cn])) != 1:\n\t\t\t\t\tone_to_one = False\n\t\t\t\t\tbreak\n\t\t\tif not one_to_one:\n\t\t\t\tself._check(False, \"ClusterName must correspond 1:1 with ClusterID\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'ClusterName' is missing\")\n\n\t\tif self._check(\"CellID\" in ds.ca, \"Column attribute 'CellID' is missing\"):\n\t\t\tself._check(ds.ca.CellID.dtype == object and np.issubdtype(ds.ca.CellID[0].dtype, np.str_), f\"Column attribute 'CellID' must be an array of strings, not '{ds.ca.CellID[0].dtype}'\")\n\t\t\tself._check(ds.ca.CellID.shape == (n_cells,), f\"Column attribute 'CellID' must be 1-dimensional array of {n_cells} elements\")\n\t\t\tself._check(len(np.unique(ds.ca.CellID)) == n_cells, \"Column attribute 'CellID' cannot contain duplicate values\")\n\n\t\tif \"Valid\" in ds.ca:\n\t\t\tself._check(np.issubdtype(ds.ca.Valid.dtype, np.int_), f\"Column attribute 'Valid' must be integer dtype, not '{ds.ca.Valid.dtype}'\")\n\t\t\tvalids = np.unique(ds.ca.Valid)\n\t\t\tself._check(np.all(np.isin(ds.ca.Valid, [0, 1])), \"Column attribute 'Valid' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ca.Valid.shape == (n_cells,), f\"Column attribute 'Valid' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'Valid' is missing\")\n\n\t\tif \"Outliers\" in ds.ca:\n\t\t\tself._check(np.issubdtype(ds.ca.Outliers.dtype, np.int_), f\"Column attribute 'Outliers' must be integer dtype, not '{ds.ca.Outliers.dtype}'\")\n\t\t\tself._check(np.all(np.isin(ds.ca.Outliers, [0, 1])), \"Column attribute 'Outliers' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ca.Outliers.shape == (n_cells,), f\"Column attribute 'Outliers' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'Outliers' is missing\")\n\n\t\tif self._check(\"Accession\" in ds.ra, \"Row attribute 'Accession' is missing\"):\n\t\t\tself._check(ds.ra.Accession.dtype == object and np.issubdtype(ds.ra.Accession[0].dtype, np.str_), f\"Row attribute 'Accession' must be an array of strings, not '{ds.ra.Accession[0].dtype}'\")\n\t\t\tself._check(ds.ra.Accession.shape == (n_genes,), f\"Row attribute 'Accession' must be 1-dimensional array of {n_genes} elements\")\n\t\t\tself._check(len(np.unique(ds.ra.Accession)) == n_genes, \"Row attribute 'Accession' cannot contain duplicate values\")\n\n\t\tif self._check(\"Gene\" in ds.ra, \"Row attribute 'Gene' is missing\"):\n\t\t\tself._check(ds.ra.Gene.dtype == object and np.issubdtype(ds.ra.Gene[0].dtype, np.str_), f\"Row attribute 'Gene' must be an array of strings, not '{ds.ra.Gene[0].dtype}'\")\n\t\t\tself._check(ds.ra.Gene.shape == (n_genes,), f\"Row attribute 'Gene' must be 1-dimensional array of {n_genes} elements\")\n\n\t\tif \"Valid\" in ds.ra:\n\t\t\tself._check(np.issubdtype(ds.ra.Valid.dtype, np.int_), f\"Row attribute 'Valid' must be integer dtype, not '{ds.ra.Valid.dtype}'\")\n\t\t\tvalids = np.unique(ds.ra.Valid)\n\t\t\tself._check(np.all(np.isin(ds.ra.Valid, [0, 1])), \"Row attribute 'Valid' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ra.Valid.shape == (n_cells,), f\"Row attribute 'Valid' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional row attribute 'Valid' is missing\")\n\n\t\tif \"Selected\" in ds.ra:\n\t\t\tself._check(np.issubdtype(ds.ra.Selected.dtype, np.int_), f\"Row attribute 'Selected' must be integer dtype, not '{ds.ra.Selected.dtype}'\")\n\t\t\tvalids = np.unique(ds.ra.Selected)\n\t\t\tself._check(np.all(np.isin(ds.ra.Selected, [0, 1])), \"Row attribute 'Selected' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ra.Selected.shape == (n_cells,), f\"Row attribute 'Selected' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional row attribute 'Selected' is missing\")\n\n\t\treturn len(self.errors) == 0","return_type":"bool","function_name":"LoomValidator.validate_conventions","stripped_code":"def validate_conventions(self, ds: loompy.LoomConnection):\n\t\t\"\"\"\n\t\tValidate the LoomConnection object against the attribute name/dtype conventions.\n\n\t\tArgs:\n\t\t\tds:\t\t\tLoomConnection object\n\t\t\n\t\tReturns:\n\t\t\tTrue if the file conforms to the conventions, else False\n\t\t\n\t\tRemarks:\n\t\t\tUpon return, the instance attributes 'self.errors' and 'self.warnings' contain\n\t\t\tlists of errors and warnings.\n\t\t\"\"\"\n\t\t(n_genes, n_cells) = ds.shape\n\n\t\tself._warn(\"Description\" in ds.attrs, \"Optional global attribute 'Description' is missing\")\n\t\tself._warn(\"Journal\" in ds.attrs, \"Optional global attribute 'Journal' is missing\")\n\t\tself._warn(\"Authors\" in ds.attrs, \"Optional global attribute 'Authors' is missing\")\n\t\tself._warn(\"Title\" in ds.attrs, \"Optional global attribute 'Title' is missing\")\n\t\tself._warn(\"Year\" in ds.attrs, \"Optional global attribute 'Year' is missing\")\n\t\tself._warn(\"CreationDate\" in ds.attrs, \"Optional global attribute 'CreationDate' is missing\")\n\n\t\tif self._check(\"ClusterID\" in ds.ca, \"Column attribute 'ClusterID' is missing\"):\n\t\t\tself._check(np.issubdtype(ds.ca.ClusterID.dtype, np.int_), \"Column attribute 'ClusterID' must be integer dtype\")\n\t\t\tself._check(len(np.unique(ds.ca.ClusterID)) == np.max(ds.ca.ClusterID) and np.min(ds.ca.ClusterID) == 0, \"Column attribute 'ClusterID' must be integers 0, 1, 2, ... with no missing values\")\n\t\t\tself._check(ds.ca.ClusterID.shape == (n_cells,), f\"Column attribute 'ClusterID' must be 1-dimensional array of {n_cells} elements\")\n\n\t\tif \"ClusterName\" in ds.ca:\n\t\t\tself._check(ds.ca.ClusterName.dtype == object and np.issubdtype(ds.ca.ClusterName[0].dtype, np.str_), \"Column attribute 'ClusterName' must be an array of strings\")\n\t\t\tself._check(ds.ca.ClusterName.shape == (n_cells,), f\"Column attribute 'ClusterName' must be 1-dimensional array of {n_cells} elements\")\n\t\t\tone_to_one = True\n\t\t\tfor cid in np.unique(ds.ca.ClusterID):\n\t\t\t\tif len(np.unique(ds.ca.ClusterName[ds.ca.ClusterID == cid])) != 1:\n\t\t\t\t\tone_to_one = False\n\t\t\t\t\tbreak\n\t\t\tfor cn in np.unique(ds.ca.ClusterName):\n\t\t\t\tif len(np.unique(ds.ca.ClusterID[ds.ca.ClusterName == cn])) != 1:\n\t\t\t\t\tone_to_one = False\n\t\t\t\t\tbreak\n\t\t\tif not one_to_one:\n\t\t\t\tself._check(False, \"ClusterName must correspond 1:1 with ClusterID\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'ClusterName' is missing\")\n\n\t\tif self._check(\"CellID\" in ds.ca, \"Column attribute 'CellID' is missing\"):\n\t\t\tself._check(ds.ca.CellID.dtype == object and np.issubdtype(ds.ca.CellID[0].dtype, np.str_), f\"Column attribute 'CellID' must be an array of strings, not '{ds.ca.CellID[0].dtype}'\")\n\t\t\tself._check(ds.ca.CellID.shape == (n_cells,), f\"Column attribute 'CellID' must be 1-dimensional array of {n_cells} elements\")\n\t\t\tself._check(len(np.unique(ds.ca.CellID)) == n_cells, \"Column attribute 'CellID' cannot contain duplicate values\")\n\n\t\tif \"Valid\" in ds.ca:\n\t\t\tself._check(np.issubdtype(ds.ca.Valid.dtype, np.int_), f\"Column attribute 'Valid' must be integer dtype, not '{ds.ca.Valid.dtype}'\")\n\t\t\tvalids = np.unique(ds.ca.Valid)\n\t\t\tself._check(np.all(np.isin(ds.ca.Valid, [0, 1])), \"Column attribute 'Valid' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ca.Valid.shape == (n_cells,), f\"Column attribute 'Valid' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'Valid' is missing\")\n\n\t\tif \"Outliers\" in ds.ca:\n\t\t\tself._check(np.issubdtype(ds.ca.Outliers.dtype, np.int_), f\"Column attribute 'Outliers' must be integer dtype, not '{ds.ca.Outliers.dtype}'\")\n\t\t\tself._check(np.all(np.isin(ds.ca.Outliers, [0, 1])), \"Column attribute 'Outliers' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ca.Outliers.shape == (n_cells,), f\"Column attribute 'Outliers' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional column attribute 'Outliers' is missing\")\n\n\t\tif self._check(\"Accession\" in ds.ra, \"Row attribute 'Accession' is missing\"):\n\t\t\tself._check(ds.ra.Accession.dtype == object and np.issubdtype(ds.ra.Accession[0].dtype, np.str_), f\"Row attribute 'Accession' must be an array of strings, not '{ds.ra.Accession[0].dtype}'\")\n\t\t\tself._check(ds.ra.Accession.shape == (n_genes,), f\"Row attribute 'Accession' must be 1-dimensional array of {n_genes} elements\")\n\t\t\tself._check(len(np.unique(ds.ra.Accession)) == n_genes, \"Row attribute 'Accession' cannot contain duplicate values\")\n\n\t\tif self._check(\"Gene\" in ds.ra, \"Row attribute 'Gene' is missing\"):\n\t\t\tself._check(ds.ra.Gene.dtype == object and np.issubdtype(ds.ra.Gene[0].dtype, np.str_), f\"Row attribute 'Gene' must be an array of strings, not '{ds.ra.Gene[0].dtype}'\")\n\t\t\tself._check(ds.ra.Gene.shape == (n_genes,), f\"Row attribute 'Gene' must be 1-dimensional array of {n_genes} elements\")\n\n\t\tif \"Valid\" in ds.ra:\n\t\t\tself._check(np.issubdtype(ds.ra.Valid.dtype, np.int_), f\"Row attribute 'Valid' must be integer dtype, not '{ds.ra.Valid.dtype}'\")\n\t\t\tvalids = np.unique(ds.ra.Valid)\n\t\t\tself._check(np.all(np.isin(ds.ra.Valid, [0, 1])), \"Row attribute 'Valid' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ra.Valid.shape == (n_cells,), f\"Row attribute 'Valid' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional row attribute 'Valid' is missing\")\n\n\t\tif \"Selected\" in ds.ra:\n\t\t\tself._check(np.issubdtype(ds.ra.Selected.dtype, np.int_), f\"Row attribute 'Selected' must be integer dtype, not '{ds.ra.Selected.dtype}'\")\n\t\t\tvalids = np.unique(ds.ra.Selected)\n\t\t\tself._check(np.all(np.isin(ds.ra.Selected, [0, 1])), \"Row attribute 'Selected' must be integers 0 or 1 only\")\n\t\t\tself._check(ds.ra.Selected.shape == (n_cells,), f\"Row attribute 'Selected' must be 1-dimensional array of {n_cells} elements\")\n\t\telse:\n\t\t\tself.warnings.append(\"Optional row attribute 'Selected' is missing\")\n\n\t\treturn len(self.errors) == 0"}
{"code":"def summary_str(self) -> str:\n        \"\"\"Return a string that summarizes the graph.\"\"\"\n        return '{}\\n'.format(self) + '\\n'.join(\n            '{}: {}'.format(label, value)\n            for label, value in self._describe_list()\n        )","return_type":"str","function_name":"BELGraph.summary_str","stripped_code":"def summary_str(self):\n        \"\"\"Return a string that summarizes the graph.\"\"\"\n        return '{}\\n'.format(self) + '\\n'.join(\n            '{}: {}'.format(label, value)\n            for label, value in self._describe_list()\n        )"}
{"code":"def raise_for_invalid_annotation_value(self, line: str, position: int, key: str, value: str) -> None:\n        \"\"\"Raise an exception if the annotation is not defined.\n\n        :raises: IllegalAnnotationValueWarning or MissingAnnotationRegexWarning\n        \"\"\"\n        if self._in_debug_mode:\n            return\n\n        if self.has_enumerated_annotation(key) and value not in self.annotation_to_term[key]:\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_regex_annotation(key) and not self.annotation_to_pattern[key].match(value):\n            raise MissingAnnotationRegexWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_local_annotation(key) and value not in self.annotation_to_local[key]:  # TODO condense\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)","return_type":"None","function_name":"ControlParser.raise_for_invalid_annotation_value","stripped_code":"def raise_for_invalid_annotation_value(self, line: str, position: int, key: str, value: str):\n        \"\"\"Raise an exception if the annotation is not defined.\n\n        :raises: IllegalAnnotationValueWarning or MissingAnnotationRegexWarning\n        \"\"\"\n        if self._in_debug_mode:\n            return\n\n        if self.has_enumerated_annotation(key) and value not in self.annotation_to_term[key]:\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_regex_annotation(key) and not self.annotation_to_pattern[key].match(value):\n            raise MissingAnnotationRegexWarning(self.get_line_number(), line, position, key, value)\n\n        elif self.has_local_annotation(key) and value not in self.annotation_to_local[key]:  # TODO condense\n            raise IllegalAnnotationValueWarning(self.get_line_number(), line, position, key, value)"}
{"code":"def uidump(self, local: _PATH = None) -> None:\n        '''Get the current interface layout file.'''\n        local = local if local else self._temp\n        self._execute('-s', self.device_sn, 'shell', 'uiautomator',\n                      'dump', '--compressed', '/data/local/tmp/uidump.xml')\n        self.pull('/data/local/tmp/uidump.xml', local)\n        ui = html.fromstring(open(local, 'rb').read())\n        self._nodes = ui.iter(tag=\"node\")","return_type":"None","function_name":"BaseAndroidDriver.uidump","stripped_code":"def uidump(self, local: _PATH = None):\n        '''Get the current interface layout file.'''\n        local = local if local else self._temp\n        self._execute('-s', self.device_sn, 'shell', 'uiautomator',\n                      'dump', '--compressed', '/data/local/tmp/uidump.xml')\n        self.pull('/data/local/tmp/uidump.xml', local)\n        ui = html.fromstring(open(local, 'rb').read())\n        self._nodes = ui.iter(tag=\"node\")"}
{"code":"def extract(self, tokens: List[Token]) -> List[Extraction]:\n        \"\"\"\n        Extracts information from a string(TEXT) with the GlossaryExtractor instance\n\n        Args:\n            token (List[Token]): list of spaCy token to be processed.\n\n        Returns:\n            List[Extraction]: the list of extraction or the empty list if there are no matches.\n\n        \"\"\"\n        results = list()\n\n        if len(tokens) > 0:\n            if self._case_sensitive:\n                new_tokens = [x.orth_ if isinstance(x, Token) else x for x in tokens]\n            else:\n                new_tokens = [x.lower_ if isinstance(x, Token) else x.lower() for x in tokens]\n        else:\n            return results\n\n        try:\n            ngrams_iter = self._generate_ngrams_with_context(new_tokens)\n            results.extend(map(lambda term: self._wrap_value_with_context(tokens, term[1], term[2]),\n                               filter(lambda term: isinstance(term[0], str),\n                                      map(lambda term: (self._glossary.get(term[0]), term[1], term[2]),\n                                          map(lambda term: (\n                                              self._combine_ngrams(term[0], self._joiner), term[1], term[2]),\n                                              ngrams_iter)))))\n        except Exception as e:\n            raise ExtractorError('GlossaryExtractor: Failed to extract with ' + self.name + '. Catch ' + str(e) + '. ')\n        return results","return_type":"List[Extraction]","function_name":"GlossaryExtractor.extract","stripped_code":"def extract(self, tokens: List[Token]):\n        \"\"\"\n        Extracts information from a string(TEXT) with the GlossaryExtractor instance\n\n        Args:\n            token (List[Token]): list of spaCy token to be processed.\n\n        Returns:\n            List[Extraction]: the list of extraction or the empty list if there are no matches.\n\n        \"\"\"\n        results = list()\n\n        if len(tokens) > 0:\n            if self._case_sensitive:\n                new_tokens = [x.orth_ if isinstance(x, Token) else x for x in tokens]\n            else:\n                new_tokens = [x.lower_ if isinstance(x, Token) else x.lower() for x in tokens]\n        else:\n            return results\n\n        try:\n            ngrams_iter = self._generate_ngrams_with_context(new_tokens)\n            results.extend(map(lambda term: self._wrap_value_with_context(tokens, term[1], term[2]),\n                               filter(lambda term: isinstance(term[0], str),\n                                      map(lambda term: (self._glossary.get(term[0]), term[1], term[2]),\n                                          map(lambda term: (\n                                              self._combine_ngrams(term[0], self._joiner), term[1], term[2]),\n                                              ngrams_iter)))))\n        except Exception as e:\n            raise ExtractorError('GlossaryExtractor: Failed to extract with ' + self.name + '. Catch ' + str(e) + '. ')\n        return results"}
{"code":"def _initialize(self,\n                    provide_data: List[mx.io.DataDesc],\n                    provide_label: List[mx.io.DataDesc],\n                    default_bucket_key: Tuple[int, int]) -> None:\n        \"\"\"\n        Initializes model components, creates scoring symbol and module, and binds it.\n\n        :param provide_data: List of data descriptors.\n        :param provide_label: List of label descriptors.\n        :param default_bucket_key: The default maximum (source, target) lengths.\n        \"\"\"\n        source = mx.sym.Variable(C.SOURCE_NAME)\n        source_words = source.split(num_outputs=self.config.config_embed_source.num_factors,\n                                    axis=2, squeeze_axis=True)[0]\n        source_length = utils.compute_lengths(source_words)\n        target = mx.sym.Variable(C.TARGET_NAME)\n        target_length = utils.compute_lengths(target)\n\n        # labels shape: (batch_size, target_length) (usually the maximum target sequence length)\n        labels = mx.sym.Variable(C.TARGET_LABEL_NAME)\n\n        data_names = [C.SOURCE_NAME, C.TARGET_NAME]\n        label_names = [C.TARGET_LABEL_NAME]\n\n        # check provide_{data,label} names\n        provide_data_names = [d[0] for d in provide_data]\n        utils.check_condition(provide_data_names == data_names,\n                              \"incompatible provide_data: %s, names should be %s\" % (provide_data_names, data_names))\n        provide_label_names = [d[0] for d in provide_label]\n        utils.check_condition(provide_label_names == label_names,\n                              \"incompatible provide_label: %s, names should be %s\" % (provide_label_names, label_names))\n\n        def sym_gen(seq_lens):\n            \"\"\"\n            Returns a (grouped) symbol containing the summed score for each sentence, as well as the entire target\n            distributions for each word.\n            Also returns data and label names for the BucketingModule.\n            \"\"\"\n            source_seq_len, target_seq_len = seq_lens\n\n            # source embedding\n            (source_embed,\n             source_embed_length,\n             source_embed_seq_len) = self.embedding_source.encode(source, source_length, source_seq_len)\n\n            # target embedding\n            (target_embed,\n             target_embed_length,\n             target_embed_seq_len) = self.embedding_target.encode(target, target_length, target_seq_len)\n\n            # encoder\n            # source_encoded: (batch_size, source_encoded_length, encoder_depth)\n            (source_encoded,\n             source_encoded_length,\n             source_encoded_seq_len) = self.encoder.encode(source_embed,\n                                                           source_embed_length,\n                                                           source_embed_seq_len)\n\n            # decoder\n            # target_decoded: (batch-size, target_len, decoder_depth)\n            target_decoded = self.decoder.decode_sequence(source_encoded, source_encoded_length, source_encoded_seq_len,\n                                                          target_embed, target_embed_length, target_embed_seq_len)\n\n            # output layer\n            # logits: (batch_size * target_seq_len, target_vocab_size)\n            logits = self.output_layer(mx.sym.reshape(data=target_decoded, shape=(-3, 0)))\n            # logits after reshape: (batch_size, target_seq_len, target_vocab_size)\n            logits = mx.sym.reshape(data=logits, shape=(-4, -1, target_embed_seq_len, 0))\n\n            if self.softmax_temperature is not None:\n                logits = logits / self.softmax_temperature\n\n            # Compute the softmax along the final dimension.\n            # target_dists: (batch_size, target_seq_len, target_vocab_size)\n            target_dists = mx.sym.softmax(data=logits, axis=2, name=C.SOFTMAX_NAME)\n\n            # Select the label probability, then take their logs.\n            # probs and scores: (batch_size, target_seq_len)\n            probs = mx.sym.pick(target_dists, labels)\n            scores = mx.sym.log(probs)\n            if self.score_type == C.SCORING_TYPE_NEGLOGPROB:\n                scores = -1 * scores\n\n            # Sum, then apply length penalty. The call to `mx.sym.where` masks out invalid values from scores.\n            # zeros and sums: (batch_size,)\n            zeros = mx.sym.zeros_like(scores)\n            sums = mx.sym.sum(mx.sym.where(labels != 0, scores, zeros), axis=1) / (self.length_penalty(target_length - 1))\n\n            # Deal with the potential presence of brevity penalty\n            # length_ratio: (batch_size,)\n            if self.constant_length_ratio > 0.0:\n                # override all ratios with the constant value\n                length_ratio = self.constant_length_ratio * mx.sym.ones_like(sums)\n            else:\n                # predict length ratio if supported\n                length_ratio = self.length_ratio(source_encoded, source_encoded_length).reshape((-1,)) \\\n                                    if self.length_ratio is not None else mx.sym.zeros_like(sums)\n            sums = sums - self.brevity_penalty(target_length - 1, length_ratio * source_encoded_length)\n\n            # Return the sums and the target distributions\n            # sums: (batch_size,) target_dists: (batch_size, target_seq_len, target_vocab_size)\n            return mx.sym.Group([sums, target_dists]), data_names, label_names\n\n        symbol, _, __ = sym_gen(default_bucket_key)\n        self.module = mx.mod.Module(symbol=symbol,\n                                    data_names=data_names,\n                                    label_names=label_names,\n                                    logger=logger,\n                                    context=self.context)\n\n        self.module.bind(data_shapes=provide_data,\n                         label_shapes=provide_label,\n                         for_training=False,\n                         force_rebind=False,\n                         grad_req='null')","return_type":"None","function_name":"ScoringModel._initialize","stripped_code":"def _initialize(self,\n                    provide_data: List[mx.io.DataDesc],\n                    provide_label: List[mx.io.DataDesc],\n                    default_bucket_key: Tuple[int, int]):\n        \"\"\"\n        Initializes model components, creates scoring symbol and module, and binds it.\n\n        :param provide_data: List of data descriptors.\n        :param provide_label: List of label descriptors.\n        :param default_bucket_key: The default maximum (source, target) lengths.\n        \"\"\"\n        source = mx.sym.Variable(C.SOURCE_NAME)\n        source_words = source.split(num_outputs=self.config.config_embed_source.num_factors,\n                                    axis=2, squeeze_axis=True)[0]\n        source_length = utils.compute_lengths(source_words)\n        target = mx.sym.Variable(C.TARGET_NAME)\n        target_length = utils.compute_lengths(target)\n\n        # labels shape: (batch_size, target_length) (usually the maximum target sequence length)\n        labels = mx.sym.Variable(C.TARGET_LABEL_NAME)\n\n        data_names = [C.SOURCE_NAME, C.TARGET_NAME]\n        label_names = [C.TARGET_LABEL_NAME]\n\n        # check provide_{data,label} names\n        provide_data_names = [d[0] for d in provide_data]\n        utils.check_condition(provide_data_names == data_names,\n                              \"incompatible provide_data: %s, names should be %s\" % (provide_data_names, data_names))\n        provide_label_names = [d[0] for d in provide_label]\n        utils.check_condition(provide_label_names == label_names,\n                              \"incompatible provide_label: %s, names should be %s\" % (provide_label_names, label_names))\n\n        def sym_gen(seq_lens):\n            \"\"\"\n            Returns a (grouped) symbol containing the summed score for each sentence, as well as the entire target\n            distributions for each word.\n            Also returns data and label names for the BucketingModule.\n            \"\"\"\n            source_seq_len, target_seq_len = seq_lens\n\n            # source embedding\n            (source_embed,\n             source_embed_length,\n             source_embed_seq_len) = self.embedding_source.encode(source, source_length, source_seq_len)\n\n            # target embedding\n            (target_embed,\n             target_embed_length,\n             target_embed_seq_len) = self.embedding_target.encode(target, target_length, target_seq_len)\n\n            # encoder\n            # source_encoded: (batch_size, source_encoded_length, encoder_depth)\n            (source_encoded,\n             source_encoded_length,\n             source_encoded_seq_len) = self.encoder.encode(source_embed,\n                                                           source_embed_length,\n                                                           source_embed_seq_len)\n\n            # decoder\n            # target_decoded: (batch-size, target_len, decoder_depth)\n            target_decoded = self.decoder.decode_sequence(source_encoded, source_encoded_length, source_encoded_seq_len,\n                                                          target_embed, target_embed_length, target_embed_seq_len)\n\n            # output layer\n            # logits: (batch_size * target_seq_len, target_vocab_size)\n            logits = self.output_layer(mx.sym.reshape(data=target_decoded, shape=(-3, 0)))\n            # logits after reshape: (batch_size, target_seq_len, target_vocab_size)\n            logits = mx.sym.reshape(data=logits, shape=(-4, -1, target_embed_seq_len, 0))\n\n            if self.softmax_temperature is not None:\n                logits = logits / self.softmax_temperature\n\n            # Compute the softmax along the final dimension.\n            # target_dists: (batch_size, target_seq_len, target_vocab_size)\n            target_dists = mx.sym.softmax(data=logits, axis=2, name=C.SOFTMAX_NAME)\n\n            # Select the label probability, then take their logs.\n            # probs and scores: (batch_size, target_seq_len)\n            probs = mx.sym.pick(target_dists, labels)\n            scores = mx.sym.log(probs)\n            if self.score_type == C.SCORING_TYPE_NEGLOGPROB:\n                scores = -1 * scores\n\n            # Sum, then apply length penalty. The call to `mx.sym.where` masks out invalid values from scores.\n            # zeros and sums: (batch_size,)\n            zeros = mx.sym.zeros_like(scores)\n            sums = mx.sym.sum(mx.sym.where(labels != 0, scores, zeros), axis=1) / (self.length_penalty(target_length - 1))\n\n            # Deal with the potential presence of brevity penalty\n            # length_ratio: (batch_size,)\n            if self.constant_length_ratio > 0.0:\n                # override all ratios with the constant value\n                length_ratio = self.constant_length_ratio * mx.sym.ones_like(sums)\n            else:\n                # predict length ratio if supported\n                length_ratio = self.length_ratio(source_encoded, source_encoded_length).reshape((-1,)) \\\n                                    if self.length_ratio is not None else mx.sym.zeros_like(sums)\n            sums = sums - self.brevity_penalty(target_length - 1, length_ratio * source_encoded_length)\n\n            # Return the sums and the target distributions\n            # sums: (batch_size,) target_dists: (batch_size, target_seq_len, target_vocab_size)\n            return mx.sym.Group([sums, target_dists]), data_names, label_names\n\n        symbol, _, __ = sym_gen(default_bucket_key)\n        self.module = mx.mod.Module(symbol=symbol,\n                                    data_names=data_names,\n                                    label_names=label_names,\n                                    logger=logger,\n                                    context=self.context)\n\n        self.module.bind(data_shapes=provide_data,\n                         label_shapes=provide_label,\n                         for_training=False,\n                         force_rebind=False,\n                         grad_req='null')"}
{"code":"def _subscribed_event_count(self) -> int:\n        \"\"\" Returns the total amount of subscribed events.\n\n        :return: Integer amount events.\n        :rtype: int\n        \"\"\"\n        event_counter = Counter()  # type: Dict[Any, int]\n\n        for key, values in self._events.items():\n            event_counter[key] = len(values)\n\n        return sum(event_counter.values())","return_type":"int","function_name":"EventBus._subscribed_event_count","stripped_code":"def _subscribed_event_count(self):\n        \"\"\" Returns the total amount of subscribed events.\n\n        :return: Integer amount events.\n        :rtype: int\n        \"\"\"\n        event_counter = Counter()  # type: Dict[Any, int]\n\n        for key, values in self._events.items():\n            event_counter[key] = len(values)\n\n        return sum(event_counter.values())"}
{"code":"def main() -> int:\n    \"\"\"\n    Utility to create and publish the Docker cache to Docker Hub\n    :return:\n    \"\"\"\n    # We need to be in the same directory than the script so the commands in the dockerfiles work as\n    # expected. But the script can be invoked from a different path\n    base = os.path.split(os.path.realpath(__file__))[0]\n    os.chdir(base)\n\n    logging.getLogger().setLevel(logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.INFO)\n    logging.getLogger('boto3').setLevel(logging.INFO)\n    logging.getLogger('urllib3').setLevel(logging.INFO)\n    logging.getLogger('s3transfer').setLevel(logging.INFO)\n\n    def script_name() -> str:\n        return os.path.split(sys.argv[0])[1]\n\n    logging.basicConfig(format='{}: %(asctime)-15s %(message)s'.format(script_name()))\n\n    parser = argparse.ArgumentParser(description=\"Utility for preserving and loading Docker cache\", epilog=\"\")\n    parser.add_argument(\"--docker-registry\",\n                        help=\"Docker hub registry name\",\n                        type=str,\n                        required=True)\n\n    args = parser.parse_args()\n\n    platforms = build_util.get_platforms()\n    try:\n        _login_dockerhub()\n        return build_save_containers(platforms=platforms, registry=args.docker_registry, load_cache=True)\n    finally:\n        _logout_dockerhub()","return_type":"int","function_name":"main","stripped_code":"def main():\n    \"\"\"\n    Utility to create and publish the Docker cache to Docker Hub\n    :return:\n    \"\"\"\n    # We need to be in the same directory than the script so the commands in the dockerfiles work as\n    # expected. But the script can be invoked from a different path\n    base = os.path.split(os.path.realpath(__file__))[0]\n    os.chdir(base)\n\n    logging.getLogger().setLevel(logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.INFO)\n    logging.getLogger('boto3').setLevel(logging.INFO)\n    logging.getLogger('urllib3').setLevel(logging.INFO)\n    logging.getLogger('s3transfer').setLevel(logging.INFO)\n\n    def script_name() -> str:\n        return os.path.split(sys.argv[0])[1]\n\n    logging.basicConfig(format='{}: %(asctime)-15s %(message)s'.format(script_name()))\n\n    parser = argparse.ArgumentParser(description=\"Utility for preserving and loading Docker cache\", epilog=\"\")\n    parser.add_argument(\"--docker-registry\",\n                        help=\"Docker hub registry name\",\n                        type=str,\n                        required=True)\n\n    args = parser.parse_args()\n\n    platforms = build_util.get_platforms()\n    try:\n        _login_dockerhub()\n        return build_save_containers(platforms=platforms, registry=args.docker_registry, load_cache=True)\n    finally:\n        _logout_dockerhub()"}
{"code":"def main() -> None:\n    \"\"\"Main entry point for console commands.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--json\",\n        help=\"output in JSON format\",\n        action=\"store_true\",\n        default=False)\n    parser.add_argument(\n        \"--config-file\", help=\"Select config file to use\", default=\".snekrc\")\n    parser.add_argument(\n        'files',\n        metavar='file',\n        nargs='*',\n        default=[],\n        help='Files to run checks against')\n    parser.add_argument(\n        \"--init\", help=\"generate snekrc\", action=\"store_true\", default=False)\n\n    args = parser.parse_args()\n\n    run_main(args)","return_type":"None","function_name":"main","stripped_code":"def main():\n    \"\"\"Main entry point for console commands.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--json\",\n        help=\"output in JSON format\",\n        action=\"store_true\",\n        default=False)\n    parser.add_argument(\n        \"--config-file\", help=\"Select config file to use\", default=\".snekrc\")\n    parser.add_argument(\n        'files',\n        metavar='file',\n        nargs='*',\n        default=[],\n        help='Files to run checks against')\n    parser.add_argument(\n        \"--init\", help=\"generate snekrc\", action=\"store_true\", default=False)\n\n    args = parser.parse_args()\n\n    run_main(args)"}
{"code":"def stop_gradient(cls, x: 'TensorFluent') -> 'TensorFluent':\n        '''Returns a copy of the input fluent with stop_gradient at tensor level.\n\n        Args:\n            x: The input fluent.\n\n        Returns:\n            A TensorFluent that stops backpropagation of gradient computations.\n        '''\n        scope = x.scope.as_list()\n        batch = x.batch\n        return TensorFluent(tf.stop_gradient(x.tensor), scope, batch)","return_type":"'TensorFluent'","function_name":"TensorFluent.stop_gradient","stripped_code":"def stop_gradient(cls, x: 'TensorFluent'):\n        '''Returns a copy of the input fluent with stop_gradient at tensor level.\n\n        Args:\n            x: The input fluent.\n\n        Returns:\n            A TensorFluent that stops backpropagation of gradient computations.\n        '''\n        scope = x.scope.as_list()\n        batch = x.batch\n        return TensorFluent(tf.stop_gradient(x.tensor), scope, batch)"}
{"code":"def message(self) -> str:\n        \"\"\"Return the message to print to the user.\"\"\"\n        ret = '{}: {}'.format(self.code, self.short_desc)\n        if self.context is not None:\n            specific_error_msg = self.context.format(*self.parameters)\n            ret += ' ({})'.format(specific_error_msg)\n        return ret","return_type":"str","function_name":"Error.message","stripped_code":"def message(self):\n        \"\"\"Return the message to print to the user.\"\"\"\n        ret = '{}: {}'.format(self.code, self.short_desc)\n        if self.context is not None:\n            specific_error_msg = self.context.format(*self.parameters)\n            ret += ' ({})'.format(specific_error_msg)\n        return ret"}
{"code":"def get_workflow(workflow_id: str, workflow_version: str) -> dict:\n    \"\"\"Get a workflow definition from the Configuration Database.\n\n    Args:\n        workflow_id (str): Workflow identifier\n        workflow_version (str): Workflow version\n\n    Returns:\n        dict, Workflow definition dictionary\n\n    \"\"\"\n    name = \"workflow_definitions:{}:{}\".format(workflow_id, workflow_version)\n    workflow = DB.get_hash_dict(name)\n    workflow['stages'] = ast.literal_eval(workflow['stages'])\n    return workflow","return_type":"dict","function_name":"get_workflow","stripped_code":"def get_workflow(workflow_id: str, workflow_version: str):\n    \"\"\"Get a workflow definition from the Configuration Database.\n\n    Args:\n        workflow_id (str): Workflow identifier\n        workflow_version (str): Workflow version\n\n    Returns:\n        dict, Workflow definition dictionary\n\n    \"\"\"\n    name = \"workflow_definitions:{}:{}\".format(workflow_id, workflow_version)\n    workflow = DB.get_hash_dict(name)\n    workflow['stages'] = ast.literal_eval(workflow['stages'])\n    return workflow"}
{"code":"def delimit(x: str, delims: Tuple[str, str]) -> str:\n    \"\"\"Delimits x, using delims[0] (left) and delims[1] (right).\"\"\"\n    return delims[0] + x + delims[1]","return_type":"str","function_name":"delimit","stripped_code":"def delimit(x: str, delims: Tuple[str, str]):\n    \"\"\"Delimits x, using delims[0] (left) and delims[1] (right).\"\"\"\n    return delims[0] + x + delims[1]"}
{"code":"def output_directory(self) -> str:\n        \"\"\"\n        Returns the directory where the project results files will be written\n        \"\"\"\n\n        return os.path.join(self.results_path, 'reports', self.uuid, 'latest')","return_type":"str","function_name":"Project.output_directory","stripped_code":"def output_directory(self):\n        \"\"\"\n        Returns the directory where the project results files will be written\n        \"\"\"\n\n        return os.path.join(self.results_path, 'reports', self.uuid, 'latest')"}
{"code":"def column_types_equal(a_coltype: TypeEngine, b_coltype: TypeEngine) -> bool:\n    \"\"\"\n    Checks that two SQLAlchemy column types are equal (by comparing ``str()``\n    versions of them).\n    \n    See http://stackoverflow.com/questions/34787794/sqlalchemy-column-type-comparison.\n    \n    IMPERFECT. \n    \"\"\"  # noqa\n    return str(a_coltype) == str(b_coltype)","return_type":"bool","function_name":"column_types_equal","stripped_code":"def column_types_equal(a_coltype: TypeEngine, b_coltype: TypeEngine):\n    \"\"\"\n    Checks that two SQLAlchemy column types are equal (by comparing ``str()``\n    versions of them).\n    \n    See http://stackoverflow.com/questions/34787794/sqlalchemy-column-type-comparison.\n    \n    IMPERFECT. \n    \"\"\"  # noqa\n    return str(a_coltype) == str(b_coltype)"}
{"code":"def words_amount_needed(self) -> int:\n        \"\"\"Calculate the needed amount of words to satisfy the entropy number.\n\n        This is for the given wordlist.\n\n        \"\"\"\n        if (\n                self.entropy_bits_req is None\n                or self.amount_n is None\n                or not self.wordlist\n        ):\n            raise ValueError(\"Can't calculate the words amount needed: \"\n                             \"wordlist is empty or entropy_bits_req or \"\n                             \"amount_n isn't set\")\n\n        # Thanks to @julianor for this tip to calculate default amount of\n        # entropy: minbitlen/log2(len(wordlist)).\n        # I set the minimum entropy bits and calculate the amount of words\n        # needed, cosidering the entropy of the wordlist.\n        # Then: entropy_w * amount_w + entropy_n * amount_n >= ENTROPY_BITS_MIN\n        entropy_n = self.entropy_bits((self.randnum_min, self.randnum_max))\n\n        # The entropy for EFF Large Wordlist is ~12.9, no need to calculate\n        entropy_w = self._wordlist_entropy_bits \\\n            if self._wordlist_entropy_bits \\\n            else self.entropy_bits(self.wordlist)\n\n        return calc_words_amount_needed(\n            self.entropy_bits_req,\n            entropy_w,\n            entropy_n,\n            self.amount_n\n        )","return_type":"int","function_name":"Passphrase.words_amount_needed","stripped_code":"def words_amount_needed(self):\n        \"\"\"Calculate the needed amount of words to satisfy the entropy number.\n\n        This is for the given wordlist.\n\n        \"\"\"\n        if (\n                self.entropy_bits_req is None\n                or self.amount_n is None\n                or not self.wordlist\n        ):\n            raise ValueError(\"Can't calculate the words amount needed: \"\n                             \"wordlist is empty or entropy_bits_req or \"\n                             \"amount_n isn't set\")\n\n        # Thanks to @julianor for this tip to calculate default amount of\n        # entropy: minbitlen/log2(len(wordlist)).\n        # I set the minimum entropy bits and calculate the amount of words\n        # needed, cosidering the entropy of the wordlist.\n        # Then: entropy_w * amount_w + entropy_n * amount_n >= ENTROPY_BITS_MIN\n        entropy_n = self.entropy_bits((self.randnum_min, self.randnum_max))\n\n        # The entropy for EFF Large Wordlist is ~12.9, no need to calculate\n        entropy_w = self._wordlist_entropy_bits \\\n            if self._wordlist_entropy_bits \\\n            else self.entropy_bits(self.wordlist)\n\n        return calc_words_amount_needed(\n            self.entropy_bits_req,\n            entropy_w,\n            entropy_n,\n            self.amount_n\n        )"}
{"code":"def logsumexp(tensor: torch.Tensor,\n              dim: int = -1,\n              keepdim: bool = False) -> torch.Tensor:\n    \"\"\"\n    A numerically stable computation of logsumexp. This is mathematically equivalent to\n    `tensor.exp().sum(dim, keep=keepdim).log()`.  This function is typically used for summing log\n    probabilities.\n\n    Parameters\n    ----------\n    tensor : torch.FloatTensor, required.\n        A tensor of arbitrary size.\n    dim : int, optional (default = -1)\n        The dimension of the tensor to apply the logsumexp to.\n    keepdim: bool, optional (default = False)\n        Whether to retain a dimension of size one at the dimension we reduce over.\n    \"\"\"\n    max_score, _ = tensor.max(dim, keepdim=keepdim)\n    if keepdim:\n        stable_vec = tensor - max_score\n    else:\n        stable_vec = tensor - max_score.unsqueeze(dim)\n    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()","return_type":"torch.Tensor","function_name":"logsumexp","stripped_code":"def logsumexp(tensor: torch.Tensor,\n              dim: int = -1,\n              keepdim: bool = False):\n    \"\"\"\n    A numerically stable computation of logsumexp. This is mathematically equivalent to\n    `tensor.exp().sum(dim, keep=keepdim).log()`.  This function is typically used for summing log\n    probabilities.\n\n    Parameters\n    ----------\n    tensor : torch.FloatTensor, required.\n        A tensor of arbitrary size.\n    dim : int, optional (default = -1)\n        The dimension of the tensor to apply the logsumexp to.\n    keepdim: bool, optional (default = False)\n        Whether to retain a dimension of size one at the dimension we reduce over.\n    \"\"\"\n    max_score, _ = tensor.max(dim, keepdim=keepdim)\n    if keepdim:\n        stable_vec = tensor - max_score\n    else:\n        stable_vec = tensor - max_score.unsqueeze(dim)\n    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()"}
{"code":"def _md5_validation(self) -> bool:\n        '''\n        Verifies the MD5 checksum.\n        Only used in the __init__() function\n        '''\n        fn = self.file if self.compressed_file is None else self.compressed_file\n\n        md5 = hashlib.md5()\n        block_size = 16384\n        with fn.open('rb') as f:\n            f.seek(-16, 2)\n            remaining = f.tell()  # File size minus checksum size\n            f.seek(0)\n            while (remaining > block_size):\n                data = f.read(block_size)\n                remaining = remaining - block_size\n                md5.update(data)\n\n            if (remaining > 0):\n                data = f.read(remaining)\n                md5.update(data)\n\n            existing_md5 = f.read(16).hex()\n\n        return md5.hexdigest() == existing_md5","return_type":"bool","function_name":"CDF._md5_validation","stripped_code":"def _md5_validation(self):\n        '''\n        Verifies the MD5 checksum.\n        Only used in the __init__() function\n        '''\n        fn = self.file if self.compressed_file is None else self.compressed_file\n\n        md5 = hashlib.md5()\n        block_size = 16384\n        with fn.open('rb') as f:\n            f.seek(-16, 2)\n            remaining = f.tell()  # File size minus checksum size\n            f.seek(0)\n            while (remaining > block_size):\n                data = f.read(block_size)\n                remaining = remaining - block_size\n                md5.update(data)\n\n            if (remaining > 0):\n                data = f.read(remaining)\n                md5.update(data)\n\n            existing_md5 = f.read(16).hex()\n\n        return md5.hexdigest() == existing_md5"}
{"code":"def clear(self) -> None:\n        \"\"\"Clears the text if it's a text entry element.\"\"\"\n        self.click()\n        for i in self.text:\n            self._parent.send_keyevents(Keys.DEL)","return_type":"None","function_name":"Elements.clear","stripped_code":"def clear(self):\n        \"\"\"Clears the text if it's a text entry element.\"\"\"\n        self.click()\n        for i in self.text:\n            self._parent.send_keyevents(Keys.DEL)"}
{"code":"def unlinked(base_dir: str) -> set:\n        \"\"\"\n        Return all paths to tails files, in specified tails base directory (recursively),\n        without symbolic links associating revocation registry identifiers.\n\n        At an Issuer, tails files should not persist long without revocation registry identifier\n        association via symbolic link. At a HolderProver, a newly downloaded tails file stays\n        unlinked until the agent stores a credential or creates a proof needing it, or else the\n        agent restarts.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :return: set of paths to tails files with no local symbolic links to them\n        \"\"\"\n\n        return {join(dp, f) for dp, dn, fn in walk(base_dir)\n            for f in fn if isfile(join(dp, f)) and not islink(join(dp, f))} - {\n                join(dirname(path_link), readlink(path_link)) for path_link in Tails.links(base_dir)}","return_type":"set","function_name":"Tails.unlinked","stripped_code":"def unlinked(base_dir: str):\n        \"\"\"\n        Return all paths to tails files, in specified tails base directory (recursively),\n        without symbolic links associating revocation registry identifiers.\n\n        At an Issuer, tails files should not persist long without revocation registry identifier\n        association via symbolic link. At a HolderProver, a newly downloaded tails file stays\n        unlinked until the agent stores a credential or creates a proof needing it, or else the\n        agent restarts.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :return: set of paths to tails files with no local symbolic links to them\n        \"\"\"\n\n        return {join(dp, f) for dp, dn, fn in walk(base_dir)\n            for f in fn if isfile(join(dp, f)) and not islink(join(dp, f))} - {\n                join(dirname(path_link), readlink(path_link)) for path_link in Tails.links(base_dir)}"}
{"code":"def save_networkfile(self, filepath: Union[str, None] = None,\n                         write_nodes: bool = True) -> None:\n        \"\"\"Save the selection as a network file.\n\n        >>> from hydpy.core.examples import prepare_full_example_2\n        >>> _, pub, TestIO = prepare_full_example_2()\n\n        In most cases, one should conveniently write network files via method\n        |NetworkManager.save_files| of class |NetworkManager|.  However,\n        using the method |Selection.save_networkfile| allows for additional\n        configuration via the arguments `filepath` and `write_nodes`:\n\n        >>> with TestIO():\n        ...     pub.selections.headwaters.save_networkfile()\n        ...     with open('headwaters.py') as networkfile:\n        ...         print(networkfile.read())\n        # -*- coding: utf-8 -*-\n        <BLANKLINE>\n        from hydpy import Node, Element\n        <BLANKLINE>\n        <BLANKLINE>\n        Node(\"dill\", variable=\"Q\",\n             keywords=\"gauge\")\n        <BLANKLINE>\n        Node(\"lahn_1\", variable=\"Q\",\n             keywords=\"gauge\")\n        <BLANKLINE>\n        <BLANKLINE>\n        Element(\"land_dill\",\n                outlets=\"dill\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        Element(\"land_lahn_1\",\n                outlets=\"lahn_1\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n\n        >>> with TestIO():\n        ...     pub.selections.headwaters.save_networkfile('test.py', False)\n        ...     with open('test.py') as networkfile:\n        ...         print(networkfile.read())\n        # -*- coding: utf-8 -*-\n        <BLANKLINE>\n        from hydpy import Node, Element\n        <BLANKLINE>\n        <BLANKLINE>\n        Element(\"land_dill\",\n                outlets=\"dill\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        Element(\"land_lahn_1\",\n                outlets=\"lahn_1\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        \"\"\"\n        if filepath is None:\n            filepath = self.name + '.py'\n        with open(filepath, 'w', encoding=\"utf-8\") as file_:\n            file_.write('# -*- coding: utf-8 -*-\\n')\n            file_.write('\\nfrom hydpy import Node, Element\\n\\n')\n            if write_nodes:\n                for node in self.nodes:\n                    file_.write('\\n' + repr(node) + '\\n')\n                file_.write('\\n')\n            for element in self.elements:\n                file_.write('\\n' + repr(element) + '\\n')","return_type":"None","function_name":"Selection.save_networkfile","stripped_code":"def save_networkfile(self, filepath: Union[str, None] = None,\n                         write_nodes: bool = True):\n        \"\"\"Save the selection as a network file.\n\n        >>> from hydpy.core.examples import prepare_full_example_2\n        >>> _, pub, TestIO = prepare_full_example_2()\n\n        In most cases, one should conveniently write network files via method\n        |NetworkManager.save_files| of class |NetworkManager|.  However,\n        using the method |Selection.save_networkfile| allows for additional\n        configuration via the arguments `filepath` and `write_nodes`:\n\n        >>> with TestIO():\n        ...     pub.selections.headwaters.save_networkfile()\n        ...     with open('headwaters.py') as networkfile:\n        ...         print(networkfile.read())\n        # -*- coding: utf-8 -*-\n        <BLANKLINE>\n        from hydpy import Node, Element\n        <BLANKLINE>\n        <BLANKLINE>\n        Node(\"dill\", variable=\"Q\",\n             keywords=\"gauge\")\n        <BLANKLINE>\n        Node(\"lahn_1\", variable=\"Q\",\n             keywords=\"gauge\")\n        <BLANKLINE>\n        <BLANKLINE>\n        Element(\"land_dill\",\n                outlets=\"dill\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        Element(\"land_lahn_1\",\n                outlets=\"lahn_1\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n\n        >>> with TestIO():\n        ...     pub.selections.headwaters.save_networkfile('test.py', False)\n        ...     with open('test.py') as networkfile:\n        ...         print(networkfile.read())\n        # -*- coding: utf-8 -*-\n        <BLANKLINE>\n        from hydpy import Node, Element\n        <BLANKLINE>\n        <BLANKLINE>\n        Element(\"land_dill\",\n                outlets=\"dill\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        Element(\"land_lahn_1\",\n                outlets=\"lahn_1\",\n                keywords=\"catchment\")\n        <BLANKLINE>\n        \"\"\"\n        if filepath is None:\n            filepath = self.name + '.py'\n        with open(filepath, 'w', encoding=\"utf-8\") as file_:\n            file_.write('# -*- coding: utf-8 -*-\\n')\n            file_.write('\\nfrom hydpy import Node, Element\\n\\n')\n            if write_nodes:\n                for node in self.nodes:\n                    file_.write('\\n' + repr(node) + '\\n')\n                file_.write('\\n')\n            for element in self.elements:\n                file_.write('\\n' + repr(element) + '\\n')"}
{"code":"def from_tokens(cls, path:PathOrStr, trn_tok:Collection[Collection[str]], trn_lbls:Collection[Union[int,float]],\n                 val_tok:Collection[Collection[str]], val_lbls:Collection[Union[int,float]], vocab:Vocab=None,\n                 tst_tok:Collection[Collection[str]]=None, classes:Collection[Any]=None, max_vocab:int=60000, min_freq:int=3,\n                 **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from tokens and labels. `kwargs` are passed to the dataloader creation.\"\n        processor = NumericalizeProcessor(vocab=vocab, max_vocab=max_vocab, min_freq=min_freq)\n        src = ItemLists(path, TextList(trn_tok, path=path, processor=processor),\n                        TextList(val_tok, path=path, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(trn_lbls, val_lbls, classes=classes)\n        if tst_tok is not None: src.add_test(TextList(tst_tok, path=path))\n        return src.databunch(**kwargs)","return_type":"DataBunch","function_name":"TextDataBunch.from_tokens","stripped_code":"def from_tokens(cls, path:PathOrStr, trn_tok:Collection[Collection[str]], trn_lbls:Collection[Union[int,float]],\n                 val_tok:Collection[Collection[str]], val_lbls:Collection[Union[int,float]], vocab:Vocab=None,\n                 tst_tok:Collection[Collection[str]]=None, classes:Collection[Any]=None, max_vocab:int=60000, min_freq:int=3,\n                 **kwargs):\n        \"Create a `TextDataBunch` from tokens and labels. `kwargs` are passed to the dataloader creation.\"\n        processor = NumericalizeProcessor(vocab=vocab, max_vocab=max_vocab, min_freq=min_freq)\n        src = ItemLists(path, TextList(trn_tok, path=path, processor=processor),\n                        TextList(val_tok, path=path, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(trn_lbls, val_lbls, classes=classes)\n        if tst_tok is not None: src.add_test(TextList(tst_tok, path=path))\n        return src.databunch(**kwargs)"}
{"code":"def update_content_encoding(self, data: Any) -> None:\n        \"\"\"Set request content encoding.\"\"\"\n        if not data:\n            return\n\n        enc = self.headers.get(hdrs.CONTENT_ENCODING, '').lower()\n        if enc:\n            if self.compress:\n                raise ValueError(\n                    'compress can not be set '\n                    'if Content-Encoding header is set')\n        elif self.compress:\n            if not isinstance(self.compress, str):\n                self.compress = 'deflate'\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\n            self.chunked = True","return_type":"None","function_name":"ClientRequest.update_content_encoding","stripped_code":"def update_content_encoding(self, data: Any):\n        \"\"\"Set request content encoding.\"\"\"\n        if not data:\n            return\n\n        enc = self.headers.get(hdrs.CONTENT_ENCODING, '').lower()\n        if enc:\n            if self.compress:\n                raise ValueError(\n                    'compress can not be set '\n                    'if Content-Encoding header is set')\n        elif self.compress:\n            if not isinstance(self.compress, str):\n                self.compress = 'deflate'\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\n            self.chunked = True"}
{"code":"def _convert(self, desired_type: Type[T], source_obj: S, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Implementing classes should implement this method to perform the conversion itself\n\n        :param desired_type: the destination type of the conversion\n        :param source_obj: the source object that should be converter\n        :param logger: a logger to use if any is available, or None\n        :param options: additional options map. Implementing classes may use 'self.get_applicable_options()' to get the\n        options that are of interest for this converter.\n        :return:\n        \"\"\"\n        pass","return_type":"T","function_name":"Converter._convert","stripped_code":"def _convert(self, desired_type: Type[T], source_obj: S, logger: Logger, options: Dict[str, Dict[str, Any]]):\n        \"\"\"\n        Implementing classes should implement this method to perform the conversion itself\n\n        :param desired_type: the destination type of the conversion\n        :param source_obj: the source object that should be converter\n        :param logger: a logger to use if any is available, or None\n        :param options: additional options map. Implementing classes may use 'self.get_applicable_options()' to get the\n        options that are of interest for this converter.\n        :return:\n        \"\"\"\n        pass"}
{"code":"def send_request(self, ssl_client: SslClient) -> str:\n        \"\"\"Send an HTTP GET to the server and return the HTTP status code.\n        \"\"\"\n        try:\n            ssl_client.write(HttpRequestGenerator.get_request(self._hostname))\n\n            # Parse the response and print the Location header\n            http_response = HttpResponseParser.parse_from_ssl_connection(ssl_client)\n            if http_response.version == 9:\n                # HTTP 0.9 => Probably not an HTTP response\n                result = self.ERR_NOT_HTTP\n            else:\n                redirect = ''\n                if 300 <= http_response.status < 400:\n                    redirect_location = http_response.getheader('Location')\n                    if redirect_location:\n                        # Add redirection URL to the result\n                        redirect = f' - {redirect_location}'\n\n                result = self.GET_RESULT_FORMAT.format(http_response.status, http_response.reason, redirect)\n        except socket.timeout:\n            result = self.ERR_HTTP_TIMEOUT\n        except IOError:\n            result = self.ERR_GENERIC\n\n        return result","return_type":"str","function_name":"HttpsHelper.send_request","stripped_code":"def send_request(self, ssl_client: SslClient):\n        \"\"\"Send an HTTP GET to the server and return the HTTP status code.\n        \"\"\"\n        try:\n            ssl_client.write(HttpRequestGenerator.get_request(self._hostname))\n\n            # Parse the response and print the Location header\n            http_response = HttpResponseParser.parse_from_ssl_connection(ssl_client)\n            if http_response.version == 9:\n                # HTTP 0.9 => Probably not an HTTP response\n                result = self.ERR_NOT_HTTP\n            else:\n                redirect = ''\n                if 300 <= http_response.status < 400:\n                    redirect_location = http_response.getheader('Location')\n                    if redirect_location:\n                        # Add redirection URL to the result\n                        redirect = f' - {redirect_location}'\n\n                result = self.GET_RESULT_FORMAT.format(http_response.status, http_response.reason, redirect)\n        except socket.timeout:\n            result = self.ERR_HTTP_TIMEOUT\n        except IOError:\n            result = self.ERR_GENERIC\n\n        return result"}
{"code":"def register_listener(self, trigger: str, func: callable, **kwargs) -> None:\n        \"\"\"\n        Register a listener for the given trigger. Raises an TypeError when the trigger is not a valid one.\n        To get a list with all valid triggers, use LISTEN_ON_OPTIONS.\n        :param trigger: the trigger on which the given callback should be used. \n        Currently supported: 'universe availability change', 'universe'\n        :param func: the callback. The parameters depend on the trigger. See README for more information\n        \"\"\"\n        if trigger in LISTEN_ON_OPTIONS:\n            if trigger == LISTEN_ON_OPTIONS[1]:  # if the trigger is universe, use the universe from args as key\n                try:\n                    self._callbacks[kwargs[LISTEN_ON_OPTIONS[1]]].append(func)\n                except:\n                    self._callbacks[kwargs[LISTEN_ON_OPTIONS[1]]] = [func]\n            try:\n                self._callbacks[trigger].append(func)\n            except:\n                self._callbacks[trigger] = [func]\n        else:\n            raise TypeError(f'The given trigger \"{trigger}\" is not a valid one!')","return_type":"None","function_name":"sACNreceiver.register_listener","stripped_code":"def register_listener(self, trigger: str, func: callable, **kwargs):\n        \"\"\"\n        Register a listener for the given trigger. Raises an TypeError when the trigger is not a valid one.\n        To get a list with all valid triggers, use LISTEN_ON_OPTIONS.\n        :param trigger: the trigger on which the given callback should be used. \n        Currently supported: 'universe availability change', 'universe'\n        :param func: the callback. The parameters depend on the trigger. See README for more information\n        \"\"\"\n        if trigger in LISTEN_ON_OPTIONS:\n            if trigger == LISTEN_ON_OPTIONS[1]:  # if the trigger is universe, use the universe from args as key\n                try:\n                    self._callbacks[kwargs[LISTEN_ON_OPTIONS[1]]].append(func)\n                except:\n                    self._callbacks[kwargs[LISTEN_ON_OPTIONS[1]]] = [func]\n            try:\n                self._callbacks[trigger].append(func)\n            except:\n                self._callbacks[trigger] = [func]\n        else:\n            raise TypeError(f'The given trigger \"{trigger}\" is not a valid one!')"}
{"code":"def send_message(\n        self, request: str, response_expected: bool, **kwargs: Any\n    ) -> Response:\n        \"\"\"\n        Transport the message to the server and return the response.\n\n        Args:\n            request: The JSON-RPC request string.\n            response_expected: Whether the request expects a response.\n\n        Returns:\n            A Response object.\n        \"\"\"\n        payload = str(request) + self.delimiter\n        self.socket.send(payload.encode(self.encoding))\n\n        response = bytes()\n        decoded = None\n\n        # Receive the response until we find the delimiter.\n        # TODO Do not wait for a response if the message sent is a notification.\n        while True:\n            response += self.socket.recv(1024)\n\n            decoded = response.decode(self.encoding)\n            if len(decoded) < self.delimiter_length:\n                continue\n\n            # TODO Check that're not in the middle of the response.\n            elif decoded[-self.delimiter_length :] == self.delimiter:\n                break\n\n        assert decoded is not None\n        return Response(decoded[: -self.delimiter_length])","return_type":"Response","function_name":"SocketClient.send_message","stripped_code":"def send_message(\n        self, request: str, response_expected: bool, **kwargs: Any\n    ):\n        \"\"\"\n        Transport the message to the server and return the response.\n\n        Args:\n            request: The JSON-RPC request string.\n            response_expected: Whether the request expects a response.\n\n        Returns:\n            A Response object.\n        \"\"\"\n        payload = str(request) + self.delimiter\n        self.socket.send(payload.encode(self.encoding))\n\n        response = bytes()\n        decoded = None\n\n        # Receive the response until we find the delimiter.\n        # TODO Do not wait for a response if the message sent is a notification.\n        while True:\n            response += self.socket.recv(1024)\n\n            decoded = response.decode(self.encoding)\n            if len(decoded) < self.delimiter_length:\n                continue\n\n            # TODO Check that're not in the middle of the response.\n            elif decoded[-self.delimiter_length :] == self.delimiter:\n                break\n\n        assert decoded is not None\n        return Response(decoded[: -self.delimiter_length])"}
{"code":"def from_pickle(path: Union[str, BinaryIO], check_version: bool = True) -> BELGraph:\n    \"\"\"Read a graph from a pickle file.\n\n    :param path: File or filename to read. Filenames ending in .gz or .bz2 will be uncompressed.\n    :param bool check_version: Checks if the graph was produced by this version of PyBEL\n    \"\"\"\n    graph = nx.read_gpickle(path)\n\n    raise_for_not_bel(graph)\n    if check_version:\n        raise_for_old_graph(graph)\n\n    return graph","return_type":"BELGraph","function_name":"from_pickle","stripped_code":"def from_pickle(path: Union[str, BinaryIO], check_version: bool = True):\n    \"\"\"Read a graph from a pickle file.\n\n    :param path: File or filename to read. Filenames ending in .gz or .bz2 will be uncompressed.\n    :param bool check_version: Checks if the graph was produced by this version of PyBEL\n    \"\"\"\n    graph = nx.read_gpickle(path)\n\n    raise_for_not_bel(graph)\n    if check_version:\n        raise_for_old_graph(graph)\n\n    return graph"}
{"code":"def env_string(name: str, required: bool=False, default: Union[Type[empty], str]=empty) -> str:\n    \"\"\"Pulls an environment variable out of the environment returning it as a\n    string. If not present in the environment and no default is specified, an\n    empty string is returned.\n\n    :param name: The name of the environment variable be pulled\n    :type name: str\n\n    :param required: Whether the environment variable is required. If ``True``\n    and the variable is not present, a ``KeyError`` is raised.\n    :type required: bool\n\n    :param default: The value to return if the environment variable is not\n    present. (Providing a default alongside setting ``required=True`` will raise\n    a ``ValueError``)\n    :type default: bool\n    \"\"\"\n    value = get_env_value(name, default=default, required=required)\n    if value is empty:\n        value = ''\n    return value","return_type":"str","function_name":"env_string","stripped_code":"def env_string(name: str, required: bool=False, default: Union[Type[empty], str]=empty):\n    \"\"\"Pulls an environment variable out of the environment returning it as a\n    string. If not present in the environment and no default is specified, an\n    empty string is returned.\n\n    :param name: The name of the environment variable be pulled\n    :type name: str\n\n    :param required: Whether the environment variable is required. If ``True``\n    and the variable is not present, a ``KeyError`` is raised.\n    :type required: bool\n\n    :param default: The value to return if the environment variable is not\n    present. (Providing a default alongside setting ``required=True`` will raise\n    a ``ValueError``)\n    :type default: bool\n    \"\"\"\n    value = get_env_value(name, default=default, required=required)\n    if value is empty:\n        value = ''\n    return value"}
{"code":"def last_modified(self, name: str = None) -> str:\n\t\t\"\"\"\n\t\tReturn a compact ISO8601 timestamp (UTC timezone) indicating when an attribute was last modified\n\n\t\tNote: if no attribute name is given (the default), the modification time of the most recently modified attribute will be returned\n\t\tNote: if the attributes do not contain a timestamp, and the mode is 'r+', a new timestamp is created and returned.\n\t\tOtherwise, the current time in UTC will be returned.\n\t\t\"\"\"\n\t\ta = [\"/row_attrs/\", \"/col_attrs/\"][self.axis]\n\n\t\tif self.ds is not None:\n\t\t\tif name is None:\n\t\t\t\tif \"last_modified\" in self.ds._file[a].attrs:\n\t\t\t\t\treturn self.ds._file[a].attrs[\"last_modified\"]\n\t\t\t\telif self.ds._file.mode == 'r+':\n\t\t\t\t\tself.ds._file[a].attrs[\"last_modified\"] = timestamp()\n\t\t\t\t\tself.ds._file.flush()\n\t\t\t\t\treturn self.ds._file[a].attrs[\"last_modified\"]\n\t\t\tif name is not None:\n\t\t\t\tif \"last_modified\" in self.ds._file[a + name].attrs:\n\t\t\t\t\treturn self.ds._file[a + name].attrs[\"last_modified\"]\n\t\t\t\telif self.ds._file.mode == 'r+':\n\t\t\t\t\tself.ds._file[a + name].attrs[\"last_modified\"] = timestamp()\n\t\t\t\t\tself.ds._file.flush()\n\t\t\t\t\treturn self.ds._file[a + name].attrs[\"last_modified\"]\n\t\treturn timestamp()","return_type":"str","function_name":"AttributeManager.last_modified","stripped_code":"def last_modified(self, name: str = None):\n\t\t\"\"\"\n\t\tReturn a compact ISO8601 timestamp (UTC timezone) indicating when an attribute was last modified\n\n\t\tNote: if no attribute name is given (the default), the modification time of the most recently modified attribute will be returned\n\t\tNote: if the attributes do not contain a timestamp, and the mode is 'r+', a new timestamp is created and returned.\n\t\tOtherwise, the current time in UTC will be returned.\n\t\t\"\"\"\n\t\ta = [\"/row_attrs/\", \"/col_attrs/\"][self.axis]\n\n\t\tif self.ds is not None:\n\t\t\tif name is None:\n\t\t\t\tif \"last_modified\" in self.ds._file[a].attrs:\n\t\t\t\t\treturn self.ds._file[a].attrs[\"last_modified\"]\n\t\t\t\telif self.ds._file.mode == 'r+':\n\t\t\t\t\tself.ds._file[a].attrs[\"last_modified\"] = timestamp()\n\t\t\t\t\tself.ds._file.flush()\n\t\t\t\t\treturn self.ds._file[a].attrs[\"last_modified\"]\n\t\t\tif name is not None:\n\t\t\t\tif \"last_modified\" in self.ds._file[a + name].attrs:\n\t\t\t\t\treturn self.ds._file[a + name].attrs[\"last_modified\"]\n\t\t\t\telif self.ds._file.mode == 'r+':\n\t\t\t\t\tself.ds._file[a + name].attrs[\"last_modified\"] = timestamp()\n\t\t\t\t\tself.ds._file.flush()\n\t\t\t\t\treturn self.ds._file[a + name].attrs[\"last_modified\"]\n\t\treturn timestamp()"}
{"code":"def weighted_sum(matrix: torch.Tensor, attention: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an\n    \"attention\" vector), and returns a weighted sum of the rows in the matrix.  This is the typical\n    computation performed after an attention mechanism.\n\n    Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle\n    higher-order tensors.  We always sum over the second-to-last dimension of the \"matrix\", and we\n    assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the\n    \"vector\".  Non-matched dimensions in the \"vector\" must be `directly after the batch dimension`.\n\n    For example, say I have a \"matrix\" with dimensions ``(batch_size, num_queries, num_words,\n    embedding_dim)``.  The attention \"vector\" then must have at least those dimensions, and could\n    have more. Both:\n\n        - ``(batch_size, num_queries, num_words)`` (distribution over words for each query)\n        - ``(batch_size, num_documents, num_queries, num_words)`` (distribution over words in a\n          query for each document)\n\n    are valid input \"vectors\", producing tensors of shape:\n    ``(batch_size, num_queries, embedding_dim)`` and\n    ``(batch_size, num_documents, num_queries, embedding_dim)`` respectively.\n    \"\"\"\n    # We'll special-case a few settings here, where there are efficient (but poorly-named)\n    # operations in pytorch that already do the computation we need.\n    if attention.dim() == 2 and matrix.dim() == 3:\n        return attention.unsqueeze(1).bmm(matrix).squeeze(1)\n    if attention.dim() == 3 and matrix.dim() == 3:\n        return attention.bmm(matrix)\n    if matrix.dim() - 1 < attention.dim():\n        expanded_size = list(matrix.size())\n        for i in range(attention.dim() - matrix.dim() + 1):\n            matrix = matrix.unsqueeze(1)\n            expanded_size.insert(i + 1, attention.size(i + 1))\n        matrix = matrix.expand(*expanded_size)\n    intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix\n    return intermediate.sum(dim=-2)","return_type":"torch.Tensor","function_name":"weighted_sum","stripped_code":"def weighted_sum(matrix: torch.Tensor, attention: torch.Tensor):\n    \"\"\"\n    Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an\n    \"attention\" vector), and returns a weighted sum of the rows in the matrix.  This is the typical\n    computation performed after an attention mechanism.\n\n    Note that while we call this a \"matrix\" of vectors and an attention \"vector\", we also handle\n    higher-order tensors.  We always sum over the second-to-last dimension of the \"matrix\", and we\n    assume that all dimensions in the \"matrix\" prior to the last dimension are matched in the\n    \"vector\".  Non-matched dimensions in the \"vector\" must be `directly after the batch dimension`.\n\n    For example, say I have a \"matrix\" with dimensions ``(batch_size, num_queries, num_words,\n    embedding_dim)``.  The attention \"vector\" then must have at least those dimensions, and could\n    have more. Both:\n\n        - ``(batch_size, num_queries, num_words)`` (distribution over words for each query)\n        - ``(batch_size, num_documents, num_queries, num_words)`` (distribution over words in a\n          query for each document)\n\n    are valid input \"vectors\", producing tensors of shape:\n    ``(batch_size, num_queries, embedding_dim)`` and\n    ``(batch_size, num_documents, num_queries, embedding_dim)`` respectively.\n    \"\"\"\n    # We'll special-case a few settings here, where there are efficient (but poorly-named)\n    # operations in pytorch that already do the computation we need.\n    if attention.dim() == 2 and matrix.dim() == 3:\n        return attention.unsqueeze(1).bmm(matrix).squeeze(1)\n    if attention.dim() == 3 and matrix.dim() == 3:\n        return attention.bmm(matrix)\n    if matrix.dim() - 1 < attention.dim():\n        expanded_size = list(matrix.size())\n        for i in range(attention.dim() - matrix.dim() + 1):\n            matrix = matrix.unsqueeze(1)\n            expanded_size.insert(i + 1, attention.size(i + 1))\n        matrix = matrix.expand(*expanded_size)\n    intermediate = attention.unsqueeze(-1).expand_as(matrix) * matrix\n    return intermediate.sum(dim=-2)"}
{"code":"def get_book_currencies(self) -> List[Commodity]:\n        \"\"\" Returns currencies used in the book \"\"\"\n        query = (\n            self.currencies_query\n                .order_by(Commodity.mnemonic)\n        )\n        return query.all()","return_type":"List[Commodity]","function_name":"CurrenciesAggregate.get_book_currencies","stripped_code":"def get_book_currencies(self):\n        \"\"\" Returns currencies used in the book \"\"\"\n        query = (\n            self.currencies_query\n                .order_by(Commodity.mnemonic)\n        )\n        return query.all()"}
{"code":"def Assert(predicate: vertex_constructor_param_types, error_message: str, label: Optional[str]=None) -> Vertex:\n    \"\"\"\n    A vertex that asserts a {@link BooleanVertex} is all true on calculation.\n    \n    :param predicate: the predicate to evaluate\n    :param error_message: a message to include in the {@link AssertionError}\n    \"\"\"\n    return Boolean(context.jvm_view().AssertVertex, label, cast_to_vertex(predicate), error_message)","return_type":"Vertex","function_name":"Assert","stripped_code":"def Assert(predicate: vertex_constructor_param_types, error_message: str, label: Optional[str]=None):\n    \"\"\"\n    A vertex that asserts a {@link BooleanVertex} is all true on calculation.\n    \n    :param predicate: the predicate to evaluate\n    :param error_message: a message to include in the {@link AssertionError}\n    \"\"\"\n    return Boolean(context.jvm_view().AssertVertex, label, cast_to_vertex(predicate), error_message)"}
{"code":"def make_pubmed_gene_group(entrez_ids: Iterable[Union[str, int]]) -> Iterable[str]:\n    \"\"\"Builds a skeleton for gene summaries\n\n    :param entrez_ids: A list of Entrez Gene identifiers to query the PubMed service\n    :return: An iterator over statement lines for NCBI Entrez Gene summaries\n    \"\"\"\n    url = PUBMED_GENE_QUERY_URL.format(','.join(str(x).strip() for x in entrez_ids))\n    response = requests.get(url)\n    tree = ElementTree.fromstring(response.content)\n\n    for x in tree.findall('./DocumentSummarySet/DocumentSummary'):\n        yield '\\n# {}'.format(x.find('Description').text)\n        yield 'SET Citation = {{\"Other\", \"PubMed Gene\", \"{}\"}}'.format(x.attrib['uid'])\n        yield 'SET Evidence = \"{}\"'.format(x.find('Summary').text.strip().replace('\\n', ''))\n        yield '\\nUNSET Evidence\\nUNSET Citation'","return_type":"Iterable[str]","function_name":"make_pubmed_gene_group","stripped_code":"def make_pubmed_gene_group(entrez_ids: Iterable[Union[str, int]]):\n    \"\"\"Builds a skeleton for gene summaries\n\n    :param entrez_ids: A list of Entrez Gene identifiers to query the PubMed service\n    :return: An iterator over statement lines for NCBI Entrez Gene summaries\n    \"\"\"\n    url = PUBMED_GENE_QUERY_URL.format(','.join(str(x).strip() for x in entrez_ids))\n    response = requests.get(url)\n    tree = ElementTree.fromstring(response.content)\n\n    for x in tree.findall('./DocumentSummarySet/DocumentSummary'):\n        yield '\\n# {}'.format(x.find('Description').text)\n        yield 'SET Citation = {{\"Other\", \"PubMed Gene\", \"{}\"}}'.format(x.attrib['uid'])\n        yield 'SET Evidence = \"{}\"'.format(x.find('Summary').text.strip().replace('\\n', ''))\n        yield '\\nUNSET Evidence\\nUNSET Citation'"}
{"code":"def clear(self) -> None:\n        \"\"\"\n        Destroys all running containers.\n        \"\"\"\n        r = self.__api.delete('containers')\n        if r.status_code != 204:\n            self.__api.handle_erroneous_response(r)","return_type":"None","function_name":"ContainerManager.clear","stripped_code":"def clear(self):\n        \"\"\"\n        Destroys all running containers.\n        \"\"\"\n        r = self.__api.delete('containers')\n        if r.status_code != 204:\n            self.__api.handle_erroneous_response(r)"}
{"code":"def start(**kwargs: Any) -> None:\n    \"\"\"Start web server.\n\n    Run until ``Ctrl-c`` pressed, or if auto-shutdown is enabled, until when\n    all browser windows are closed.\n\n    This function accepts keyword areguments same as :func:`start_server` and\n    all arguments passed to it.\n    \"\"\"\n    start_server(**kwargs)\n    try:\n        asyncio.get_event_loop().run_forever()\n    except KeyboardInterrupt:\n        stop_server()","return_type":"None","function_name":"start","stripped_code":"def start(**kwargs: Any):\n    \"\"\"Start web server.\n\n    Run until ``Ctrl-c`` pressed, or if auto-shutdown is enabled, until when\n    all browser windows are closed.\n\n    This function accepts keyword areguments same as :func:`start_server` and\n    all arguments passed to it.\n    \"\"\"\n    start_server(**kwargs)\n    try:\n        asyncio.get_event_loop().run_forever()\n    except KeyboardInterrupt:\n        stop_server()"}
{"code":"def long_name(self) -> str:\n        \"\"\"\n        Shortcut property, if alias exists, this will provide the alias with name\n        in parenthesis. Otherwise, this will return the name\n        \"\"\"\n        if self.chat_alias:\n            return \"{0} ({1})\".format(self.chat_alias, self.chat_name)\n        else:\n            return self.chat_name","return_type":"str","function_name":"EFBChat.long_name","stripped_code":"def long_name(self):\n        \"\"\"\n        Shortcut property, if alias exists, this will provide the alias with name\n        in parenthesis. Otherwise, this will return the name\n        \"\"\"\n        if self.chat_alias:\n            return \"{0} ({1})\".format(self.chat_alias, self.chat_name)\n        else:\n            return self.chat_name"}
{"code":"def check_scicrunch_for_label(self, label: str) -> dict:\n        \"\"\" Sees if label with your user ID already exists\n\n        There are can be multiples of the same label in interlex, but there should only be one\n        label with your user id. Therefore you can create labels if there already techniqually\n        exist, but not if you are the one to create it.\n        \"\"\"\n        list_of_crude_matches = self.crude_search_scicrunch_via_label(label)\n        for crude_match in list_of_crude_matches:\n            # If labels match\n            if crude_match['label'].lower().strip() == label.lower().strip():\n                complete_data_of_crude_match = self.get_entity(crude_match['ilx'])\n                crude_match_label = crude_match['label']\n                crude_match_user_id = complete_data_of_crude_match['uid']\n                # If label was created by you\n                if str(self.user_id) == str(crude_match_user_id):\n                    return complete_data_of_crude_match # You created the entity already\n        # No label AND user id match\n        return {}","return_type":"dict","function_name":"InterLexClient.check_scicrunch_for_label","stripped_code":"def check_scicrunch_for_label(self, label: str):\n        \"\"\" Sees if label with your user ID already exists\n\n        There are can be multiples of the same label in interlex, but there should only be one\n        label with your user id. Therefore you can create labels if there already techniqually\n        exist, but not if you are the one to create it.\n        \"\"\"\n        list_of_crude_matches = self.crude_search_scicrunch_via_label(label)\n        for crude_match in list_of_crude_matches:\n            # If labels match\n            if crude_match['label'].lower().strip() == label.lower().strip():\n                complete_data_of_crude_match = self.get_entity(crude_match['ilx'])\n                crude_match_label = crude_match['label']\n                crude_match_user_id = complete_data_of_crude_match['uid']\n                # If label was created by you\n                if str(self.user_id) == str(crude_match_user_id):\n                    return complete_data_of_crude_match # You created the entity already\n        # No label AND user id match\n        return {}"}
{"code":"def included(powernode:str, inclusions:dict, nodes_only=False) -> iter:\n    \"\"\"Yield (power)nodes below given powernode (contained by it,\n    or contained by a powernode contained by it, etc).\n\n    >>> sorted(included('p1', {'p1': ('p2', 1), 'p2': (3,), 1: (), 3: ()}), key=str)\n    [1, 3, 'p2']\n    >>> sorted(included('p1', {'p1': ('p2', 1), 'p2': (3,), 1: (), 3: ()}, nodes_only=True), key=str)\n    [1, 3]\n\n    \"\"\"\n    if nodes_only:\n        condition = lambda e: e != powernode and inclusions[e] == ()\n    else:\n        condition = lambda e: e != powernode\n    yield from (elem for elem in utils.walk(powernode, (inclusions,))\n                if condition(elem))","return_type":"iter","function_name":"included","stripped_code":"def included(powernode:str, inclusions:dict, nodes_only=False):\n    \"\"\"Yield (power)nodes below given powernode (contained by it,\n    or contained by a powernode contained by it, etc).\n\n    >>> sorted(included('p1', {'p1': ('p2', 1), 'p2': (3,), 1: (), 3: ()}), key=str)\n    [1, 3, 'p2']\n    >>> sorted(included('p1', {'p1': ('p2', 1), 'p2': (3,), 1: (), 3: ()}, nodes_only=True), key=str)\n    [1, 3]\n\n    \"\"\"\n    if nodes_only:\n        condition = lambda e: e != powernode and inclusions[e] == ()\n    else:\n        condition = lambda e: e != powernode\n    yield from (elem for elem in utils.walk(powernode, (inclusions,))\n                if condition(elem))"}
{"code":"def check_tx_with_confirmations(self, tx_hash: str, confirmations: int) -> bool:\n        \"\"\"\n        Check tx hash and make sure it has the confirmations required\n        :param w3: Web3 instance\n        :param tx_hash: Hash of the tx\n        :param confirmations: Minimum number of confirmations required\n        :return: True if tx was mined with the number of confirmations required, False otherwise\n        \"\"\"\n        tx_receipt = self.w3.eth.getTransactionReceipt(tx_hash)\n        if not tx_receipt or tx_receipt['blockNumber'] is None:\n            # If tx_receipt exists but blockNumber is None, tx is still pending (just Parity)\n            return False\n        else:\n            return (self.w3.eth.blockNumber - tx_receipt['blockNumber']) >= confirmations","return_type":"bool","function_name":"EthereumClient.check_tx_with_confirmations","stripped_code":"def check_tx_with_confirmations(self, tx_hash: str, confirmations: int):\n        \"\"\"\n        Check tx hash and make sure it has the confirmations required\n        :param w3: Web3 instance\n        :param tx_hash: Hash of the tx\n        :param confirmations: Minimum number of confirmations required\n        :return: True if tx was mined with the number of confirmations required, False otherwise\n        \"\"\"\n        tx_receipt = self.w3.eth.getTransactionReceipt(tx_hash)\n        if not tx_receipt or tx_receipt['blockNumber'] is None:\n            # If tx_receipt exists but blockNumber is None, tx is still pending (just Parity)\n            return False\n        else:\n            return (self.w3.eth.blockNumber - tx_receipt['blockNumber']) >= confirmations"}
{"code":"def quoted_or_list(items: List[str]) -> Optional[str]:\n    \"\"\"Given [A, B, C] return \"'A', 'B', or 'C'\".\n\n    Note: We use single quotes here, since these are also used by repr().\n    \"\"\"\n    return or_list([f\"'{item}'\" for item in items])","return_type":"Optional[str]","function_name":"quoted_or_list","stripped_code":"def quoted_or_list(items: List[str]):\n    \"\"\"Given [A, B, C] return \"'A', 'B', or 'C'\".\n\n    Note: We use single quotes here, since these are also used by repr().\n    \"\"\"\n    return or_list([f\"'{item}'\" for item in items])"}
{"code":"def handle_cancelpayment(\n        payment_state: InitiatorPaymentState,\n        channelidentifiers_to_channels: ChannelMap,\n) -> TransitionResult[InitiatorPaymentState]:\n    \"\"\" Cancel the payment and all related transfers. \"\"\"\n    # Cannot cancel a transfer after the secret is revealed\n    events = list()\n    for initiator_state in payment_state.initiator_transfers.values():\n        channel_identifier = initiator_state.channel_identifier\n        channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n        if not channel_state:\n            continue\n\n        if can_cancel(initiator_state):\n            transfer_description = initiator_state.transfer_description\n            cancel_events = cancel_current_route(payment_state, initiator_state)\n\n            initiator_state.transfer_state = 'transfer_cancelled'\n\n            cancel = EventPaymentSentFailed(\n                payment_network_identifier=channel_state.payment_network_identifier,\n                token_network_identifier=TokenNetworkID(channel_state.token_network_identifier),\n                identifier=transfer_description.payment_identifier,\n                target=transfer_description.target,\n                reason='user canceled payment',\n            )\n            cancel_events.append(cancel)\n\n            events.extend(cancel_events)\n\n    return TransitionResult(payment_state, events)","return_type":"TransitionResult[InitiatorPaymentState]","function_name":"handle_cancelpayment","stripped_code":"def handle_cancelpayment(\n        payment_state: InitiatorPaymentState,\n        channelidentifiers_to_channels: ChannelMap,\n):\n    \"\"\" Cancel the payment and all related transfers. \"\"\"\n    # Cannot cancel a transfer after the secret is revealed\n    events = list()\n    for initiator_state in payment_state.initiator_transfers.values():\n        channel_identifier = initiator_state.channel_identifier\n        channel_state = channelidentifiers_to_channels.get(channel_identifier)\n\n        if not channel_state:\n            continue\n\n        if can_cancel(initiator_state):\n            transfer_description = initiator_state.transfer_description\n            cancel_events = cancel_current_route(payment_state, initiator_state)\n\n            initiator_state.transfer_state = 'transfer_cancelled'\n\n            cancel = EventPaymentSentFailed(\n                payment_network_identifier=channel_state.payment_network_identifier,\n                token_network_identifier=TokenNetworkID(channel_state.token_network_identifier),\n                identifier=transfer_description.payment_identifier,\n                target=transfer_description.target,\n                reason='user canceled payment',\n            )\n            cancel_events.append(cancel)\n\n            events.extend(cancel_events)\n\n    return TransitionResult(payment_state, events)"}
{"code":"def decode(self) -> Iterable:\n        \"\"\"Start of decode process. Returns final results.\"\"\"\n        if self.data[0:1] not in (b'd', b'l'):\n            return self.__wrap_with_tuple()\n        return self.__parse()","return_type":"Iterable","function_name":"Decoder.decode","stripped_code":"def decode(self):\n        \"\"\"Start of decode process. Returns final results.\"\"\"\n        if self.data[0:1] not in (b'd', b'l'):\n            return self.__wrap_with_tuple()\n        return self.__parse()"}
{"code":"def _init_setup_console_data(self, order: str = \"C\") -> None:\n        \"\"\"Setup numpy arrays over libtcod data buffers.\"\"\"\n        global _root_console\n        self._key_color = None\n        if self.console_c == ffi.NULL:\n            _root_console = self\n            self._console_data = lib.TCOD_ctx.root\n        else:\n            self._console_data = ffi.cast(\n                \"struct TCOD_Console*\", self.console_c\n            )\n\n        self._tiles = np.frombuffer(\n            ffi.buffer(self._console_data.tiles[0 : self.width * self.height]),\n            dtype=self.DTYPE,\n        ).reshape((self.height, self.width))\n\n        self._order = tcod._internal.verify_order(order)","return_type":"None","function_name":"Console._init_setup_console_data","stripped_code":"def _init_setup_console_data(self, order: str = \"C\"):\n        \"\"\"Setup numpy arrays over libtcod data buffers.\"\"\"\n        global _root_console\n        self._key_color = None\n        if self.console_c == ffi.NULL:\n            _root_console = self\n            self._console_data = lib.TCOD_ctx.root\n        else:\n            self._console_data = ffi.cast(\n                \"struct TCOD_Console*\", self.console_c\n            )\n\n        self._tiles = np.frombuffer(\n            ffi.buffer(self._console_data.tiles[0 : self.width * self.height]),\n            dtype=self.DTYPE,\n        ).reshape((self.height, self.width))\n\n        self._order = tcod._internal.verify_order(order)"}
{"code":"def run(self, ket: State) -> State:\n        \"\"\"Apply the action of this gate upon a state\"\"\"\n        qubits = self.qubits\n        indices = [ket.qubits.index(q) for q in qubits]\n        tensor = bk.tensormul(self.tensor, ket.tensor, indices)\n        return State(tensor, ket.qubits, ket.memory)","return_type":"State","function_name":"Gate.run","stripped_code":"def run(self, ket: State):\n        \"\"\"Apply the action of this gate upon a state\"\"\"\n        qubits = self.qubits\n        indices = [ket.qubits.index(q) for q in qubits]\n        tensor = bk.tensormul(self.tensor, ket.tensor, indices)\n        return State(tensor, ket.qubits, ket.memory)"}
{"code":"def square_distance(a: Square, b: Square) -> int:\n    \"\"\"\n    Gets the distance (i.e., the number of king steps) from square *a* to *b*.\n    \"\"\"\n    return max(abs(square_file(a) - square_file(b)), abs(square_rank(a) - square_rank(b)))","return_type":"int","function_name":"square_distance","stripped_code":"def square_distance(a: Square, b: Square):\n    \"\"\"\n    Gets the distance (i.e., the number of king steps) from square *a* to *b*.\n    \"\"\"\n    return max(abs(square_file(a) - square_file(b)), abs(square_rank(a) - square_rank(b)))"}
{"code":"def get_network_by_id(self, network_id: int) -> Network:\n        \"\"\"Get a network from the database by its identifier.\"\"\"\n        return self.session.query(Network).get(network_id)","return_type":"Network","function_name":"NetworkManager.get_network_by_id","stripped_code":"def get_network_by_id(self, network_id: int):\n        \"\"\"Get a network from the database by its identifier.\"\"\"\n        return self.session.query(Network).get(network_id)"}
{"code":"def get_column_names(engine: Engine, tablename: str) -> List[str]:\n    \"\"\"\n    Get all the database column names for the specified table.\n    \"\"\"\n    return [info.name for info in gen_columns_info(engine, tablename)]","return_type":"List[str]","function_name":"get_column_names","stripped_code":"def get_column_names(engine: Engine, tablename: str):\n    \"\"\"\n    Get all the database column names for the specified table.\n    \"\"\"\n    return [info.name for info in gen_columns_info(engine, tablename)]"}
{"code":"def parse_reading(val: str) -> Optional[float]:\n    \"\"\" Convert reading value to float (if possible) \"\"\"\n    try:\n        return float(val)\n    except ValueError:\n        logging.warning('Reading of \"%s\" is not a number', val)\n        return None","return_type":"Optional[float]","function_name":"parse_reading","stripped_code":"def parse_reading(val: str):\n    \"\"\" Convert reading value to float (if possible) \"\"\"\n    try:\n        return float(val)\n    except ValueError:\n        logging.warning('Reading of \"%s\" is not a number', val)\n        return None"}
{"code":"def defer(\n            self,\n            func: typing.Callable[[], typing.Any],\n            until: typing.Union[int, float]=-1,\n    ) -> typing.Any:\n        \"\"\"Defer the execution of a function until some clock value.\n\n        Args:\n            func (typing.Callable[[], typing.Any]): A callable that accepts no\n                arguments. All return values are ignored.\n            until (typing.Union[int, float]): A numeric value that represents\n                the clock time when the callback becomes available for\n                execution. Values that are less than the current time result in\n                the function being called at the next opportunity.\n\n        Returns:\n            typing.Any: An opaque identifier that represents the callback\n                uniquely within the processor. This identifier is used to\n                modify the callback scheduling.\n\n        Note:\n            The time given should not be considered absolute. It represents\n            the time when the callback becomes available to execute. It may\n            be much later than the given time value when the function actually\n            executes depending on the implementation.\n        \"\"\"\n        raise NotImplementedError()","return_type":"typing.Any","function_name":"ITime.defer","stripped_code":"def defer(\n            self,\n            func: typing.Callable[[], typing.Any],\n            until: typing.Union[int, float]=-1,\n    ):\n        \"\"\"Defer the execution of a function until some clock value.\n\n        Args:\n            func (typing.Callable[[], typing.Any]): A callable that accepts no\n                arguments. All return values are ignored.\n            until (typing.Union[int, float]): A numeric value that represents\n                the clock time when the callback becomes available for\n                execution. Values that are less than the current time result in\n                the function being called at the next opportunity.\n\n        Returns:\n            typing.Any: An opaque identifier that represents the callback\n                uniquely within the processor. This identifier is used to\n                modify the callback scheduling.\n\n        Note:\n            The time given should not be considered absolute. It represents\n            the time when the callback becomes available to execute. It may\n            be much later than the given time value when the function actually\n            executes depending on the implementation.\n        \"\"\"\n        raise NotImplementedError()"}
{"code":"def write(self, data: Union[bytes, memoryview]) -> \"Future[None]\":\n        \"\"\"Asynchronously write the given data to this stream.\n\n        This method returns a `.Future` that resolves (with a result\n        of ``None``) when the write has been completed.\n\n        The ``data`` argument may be of type `bytes` or `memoryview`.\n\n        .. versionchanged:: 4.0\n            Now returns a `.Future` if no callback is given.\n\n        .. versionchanged:: 4.5\n            Added support for `memoryview` arguments.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.\n\n        \"\"\"\n        self._check_closed()\n        if data:\n            if (\n                self.max_write_buffer_size is not None\n                and len(self._write_buffer) + len(data) > self.max_write_buffer_size\n            ):\n                raise StreamBufferFullError(\"Reached maximum write buffer size\")\n            self._write_buffer.append(data)\n            self._total_write_index += len(data)\n        future = Future()  # type: Future[None]\n        future.add_done_callback(lambda f: f.exception())\n        self._write_futures.append((self._total_write_index, future))\n        if not self._connecting:\n            self._handle_write()\n            if self._write_buffer:\n                self._add_io_state(self.io_loop.WRITE)\n            self._maybe_add_error_listener()\n        return future","return_type":"\"Future[None]\"","function_name":"BaseIOStream.write","stripped_code":"def write(self, data: Union[bytes, memoryview]):\n        \"\"\"Asynchronously write the given data to this stream.\n\n        This method returns a `.Future` that resolves (with a result\n        of ``None``) when the write has been completed.\n\n        The ``data`` argument may be of type `bytes` or `memoryview`.\n\n        .. versionchanged:: 4.0\n            Now returns a `.Future` if no callback is given.\n\n        .. versionchanged:: 4.5\n            Added support for `memoryview` arguments.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.\n\n        \"\"\"\n        self._check_closed()\n        if data:\n            if (\n                self.max_write_buffer_size is not None\n                and len(self._write_buffer) + len(data) > self.max_write_buffer_size\n            ):\n                raise StreamBufferFullError(\"Reached maximum write buffer size\")\n            self._write_buffer.append(data)\n            self._total_write_index += len(data)\n        future = Future()  # type: Future[None]\n        future.add_done_callback(lambda f: f.exception())\n        self._write_futures.append((self._total_write_index, future))\n        if not self._connecting:\n            self._handle_write()\n            if self._write_buffer:\n                self._add_io_state(self.io_loop.WRITE)\n            self._maybe_add_error_listener()\n        return future"}
{"code":"def _parse_multifile(self, desired_type: Type[T], obj: PersistedObject,\n                         parsing_plan_for_children: Dict[str, AnyParser._RecursiveParsingPlan],\n                         logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        raise Exception('This should never happen, since this parser relies on underlying parsers')","return_type":"T","function_name":"DelegatingParser._parse_multifile","stripped_code":"def _parse_multifile(self, desired_type: Type[T], obj: PersistedObject,\n                         parsing_plan_for_children: Dict[str, AnyParser._RecursiveParsingPlan],\n                         logger: Logger, options: Dict[str, Dict[str, Any]]):\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        raise Exception('This should never happen, since this parser relies on underlying parsers')"}
{"code":"def visit_SetComp(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s representation as set comprehension.\"\"\"\n        return f\"{{{self.visit(node.elt)} \" \\\n               f\"{' '.join(self.visit(gen) for gen in node.generators)}}}\"","return_type":"str","function_name":"SourceGenerator.visit_SetComp","stripped_code":"def visit_SetComp(self, node: AST, dfltChaining: bool = True):\n        \"\"\"Return `node`s representation as set comprehension.\"\"\"\n        return f\"{{{self.visit(node.elt)} \" \\\n               f\"{' '.join(self.visit(gen) for gen in node.generators)}}}\""}
{"code":"def bsp_split_once(\n    node: tcod.bsp.BSP, horizontal: bool, position: int\n) -> None:\n    \"\"\"\n    .. deprecated:: 2.0\n       Use :any:`BSP.split_once` instead.\n    \"\"\"\n    node.split_once(horizontal, position)","return_type":"None","function_name":"bsp_split_once","stripped_code":"def bsp_split_once(\n    node: tcod.bsp.BSP, horizontal: bool, position: int\n):\n    \"\"\"\n    .. deprecated:: 2.0\n       Use :any:`BSP.split_once` instead.\n    \"\"\"\n    node.split_once(horizontal, position)"}
{"code":"def parse_card_transfer_metainfo(protobuf: bytes, deck_version: int) -> dict:\n    '''decode card_spawn protobuf message and validate it against deck.version\n    :protobuf - bytes from op_return message\n    :deck_version - integer\n    '''\n\n    card = CardTransferProto()\n    card.ParseFromString(protobuf)\n\n    if not card.version == deck_version:\n        raise CardVersionMismatch({'error': 'card version does not match deck version.'})\n\n    return {\n        \"version\": card.version,\n        \"number_of_decimals\": card.number_of_decimals,\n        \"amount\": list(card.amount),\n        \"asset_specific_data\": card.asset_specific_data\n    }","return_type":"dict","function_name":"parse_card_transfer_metainfo","stripped_code":"def parse_card_transfer_metainfo(protobuf: bytes, deck_version: int):\n    '''decode card_spawn protobuf message and validate it against deck.version\n    :protobuf - bytes from op_return message\n    :deck_version - integer\n    '''\n\n    card = CardTransferProto()\n    card.ParseFromString(protobuf)\n\n    if not card.version == deck_version:\n        raise CardVersionMismatch({'error': 'card version does not match deck version.'})\n\n    return {\n        \"version\": card.version,\n        \"number_of_decimals\": card.number_of_decimals,\n        \"amount\": list(card.amount),\n        \"asset_specific_data\": card.asset_specific_data\n    }"}
{"code":"def phys_name(self, item: str) -> str:\n        \"\"\"Return the physical (mapped) name of item.\n\n        :param item: logical table name\n        :return: physical name of table\n        \"\"\"\n        v = self.__dict__[item]\n        return v if v is not None else item","return_type":"str","function_name":"_I2B2Tables.phys_name","stripped_code":"def phys_name(self, item: str):\n        \"\"\"Return the physical (mapped) name of item.\n\n        :param item: logical table name\n        :return: physical name of table\n        \"\"\"\n        v = self.__dict__[item]\n        return v if v is not None else item"}
{"code":"def _run(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Wraps around the process body (the function that implements a process within the simulation) so as to catch the\n        eventual Interrupt that may terminate the process.\n        \"\"\"\n        try:\n            self._body(*args, **kwargs)\n            if _logger is not None:\n                _log(INFO, \"Process\", self.local.name, \"die-finish\")\n        except Interrupt:\n            if _logger is not None:\n                _log(INFO, \"Process\", self.local.name, \"die-interrupt\")","return_type":"None","function_name":"Process._run","stripped_code":"def _run(self, *args: Any, **kwargs: Any):\n        \"\"\"\n        Wraps around the process body (the function that implements a process within the simulation) so as to catch the\n        eventual Interrupt that may terminate the process.\n        \"\"\"\n        try:\n            self._body(*args, **kwargs)\n            if _logger is not None:\n                _log(INFO, \"Process\", self.local.name, \"die-finish\")\n        except Interrupt:\n            if _logger is not None:\n                _log(INFO, \"Process\", self.local.name, \"die-interrupt\")"}
{"code":"def ungeometrize_shapes(geo_shapes) -> DataFrame:\n    \"\"\"\n    The inverse of :func:`geometrize_shapes`.\n    Produces the columns:\n\n    - ``'shape_id'``\n    - ``'shape_pt_sequence'``\n    - ``'shape_pt_lon'``\n    - ``'shape_pt_lat'``\n\n    If ``geo_shapes`` is in UTM coordinates (has a UTM CRS property),\n    then convert thoes UTM coordinates back to WGS84 coordinates,\n    which is the standard for a GTFS shapes table.\n    \"\"\"\n    geo_shapes = geo_shapes.to_crs(cs.WGS84)\n\n    F = []\n    for index, row in geo_shapes.iterrows():\n        F.extend(\n            [\n                [row[\"shape_id\"], i, x, y]\n                for i, (x, y) in enumerate(row[\"geometry\"].coords)\n            ]\n        )\n\n    return pd.DataFrame(\n        F,\n        columns=[\n            \"shape_id\",\n            \"shape_pt_sequence\",\n            \"shape_pt_lon\",\n            \"shape_pt_lat\",\n        ],\n    )","return_type":"DataFrame","function_name":"ungeometrize_shapes","stripped_code":"def ungeometrize_shapes(geo_shapes):\n    \"\"\"\n    The inverse of :func:`geometrize_shapes`.\n    Produces the columns:\n\n    - ``'shape_id'``\n    - ``'shape_pt_sequence'``\n    - ``'shape_pt_lon'``\n    - ``'shape_pt_lat'``\n\n    If ``geo_shapes`` is in UTM coordinates (has a UTM CRS property),\n    then convert thoes UTM coordinates back to WGS84 coordinates,\n    which is the standard for a GTFS shapes table.\n    \"\"\"\n    geo_shapes = geo_shapes.to_crs(cs.WGS84)\n\n    F = []\n    for index, row in geo_shapes.iterrows():\n        F.extend(\n            [\n                [row[\"shape_id\"], i, x, y]\n                for i, (x, y) in enumerate(row[\"geometry\"].coords)\n            ]\n        )\n\n    return pd.DataFrame(\n        F,\n        columns=[\n            \"shape_id\",\n            \"shape_pt_sequence\",\n            \"shape_pt_lon\",\n            \"shape_pt_lat\",\n        ],\n    )"}
{"code":"def wrap_exception(func: Callable) -> Callable:\n    \"\"\"Decorator to wrap BTLEExceptions into BluetoothBackendException.\"\"\"\n    try:\n        # only do the wrapping if bluepy is installed.\n        # otherwise it's pointless anyway\n        from bluepy.btle import BTLEException\n    except ImportError:\n        return func\n\n    def _func_wrapper(*args, **kwargs):\n        error_count = 0\n        last_error = None\n        while error_count < RETRY_LIMIT:\n            try:\n                return func(*args, **kwargs)\n            except BTLEException as exception:\n                error_count += 1\n                last_error = exception\n                time.sleep(RETRY_DELAY)\n                _LOGGER.debug('Call to %s failed, try %d of %d', func, error_count, RETRY_LIMIT)\n        raise BluetoothBackendException() from last_error\n\n    return _func_wrapper","return_type":"Callable","function_name":"wrap_exception","stripped_code":"def wrap_exception(func: Callable):\n    \"\"\"Decorator to wrap BTLEExceptions into BluetoothBackendException.\"\"\"\n    try:\n        # only do the wrapping if bluepy is installed.\n        # otherwise it's pointless anyway\n        from bluepy.btle import BTLEException\n    except ImportError:\n        return func\n\n    def _func_wrapper(*args, **kwargs):\n        error_count = 0\n        last_error = None\n        while error_count < RETRY_LIMIT:\n            try:\n                return func(*args, **kwargs)\n            except BTLEException as exception:\n                error_count += 1\n                last_error = exception\n                time.sleep(RETRY_DELAY)\n                _LOGGER.debug('Call to %s failed, try %d of %d', func, error_count, RETRY_LIMIT)\n        raise BluetoothBackendException() from last_error\n\n    return _func_wrapper"}
{"code":"def prepare_modules(module_paths: list, available: dict) -> dict:\n    \"\"\"\n    Scan all paths for external modules and form key-value dict.\n    :param module_paths: list of external modules (either python packages or third-party scripts)\n    :param available: dict of all registered python modules (can contain python modules from module_paths)\n    :return: dict of external modules, where keys are filenames (same as stepnames) and values are the paths\n    \"\"\"\n    indexed = {}\n    for path in module_paths:\n        if not os.path.exists(path) and path not in available:\n            err = 'No such path: ' + path\n            error(err)\n        else:\n            for f in os.listdir(path):\n                mod_path = join(path, f)\n                if f in indexed:\n                    warning('Override ' + indexed[f] + ' with ' + mod_path)\n                indexed[f] = mod_path\n    return indexed","return_type":"dict","function_name":"prepare_modules","stripped_code":"def prepare_modules(module_paths: list, available: dict):\n    \"\"\"\n    Scan all paths for external modules and form key-value dict.\n    :param module_paths: list of external modules (either python packages or third-party scripts)\n    :param available: dict of all registered python modules (can contain python modules from module_paths)\n    :return: dict of external modules, where keys are filenames (same as stepnames) and values are the paths\n    \"\"\"\n    indexed = {}\n    for path in module_paths:\n        if not os.path.exists(path) and path not in available:\n            err = 'No such path: ' + path\n            error(err)\n        else:\n            for f in os.listdir(path):\n                mod_path = join(path, f)\n                if f in indexed:\n                    warning('Override ' + indexed[f] + ' with ' + mod_path)\n                indexed[f] = mod_path\n    return indexed"}
{"code":"def add_untagged(self, *responses: 'Response') -> None:\n        \"\"\"Add an untagged response. These responses are shown before the\n        parent response.\n\n        Args:\n            responses: The untagged responses to add.\n\n        \"\"\"\n        for resp in responses:\n            try:\n                merge_key = resp.merge_key\n            except TypeError:\n                self._untagged.append(resp)\n            else:\n                key = (type(resp), merge_key)\n                try:\n                    untagged_idx = self._mergeable[key]\n                except KeyError:\n                    untagged_idx = len(self._untagged)\n                    self._mergeable[key] = untagged_idx\n                    self._untagged.append(resp)\n                else:\n                    merged = self._untagged[untagged_idx].merge(resp)\n                    self._untagged[untagged_idx] = merged\n        self._raw = None","return_type":"None","function_name":"Response.add_untagged","stripped_code":"def add_untagged(self, *responses: 'Response'):\n        \"\"\"Add an untagged response. These responses are shown before the\n        parent response.\n\n        Args:\n            responses: The untagged responses to add.\n\n        \"\"\"\n        for resp in responses:\n            try:\n                merge_key = resp.merge_key\n            except TypeError:\n                self._untagged.append(resp)\n            else:\n                key = (type(resp), merge_key)\n                try:\n                    untagged_idx = self._mergeable[key]\n                except KeyError:\n                    untagged_idx = len(self._untagged)\n                    self._mergeable[key] = untagged_idx\n                    self._untagged.append(resp)\n                else:\n                    merged = self._untagged[untagged_idx].merge(resp)\n                    self._untagged[untagged_idx] = merged\n        self._raw = None"}
{"code":"def gen_typedefs(self) -> str:\n        \"\"\" Generate python type declarations for all defined types \"\"\"\n        rval = []\n        for typ in self.schema.types.values():\n            typname = self.python_name_for(typ.name)\n            parent = self.python_name_for(typ.typeof)\n            rval.append(f'class {typname}({parent}):\\n\\tpass')\n        return '\\n\\n\\n'.join(rval) + ('\\n' if rval else '')","return_type":"str","function_name":"PythonGenerator.gen_typedefs","stripped_code":"def gen_typedefs(self):\n        \"\"\" Generate python type declarations for all defined types \"\"\"\n        rval = []\n        for typ in self.schema.types.values():\n            typname = self.python_name_for(typ.name)\n            parent = self.python_name_for(typ.typeof)\n            rval.append(f'class {typname}({parent}):\\n\\tpass')\n        return '\\n\\n\\n'.join(rval) + ('\\n' if rval else '')"}
{"code":"def predict_on_batch(self, data: Union[list, tuple],\n                         return_indexes: bool = False) -> List[List[str]]:\n        \"\"\"\n        Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences\n        \"\"\"\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[List[str]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer","return_type":"List[List[str]]","function_name":"CharacterTagger.predict_on_batch","stripped_code":"def predict_on_batch(self, data: Union[list, tuple],\n                         return_indexes: bool = False):\n        \"\"\"\n        Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences\n        \"\"\"\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[List[str]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer"}
{"code":"def trunc_str(s: str) -> str:\n    \"\"\"Truncate strings to maximum length.\"\"\"\n    if len(s) > max_str_size:\n        i = max(0, (max_str_size - 3) // 2)\n        j = max(0, max_str_size - 3 - i)\n        s = s[:i] + \"...\" + s[-j:]\n    return s","return_type":"str","function_name":"trunc_str","stripped_code":"def trunc_str(s: str):\n    \"\"\"Truncate strings to maximum length.\"\"\"\n    if len(s) > max_str_size:\n        i = max(0, (max_str_size - 3) // 2)\n        j = max(0, max_str_size - 3 - i)\n        s = s[:i] + \"...\" + s[-j:]\n    return s"}
{"code":"def in_project_directory() -> bool:\n    \"\"\"\n    Returns whether or not the current working directory is a Cauldron project\n    directory, which contains a cauldron.json file.\n    \"\"\"\n    current_directory = os.path.realpath(os.curdir)\n    project_path = os.path.join(current_directory, 'cauldron.json')\n    return os.path.exists(project_path) and os.path.isfile(project_path)","return_type":"bool","function_name":"in_project_directory","stripped_code":"def in_project_directory():\n    \"\"\"\n    Returns whether or not the current working directory is a Cauldron project\n    directory, which contains a cauldron.json file.\n    \"\"\"\n    current_directory = os.path.realpath(os.curdir)\n    project_path = os.path.join(current_directory, 'cauldron.json')\n    return os.path.exists(project_path) and os.path.isfile(project_path)"}
{"code":"def extracts(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            'action': 'query',\n            'prop': 'extracts',\n            'titles': page.title\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params['explaintext'] = 1\n            params['exsectionformat'] = 'wiki'\n        # elif self.extract_format == ExtractFormat.PLAIN:\n        #    params['explaintext'] = 1\n        #    params['exsectionformat'] = 'plain'\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n        self._common_attributes(raw['query'], page)\n        pages = raw['query']['pages']\n        for k, v in pages.items():\n            if k == '-1':\n                page._attributes['pageid'] = -1\n                return ''\n            else:\n                return self._build_extracts(v, page)\n        return ''","return_type":"str","function_name":"Wikipedia.extracts","stripped_code":"def extracts(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ):\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            'action': 'query',\n            'prop': 'extracts',\n            'titles': page.title\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params['explaintext'] = 1\n            params['exsectionformat'] = 'wiki'\n        # elif self.extract_format == ExtractFormat.PLAIN:\n        #    params['explaintext'] = 1\n        #    params['exsectionformat'] = 'plain'\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n        self._common_attributes(raw['query'], page)\n        pages = raw['query']['pages']\n        for k, v in pages.items():\n            if k == '-1':\n                page._attributes['pageid'] = -1\n                return ''\n            else:\n                return self._build_extracts(v, page)\n        return ''"}
{"code":"def hashfile(name: str) -> str:\n        \"\"\"\n        Gets a hash of a file using block parsing\n\n        http://stackoverflow.com/questions/3431825/generating-a-md5-checksum-of-a-file\n        Using SHA512 for long-term support (hehehehe)\n        \"\"\"\n        hasher = hashlib.sha512()\n        with open(name, 'rb') as openfile:\n            for chunk in iter(lambda: openfile.read(4096), b''):\n                hasher.update(chunk)\n        return hasher.hexdigest()","return_type":"str","function_name":"DataStore.hashfile","stripped_code":"def hashfile(name: str):\n        \"\"\"\n        Gets a hash of a file using block parsing\n\n        http://stackoverflow.com/questions/3431825/generating-a-md5-checksum-of-a-file\n        Using SHA512 for long-term support (hehehehe)\n        \"\"\"\n        hasher = hashlib.sha512()\n        with open(name, 'rb') as openfile:\n            for chunk in iter(lambda: openfile.read(4096), b''):\n                hasher.update(chunk)\n        return hasher.hexdigest()"}
{"code":"def set_circular(self, circular: bool, chain: List[Table] = None) -> None:\n        \"\"\"\n        Mark this table as circular (or not).\n\n        Args:\n            circular: is it circular?\n            chain: if it's circular, this should be the list of tables\n                participating in the circular chain\n        \"\"\"\n        self.circular = circular\n        self.circular_chain = chain or []","return_type":"None","function_name":"TableDependencyClassification.set_circular","stripped_code":"def set_circular(self, circular: bool, chain: List[Table] = None):\n        \"\"\"\n        Mark this table as circular (or not).\n\n        Args:\n            circular: is it circular?\n            chain: if it's circular, this should be the list of tables\n                participating in the circular chain\n        \"\"\"\n        self.circular = circular\n        self.circular_chain = chain or []"}
{"code":"def to_html(graph: BELGraph, chart: Optional[str] = None) -> str:\n    \"\"\"Render the graph as an HTML string.\n\n    Common usage may involve writing to a file like:\n\n    >>> from pybel.examples import sialic_acid_graph\n    >>> with open('ideogram_output.html', 'w') as file:\n    ...     print(to_html(sialic_acid_graph), file=file)\n    \"\"\"\n    with open(os.path.join(HERE, 'index.html'), 'rt') as f:\n        html_template = Template(f.read())\n\n    return html_template.render(**_get_context(graph, chart=chart))","return_type":"str","function_name":"to_html","stripped_code":"def to_html(graph: BELGraph, chart: Optional[str] = None):\n    \"\"\"Render the graph as an HTML string.\n\n    Common usage may involve writing to a file like:\n\n    >>> from pybel.examples import sialic_acid_graph\n    >>> with open('ideogram_output.html', 'w') as file:\n    ...     print(to_html(sialic_acid_graph), file=file)\n    \"\"\"\n    with open(os.path.join(HERE, 'index.html'), 'rt') as f:\n        html_template = Template(f.read())\n\n    return html_template.render(**_get_context(graph, chart=chart))"}
{"code":"def current_timestamp(self) -> datetime:\n        \"\"\"Get the current state timestamp.\"\"\"\n        timestamp = DB.get_hash_value(self._key, 'current_timestamp')\n        return datetime_from_isoformat(timestamp)","return_type":"datetime","function_name":"StateObject.current_timestamp","stripped_code":"def current_timestamp(self):\n        \"\"\"Get the current state timestamp.\"\"\"\n        timestamp = DB.get_hash_value(self._key, 'current_timestamp')\n        return datetime_from_isoformat(timestamp)"}
{"code":"def unique_name(device: InputDevice) -> str:\n    \"\"\"\n    Construct a unique name for the device based on, in order if available, the uniq ID, the phys ID and\n    finally a concatenation of vendor, product, version and filename.\n\n    :param device:\n        An InputDevice instance to query\n    :return:\n        A string containing as unique as possible a name for the physical entity represented by the device\n    \"\"\"\n    if device.uniq:\n        return device.uniq\n    elif device.phys:\n        return device.phys.split('/')[0]\n    return '{}-{}-{}-{}'.format(device.info.vendor, device.info.product, device.info.version, device.path)","return_type":"str","function_name":"unique_name","stripped_code":"def unique_name(device: InputDevice):\n    \"\"\"\n    Construct a unique name for the device based on, in order if available, the uniq ID, the phys ID and\n    finally a concatenation of vendor, product, version and filename.\n\n    :param device:\n        An InputDevice instance to query\n    :return:\n        A string containing as unique as possible a name for the physical entity represented by the device\n    \"\"\"\n    if device.uniq:\n        return device.uniq\n    elif device.phys:\n        return device.phys.split('/')[0]\n    return '{}-{}-{}-{}'.format(device.info.vendor, device.info.product, device.info.version, device.path)"}
{"code":"def add_cssfile(self, src: str) -> None:\n        \"\"\"Add CSS file to load at this document's header.\"\"\"\n        self.head.appendChild(Link(rel='stylesheet', href=src))","return_type":"None","function_name":"WdomDocument.add_cssfile","stripped_code":"def add_cssfile(self, src: str):\n        \"\"\"Add CSS file to load at this document's header.\"\"\"\n        self.head.appendChild(Link(rel='stylesheet', href=src))"}
{"code":"def close(self, timeout: int = 5) -> None:\n        \"\"\"Stop a ffmpeg instance.\n\n        Return a coroutine\n        \"\"\"\n        if self._read_task is not None and not self._read_task.cancelled():\n            self._read_task.cancel()\n\n        return super().close(timeout)","return_type":"None","function_name":"HAFFmpegWorker.close","stripped_code":"def close(self, timeout: int = 5):\n        \"\"\"Stop a ffmpeg instance.\n\n        Return a coroutine\n        \"\"\"\n        if self._read_task is not None and not self._read_task.cancelled():\n            self._read_task.cancel()\n\n        return super().close(timeout)"}
{"code":"def network_protocol(self, layer: Optional[Layer] = None) -> str:\n        \"\"\"Get a random network protocol form OSI model.\n\n        :param layer: Enum object Layer.\n        :return: Protocol name.\n\n        :Example:\n            AMQP\n        \"\"\"\n        key = self._validate_enum(item=layer, enum=Layer)\n        protocols = NETWORK_PROTOCOLS[key]\n        return self.random.choice(protocols)","return_type":"str","function_name":"Internet.network_protocol","stripped_code":"def network_protocol(self, layer: Optional[Layer] = None):\n        \"\"\"Get a random network protocol form OSI model.\n\n        :param layer: Enum object Layer.\n        :return: Protocol name.\n\n        :Example:\n            AMQP\n        \"\"\"\n        key = self._validate_enum(item=layer, enum=Layer)\n        protocols = NETWORK_PROTOCOLS[key]\n        return self.random.choice(protocols)"}
{"code":"def plot_file(self, name: str=None, time: int=None) -> None:\n        \"\"\"\n        Plot specific time for provided datafile.\n        If no time provided, will plot middle.\n\n        :param: savefile name\n        :param: time/data column\n        \"\"\"\n        if not time:\n            time = int(len(self.times) / 2)\n        if not name:\n            name = './img/' + self.filename + '.png'\n        yhat, residuals, residual_mean, noise = self._get_fit(time)\n        plt.figure()\n        plt.scatter(self.domain, self.averagedata[:, time], alpha=0.2)\n        plt.plot(yhat)\n        plt.savefig(name)","return_type":"None","function_name":"LineFit.plot_file","stripped_code":"def plot_file(self, name: str=None, time: int=None):\n        \"\"\"\n        Plot specific time for provided datafile.\n        If no time provided, will plot middle.\n\n        :param: savefile name\n        :param: time/data column\n        \"\"\"\n        if not time:\n            time = int(len(self.times) / 2)\n        if not name:\n            name = './img/' + self.filename + '.png'\n        yhat, residuals, residual_mean, noise = self._get_fit(time)\n        plt.figure()\n        plt.scatter(self.domain, self.averagedata[:, time], alpha=0.2)\n        plt.plot(yhat)\n        plt.savefig(name)"}
{"code":"def safe_read_file(file_path: Path) -> str:\n    \"\"\"Read a text file. Several text encodings are tried until\n    the file content is correctly decoded.\n\n    :raise GuesslangError: when the file encoding is not supported\n    :param file_path: path to the input file\n    :return: text file content\n    \"\"\"\n    for encoding in FILE_ENCODINGS:\n        try:\n            return file_path.read_text(encoding=encoding)\n        except UnicodeError:\n            pass  # Ignore encoding error\n\n    raise GuesslangError('Encoding not supported for {!s}'.format(file_path))","return_type":"str","function_name":"safe_read_file","stripped_code":"def safe_read_file(file_path: Path):\n    \"\"\"Read a text file. Several text encodings are tried until\n    the file content is correctly decoded.\n\n    :raise GuesslangError: when the file encoding is not supported\n    :param file_path: path to the input file\n    :return: text file content\n    \"\"\"\n    for encoding in FILE_ENCODINGS:\n        try:\n            return file_path.read_text(encoding=encoding)\n        except UnicodeError:\n            pass  # Ignore encoding error\n\n    raise GuesslangError('Encoding not supported for {!s}'.format(file_path))"}
{"code":"def _handle_launch(self, request: dict) -> dict:\n        \"\"\"Handles LaunchRequest Alexa request.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: \"response\" part of response dict conforming Alexa specification.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.config['start_message']\n                },\n                'card': {\n                    'type': 'Simple',\n                    'content': self.config['start_message']\n                }\n            }\n        }\n\n        response = self._generate_response(response, request)\n\n        return response","return_type":"dict","function_name":"Conversation._handle_launch","stripped_code":"def _handle_launch(self, request: dict):\n        \"\"\"Handles LaunchRequest Alexa request.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: \"response\" part of response dict conforming Alexa specification.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.config['start_message']\n                },\n                'card': {\n                    'type': 'Simple',\n                    'content': self.config['start_message']\n                }\n            }\n        }\n\n        response = self._generate_response(response, request)\n\n        return response"}
{"code":"def dict_from_environs(prefix: str, name: str, args: list) -> dict:\n    \"\"\"\n    Return a dict of environment variables correlating to the arguments list, main name and prefix like so:\n    [prefix]_[name]_[arg]\n\n    :param prefix: The environ prefix to use\n    :param name: Main part\n    :param args: List of args to iterate over\n    :return: A dict of found environ values\n    \"\"\"\n    environs = {}\n    log.debug(\"starting to collect environs using prefix: '%s'\", prefix)\n    for arg in args:\n        environ = f\"{prefix}{name}_{arg}\".upper()\n        if os.environ.get(environ):\n            environs[arg] = os.environ[environ]\n    return environs","return_type":"dict","function_name":"dict_from_environs","stripped_code":"def dict_from_environs(prefix: str, name: str, args: list):\n    \"\"\"\n    Return a dict of environment variables correlating to the arguments list, main name and prefix like so:\n    [prefix]_[name]_[arg]\n\n    :param prefix: The environ prefix to use\n    :param name: Main part\n    :param args: List of args to iterate over\n    :return: A dict of found environ values\n    \"\"\"\n    environs = {}\n    log.debug(\"starting to collect environs using prefix: '%s'\", prefix)\n    for arg in args:\n        environ = f\"{prefix}{name}_{arg}\".upper()\n        if os.environ.get(environ):\n            environs[arg] = os.environ[environ]\n    return environs"}
{"code":"def func_factory(p, method: str) -> callable:\n    \"\"\"\n    Dynamically generates callback commands to correlate to provider public methods\n\n    :param p: A :class:`notifiers.core.Provider` object\n    :param method: A string correlating to a provider method\n    :return: A callback func\n    \"\"\"\n\n    def callback(pretty: bool = False):\n        res = getattr(p, method)\n        dump = partial(json.dumps, indent=4) if pretty else partial(json.dumps)\n        click.echo(dump(res))\n\n    return callback","return_type":"callable","function_name":"func_factory","stripped_code":"def func_factory(p, method: str):\n    \"\"\"\n    Dynamically generates callback commands to correlate to provider public methods\n\n    :param p: A :class:`notifiers.core.Provider` object\n    :param method: A string correlating to a provider method\n    :return: A callback func\n    \"\"\"\n\n    def callback(pretty: bool = False):\n        res = getattr(p, method)\n        dump = partial(json.dumps, indent=4) if pretty else partial(json.dumps)\n        click.echo(dump(res))\n\n    return callback"}
{"code":"def on_train_begin(self, **kwargs:Any)->None:\n        \"Initializes the best value.\"\n        self.best = float('inf') if self.operator == np.less else -float('inf')","return_type":"None","function_name":"TrackerCallback.on_train_begin","stripped_code":"def on_train_begin(self, **kwargs:Any):\n        \"Initializes the best value.\"\n        self.best = float('inf') if self.operator == np.less else -float('inf')"}
{"code":"def index_fields(self, vocab: Vocabulary) -> None:\n        \"\"\"\n        Indexes all fields in this ``Instance`` using the provided ``Vocabulary``.\n        This `mutates` the current object, it does not return a new ``Instance``.\n        A ``DataIterator`` will call this on each pass through a dataset; we use the ``indexed``\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you've\n        indexed your instances, you might get unexpected behavior.\n        \"\"\"\n        if not self.indexed:\n            self.indexed = True\n            for field in self.fields.values():\n                field.index(vocab)","return_type":"None","function_name":"Instance.index_fields","stripped_code":"def index_fields(self, vocab: Vocabulary):\n        \"\"\"\n        Indexes all fields in this ``Instance`` using the provided ``Vocabulary``.\n        This `mutates` the current object, it does not return a new ``Instance``.\n        A ``DataIterator`` will call this on each pass through a dataset; we use the ``indexed``\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you've\n        indexed your instances, you might get unexpected behavior.\n        \"\"\"\n        if not self.indexed:\n            self.indexed = True\n            for field in self.fields.values():\n                field.index(vocab)"}
{"code":"def inline(self) -> str:\n        \"\"\"\n        Return endpoint string\n\n        :return:\n        \"\"\"\n        inlined = [str(info) for info in (self.server, self.port) if info]\n        return ESSubscribtionEndpoint.API + \" \" + \" \".join(inlined)","return_type":"str","function_name":"ESSubscribtionEndpoint.inline","stripped_code":"def inline(self):\n        \"\"\"\n        Return endpoint string\n\n        :return:\n        \"\"\"\n        inlined = [str(info) for info in (self.server, self.port) if info]\n        return ESSubscribtionEndpoint.API + \" \" + \" \".join(inlined)"}
{"code":"def generate_base(path: str) -> str:\n    \"\"\" Convert path, which can be a URL or a file path into a base URI\n\n    :param path: file location or url\n    :return: file location or url sans actual name\n    \"\"\"\n    if ':' in path:\n        parts = urlparse(path)\n        parts_dict = parts._asdict()\n        parts_dict['path'] = os.path.split(parts.path)[0] if '/' in parts.path else ''\n        return urlunparse(ParseResult(**parts_dict)) + '/'\n    else:\n        return (os.path.split(path)[0] if '/' in path else '') + '/'","return_type":"str","function_name":"generate_base","stripped_code":"def generate_base(path: str):\n    \"\"\" Convert path, which can be a URL or a file path into a base URI\n\n    :param path: file location or url\n    :return: file location or url sans actual name\n    \"\"\"\n    if ':' in path:\n        parts = urlparse(path)\n        parts_dict = parts._asdict()\n        parts_dict['path'] = os.path.split(parts.path)[0] if '/' in parts.path else ''\n        return urlunparse(ParseResult(**parts_dict)) + '/'\n    else:\n        return (os.path.split(path)[0] if '/' in path else '') + '/'"}
{"code":"def uci(self, move: Move, *, chess960: Optional[bool] = None) -> str:\n        \"\"\"\n        Gets the UCI notation of the move.\n\n        *chess960* defaults to the mode of the board. Pass ``True`` to force\n        Chess960 mode.\n        \"\"\"\n        if chess960 is None:\n            chess960 = self.chess960\n\n        move = self._to_chess960(move)\n        move = self._from_chess960(chess960, move.from_square, move.to_square, move.promotion, move.drop)\n        return move.uci()","return_type":"str","function_name":"Board.uci","stripped_code":"def uci(self, move: Move, *, chess960: Optional[bool] = None):\n        \"\"\"\n        Gets the UCI notation of the move.\n\n        *chess960* defaults to the mode of the board. Pass ``True`` to force\n        Chess960 mode.\n        \"\"\"\n        if chess960 is None:\n            chess960 = self.chess960\n\n        move = self._to_chess960(move)\n        move = self._from_chess960(chess960, move.from_square, move.to_square, move.promotion, move.drop)\n        return move.uci()"}
{"code":"def score(self, file: str = '', code: str = '', out: 'SASdata' = None) -> 'SASdata':\n        \"\"\"\n        This method is meant to update a SAS Data object with a model score file.\n\n        :param file: a file reference to the SAS score code\n        :param code: a string of the valid SAS score code\n        :param out: Where to the write the file. Defaults to update in place\n        :return: The Scored SAS Data object.\n        \"\"\"\n        if out is not None:\n            outTable = out.table\n            outLibref = out.libref\n        else:\n            outTable = self.table\n            outLibref = self.libref\n        codestr = code\n        code = \"data %s.%s%s;\" % (outLibref, outTable, self._dsopts())\n        code += \"set %s.%s%s;\" % (self.libref, self.table, self._dsopts())\n        if len(file)>0:\n            code += '%%include \"%s\";' % file\n        else:\n            code += \"%s;\" %codestr\n        code += \"run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return None\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll","return_type":"'SASdata'","function_name":"SASdata.score","stripped_code":"def score(self, file: str = '', code: str = '', out: 'SASdata' = None):\n        \"\"\"\n        This method is meant to update a SAS Data object with a model score file.\n\n        :param file: a file reference to the SAS score code\n        :param code: a string of the valid SAS score code\n        :param out: Where to the write the file. Defaults to update in place\n        :return: The Scored SAS Data object.\n        \"\"\"\n        if out is not None:\n            outTable = out.table\n            outLibref = out.libref\n        else:\n            outTable = self.table\n            outLibref = self.libref\n        codestr = code\n        code = \"data %s.%s%s;\" % (outLibref, outTable, self._dsopts())\n        code += \"set %s.%s%s;\" % (self.libref, self.table, self._dsopts())\n        if len(file)>0:\n            code += '%%include \"%s\";' % file\n        else:\n            code += \"%s;\" %codestr\n        code += \"run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return None\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll"}
{"code":"def stack_push(self, value: Union[int, bytes]) -> None:\n        \"\"\"\n        Push ``value`` onto the stack.\n\n        Raise `eth.exceptions.StackDepthLimit` if the stack is full.\n        \"\"\"\n        return self._stack.push(value)","return_type":"None","function_name":"BaseComputation.stack_push","stripped_code":"def stack_push(self, value: Union[int, bytes]):\n        \"\"\"\n        Push ``value`` onto the stack.\n\n        Raise `eth.exceptions.StackDepthLimit` if the stack is full.\n        \"\"\"\n        return self._stack.push(value)"}
{"code":"def migrate(belstr: str) -> str:\n    \"\"\"Migrate BEL 1 to 2.0.0\n\n    Args:\n        bel: BEL 1\n\n    Returns:\n        bel: BEL 2\n    \"\"\"\n\n    bo.ast = bel.lang.partialparse.get_ast_obj(belstr, \"2.0.0\")\n\n    return migrate_ast(bo.ast).to_string()","return_type":"str","function_name":"migrate","stripped_code":"def migrate(belstr: str):\n    \"\"\"Migrate BEL 1 to 2.0.0\n\n    Args:\n        bel: BEL 1\n\n    Returns:\n        bel: BEL 2\n    \"\"\"\n\n    bo.ast = bel.lang.partialparse.get_ast_obj(belstr, \"2.0.0\")\n\n    return migrate_ast(bo.ast).to_string()"}
{"code":"def already_resolved(self, pattern: QueryTriple) -> bool:\n        \"\"\" Determine whether pattern has already been loaded into the cache.\n\n        The \"wild card\" - `(None, None, None)` - always counts as resolved.\n\n        :param pattern: pattern to check\n        :return: True it is a subset of elements already loaded\n        \"\"\"\n        if self.sparql_locked or pattern == (None, None, None):\n            return True\n        for resolved_node in self.resolved_nodes:\n            if resolved_node != (None, None, None) and \\\n                    (pattern[0] == resolved_node[0] or resolved_node[0] is None) and \\\n                    (pattern[1] == resolved_node[1] or resolved_node[1] is None) and\\\n                    (pattern[2] == resolved_node[2] or resolved_node[2] is None):\n                return True\n        return False","return_type":"bool","function_name":"SlurpyGraph.already_resolved","stripped_code":"def already_resolved(self, pattern: QueryTriple):\n        \"\"\" Determine whether pattern has already been loaded into the cache.\n\n        The \"wild card\" - `(None, None, None)` - always counts as resolved.\n\n        :param pattern: pattern to check\n        :return: True it is a subset of elements already loaded\n        \"\"\"\n        if self.sparql_locked or pattern == (None, None, None):\n            return True\n        for resolved_node in self.resolved_nodes:\n            if resolved_node != (None, None, None) and \\\n                    (pattern[0] == resolved_node[0] or resolved_node[0] is None) and \\\n                    (pattern[1] == resolved_node[1] or resolved_node[1] is None) and\\\n                    (pattern[2] == resolved_node[2] or resolved_node[2] is None):\n                return True\n        return False"}
{"code":"def flush_body(self) -> bool:\n        \"\"\"\n        \u53d1\u9001\u5185\u5bb9\u4f53\n        \"\"\"\n        if self._body is None:\n            return False\n        elif isinstance(self._body, bytes):\n            self.write(self._body)\n            return True\n        return False","return_type":"bool","function_name":"Response.flush_body","stripped_code":"def flush_body(self):\n        \"\"\"\n        \u53d1\u9001\u5185\u5bb9\u4f53\n        \"\"\"\n        if self._body is None:\n            return False\n        elif isinstance(self._body, bytes):\n            self.write(self._body)\n            return True\n        return False"}
{"code":"def same_color(self, objects: Set[Object]) -> Set[Object]:\n        \"\"\"\n        Filters the set of objects, and returns those objects whose color is the most frequent\n        color in the initial set of objects, if the highest frequency is greater than 1, or an\n        empty set otherwise.\n\n        This is an unusual name for what the method does, but just as ``blue`` filters objects to\n        those that are blue, this filters objects to those that are of the same color.\n        \"\"\"\n        return self._get_objects_with_same_attribute(objects, lambda x: x.color)","return_type":"Set[Object]","function_name":"NlvrLanguage.same_color","stripped_code":"def same_color(self, objects: Set[Object]):\n        \"\"\"\n        Filters the set of objects, and returns those objects whose color is the most frequent\n        color in the initial set of objects, if the highest frequency is greater than 1, or an\n        empty set otherwise.\n\n        This is an unusual name for what the method does, but just as ``blue`` filters objects to\n        those that are blue, this filters objects to those that are of the same color.\n        \"\"\"\n        return self._get_objects_with_same_attribute(objects, lambda x: x.color)"}
{"code":"def similarity(word1: str, word2: str) -> float:\n    \"\"\"\n    Get cosine similarity between two words.\n    If a word is not in the vocabulary, KeyError will be raised.\n\n    :param string word1: first word\n    :param string word2: second word\n    :return: the cosine similarity between the two word vectors\n    \"\"\"\n    return _MODEL.similarity(word1, word2)","return_type":"float","function_name":"similarity","stripped_code":"def similarity(word1: str, word2: str):\n    \"\"\"\n    Get cosine similarity between two words.\n    If a word is not in the vocabulary, KeyError will be raised.\n\n    :param string word1: first word\n    :param string word2: second word\n    :return: the cosine similarity between the two word vectors\n    \"\"\"\n    return _MODEL.similarity(word1, word2)"}
{"code":"def alexa(self) -> list:\n        \"\"\"Returns list of Amazon Alexa compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            alexa_controls: Amazon Alexa representation of RichMessage instance nested\n                controls.\n        \"\"\"\n        alexa_controls = [control.alexa() for control in self.controls]\n        return alexa_controls","return_type":"list","function_name":"RichMessage.alexa","stripped_code":"def alexa(self):\n        \"\"\"Returns list of Amazon Alexa compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            alexa_controls: Amazon Alexa representation of RichMessage instance nested\n                controls.\n        \"\"\"\n        alexa_controls = [control.alexa() for control in self.controls]\n        return alexa_controls"}
{"code":"def from_jsons(graph_json_str: str, check_version: bool = True) -> BELGraph:\n    \"\"\"Read a BEL graph from a Node-Link JSON string.\"\"\"\n    graph_json_dict = json.loads(graph_json_str)\n    return from_json(graph_json_dict, check_version=check_version)","return_type":"BELGraph","function_name":"from_jsons","stripped_code":"def from_jsons(graph_json_str: str, check_version: bool = True):\n    \"\"\"Read a BEL graph from a Node-Link JSON string.\"\"\"\n    graph_json_dict = json.loads(graph_json_str)\n    return from_json(graph_json_dict, check_version=check_version)"}
{"code":"def queue_scan_command(self, server_info: ServerConnectivityInfo, scan_command: PluginScanCommand) -> None:\n        \"\"\"Queue a scan command targeting a specific server.\n\n        Args:\n            server_info: The server's connectivity information. The test_connectivity_to_server() method must have been\n                called first to ensure that the server is online and accessible.\n            scan_command: The scan command to run against this server.\n        \"\"\"\n        # Ensure we have the right processes and queues in place for this hostname\n        self._check_and_create_process(server_info.hostname)\n\n        # Add the task to the right queue\n        self._queued_tasks_nb += 1\n        if scan_command.is_aggressive:\n            # Aggressive commands should not be run in parallel against\n            # a given server so we use the priority queues to prevent this\n            self._hostname_queues_dict[server_info.hostname].put((server_info, scan_command))\n        else:\n            # Normal commands get put in the standard/shared queue\n            self._task_queue.put((server_info, scan_command))","return_type":"None","function_name":"ConcurrentScanner.queue_scan_command","stripped_code":"def queue_scan_command(self, server_info: ServerConnectivityInfo, scan_command: PluginScanCommand):\n        \"\"\"Queue a scan command targeting a specific server.\n\n        Args:\n            server_info: The server's connectivity information. The test_connectivity_to_server() method must have been\n                called first to ensure that the server is online and accessible.\n            scan_command: The scan command to run against this server.\n        \"\"\"\n        # Ensure we have the right processes and queues in place for this hostname\n        self._check_and_create_process(server_info.hostname)\n\n        # Add the task to the right queue\n        self._queued_tasks_nb += 1\n        if scan_command.is_aggressive:\n            # Aggressive commands should not be run in parallel against\n            # a given server so we use the priority queues to prevent this\n            self._hostname_queues_dict[server_info.hostname].put((server_info, scan_command))\n        else:\n            # Normal commands get put in the standard/shared queue\n            self._task_queue.put((server_info, scan_command))"}
{"code":"def is_zeroing(self, move: Move) -> bool:\n        \"\"\"Checks if the given pseudo-legal move is a capture or pawn move.\"\"\"\n        return bool(BB_SQUARES[move.from_square] & self.pawns or BB_SQUARES[move.to_square] & self.occupied_co[not self.turn])","return_type":"bool","function_name":"Board.is_zeroing","stripped_code":"def is_zeroing(self, move: Move):\n        \"\"\"Checks if the given pseudo-legal move is a capture or pawn move.\"\"\"\n        return bool(BB_SQUARES[move.from_square] & self.pawns or BB_SQUARES[move.to_square] & self.occupied_co[not self.turn])"}
{"code":"def parse_datetime(value: Union[datetime, StrIntFloat]) -> datetime:\n    \"\"\"\n    Parse a datetime/int/float/string and return a datetime.datetime.\n\n    This function supports time zone offsets. When the input contains one,\n    the output uses a timezone with a fixed offset from UTC.\n\n    Raise ValueError if the input is well formatted but not a valid datetime.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, datetime):\n        return value\n\n    number = get_numeric(value)\n    if number is not None:\n        return from_unix_seconds(number)\n\n    match = datetime_re.match(cast(str, value))\n    if not match:\n        raise errors.DateTimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo_str = kw.pop('tzinfo')\n    if tzinfo_str == 'Z':\n        tzinfo = timezone.utc\n    elif tzinfo_str is not None:\n        offset_mins = int(tzinfo_str[-2:]) if len(tzinfo_str) > 3 else 0\n        offset = 60 * int(tzinfo_str[1:3]) + offset_mins\n        if tzinfo_str[0] == '-':\n            offset = -offset\n        tzinfo = timezone(timedelta(minutes=offset))\n    else:\n        tzinfo = None\n\n    kw_: Dict[str, Union[int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    with change_exception(errors.DateTimeError, ValueError):\n        return datetime(**kw_)","return_type":"datetime","function_name":"parse_datetime","stripped_code":"def parse_datetime(value: Union[datetime, StrIntFloat]):\n    \"\"\"\n    Parse a datetime/int/float/string and return a datetime.datetime.\n\n    This function supports time zone offsets. When the input contains one,\n    the output uses a timezone with a fixed offset from UTC.\n\n    Raise ValueError if the input is well formatted but not a valid datetime.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n    if isinstance(value, datetime):\n        return value\n\n    number = get_numeric(value)\n    if number is not None:\n        return from_unix_seconds(number)\n\n    match = datetime_re.match(cast(str, value))\n    if not match:\n        raise errors.DateTimeError()\n\n    kw = match.groupdict()\n    if kw['microsecond']:\n        kw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\n    tzinfo_str = kw.pop('tzinfo')\n    if tzinfo_str == 'Z':\n        tzinfo = timezone.utc\n    elif tzinfo_str is not None:\n        offset_mins = int(tzinfo_str[-2:]) if len(tzinfo_str) > 3 else 0\n        offset = 60 * int(tzinfo_str[1:3]) + offset_mins\n        if tzinfo_str[0] == '-':\n            offset = -offset\n        tzinfo = timezone(timedelta(minutes=offset))\n    else:\n        tzinfo = None\n\n    kw_: Dict[str, Union[int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}\n    kw_['tzinfo'] = tzinfo\n\n    with change_exception(errors.DateTimeError, ValueError):\n        return datetime(**kw_)"}
{"code":"def console_set_char_background(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    col: Tuple[int, int, int],\n    flag: int = BKGND_SET,\n) -> None:\n    \"\"\"Change the background color of x,y to col using a blend mode.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        col (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n        flag (int): Blending mode to use, defaults to BKGND_SET.\n    \"\"\"\n    lib.TCOD_console_set_char_background(_console(con), x, y, col, flag)","return_type":"None","function_name":"console_set_char_background","stripped_code":"def console_set_char_background(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    col: Tuple[int, int, int],\n    flag: int = BKGND_SET,\n):\n    \"\"\"Change the background color of x,y to col using a blend mode.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        col (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n        flag (int): Blending mode to use, defaults to BKGND_SET.\n    \"\"\"\n    lib.TCOD_console_set_char_background(_console(con), x, y, col, flag)"}
{"code":"def get_matrix_index(graph: BELGraph) -> Set[str]:\n    \"\"\"Return set of HGNC names from Proteins/Rnas/Genes/miRNA, nodes that can be used by SPIA.\"\"\"\n    # TODO: Using HGNC Symbols for now\n    return {\n        node.name\n        for node in graph\n        if isinstance(node, CentralDogma) and node.namespace.upper() == 'HGNC'\n    }","return_type":"Set[str]","function_name":"get_matrix_index","stripped_code":"def get_matrix_index(graph: BELGraph):\n    \"\"\"Return set of HGNC names from Proteins/Rnas/Genes/miRNA, nodes that can be used by SPIA.\"\"\"\n    # TODO: Using HGNC Symbols for now\n    return {\n        node.name\n        for node in graph\n        if isinstance(node, CentralDogma) and node.namespace.upper() == 'HGNC'\n    }"}
{"code":"def send_venue(\n        self,\n        chat_id: Union[int, str],\n        latitude: float,\n        longitude: float,\n        title: str,\n        address: str,\n        foursquare_id: str = \"\",\n        foursquare_type: str = \"\",\n        disable_notification: bool = None,\n        reply_to_message_id: int = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"pyrogram.Message\":\n        \"\"\"Use this method to send information about a venue.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n                For your personal cloud (Saved Messages) you can simply use \"me\" or \"self\".\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n            latitude (``float``):\n                Latitude of the venue.\n\n            longitude (``float``):\n                Longitude of the venue.\n\n            title (``str``):\n                Name of the venue.\n\n            address (``str``):\n                Address of the venue.\n\n            foursquare_id (``str``, *optional*):\n                Foursquare identifier of the venue.\n\n            foursquare_type (``str``, *optional*):\n                Foursquare type of the venue, if known.\n                (For example, \"arts_entertainment/default\", \"arts_entertainment/aquarium\" or \"food/icecream\".)\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message\n\n            reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*):\n                Additional interface options. An object for an inline keyboard, custom reply keyboard,\n                instructions to remove reply keyboard or to force a reply from the user.\n\n        Returns:\n            On success, the sent :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        r = self.send(\n            functions.messages.SendMedia(\n                peer=self.resolve_peer(chat_id),\n                media=types.InputMediaVenue(\n                    geo_point=types.InputGeoPoint(\n                        lat=latitude,\n                        long=longitude\n                    ),\n                    title=title,\n                    address=address,\n                    provider=\"\",\n                    venue_id=foursquare_id,\n                    venue_type=foursquare_type\n                ),\n                message=\"\",\n                silent=disable_notification or None,\n                reply_to_msg_id=reply_to_message_id,\n                random_id=self.rnd_id(),\n                reply_markup=reply_markup.write() if reply_markup else None\n            )\n        )\n\n        for i in r.updates:\n            if isinstance(i, (types.UpdateNewMessage, types.UpdateNewChannelMessage)):\n                return pyrogram.Message._parse(\n                    self, i.message,\n                    {i.id: i for i in r.users},\n                    {i.id: i for i in r.chats}\n                )","return_type":"\"pyrogram.Message\"","function_name":"SendVenue.send_venue","stripped_code":"def send_venue(\n        self,\n        chat_id: Union[int, str],\n        latitude: float,\n        longitude: float,\n        title: str,\n        address: str,\n        foursquare_id: str = \"\",\n        foursquare_type: str = \"\",\n        disable_notification: bool = None,\n        reply_to_message_id: int = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ):\n        \"\"\"Use this method to send information about a venue.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n                For your personal cloud (Saved Messages) you can simply use \"me\" or \"self\".\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n            latitude (``float``):\n                Latitude of the venue.\n\n            longitude (``float``):\n                Longitude of the venue.\n\n            title (``str``):\n                Name of the venue.\n\n            address (``str``):\n                Address of the venue.\n\n            foursquare_id (``str``, *optional*):\n                Foursquare identifier of the venue.\n\n            foursquare_type (``str``, *optional*):\n                Foursquare type of the venue, if known.\n                (For example, \"arts_entertainment/default\", \"arts_entertainment/aquarium\" or \"food/icecream\".)\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message\n\n            reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*):\n                Additional interface options. An object for an inline keyboard, custom reply keyboard,\n                instructions to remove reply keyboard or to force a reply from the user.\n\n        Returns:\n            On success, the sent :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        r = self.send(\n            functions.messages.SendMedia(\n                peer=self.resolve_peer(chat_id),\n                media=types.InputMediaVenue(\n                    geo_point=types.InputGeoPoint(\n                        lat=latitude,\n                        long=longitude\n                    ),\n                    title=title,\n                    address=address,\n                    provider=\"\",\n                    venue_id=foursquare_id,\n                    venue_type=foursquare_type\n                ),\n                message=\"\",\n                silent=disable_notification or None,\n                reply_to_msg_id=reply_to_message_id,\n                random_id=self.rnd_id(),\n                reply_markup=reply_markup.write() if reply_markup else None\n            )\n        )\n\n        for i in r.updates:\n            if isinstance(i, (types.UpdateNewMessage, types.UpdateNewChannelMessage)):\n                return pyrogram.Message._parse(\n                    self, i.message,\n                    {i.id: i for i in r.users},\n                    {i.id: i for i in r.chats}\n                )"}
{"code":"def as_bel(self) -> str:\n        \"\"\"Return this fragment variant as a BEL string.\"\"\"\n        res = '\"{}\"'.format(self.range)\n\n        if FRAGMENT_DESCRIPTION in self:\n            res += ', \"{}\"'.format(self[FRAGMENT_DESCRIPTION])\n\n        return 'frag({})'.format(res)","return_type":"str","function_name":"Fragment.as_bel","stripped_code":"def as_bel(self):\n        \"\"\"Return this fragment variant as a BEL string.\"\"\"\n        res = '\"{}\"'.format(self.range)\n\n        if FRAGMENT_DESCRIPTION in self:\n            res += ', \"{}\"'.format(self[FRAGMENT_DESCRIPTION])\n\n        return 'frag({})'.format(res)"}
{"code":"def action_sequence_to_logical_form(self, action_sequence: List[str]) -> str:\n        \"\"\"\n        Takes an action sequence as produced by :func:`logical_form_to_action_sequence`, which is a\n        linearization of an abstract syntax tree, and reconstructs the logical form defined by that\n        abstract syntax tree.\n        \"\"\"\n        # Basic outline: we assume that the bracketing that we get in the RHS of each action is the\n        # correct bracketing for reconstructing the logical form.  This is true when there is no\n        # currying in the action sequence.  Given this assumption, we just need to construct a tree\n        # from the action sequence, then output all of the leaves in the tree, with brackets around\n        # the children of all non-terminal nodes.\n\n        remaining_actions = [action.split(\" -> \") for action in action_sequence]\n        tree = Tree(remaining_actions[0][1], [])\n\n        try:\n            remaining_actions = self._construct_node_from_actions(tree, remaining_actions[1:])\n        except ParsingError:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            raise\n\n        if remaining_actions:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            logger.error(\"Remaining actions were: %s\", remaining_actions)\n            raise ParsingError(\"Extra actions in action sequence\")\n        return nltk_tree_to_logical_form(tree)","return_type":"str","function_name":"DomainLanguage.action_sequence_to_logical_form","stripped_code":"def action_sequence_to_logical_form(self, action_sequence: List[str]):\n        \"\"\"\n        Takes an action sequence as produced by :func:`logical_form_to_action_sequence`, which is a\n        linearization of an abstract syntax tree, and reconstructs the logical form defined by that\n        abstract syntax tree.\n        \"\"\"\n        # Basic outline: we assume that the bracketing that we get in the RHS of each action is the\n        # correct bracketing for reconstructing the logical form.  This is true when there is no\n        # currying in the action sequence.  Given this assumption, we just need to construct a tree\n        # from the action sequence, then output all of the leaves in the tree, with brackets around\n        # the children of all non-terminal nodes.\n\n        remaining_actions = [action.split(\" -> \") for action in action_sequence]\n        tree = Tree(remaining_actions[0][1], [])\n\n        try:\n            remaining_actions = self._construct_node_from_actions(tree, remaining_actions[1:])\n        except ParsingError:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            raise\n\n        if remaining_actions:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            logger.error(\"Remaining actions were: %s\", remaining_actions)\n            raise ParsingError(\"Extra actions in action sequence\")\n        return nltk_tree_to_logical_form(tree)"}
{"code":"def sample_forward_transitions(self, batch_size, batch_info,\n                                   forward_steps: int, discount_factor: float) -> Transitions:\n        \"\"\"\n        Sample transitions from replay buffer with _forward steps_.\n        That is, instead of getting a transition s_t -> s_t+1 with reward r,\n        get a transition s_t -> s_t+n with sum of intermediate rewards.\n\n        Used in a variant of Deep Q-Learning\n        \"\"\"\n        probs, indexes, tree_idxs = self.backend.sample_batch_transitions(batch_size, forward_steps)\n\n        return self._get_transitions(\n            probs, indexes, tree_idxs, batch_info\n        )","return_type":"Transitions","function_name":"PrioritizedCircularReplayBuffer.sample_forward_transitions","stripped_code":"def sample_forward_transitions(self, batch_size, batch_info,\n                                   forward_steps: int, discount_factor: float):\n        \"\"\"\n        Sample transitions from replay buffer with _forward steps_.\n        That is, instead of getting a transition s_t -> s_t+1 with reward r,\n        get a transition s_t -> s_t+n with sum of intermediate rewards.\n\n        Used in a variant of Deep Q-Learning\n        \"\"\"\n        probs, indexes, tree_idxs = self.backend.sample_batch_transitions(batch_size, forward_steps)\n\n        return self._get_transitions(\n            probs, indexes, tree_idxs, batch_info\n        )"}
{"code":"def revoc_info(creds: dict, filt: dict = None) -> dict:\n    \"\"\"\n    Given a creds structure, return a dict mapping pairs\n    (revocation registry identifier, credential revocation identifier)\n    to (decoded) attribute name:value dicts.\n\n    If the caller includes a filter of attribute:value pairs, retain only matching attributes.\n\n    :param creds: creds structure returned by HolderProver.get_creds() as above\n    :param filt: dict mapping attributes to values of interest; e.g.,\n\n    ::\n\n        {\n            'legalName': 'Flan Nebula',\n            'effectiveDate': '2018-01-01',\n            'endDate': None\n        }\n\n    :return: dict mapping (rev_reg_id, cred_rev_id) pairs to decoded attributes\n    \"\"\"\n\n    rv = {}\n    for uuid2creds in (creds.get('attrs', {}), creds.get('predicates', {})):\n        for inner_creds in uuid2creds.values():\n            for cred in inner_creds:\n                cred_info = cred['cred_info']\n                (rr_id, cr_id) = (cred_info['rev_reg_id'], cred_info['cred_rev_id'])\n                if (rr_id, cr_id) in rv or rr_id is None or cr_id is None:\n                    continue\n                if not filt:\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n                    continue\n                if ({attr: str(filt[attr]) for attr in filt}.items() <= cred_info['attrs'].items()):\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n    return rv","return_type":"dict","function_name":"revoc_info","stripped_code":"def revoc_info(creds: dict, filt: dict = None):\n    \"\"\"\n    Given a creds structure, return a dict mapping pairs\n    (revocation registry identifier, credential revocation identifier)\n    to (decoded) attribute name:value dicts.\n\n    If the caller includes a filter of attribute:value pairs, retain only matching attributes.\n\n    :param creds: creds structure returned by HolderProver.get_creds() as above\n    :param filt: dict mapping attributes to values of interest; e.g.,\n\n    ::\n\n        {\n            'legalName': 'Flan Nebula',\n            'effectiveDate': '2018-01-01',\n            'endDate': None\n        }\n\n    :return: dict mapping (rev_reg_id, cred_rev_id) pairs to decoded attributes\n    \"\"\"\n\n    rv = {}\n    for uuid2creds in (creds.get('attrs', {}), creds.get('predicates', {})):\n        for inner_creds in uuid2creds.values():\n            for cred in inner_creds:\n                cred_info = cred['cred_info']\n                (rr_id, cr_id) = (cred_info['rev_reg_id'], cred_info['cred_rev_id'])\n                if (rr_id, cr_id) in rv or rr_id is None or cr_id is None:\n                    continue\n                if not filt:\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n                    continue\n                if ({attr: str(filt[attr]) for attr in filt}.items() <= cred_info['attrs'].items()):\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n    return rv"}
{"code":"def main(self) -> None:\n        \"\"\"\n        Main entry point. Runs :func:`service`.\n        \"\"\"\n        # Actual main service code.\n        try:\n            self.service()\n        except Exception as e:\n            self.error(\"Unexpected exception: {e}\\n{t}\".format(\n                e=e, t=traceback.format_exc()))","return_type":"None","function_name":"WindowsService.main","stripped_code":"def main(self):\n        \"\"\"\n        Main entry point. Runs :func:`service`.\n        \"\"\"\n        # Actual main service code.\n        try:\n            self.service()\n        except Exception as e:\n            self.error(\"Unexpected exception: {e}\\n{t}\".format(\n                e=e, t=traceback.format_exc()))"}
{"code":"def parse_docstring(self, func_or_method: typing.Callable) -> dict:\n        \"\"\"\n        Given a function, parse the docstring as YAML and return a dictionary of info.\n        \"\"\"\n        docstring = func_or_method.__doc__\n        if not docstring:\n            return {}\n\n        # We support having regular docstrings before the schema\n        # definition. Here we return just the schema part from\n        # the docstring.\n        docstring = docstring.split(\"---\")[-1]\n\n        parsed = yaml.safe_load(docstring)\n\n        if not isinstance(parsed, dict):\n            # A regular docstring (not yaml formatted) can return\n            # a simple string here, which wouldn't follow the schema.\n            return {}\n\n        return parsed","return_type":"dict","function_name":"BaseSchemaGenerator.parse_docstring","stripped_code":"def parse_docstring(self, func_or_method: typing.Callable):\n        \"\"\"\n        Given a function, parse the docstring as YAML and return a dictionary of info.\n        \"\"\"\n        docstring = func_or_method.__doc__\n        if not docstring:\n            return {}\n\n        # We support having regular docstrings before the schema\n        # definition. Here we return just the schema part from\n        # the docstring.\n        docstring = docstring.split(\"---\")[-1]\n\n        parsed = yaml.safe_load(docstring)\n\n        if not isinstance(parsed, dict):\n            # A regular docstring (not yaml formatted) can return\n            # a simple string here, which wouldn't follow the schema.\n            return {}\n\n        return parsed"}
{"code":"def app_context_processor(self, func: Callable) -> Callable:\n        \"\"\"Add a context processor function to the app.\n\n        This is designed to be used as a decorator, and has the same\n        arguments as :meth:`~quart.Quart.context_processor`. This will\n        add context to all templates rendered. An example usage,\n\n        .. code-block:: python\n\n            blueprint = Blueprint(__name__)\n            @blueprint.app_context_processor\n            def processor():\n                ...\n        \"\"\"\n        self.record_once(lambda state: state.app.context_processor(func))\n        return func","return_type":"Callable","function_name":"Blueprint.app_context_processor","stripped_code":"def app_context_processor(self, func: Callable):\n        \"\"\"Add a context processor function to the app.\n\n        This is designed to be used as a decorator, and has the same\n        arguments as :meth:`~quart.Quart.context_processor`. This will\n        add context to all templates rendered. An example usage,\n\n        .. code-block:: python\n\n            blueprint = Blueprint(__name__)\n            @blueprint.app_context_processor\n            def processor():\n                ...\n        \"\"\"\n        self.record_once(lambda state: state.app.context_processor(func))\n        return func"}
{"code":"def parse_metadata(metadata_obj: Metadata, metadata_dictionary: dict) -> None:\n    \"\"\" Adds to a Metadata object any DublinCore or dts:Extensions object\n    found in the given dictionary\n\n    :param metadata_obj:\n    :param metadata_dictionary:\n    \"\"\"\n    for key, value_set in metadata_dictionary.get(\"https://w3id.org/dts/api#dublincore\", [{}])[0].items():\n        term = URIRef(key)\n        for value_dict in value_set:\n            metadata_obj.add(term, *dict_to_literal(value_dict))\n\n    for key, value_set in metadata_dictionary.get(\"https://w3id.org/dts/api#extensions\", [{}])[0].items():\n        term = URIRef(key)\n        for value_dict in value_set:\n            metadata_obj.add(term, *dict_to_literal(value_dict))","return_type":"None","function_name":"parse_metadata","stripped_code":"def parse_metadata(metadata_obj: Metadata, metadata_dictionary: dict):\n    \"\"\" Adds to a Metadata object any DublinCore or dts:Extensions object\n    found in the given dictionary\n\n    :param metadata_obj:\n    :param metadata_dictionary:\n    \"\"\"\n    for key, value_set in metadata_dictionary.get(\"https://w3id.org/dts/api#dublincore\", [{}])[0].items():\n        term = URIRef(key)\n        for value_dict in value_set:\n            metadata_obj.add(term, *dict_to_literal(value_dict))\n\n    for key, value_set in metadata_dictionary.get(\"https://w3id.org/dts/api#extensions\", [{}])[0].items():\n        term = URIRef(key)\n        for value_dict in value_set:\n            metadata_obj.add(term, *dict_to_literal(value_dict))"}
{"code":"def treeboost(self, data: ['SASdata', str] = None,\n                  assess: str = None,\n                  code: str = None,\n                  freq: str = None,\n                  importance: str = None,\n                  input: [str, list, dict] = None,\n                  performance: str = None,\n                  save: [str, bool] = True,\n                  score: [str, bool, 'SASdata'] = True,\n                  subseries: str = None,\n                  target: [str, list, dict] = None,\n                  procopts: str = None,\n                  stmtpassthrough: str = None,\n                  **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the TREEBOOST procedure\n\n        Documentation link:\n        https://support.sas.com/documentation/solutions/miner/emhp/14.1/emhpprcref.pdf\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm assess: The assess variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm importance: The importance variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm performance: The performance variable can only be a string type.\n        :parm save: The save variable can be a string or boolean type. The default is True to create common output datasets\n        :parm score: The score variable can only be a string type.\n        :parm subseries: The subseries variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"","return_type":"'SASresults'","function_name":"SASml.treeboost","stripped_code":"def treeboost(self, data: ['SASdata', str] = None,\n                  assess: str = None,\n                  code: str = None,\n                  freq: str = None,\n                  importance: str = None,\n                  input: [str, list, dict] = None,\n                  performance: str = None,\n                  save: [str, bool] = True,\n                  score: [str, bool, 'SASdata'] = True,\n                  subseries: str = None,\n                  target: [str, list, dict] = None,\n                  procopts: str = None,\n                  stmtpassthrough: str = None,\n                  **kwargs: dict):\n        \"\"\"\n        Python method to call the TREEBOOST procedure\n\n        Documentation link:\n        https://support.sas.com/documentation/solutions/miner/emhp/14.1/emhpprcref.pdf\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm assess: The assess variable can only be a string type.\n        :parm code: The code variable can only be a string type.\n        :parm freq: The freq variable can only be a string type.\n        :parm importance: The importance variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm performance: The performance variable can only be a string type.\n        :parm save: The save variable can be a string or boolean type. The default is True to create common output datasets\n        :parm score: The score variable can only be a string type.\n        :parm subseries: The subseries variable can only be a string type.\n        :parm target: The target variable can be a string, list or dict type. It refers to the dependent, y, or label variable. This parameter is required\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\""}
{"code":"def line(h1: Histogram1D, **kwargs) -> dict:\n    \"\"\"Line plot of 1D histogram values.\n\n    Points are horizontally placed in bin centers.\n\n    Parameters\n    ----------\n    h1 : physt.histogram1d.Histogram1D\n        Dimensionality of histogram for which it is applicable\n    \"\"\"\n\n    lw = kwargs.pop(\"lw\", DEFAULT_STROKE_WIDTH)\n\n    mark_template = [{\n        \"type\": \"line\",\n        \"encode\": {\n            \"enter\": {\n                \"x\": {\"scale\": \"xscale\", \"field\": \"x\"},\n                \"y\": {\"scale\": \"yscale\", \"field\": \"y\"},\n                \"stroke\": {\"scale\": \"series\", \"field\": \"c\"},\n                \"strokeWidth\": {\"value\": lw}\n            }\n        },\n        \"from\": {\"data\": \"series\"},\n    }]\n    vega = _scatter_or_line(h1, mark_template=mark_template, kwargs=kwargs)\n    return vega","return_type":"dict","function_name":"line","stripped_code":"def line(h1: Histogram1D, **kwargs):\n    \"\"\"Line plot of 1D histogram values.\n\n    Points are horizontally placed in bin centers.\n\n    Parameters\n    ----------\n    h1 : physt.histogram1d.Histogram1D\n        Dimensionality of histogram for which it is applicable\n    \"\"\"\n\n    lw = kwargs.pop(\"lw\", DEFAULT_STROKE_WIDTH)\n\n    mark_template = [{\n        \"type\": \"line\",\n        \"encode\": {\n            \"enter\": {\n                \"x\": {\"scale\": \"xscale\", \"field\": \"x\"},\n                \"y\": {\"scale\": \"yscale\", \"field\": \"y\"},\n                \"stroke\": {\"scale\": \"series\", \"field\": \"c\"},\n                \"strokeWidth\": {\"value\": lw}\n            }\n        },\n        \"from\": {\"data\": \"series\"},\n    }]\n    vega = _scatter_or_line(h1, mark_template=mark_template, kwargs=kwargs)\n    return vega"}
{"code":"def transform(self, target_type: Type[T], value: F, context: PipelineContext = None) -> T:\n        \"\"\"Transforms an object to a new type.\n\n        Args:\n            target_type: The type to be converted to.\n            value: The object to be transformed.\n            context: The context of the transformation (mutable).\n        \"\"\"\n        pass","return_type":"T","function_name":"DataTransformer.transform","stripped_code":"def transform(self, target_type: Type[T], value: F, context: PipelineContext = None):\n        \"\"\"Transforms an object to a new type.\n\n        Args:\n            target_type: The type to be converted to.\n            value: The object to be transformed.\n            context: The context of the transformation (mutable).\n        \"\"\"\n        pass"}
{"code":"def fetch_closed_orders(self, limit: int) -> List[Order]:\n        \"\"\"Fetch latest closed orders, must provide a limit.\"\"\"\n        return self._fetch_orders_limit(self._closed_orders, limit)","return_type":"List[Order]","function_name":"TradingClient.fetch_closed_orders","stripped_code":"def fetch_closed_orders(self, limit: int):\n        \"\"\"Fetch latest closed orders, must provide a limit.\"\"\"\n        return self._fetch_orders_limit(self._closed_orders, limit)"}
{"code":"def get_explore_posts(self) -> Iterator[Post]:\n        \"\"\"Get Posts which are worthy of exploring suggested by Instagram.\n\n        :return: Iterator over Posts of the user's suggested posts.\n        \"\"\"\n        data = self.context.get_json('explore/', {})\n        yield from (Post(self.context, node)\n                    for node in self.context.graphql_node_list(\"df0dcc250c2b18d9fd27c5581ef33c7c\",\n                                                               {}, 'https://www.instagram.com/explore/',\n                                                               lambda d: d['data']['user']['edge_web_discover_media'],\n                                                               data['rhx_gis']))","return_type":"Iterator[Post]","function_name":"Instaloader.get_explore_posts","stripped_code":"def get_explore_posts(self):\n        \"\"\"Get Posts which are worthy of exploring suggested by Instagram.\n\n        :return: Iterator over Posts of the user's suggested posts.\n        \"\"\"\n        data = self.context.get_json('explore/', {})\n        yield from (Post(self.context, node)\n                    for node in self.context.graphql_node_list(\"df0dcc250c2b18d9fd27c5581ef33c7c\",\n                                                               {}, 'https://www.instagram.com/explore/',\n                                                               lambda d: d['data']['user']['edge_web_discover_media'],\n                                                               data['rhx_gis']))"}
{"code":"def share(\n        self,\n        share_id: str,\n        token: dict = None,\n        augment: bool = False,\n        prot: str = \"https\",\n    ) -> dict:\n        \"\"\"Get information about a specific share and its applications.\n\n        :param str token: API auth token\n        :param str share_id: share UUID\n        :param bool augment: option to improve API response by adding\n         some tags on the fly.\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n        # passing auth parameter\n        share_url = \"{}://v1.{}.isogeo.com/shares/{}\".format(\n            prot, self.api_url, share_id\n        )\n        share_req = self.get(\n            share_url, headers=self.header, proxies=self.proxies, verify=self.ssl\n        )\n\n        # checking response\n        checker.check_api_response(share_req)\n\n        # enhance share model\n        share = share_req.json()\n        if augment:\n            share = utils.share_extender(\n                share, self.search(whole_share=1, share=share_id).get(\"results\")\n            )\n        else:\n            pass\n\n        # end of method\n        return share","return_type":"dict","function_name":"Isogeo.share","stripped_code":"def share(\n        self,\n        share_id: str,\n        token: dict = None,\n        augment: bool = False,\n        prot: str = \"https\",\n    ):\n        \"\"\"Get information about a specific share and its applications.\n\n        :param str token: API auth token\n        :param str share_id: share UUID\n        :param bool augment: option to improve API response by adding\n         some tags on the fly.\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n        # passing auth parameter\n        share_url = \"{}://v1.{}.isogeo.com/shares/{}\".format(\n            prot, self.api_url, share_id\n        )\n        share_req = self.get(\n            share_url, headers=self.header, proxies=self.proxies, verify=self.ssl\n        )\n\n        # checking response\n        checker.check_api_response(share_req)\n\n        # enhance share model\n        share = share_req.json()\n        if augment:\n            share = utils.share_extender(\n                share, self.search(whole_share=1, share=share_id).get(\"results\")\n            )\n        else:\n            pass\n\n        # end of method\n        return share"}
{"code":"def increment(self, key: Any, by: int = 1) -> None:\n        \"\"\" Increments the value set against a key.  If the key is not present, 0 is assumed as the initial state \"\"\"\n        if key is not None:\n            self[key] = self.get(key, 0) + by","return_type":"None","function_name":"Map.increment","stripped_code":"def increment(self, key: Any, by: int = 1):\n        \"\"\" Increments the value set against a key.  If the key is not present, 0 is assumed as the initial state \"\"\"\n        if key is not None:\n            self[key] = self.get(key, 0) + by"}
{"code":"def getItemWidth(self) -> int:\n        \"\"\"\n        Only for transactions derived from HArray\n\n        :return: width of item in original array\n        \"\"\"\n        if not isinstance(self.dtype, HArray):\n            raise TypeError()\n        return (self.bitAddrEnd - self.bitAddr) // self.itemCnt","return_type":"int","function_name":"TransTmpl.getItemWidth","stripped_code":"def getItemWidth(self):\n        \"\"\"\n        Only for transactions derived from HArray\n\n        :return: width of item in original array\n        \"\"\"\n        if not isinstance(self.dtype, HArray):\n            raise TypeError()\n        return (self.bitAddrEnd - self.bitAddr) // self.itemCnt"}
{"code":"def get_func_name(self, hsh: bytes) -> str:\n        \"\"\"Returns the name of the normal function with the selector ``hsh``,\n        or ``'{fallback}'`` if no such function exists.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        sig = self._function_signatures_by_selector.get(hsh)\n        return '{fallback}' if sig is None else sig[:sig.find('(')]","return_type":"str","function_name":"SolidityMetadata.get_func_name","stripped_code":"def get_func_name(self, hsh: bytes):\n        \"\"\"Returns the name of the normal function with the selector ``hsh``,\n        or ``'{fallback}'`` if no such function exists.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        sig = self._function_signatures_by_selector.get(hsh)\n        return '{fallback}' if sig is None else sig[:sig.find('(')]"}
{"code":"def trc(postfix: Optional[str] = None, *, depth=1) -> logging.Logger:\n    \"\"\"\n    Automatically generate a logger from the calling function\n\n    :param postfix: append another logger name on top this\n    :param depth: depth of the call stack at which to capture the caller name\n    :return: instance of a logger with a correct path to a current caller\n\n    \"\"\"\n    x = inspect.stack()[depth]\n\n    code = x[0].f_code\n    func = [obj for obj in gc.get_referrers(code) if inspect.isfunction(obj)][0]\n\n    mod = inspect.getmodule(x.frame)\n\n    parts = (mod.__name__, func.__qualname__)\n\n    if postfix:\n        parts += (postfix,)\n\n    logger_name = '.'.join(parts)\n\n    return logging.getLogger(logger_name)","return_type":"logging.Logger","function_name":"trc","stripped_code":"def trc(postfix: Optional[str] = None, *, depth=1):\n    \"\"\"\n    Automatically generate a logger from the calling function\n\n    :param postfix: append another logger name on top this\n    :param depth: depth of the call stack at which to capture the caller name\n    :return: instance of a logger with a correct path to a current caller\n\n    \"\"\"\n    x = inspect.stack()[depth]\n\n    code = x[0].f_code\n    func = [obj for obj in gc.get_referrers(code) if inspect.isfunction(obj)][0]\n\n    mod = inspect.getmodule(x.frame)\n\n    parts = (mod.__name__, func.__qualname__)\n\n    if postfix:\n        parts += (postfix,)\n\n    logger_name = '.'.join(parts)\n\n    return logging.getLogger(logger_name)"}
{"code":"def ULE(a: BitVec, b: BitVec) -> Bool:\n    \"\"\"Create an unsigned less than expression.\n\n    :param a:\n    :param b:\n    :return:\n    \"\"\"\n    return Or(ULT(a, b), a == b)","return_type":"Bool","function_name":"ULE","stripped_code":"def ULE(a: BitVec, b: BitVec):\n    \"\"\"Create an unsigned less than expression.\n\n    :param a:\n    :param b:\n    :return:\n    \"\"\"\n    return Or(ULT(a, b), a == b)"}
{"code":"def verify_ws2p_head(self, head: Any) -> bool:\n        \"\"\"\n        Check specified document\n        :param Any head:\n        :return:\n        \"\"\"\n        signature = base64.b64decode(head.signature)\n        inline = head.inline()\n        prepended = signature + bytes(inline, 'ascii')\n\n        try:\n            self.verify(prepended)\n            return True\n        except ValueError:\n            return False","return_type":"bool","function_name":"VerifyingKey.verify_ws2p_head","stripped_code":"def verify_ws2p_head(self, head: Any):\n        \"\"\"\n        Check specified document\n        :param Any head:\n        :return:\n        \"\"\"\n        signature = base64.b64decode(head.signature)\n        inline = head.inline()\n        prepended = signature + bytes(inline, 'ascii')\n\n        try:\n            self.verify(prepended)\n            return True\n        except ValueError:\n            return False"}
{"code":"def altimeter(alt: Number, unit: str = 'hPa') -> str:\n    \"\"\"\n    Formats the altimter element into a string with hPa and inHg values\n\n    Ex: 30.11 inHg (10.20 hPa)\n    \"\"\"\n    if not (alt and unit in ('hPa', 'inHg')):\n        return ''\n    if unit == 'hPa':\n        value = alt.repr\n        converted = alt.value / 33.8638866667\n        converted = str(round(converted, 2)) + ' inHg'  # type: ignore\n    elif unit == 'inHg':\n        value = alt.repr[:2] + '.' + alt.repr[2:]\n        converted = float(value) * 33.8638866667\n        converted = str(int(round(converted))) + ' hPa'  # type: ignore\n    return f'{value} {unit} ({converted})'","return_type":"str","function_name":"altimeter","stripped_code":"def altimeter(alt: Number, unit: str = 'hPa'):\n    \"\"\"\n    Formats the altimter element into a string with hPa and inHg values\n\n    Ex: 30.11 inHg (10.20 hPa)\n    \"\"\"\n    if not (alt and unit in ('hPa', 'inHg')):\n        return ''\n    if unit == 'hPa':\n        value = alt.repr\n        converted = alt.value / 33.8638866667\n        converted = str(round(converted, 2)) + ' inHg'  # type: ignore\n    elif unit == 'inHg':\n        value = alt.repr[:2] + '.' + alt.repr[2:]\n        converted = float(value) * 33.8638866667\n        converted = str(int(round(converted))) + ' hPa'  # type: ignore\n    return f'{value} {unit} ({converted})'"}
{"code":"def set_dimensional_calibrations(self, dimensional_calibrations: typing.List[CalibrationModule.Calibration]) -> None:\n        \"\"\"Set the dimensional calibrations.\n\n        :param dimensional_calibrations: A list of calibrations, must match the dimensions of the data.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        self.__data_item.set_dimensional_calibrations(dimensional_calibrations)","return_type":"None","function_name":"DataItem.set_dimensional_calibrations","stripped_code":"def set_dimensional_calibrations(self, dimensional_calibrations: typing.List[CalibrationModule.Calibration]):\n        \"\"\"Set the dimensional calibrations.\n\n        :param dimensional_calibrations: A list of calibrations, must match the dimensions of the data.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n        self.__data_item.set_dimensional_calibrations(dimensional_calibrations)"}
{"code":"def _determine_outliers_index(hist: Hist,\n                              moving_average_threshold: float = 1.0,\n                              number_of_values_to_search_ahead: int = 5,\n                              limit_of_number_of_values_below_threshold: int = None) -> int:\n    \"\"\" Determine the location of where outliers begin in a 1D histogram.\n\n    When the moving average falls below the limit, we consider the outliers to have begun.\n\n    To determine the location of outliers:\n\n    - Calculate the moving average for number_of_values_to_search_ahead values.\n    - First, the moving average must go above the limit at least once to guard against a random cut\n      in a low pt bin causing most of the data to be cut out.\n    - Next, we look for a consecutive number of entries below limit_of_number_of_values_below_threshold.\n    - If we meet that condition, we have found the index where the outliers begin. We then return the ROOT\n      bin index of the value.\n    - If not, we return -1.\n\n    Note:\n        The index returned is when the moving average first drops below the threshold for a moving average\n        calculated with that bin at the center. This is somewhat different from a standard moving average\n        calculation which would only look forward in the array.\n\n    Args:\n        hist: Histogram to be checked for outliers.\n        moving_average_threshold: Value of moving average under which we consider the moving average\n            to be 0. Default: 2.\n        number_of_values_to_search_ahead: Number of values to search ahead in the array when calculating\n            the moving average. Default: 5.\n        limit_of_number_of_values_below_threshold: Number of consecutive bins below the threshold to be considered\n            the beginning of outliers. Default: None, which will correspond to number_of_values_to_search_ahead - 1.\n    Returns:\n        ROOT (ie 1-indexed) index of the histogram axes where the outliers begin.\n    \"\"\"\n    # Validation\n    import ROOT\n    if isinstance(hist, (ROOT.TH2, ROOT.TH3, ROOT.THnBase)):\n        raise ValueError(\n            f\"Given histogram '{hist.GetName()}' of type {type(hist)}, but can only\"\n            \" determine the outlier location of a 1D histogram. Please project to\"\n            \" the particle level axis first.\"\n        )\n\n    if limit_of_number_of_values_below_threshold is None:\n        # In principle, this could be another value. However, this is what was used in the previous outliers\n        # removal implementation.\n        limit_of_number_of_values_below_threshold = number_of_values_to_search_ahead - 1\n\n    # It is much more convenient to work with a numpy array.\n    hist_to_check = histogram.Histogram1D.from_existing_hist(hist)\n\n    # Calculate the moving average for the entire axis, looking ahead including the current bin + 4 = 5 ahead.\n    number_of_values_to_search_ahead = 5\n    moving_average = utils.moving_average(hist_to_check.y, n = number_of_values_to_search_ahead)\n\n    #logger.debug(f\"y: {hist_to_check.y}\")\n    #logger.debug(f\"moving_average: {moving_average}\")\n\n    cut_index = _determine_outliers_for_moving_average(\n        moving_average = moving_average,\n        moving_average_threshold = moving_average_threshold,\n        number_of_values_to_search_ahead = number_of_values_to_search_ahead,\n        limit_of_number_of_values_below_threshold = limit_of_number_of_values_below_threshold,\n    )\n\n    if cut_index != -1:\n        # ROOT histograms are 1 indexed, so we add another 1.\n        cut_index += 1\n\n    return cut_index","return_type":"int","function_name":"_determine_outliers_index","stripped_code":"def _determine_outliers_index(hist: Hist,\n                              moving_average_threshold: float = 1.0,\n                              number_of_values_to_search_ahead: int = 5,\n                              limit_of_number_of_values_below_threshold: int = None):\n    \"\"\" Determine the location of where outliers begin in a 1D histogram.\n\n    When the moving average falls below the limit, we consider the outliers to have begun.\n\n    To determine the location of outliers:\n\n    - Calculate the moving average for number_of_values_to_search_ahead values.\n    - First, the moving average must go above the limit at least once to guard against a random cut\n      in a low pt bin causing most of the data to be cut out.\n    - Next, we look for a consecutive number of entries below limit_of_number_of_values_below_threshold.\n    - If we meet that condition, we have found the index where the outliers begin. We then return the ROOT\n      bin index of the value.\n    - If not, we return -1.\n\n    Note:\n        The index returned is when the moving average first drops below the threshold for a moving average\n        calculated with that bin at the center. This is somewhat different from a standard moving average\n        calculation which would only look forward in the array.\n\n    Args:\n        hist: Histogram to be checked for outliers.\n        moving_average_threshold: Value of moving average under which we consider the moving average\n            to be 0. Default: 2.\n        number_of_values_to_search_ahead: Number of values to search ahead in the array when calculating\n            the moving average. Default: 5.\n        limit_of_number_of_values_below_threshold: Number of consecutive bins below the threshold to be considered\n            the beginning of outliers. Default: None, which will correspond to number_of_values_to_search_ahead - 1.\n    Returns:\n        ROOT (ie 1-indexed) index of the histogram axes where the outliers begin.\n    \"\"\"\n    # Validation\n    import ROOT\n    if isinstance(hist, (ROOT.TH2, ROOT.TH3, ROOT.THnBase)):\n        raise ValueError(\n            f\"Given histogram '{hist.GetName()}' of type {type(hist)}, but can only\"\n            \" determine the outlier location of a 1D histogram. Please project to\"\n            \" the particle level axis first.\"\n        )\n\n    if limit_of_number_of_values_below_threshold is None:\n        # In principle, this could be another value. However, this is what was used in the previous outliers\n        # removal implementation.\n        limit_of_number_of_values_below_threshold = number_of_values_to_search_ahead - 1\n\n    # It is much more convenient to work with a numpy array.\n    hist_to_check = histogram.Histogram1D.from_existing_hist(hist)\n\n    # Calculate the moving average for the entire axis, looking ahead including the current bin + 4 = 5 ahead.\n    number_of_values_to_search_ahead = 5\n    moving_average = utils.moving_average(hist_to_check.y, n = number_of_values_to_search_ahead)\n\n    #logger.debug(f\"y: {hist_to_check.y}\")\n    #logger.debug(f\"moving_average: {moving_average}\")\n\n    cut_index = _determine_outliers_for_moving_average(\n        moving_average = moving_average,\n        moving_average_threshold = moving_average_threshold,\n        number_of_values_to_search_ahead = number_of_values_to_search_ahead,\n        limit_of_number_of_values_below_threshold = limit_of_number_of_values_below_threshold,\n    )\n\n    if cut_index != -1:\n        # ROOT histograms are 1 indexed, so we add another 1.\n        cut_index += 1\n\n    return cut_index"}
{"code":"def make_shell_context(self) -> dict:\n        \"\"\"Create a context for interactive shell usage.\n\n        The :attr:`shell_context_processors` can be used to add\n        additional context.\n        \"\"\"\n        context = {'app': self, 'g': g}\n        for processor in self.shell_context_processors:\n            context.update(processor())\n        return context","return_type":"dict","function_name":"Quart.make_shell_context","stripped_code":"def make_shell_context(self):\n        \"\"\"Create a context for interactive shell usage.\n\n        The :attr:`shell_context_processors` can be used to add\n        additional context.\n        \"\"\"\n        context = {'app': self, 'g': g}\n        for processor in self.shell_context_processors:\n            context.update(processor())\n        return context"}
{"code":"def generate_sbi_json(num_pbs: int = 3, project: str = 'sip',\n                      programme_block: str = 'sip_demos',\n                      pb_config: Union[dict, List[dict]] = None,\n                      workflow_config:\n                      Union[dict, List[dict]] = None,\n                      register_workflows=True) -> str:\n    \"\"\"Return a JSON string used to configure an SBI.\"\"\"\n    return json.dumps(generate_sbi_config(num_pbs, project, programme_block,\n                                          pb_config,\n                                          workflow_config, register_workflows))","return_type":"str","function_name":"generate_sbi_json","stripped_code":"def generate_sbi_json(num_pbs: int = 3, project: str = 'sip',\n                      programme_block: str = 'sip_demos',\n                      pb_config: Union[dict, List[dict]] = None,\n                      workflow_config:\n                      Union[dict, List[dict]] = None,\n                      register_workflows=True):\n    \"\"\"Return a JSON string used to configure an SBI.\"\"\"\n    return json.dumps(generate_sbi_config(num_pbs, project, programme_block,\n                                          pb_config,\n                                          workflow_config, register_workflows))"}
{"code":"def natural_name(self) -> str:\n        \"\"\"Valid python identifier representation of the expession.\"\"\"\n        name = self.expression.strip()\n        for op in operators:\n            name = name.replace(op, operator_to_identifier[op])\n        return wt_kit.string2identifier(name)","return_type":"str","function_name":"Axis.natural_name","stripped_code":"def natural_name(self):\n        \"\"\"Valid python identifier representation of the expession.\"\"\"\n        name = self.expression.strip()\n        for op in operators:\n            name = name.replace(op, operator_to_identifier[op])\n        return wt_kit.string2identifier(name)"}
{"code":"def print_gate(gate: Gate, ndigits: int = 2,\n               file: TextIO = None) -> None:\n    \"\"\"Pretty print a gate tensor\n\n    Args:\n        gate:\n        ndigits:\n        file: Stream to which to write. Defaults to stdout\n    \"\"\"\n    N = gate.qubit_nb\n    gate_tensor = gate.vec.asarray()\n    lines = []\n    for index, amplitude in np.ndenumerate(gate_tensor):\n        ket = \"\".join([str(n) for n in index[0:N]])\n        bra = \"\".join([str(index[n]) for n in range(N, 2*N)])\n        if round(abs(amplitude)**2, ndigits) > 0.0:\n            lines.append('{} -> {} : {}'.format(bra, ket, amplitude))\n    lines.sort(key=lambda x: int(x[0:N]))\n    print('\\n'.join(lines), file=file)","return_type":"None","function_name":"print_gate","stripped_code":"def print_gate(gate: Gate, ndigits: int = 2,\n               file: TextIO = None):\n    \"\"\"Pretty print a gate tensor\n\n    Args:\n        gate:\n        ndigits:\n        file: Stream to which to write. Defaults to stdout\n    \"\"\"\n    N = gate.qubit_nb\n    gate_tensor = gate.vec.asarray()\n    lines = []\n    for index, amplitude in np.ndenumerate(gate_tensor):\n        ket = \"\".join([str(n) for n in index[0:N]])\n        bra = \"\".join([str(index[n]) for n in range(N, 2*N)])\n        if round(abs(amplitude)**2, ndigits) > 0.0:\n            lines.append('{} -> {} : {}'.format(bra, ket, amplitude))\n    lines.sort(key=lambda x: int(x[0:N]))\n    print('\\n'.join(lines), file=file)"}
{"code":"def multiple(layer: int, limit: int) -> Set[str]:\n\t\"\"\"Returns a set of strings to be used as Slots with Pabianas default Clock.\n\n\tArgs:\n\t\tlayer: The layer in the hierarchy this Area is placed in.\n\t\t\tTechnically, the number specifies how many of the Clocks signals are relevant to the Area.\n\t\t\tBetween 1 and limit.\n\t\tlimit: The number of layers of the hierarchy.\n\t\"\"\"\n\treturn {str(x).zfill(2) for x in [2**x for x in range(limit)] if x % 2**(layer - 1) == 0}","return_type":"Set[str]","function_name":"multiple","stripped_code":"def multiple(layer: int, limit: int):\n\t\"\"\"Returns a set of strings to be used as Slots with Pabianas default Clock.\n\n\tArgs:\n\t\tlayer: The layer in the hierarchy this Area is placed in.\n\t\t\tTechnically, the number specifies how many of the Clocks signals are relevant to the Area.\n\t\t\tBetween 1 and limit.\n\t\tlimit: The number of layers of the hierarchy.\n\t\"\"\"\n\treturn {str(x).zfill(2) for x in [2**x for x in range(limit)] if x % 2**(layer - 1) == 0}"}
{"code":"def construct_url(ip_address: str) -> str:\r\n    \"\"\"Construct the URL with a given IP address.\"\"\"\r\n    if 'http://' not in ip_address and 'https://' not in ip_address:\r\n        ip_address = '{}{}'.format('http://', ip_address)\r\n    if ip_address[-1] == '/':\r\n        ip_address = ip_address[:-1]\r\n    return ip_address","return_type":"str","function_name":"construct_url","stripped_code":"def construct_url(ip_address: str):\r\n    \"\"\"Construct the URL with a given IP address.\"\"\"\r\n    if 'http://' not in ip_address and 'https://' not in ip_address:\r\n        ip_address = '{}{}'.format('http://', ip_address)\r\n    if ip_address[-1] == '/':\r\n        ip_address = ip_address[:-1]\r\n    return ip_address"}
{"code":"def raise_for_redefined_namespace(self, line: str, position: int, namespace: str) -> None:\n        \"\"\"Raise an exception if a namespace is already defined.\n\n        :raises: RedefinedNamespaceError\n        \"\"\"\n        if self.disallow_redefinition and self.has_namespace(namespace):\n            raise RedefinedNamespaceError(self.get_line_number(), line, position, namespace)","return_type":"None","function_name":"MetadataParser.raise_for_redefined_namespace","stripped_code":"def raise_for_redefined_namespace(self, line: str, position: int, namespace: str):\n        \"\"\"Raise an exception if a namespace is already defined.\n\n        :raises: RedefinedNamespaceError\n        \"\"\"\n        if self.disallow_redefinition and self.has_namespace(namespace):\n            raise RedefinedNamespaceError(self.get_line_number(), line, position, namespace)"}
{"code":"def aspirate(self,\n                 volume: float = None,\n                 location: Union[types.Location, Well] = None,\n                 rate: float = 1.0) -> 'InstrumentContext':\n        \"\"\"\n        Aspirate a volume of liquid (in microliters/uL) using this pipette\n        from the specified location\n\n        If only a volume is passed, the pipette will aspirate\n        from its current position. If only a location is passed,\n        :py:meth:`aspirate` will default to its :py:attr:`max_volume`.\n\n        :param volume: The volume to aspirate, in microliters. If not\n                       specified, :py:attr:`max_volume`.\n        :type volume: int or float\n        :param location: Where to aspirate from. If `location` is a\n                         :py:class:`.Well`, the robot will aspirate from\n                         :py:attr:`well_bottom_clearance` mm\n                         above the bottom of the well. If `location` is a\n                         :py:class:`.Location` (i.e. the result of\n                         :py:meth:`.Well.top` or :py:meth:`.Well.bottom`), the\n                         robot will aspirate from the exact specified location.\n                         If unspecified, the robot will aspirate from the\n                         current position.\n        :param rate: The relative plunger speed for this aspirate. During\n                     this aspirate, the speed of the plunger will be\n                     `rate` * :py:attr:`aspirate_speed`. If not specified,\n                     defaults to 1.0 (speed will not be modified).\n        :type rate: float\n        :returns: This instance.\n        \"\"\"\n        self._log.debug(\"aspirate {} from {} at {}\"\n                        .format(volume,\n                                location if location else 'current position',\n                                rate))\n\n        if isinstance(location, Well):\n            point, well = location.bottom()\n            loc = types.Location(\n                point + types.Point(0, 0, self.well_bottom_clearance),\n                well)\n            self.move_to(loc)\n        elif isinstance(location, types.Location):\n            loc = location\n            self.move_to(location)\n        elif location is not None:\n            raise TypeError(\n                'location should be a Well or Location, but it is {}'\n                .format(location))\n        elif self._ctx.location_cache:\n            loc = self._ctx.location_cache\n        else:\n            raise RuntimeError(\n                \"If aspirate is called without an explicit location, another\"\n                \" method that moves to a location (such as move_to or \"\n                \"dispense) must previously have been called so the robot \"\n                \"knows where it is.\")\n        cmds.do_publish(self.broker, cmds.aspirate, self.aspirate,\n                        'before', None, None, self, volume, loc, rate)\n        self._hw_manager.hardware.aspirate(self._mount, volume, rate)\n        cmds.do_publish(self.broker, cmds.aspirate, self.aspirate,\n                        'after', self, None, self, volume, loc, rate)\n        return self","return_type":"'InstrumentContext'","function_name":"InstrumentContext.aspirate","stripped_code":"def aspirate(self,\n                 volume: float = None,\n                 location: Union[types.Location, Well] = None,\n                 rate: float = 1.0):\n        \"\"\"\n        Aspirate a volume of liquid (in microliters/uL) using this pipette\n        from the specified location\n\n        If only a volume is passed, the pipette will aspirate\n        from its current position. If only a location is passed,\n        :py:meth:`aspirate` will default to its :py:attr:`max_volume`.\n\n        :param volume: The volume to aspirate, in microliters. If not\n                       specified, :py:attr:`max_volume`.\n        :type volume: int or float\n        :param location: Where to aspirate from. If `location` is a\n                         :py:class:`.Well`, the robot will aspirate from\n                         :py:attr:`well_bottom_clearance` mm\n                         above the bottom of the well. If `location` is a\n                         :py:class:`.Location` (i.e. the result of\n                         :py:meth:`.Well.top` or :py:meth:`.Well.bottom`), the\n                         robot will aspirate from the exact specified location.\n                         If unspecified, the robot will aspirate from the\n                         current position.\n        :param rate: The relative plunger speed for this aspirate. During\n                     this aspirate, the speed of the plunger will be\n                     `rate` * :py:attr:`aspirate_speed`. If not specified,\n                     defaults to 1.0 (speed will not be modified).\n        :type rate: float\n        :returns: This instance.\n        \"\"\"\n        self._log.debug(\"aspirate {} from {} at {}\"\n                        .format(volume,\n                                location if location else 'current position',\n                                rate))\n\n        if isinstance(location, Well):\n            point, well = location.bottom()\n            loc = types.Location(\n                point + types.Point(0, 0, self.well_bottom_clearance),\n                well)\n            self.move_to(loc)\n        elif isinstance(location, types.Location):\n            loc = location\n            self.move_to(location)\n        elif location is not None:\n            raise TypeError(\n                'location should be a Well or Location, but it is {}'\n                .format(location))\n        elif self._ctx.location_cache:\n            loc = self._ctx.location_cache\n        else:\n            raise RuntimeError(\n                \"If aspirate is called without an explicit location, another\"\n                \" method that moves to a location (such as move_to or \"\n                \"dispense) must previously have been called so the robot \"\n                \"knows where it is.\")\n        cmds.do_publish(self.broker, cmds.aspirate, self.aspirate,\n                        'before', None, None, self, volume, loc, rate)\n        self._hw_manager.hardware.aspirate(self._mount, volume, rate)\n        cmds.do_publish(self.broker, cmds.aspirate, self.aspirate,\n                        'after', self, None, self, volume, loc, rate)\n        return self"}
{"code":"def info_qry(tickers, flds) -> str:\n    \"\"\"\n    Logging info for given tickers and fields\n\n    Args:\n        tickers: tickers\n        flds: fields\n\n    Returns:\n        str\n\n    Examples:\n        >>> print(info_qry(\n        ...     tickers=['NVDA US Equity'], flds=['Name', 'Security_Name']\n        ... ))\n        tickers: ['NVDA US Equity']\n        fields:  ['Name', 'Security_Name']\n    \"\"\"\n    full_list = '\\n'.join([f'tickers: {tickers[:8]}'] + [\n        f'         {tickers[n:(n + 8)]}' for n in range(8, len(tickers), 8)\n    ])\n    return f'{full_list}\\nfields:  {flds}'","return_type":"str","function_name":"info_qry","stripped_code":"def info_qry(tickers, flds):\n    \"\"\"\n    Logging info for given tickers and fields\n\n    Args:\n        tickers: tickers\n        flds: fields\n\n    Returns:\n        str\n\n    Examples:\n        >>> print(info_qry(\n        ...     tickers=['NVDA US Equity'], flds=['Name', 'Security_Name']\n        ... ))\n        tickers: ['NVDA US Equity']\n        fields:  ['Name', 'Security_Name']\n    \"\"\"\n    full_list = '\\n'.join([f'tickers: {tickers[:8]}'] + [\n        f'         {tickers[n:(n + 8)]}' for n in range(8, len(tickers), 8)\n    ])\n    return f'{full_list}\\nfields:  {flds}'"}
{"code":"def containers(self) -> list:\n        \"\"\"\n        Returns all containers on a deck as a list\n        \"\"\"\n\n        all_containers: List = list()\n        for slot in self:\n            all_containers += slot.get_children_list()\n\n        for container in all_containers:\n            if getattr(container, 'stackable', False):\n                all_containers += container.get_children_list()\n\n        return all_containers","return_type":"list","function_name":"Deck.containers","stripped_code":"def containers(self):\n        \"\"\"\n        Returns all containers on a deck as a list\n        \"\"\"\n\n        all_containers: List = list()\n        for slot in self:\n            all_containers += slot.get_children_list()\n\n        for container in all_containers:\n            if getattr(container, 'stackable', False):\n                all_containers += container.get_children_list()\n\n        return all_containers"}
{"code":"def _send_direct_message_new(self, messageobject: Dict[str, Dict]) -> Any:\n        \"\"\"\n        :reference: https://developer.twitter.com/en/docs/direct-messages/sending-and-receiving/api-reference/new-event.html\n        \"\"\"\n        headers, post_data = _buildmessageobject(messageobject)\n        newdm_path = \"/direct_messages/events/new.json\"\n\n        return tweepy.binder.bind_api(\n            api=self.api,\n            path=newdm_path,\n            method=\"POST\",\n            require_auth=True,\n        )(post_data=post_data, headers=headers)","return_type":"Any","function_name":"BirdsiteSkeleton._send_direct_message_new","stripped_code":"def _send_direct_message_new(self, messageobject: Dict[str, Dict]):\n        \"\"\"\n        :reference: https://developer.twitter.com/en/docs/direct-messages/sending-and-receiving/api-reference/new-event.html\n        \"\"\"\n        headers, post_data = _buildmessageobject(messageobject)\n        newdm_path = \"/direct_messages/events/new.json\"\n\n        return tweepy.binder.bind_api(\n            api=self.api,\n            path=newdm_path,\n            method=\"POST\",\n            require_auth=True,\n        )(post_data=post_data, headers=headers)"}
{"code":"def read_char(self, c: str) -> bool:\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()","return_type":"bool","function_name":"BasicParser.read_char","stripped_code":"def read_char(self, c: str):\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()"}
{"code":"def ok_schema_id(token: str) -> bool:\n    \"\"\"\n    Whether input token looks like a valid schema identifier;\n    i.e., <issuer-did>:2:<name>:<version>.\n\n    :param token: candidate string\n    :return: whether input token looks like a valid schema identifier\n    \"\"\"\n\n    return bool(re.match('[{}]{{21,22}}:2:.+:[0-9.]+$'.format(B58), token or ''))","return_type":"bool","function_name":"ok_schema_id","stripped_code":"def ok_schema_id(token: str):\n    \"\"\"\n    Whether input token looks like a valid schema identifier;\n    i.e., <issuer-did>:2:<name>:<version>.\n\n    :param token: candidate string\n    :return: whether input token looks like a valid schema identifier\n    \"\"\"\n\n    return bool(re.match('[{}]{{21,22}}:2:.+:[0-9.]+$'.format(B58), token or ''))"}
{"code":"def _extract_session_id(ssl_session: nassl._nassl.SSL_SESSION) -> str:\n        \"\"\"Extract the SSL session ID from a SSL session object or raises IndexError if the session ID was not set.\n        \"\"\"\n        session_string = ((ssl_session.as_text()).split('Session-ID:'))[1]\n        session_id = (session_string.split('Session-ID-ctx:'))[0].strip()\n        return session_id","return_type":"str","function_name":"SessionResumptionPlugin._extract_session_id","stripped_code":"def _extract_session_id(ssl_session: nassl._nassl.SSL_SESSION):\n        \"\"\"Extract the SSL session ID from a SSL session object or raises IndexError if the session ID was not set.\n        \"\"\"\n        session_string = ((ssl_session.as_text()).split('Session-ID:'))[1]\n        session_id = (session_string.split('Session-ID-ctx:'))[0].strip()\n        return session_id"}
{"code":"def refresh(\n        *modules: typing.Union[str, types.ModuleType],\n        recursive: bool = False,\n        force: bool = False\n) -> bool:\n    \"\"\"\n    Checks the specified module or modules for changes and reloads them if\n    they have been changed since the module was first imported or last\n    refreshed.\n\n    :param modules:\n        One or more module objects that should be refreshed if they the\n        currently loaded versions are out of date. The package name for\n        modules can also be used.\n    :param recursive:\n        When true, any imported sub-modules of this module will also be\n        refreshed if they have been updated.\n    :param force:\n        When true, all modules will be refreshed even if it doesn't appear\n        that they have been updated.\n    :return:\n        True or False depending on whether any modules were refreshed by this\n        call.\n    \"\"\"\n\n    out = []\n    for module in modules:\n        out.append(reload_module(module, recursive, force))\n\n    return any(out)","return_type":"bool","function_name":"refresh","stripped_code":"def refresh(\n        *modules: typing.Union[str, types.ModuleType],\n        recursive: bool = False,\n        force: bool = False\n):\n    \"\"\"\n    Checks the specified module or modules for changes and reloads them if\n    they have been changed since the module was first imported or last\n    refreshed.\n\n    :param modules:\n        One or more module objects that should be refreshed if they the\n        currently loaded versions are out of date. The package name for\n        modules can also be used.\n    :param recursive:\n        When true, any imported sub-modules of this module will also be\n        refreshed if they have been updated.\n    :param force:\n        When true, all modules will be refreshed even if it doesn't appear\n        that they have been updated.\n    :return:\n        True or False depending on whether any modules were refreshed by this\n        call.\n    \"\"\"\n\n    out = []\n    for module in modules:\n        out.append(reload_module(module, recursive, force))\n\n    return any(out)"}
{"code":"def visibility(vis: Number, unit: str = 'm') -> str:\n    \"\"\"\n    Format visibility details into a spoken word string\n    \"\"\"\n    if not vis:\n        return 'Visibility unknown'\n    if vis.value is None or '/' in vis.repr:\n        ret_vis = vis.spoken\n    else:\n        ret_vis = translate.visibility(vis, unit=unit)\n        if unit == 'm':\n            unit = 'km'\n        ret_vis = ret_vis[:ret_vis.find(' (')].lower().replace(unit, '').strip()\n        ret_vis = core.spoken_number(core.remove_leading_zeros(ret_vis))\n    ret = 'Visibility ' + ret_vis\n    if unit in SPOKEN_UNITS:\n        if '/' in vis.repr and 'half' not in ret:\n            ret += ' of a'\n        ret += ' ' + SPOKEN_UNITS[unit]\n        if not (('one half' in ret and ' and ' not in ret) or 'of a' in ret):\n            ret += 's'\n    else:\n        ret += unit\n    return ret","return_type":"str","function_name":"visibility","stripped_code":"def visibility(vis: Number, unit: str = 'm'):\n    \"\"\"\n    Format visibility details into a spoken word string\n    \"\"\"\n    if not vis:\n        return 'Visibility unknown'\n    if vis.value is None or '/' in vis.repr:\n        ret_vis = vis.spoken\n    else:\n        ret_vis = translate.visibility(vis, unit=unit)\n        if unit == 'm':\n            unit = 'km'\n        ret_vis = ret_vis[:ret_vis.find(' (')].lower().replace(unit, '').strip()\n        ret_vis = core.spoken_number(core.remove_leading_zeros(ret_vis))\n    ret = 'Visibility ' + ret_vis\n    if unit in SPOKEN_UNITS:\n        if '/' in vis.repr and 'half' not in ret:\n            ret += ' of a'\n        ret += ' ' + SPOKEN_UNITS[unit]\n        if not (('one half' in ret and ' and ' not in ret) or 'of a' in ret):\n            ret += 's'\n    else:\n        ret += unit\n    return ret"}
{"code":"def do_reload(module: types.ModuleType, newer_than: int) -> bool:\n    \"\"\"\n    Executes the reload of the specified module if the source file that it was\n    loaded from was updated more recently than the specified time\n\n    :param module:\n        A module object to be reloaded\n    :param newer_than:\n        The time in seconds since epoch that should be used to determine if\n        the module needs to be reloaded. If the module source was modified\n        more recently than this time, the module will be refreshed.\n    :return:\n        Whether or not the module was reloaded\n    \"\"\"\n    path = getattr(module, '__file__')\n    directory = getattr(module, '__path__', [None])[0]\n\n    if path is None and directory:\n        path = os.path.join(directory, '__init__.py')\n\n    last_modified = os.path.getmtime(path)\n\n    if last_modified < newer_than:\n        return False\n\n    try:\n        importlib.reload(module)\n        return True\n    except ImportError:\n        return False","return_type":"bool","function_name":"do_reload","stripped_code":"def do_reload(module: types.ModuleType, newer_than: int):\n    \"\"\"\n    Executes the reload of the specified module if the source file that it was\n    loaded from was updated more recently than the specified time\n\n    :param module:\n        A module object to be reloaded\n    :param newer_than:\n        The time in seconds since epoch that should be used to determine if\n        the module needs to be reloaded. If the module source was modified\n        more recently than this time, the module will be refreshed.\n    :return:\n        Whether or not the module was reloaded\n    \"\"\"\n    path = getattr(module, '__file__')\n    directory = getattr(module, '__path__', [None])[0]\n\n    if path is None and directory:\n        path = os.path.join(directory, '__init__.py')\n\n    last_modified = os.path.getmtime(path)\n\n    if last_modified < newer_than:\n        return False\n\n    try:\n        importlib.reload(module)\n        return True\n    except ImportError:\n        return False"}
{"code":"def call(self, func_name, args=None, *,\n             timeout=-1.0, push_subscribe=False) -> _MethodRet:\n        \"\"\"\n            Call request coroutine. It is a call with a new behaviour\n            (return result of a Tarantool procedure is not wrapped into\n            an extra tuple). If you're connecting to Tarantool with\n            version < 1.7, then this call method acts like a call16 method\n\n            Examples:\n\n            .. code-block:: pycon\n\n                # tarantool function:\n                # function f(...)\n                #     return ...\n                # end\n\n                >>> await conn.call('f')\n                <Response sync=3 rowcount=0 data=[]>\n\n                >>> await conn.call('f', [20, 42])\n                <Response sync=3 rowcount=2 data=[20, 42]>\n\n            :param func_name: function name to call\n            :param args: arguments to pass to the function (list object)\n            :param timeout: Request timeout\n            :param push_subscribe: Subscribe to push notifications\n\n            :returns: :class:`asynctnt.Response` instance\n        \"\"\"\n        return self._db.call(func_name, args,\n                             timeout=timeout, push_subscribe=push_subscribe)","return_type":"_MethodRet","function_name":"Connection.call","stripped_code":"def call(self, func_name, args=None, *,\n             timeout=-1.0, push_subscribe=False):\n        \"\"\"\n            Call request coroutine. It is a call with a new behaviour\n            (return result of a Tarantool procedure is not wrapped into\n            an extra tuple). If you're connecting to Tarantool with\n            version < 1.7, then this call method acts like a call16 method\n\n            Examples:\n\n            .. code-block:: pycon\n\n                # tarantool function:\n                # function f(...)\n                #     return ...\n                # end\n\n                >>> await conn.call('f')\n                <Response sync=3 rowcount=0 data=[]>\n\n                >>> await conn.call('f', [20, 42])\n                <Response sync=3 rowcount=2 data=[20, 42]>\n\n            :param func_name: function name to call\n            :param args: arguments to pass to the function (list object)\n            :param timeout: Request timeout\n            :param push_subscribe: Subscribe to push notifications\n\n            :returns: :class:`asynctnt.Response` instance\n        \"\"\"\n        return self._db.call(func_name, args,\n                             timeout=timeout, push_subscribe=push_subscribe)"}
{"code":"def is_path_sibling_creatable(pathname: str) -> bool:\n    \"\"\"Checks whether current user can create siblings of the given path.\n\n    Returns\n    -------\n    `True` if the current user has sufficient permissions to create siblings\n    (i.e., arbitrary files in the parent directory) of the passed pathname;\n    `False` otherwise.\n    \"\"\"\n    # Parent directory of the passed path. If empty, we substitute the current\n    # working directory (CWD) instead.\n    dirname = os.path.dirname(pathname) or os.getcwd()\n\n    try:\n        # For safety, explicitly close and hence delete this temporary file\n        # immediately after creating it in the passed path's parent directory.\n        with tempfile.TemporaryFile(dir=dirname):\n            pass\n        return True\n    # While the exact type of exception raised by the above function depends on\n    # the current version of the Python interpreter, all such types subclass\n    # the following exception superclass.\n    except EnvironmentError:\n        return False","return_type":"bool","function_name":"is_path_sibling_creatable","stripped_code":"def is_path_sibling_creatable(pathname: str):\n    \"\"\"Checks whether current user can create siblings of the given path.\n\n    Returns\n    -------\n    `True` if the current user has sufficient permissions to create siblings\n    (i.e., arbitrary files in the parent directory) of the passed pathname;\n    `False` otherwise.\n    \"\"\"\n    # Parent directory of the passed path. If empty, we substitute the current\n    # working directory (CWD) instead.\n    dirname = os.path.dirname(pathname) or os.getcwd()\n\n    try:\n        # For safety, explicitly close and hence delete this temporary file\n        # immediately after creating it in the passed path's parent directory.\n        with tempfile.TemporaryFile(dir=dirname):\n            pass\n        return True\n    # While the exact type of exception raised by the above function depends on\n    # the current version of the Python interpreter, all such types subclass\n    # the following exception superclass.\n    except EnvironmentError:\n        return False"}
{"code":"def dict_to_pendulumdate(d: Dict[str, Any],\n                         pendulumdate_class: ClassType) -> Date:\n    \"\"\"\n    Converts a ``dict`` object back to a ``pendulum.Date``.\n    \"\"\"\n    # noinspection PyTypeChecker\n    return pendulum.parse(d['iso']).date()","return_type":"Date","function_name":"dict_to_pendulumdate","stripped_code":"def dict_to_pendulumdate(d: Dict[str, Any],\n                         pendulumdate_class: ClassType):\n    \"\"\"\n    Converts a ``dict`` object back to a ``pendulum.Date``.\n    \"\"\"\n    # noinspection PyTypeChecker\n    return pendulum.parse(d['iso']).date()"}
{"code":"def error(self, msg: str) -> None:\n        \"\"\"\n        Write an error message to the Windows Application log\n        (\u00b1 to the Python disk log).\n        \"\"\"\n        # noinspection PyUnresolvedReferences\n        s = \"{}: {}\".format(self.fullname, msg)\n        servicemanager.LogErrorMsg(s)\n        if self.debugging:\n            log.warning(s)","return_type":"None","function_name":"ProcessManager.error","stripped_code":"def error(self, msg: str):\n        \"\"\"\n        Write an error message to the Windows Application log\n        (\u00b1 to the Python disk log).\n        \"\"\"\n        # noinspection PyUnresolvedReferences\n        s = \"{}: {}\".format(self.fullname, msg)\n        servicemanager.LogErrorMsg(s)\n        if self.debugging:\n            log.warning(s)"}
{"code":"def signal_alias_exists(alias: str) -> bool:\n        \"\"\"\n        Checks if signal alias exists.\n        :param alias: Signal alias.\n        :return:\n        \"\"\"\n        if SignalDispatcher.signals.get(alias):\n            return True\n\n        return False","return_type":"bool","function_name":"SignalDispatcher.signal_alias_exists","stripped_code":"def signal_alias_exists(alias: str):\n        \"\"\"\n        Checks if signal alias exists.\n        :param alias: Signal alias.\n        :return:\n        \"\"\"\n        if SignalDispatcher.signals.get(alias):\n            return True\n\n        return False"}
{"code":"def normalize_per_cell_weinreb16_deprecated(\n    X,\n    max_fraction=1,\n    mult_with_mean=False,\n) -> np.ndarray:\n    \"\"\"Normalize each cell [Weinreb17]_.\n\n    This is a deprecated version. See `normalize_per_cell` instead.\n\n    Normalize each cell by UMI count, so that every cell has the same total\n    count.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Expression matrix. Rows correspond to cells and columns to genes.\n    max_fraction : float, optional\n        Only use genes that make up more than max_fraction of the total\n        reads in every cell.\n    mult_with_mean: bool, optional\n        Multiply the result with the mean of total counts.\n\n    Returns\n    -------\n    Normalized version of the original expression matrix.\n    \"\"\"\n    if max_fraction < 0 or max_fraction > 1:\n        raise ValueError('Choose max_fraction between 0 and 1.')\n\n    counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n    gene_subset = np.all(X <= counts_per_cell[:, None] * max_fraction, axis=0)\n    if issparse(X): gene_subset = gene_subset.A1\n    tc_include = X[:, gene_subset].sum(1).A1 if issparse(X) else X[:, gene_subset].sum(1)\n\n    X_norm = X.multiply(csr_matrix(1/tc_include[:, None])) if issparse(X) else X / tc_include[:, None]\n    if mult_with_mean:\n        X_norm *= np.mean(counts_per_cell)\n\n    return X_norm","return_type":"np.ndarray","function_name":"normalize_per_cell_weinreb16_deprecated","stripped_code":"def normalize_per_cell_weinreb16_deprecated(\n    X,\n    max_fraction=1,\n    mult_with_mean=False,\n):\n    \"\"\"Normalize each cell [Weinreb17]_.\n\n    This is a deprecated version. See `normalize_per_cell` instead.\n\n    Normalize each cell by UMI count, so that every cell has the same total\n    count.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Expression matrix. Rows correspond to cells and columns to genes.\n    max_fraction : float, optional\n        Only use genes that make up more than max_fraction of the total\n        reads in every cell.\n    mult_with_mean: bool, optional\n        Multiply the result with the mean of total counts.\n\n    Returns\n    -------\n    Normalized version of the original expression matrix.\n    \"\"\"\n    if max_fraction < 0 or max_fraction > 1:\n        raise ValueError('Choose max_fraction between 0 and 1.')\n\n    counts_per_cell = X.sum(1).A1 if issparse(X) else X.sum(1)\n    gene_subset = np.all(X <= counts_per_cell[:, None] * max_fraction, axis=0)\n    if issparse(X): gene_subset = gene_subset.A1\n    tc_include = X[:, gene_subset].sum(1).A1 if issparse(X) else X[:, gene_subset].sum(1)\n\n    X_norm = X.multiply(csr_matrix(1/tc_include[:, None])) if issparse(X) else X / tc_include[:, None]\n    if mult_with_mean:\n        X_norm *= np.mean(counts_per_cell)\n\n    return X_norm"}
{"code":"def place_limit_order(self, side: Side, amount: Number, price: Number) -> Order:\n        \"\"\"Place a limit order.\"\"\"\n        return self.place_order(side, OrderType.LIMIT, amount, price)","return_type":"Order","function_name":"TradingClient.place_limit_order","stripped_code":"def place_limit_order(self, side: Side, amount: Number, price: Number):\n        \"\"\"Place a limit order.\"\"\"\n        return self.place_order(side, OrderType.LIMIT, amount, price)"}
{"code":"def address(self) -> str:\n        '''generate an address from pubkey'''\n\n        return str(self._public_key.to_address(\n                   net_query(self.network))\n                   )","return_type":"str","function_name":"Kutil.address","stripped_code":"def address(self):\n        '''generate an address from pubkey'''\n\n        return str(self._public_key.to_address(\n                   net_query(self.network))\n                   )"}
{"code":"def prefix(url_prefix: str,\n           children: Iterable[Union[Route, RouteGenerator]],\n           ) -> RouteGenerator:\n    \"\"\"\n    Sets a prefix on all of the child routes passed to it. It also supports nesting, eg::\n\n        routes = lambda: [\n            prefix('/foobar', [\n                controller('/one', OneController),\n                controller('/two', TwoController),\n                prefix('/baz', [\n                    controller('/three', ThreeController),\n                    controller('/four', FourController),\n                ])\n            ])\n        ]\n\n    :param url_prefix: The url prefix to set on the child routes\n    :param children:\n    \"\"\"\n    for route in _reduce_routes(children):\n        route = route.copy()\n        route.rule = join(url_prefix, route.rule,\n                          trailing_slash=route.rule.endswith('/'))\n        yield route","return_type":"RouteGenerator","function_name":"prefix","stripped_code":"def prefix(url_prefix: str,\n           children: Iterable[Union[Route, RouteGenerator]],\n           ):\n    \"\"\"\n    Sets a prefix on all of the child routes passed to it. It also supports nesting, eg::\n\n        routes = lambda: [\n            prefix('/foobar', [\n                controller('/one', OneController),\n                controller('/two', TwoController),\n                prefix('/baz', [\n                    controller('/three', ThreeController),\n                    controller('/four', FourController),\n                ])\n            ])\n        ]\n\n    :param url_prefix: The url prefix to set on the child routes\n    :param children:\n    \"\"\"\n    for route in _reduce_routes(children):\n        route = route.copy()\n        route.rule = join(url_prefix, route.rule,\n                          trailing_slash=route.rule.endswith('/'))\n        yield route"}
{"code":"def handle_oneof(oneof_schema: list) -> tuple:\n    \"\"\"\n    Custom handle of `oneOf` JSON schema validator. Tried to match primitive type and see if it should be allowed\n     to be passed multiple timns into a command\n\n    :param oneof_schema: `oneOf` JSON schema\n    :return: Tuple of :class:`click.ParamType`, ``multiple`` flag and ``description`` of option\n    \"\"\"\n    oneof_dict = {schema[\"type\"]: schema for schema in oneof_schema}\n    click_type = None\n    multiple = False\n    description = None\n    for key, value in oneof_dict.items():\n        if key == \"array\":\n            continue\n        elif key in SCHEMA_BASE_MAP:\n            if oneof_dict.get(\"array\") and oneof_dict[\"array\"][\"items\"][\"type\"] == key:\n                multiple = True\n            # Found a match to a primitive type\n            click_type = SCHEMA_BASE_MAP[key]\n            description = value.get(\"title\")\n            break\n    return click_type, multiple, description","return_type":"tuple","function_name":"handle_oneof","stripped_code":"def handle_oneof(oneof_schema: list):\n    \"\"\"\n    Custom handle of `oneOf` JSON schema validator. Tried to match primitive type and see if it should be allowed\n     to be passed multiple timns into a command\n\n    :param oneof_schema: `oneOf` JSON schema\n    :return: Tuple of :class:`click.ParamType`, ``multiple`` flag and ``description`` of option\n    \"\"\"\n    oneof_dict = {schema[\"type\"]: schema for schema in oneof_schema}\n    click_type = None\n    multiple = False\n    description = None\n    for key, value in oneof_dict.items():\n        if key == \"array\":\n            continue\n        elif key in SCHEMA_BASE_MAP:\n            if oneof_dict.get(\"array\") and oneof_dict[\"array\"][\"items\"][\"type\"] == key:\n                multiple = True\n            # Found a match to a primitive type\n            click_type = SCHEMA_BASE_MAP[key]\n            description = value.get(\"title\")\n            break\n    return click_type, multiple, description"}
{"code":"def gene_coordinates(host, org, gene, chr_exclude=[]) -> pd.DataFrame:\n    \"\"\"Retrieve gene coordinates for specific organism through BioMart.\n    Parameters\n    ----------\n    host : {{'www.ensembl.org', ...}}\n        A valid BioMart host URL. Can be used to control genome build.\n    org : {{'hsapiens', 'mmusculus', 'drerio'}}\n        Organism to query. Currently available are human ('hsapiens'), mouse\n        ('mmusculus') and zebrafish ('drerio').\n    gene :\n        The gene symbol (e.g. 'hgnc_symbol' for human) for which to retrieve\n        coordinates.\n    chr_exclude :\n        A list of chromosomes to exclude from query.\n    Returns\n    -------\n    A `pd.DataFrame` containing gene coordinates for the specified gene symbol.\n    \"\"\"\n    try:\n        from bioservices import biomart\n    except ImportError:\n        raise ImportError(\n            'You need to install the `bioservices` module.')\n    from io import StringIO\n    s = biomart.BioMart(host=host)\n\n    # building query\n    s.new_query()\n    if org == 'hsapiens':\n        s.add_dataset_to_xml('hsapiens_gene_ensembl')\n        s.add_attribute_to_xml('hgnc_symbol')\n    elif org == 'mmusculus':\n        s.add_dataset_to_xml('mmusculus_gene_ensembl')\n        s.add_attribute_to_xml('mgi_symbol')\n    elif org == 'drerio':\n        s.add_dataset_to_xml('drerio_gene_ensembl')\n        s.add_attribute_to_xml('zfin_id_symbol')\n    else:\n        logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True)\n        return None\n    s.add_attribute_to_xml('chromosome_name')\n    s.add_attribute_to_xml('start_position')\n    s.add_attribute_to_xml('end_position')\n    xml = s.get_xml()\n\n    # parsing gene coordinates\n    res = pd.read_csv(StringIO(s.query(xml)), sep='\\t', header=None)\n    res.columns = ['symbol', 'chromosome_name', 'start', 'end']\n    res = res.dropna()\n    res = res[~res['chromosome_name'].isin(chr_exclude)]\n    res = res.set_index('symbol')\n\n    return res.loc[[gene], :]","return_type":"pd.DataFrame","function_name":"gene_coordinates","stripped_code":"def gene_coordinates(host, org, gene, chr_exclude=[]):\n    \"\"\"Retrieve gene coordinates for specific organism through BioMart.\n    Parameters\n    ----------\n    host : {{'www.ensembl.org', ...}}\n        A valid BioMart host URL. Can be used to control genome build.\n    org : {{'hsapiens', 'mmusculus', 'drerio'}}\n        Organism to query. Currently available are human ('hsapiens'), mouse\n        ('mmusculus') and zebrafish ('drerio').\n    gene :\n        The gene symbol (e.g. 'hgnc_symbol' for human) for which to retrieve\n        coordinates.\n    chr_exclude :\n        A list of chromosomes to exclude from query.\n    Returns\n    -------\n    A `pd.DataFrame` containing gene coordinates for the specified gene symbol.\n    \"\"\"\n    try:\n        from bioservices import biomart\n    except ImportError:\n        raise ImportError(\n            'You need to install the `bioservices` module.')\n    from io import StringIO\n    s = biomart.BioMart(host=host)\n\n    # building query\n    s.new_query()\n    if org == 'hsapiens':\n        s.add_dataset_to_xml('hsapiens_gene_ensembl')\n        s.add_attribute_to_xml('hgnc_symbol')\n    elif org == 'mmusculus':\n        s.add_dataset_to_xml('mmusculus_gene_ensembl')\n        s.add_attribute_to_xml('mgi_symbol')\n    elif org == 'drerio':\n        s.add_dataset_to_xml('drerio_gene_ensembl')\n        s.add_attribute_to_xml('zfin_id_symbol')\n    else:\n        logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True)\n        return None\n    s.add_attribute_to_xml('chromosome_name')\n    s.add_attribute_to_xml('start_position')\n    s.add_attribute_to_xml('end_position')\n    xml = s.get_xml()\n\n    # parsing gene coordinates\n    res = pd.read_csv(StringIO(s.query(xml)), sep='\\t', header=None)\n    res.columns = ['symbol', 'chromosome_name', 'start', 'end']\n    res = res.dropna()\n    res = res[~res['chromosome_name'].isin(chr_exclude)]\n    res = res.set_index('symbol')\n\n    return res.loc[[gene], :]"}
{"code":"def result(self, *, claim_draw: bool = False) -> str:\n        \"\"\"\n        Gets the game result.\n\n        ``1-0``, ``0-1`` or ``1/2-1/2`` if the\n        :func:`game is over <chess.Board.is_game_over()>`. Otherwise, the\n        result is undetermined: ``*``.\n        \"\"\"\n        # Chess variant support.\n        if self.is_variant_loss():\n            return \"0-1\" if self.turn == WHITE else \"1-0\"\n        elif self.is_variant_win():\n            return \"1-0\" if self.turn == WHITE else \"0-1\"\n        elif self.is_variant_draw():\n            return \"1/2-1/2\"\n\n        # Checkmate.\n        if self.is_checkmate():\n            return \"0-1\" if self.turn == WHITE else \"1-0\"\n\n        # Draw claimed.\n        if claim_draw and self.can_claim_draw():\n            return \"1/2-1/2\"\n\n        # Seventyfive-move rule or fivefold repetition.\n        if self.is_seventyfive_moves() or self.is_fivefold_repetition():\n            return \"1/2-1/2\"\n\n        # Insufficient material.\n        if self.is_insufficient_material():\n            return \"1/2-1/2\"\n\n        # Stalemate.\n        if not any(self.generate_legal_moves()):\n            return \"1/2-1/2\"\n\n        # Undetermined.\n        return \"*\"","return_type":"str","function_name":"Board.result","stripped_code":"def result(self, *, claim_draw: bool = False):\n        \"\"\"\n        Gets the game result.\n\n        ``1-0``, ``0-1`` or ``1/2-1/2`` if the\n        :func:`game is over <chess.Board.is_game_over()>`. Otherwise, the\n        result is undetermined: ``*``.\n        \"\"\"\n        # Chess variant support.\n        if self.is_variant_loss():\n            return \"0-1\" if self.turn == WHITE else \"1-0\"\n        elif self.is_variant_win():\n            return \"1-0\" if self.turn == WHITE else \"0-1\"\n        elif self.is_variant_draw():\n            return \"1/2-1/2\"\n\n        # Checkmate.\n        if self.is_checkmate():\n            return \"0-1\" if self.turn == WHITE else \"1-0\"\n\n        # Draw claimed.\n        if claim_draw and self.can_claim_draw():\n            return \"1/2-1/2\"\n\n        # Seventyfive-move rule or fivefold repetition.\n        if self.is_seventyfive_moves() or self.is_fivefold_repetition():\n            return \"1/2-1/2\"\n\n        # Insufficient material.\n        if self.is_insufficient_material():\n            return \"1/2-1/2\"\n\n        # Stalemate.\n        if not any(self.generate_legal_moves()):\n            return \"1/2-1/2\"\n\n        # Undetermined.\n        return \"*\""}
{"code":"def matches(self, txt: str) -> bool:\n        \"\"\"Determine whether txt matches pattern\n\n        :param txt: text to check\n        :return: True if match\n        \"\"\"\n        # rval = ref.getText()[1:-1].encode('utf-8').decode('unicode-escape')\n        if r'\\\\u' in self.pattern_re.pattern:\n            txt = txt.encode('utf-8').decode('unicode-escape')\n        match = self.pattern_re.match(txt)\n        return match is not None and match.end() == len(txt)","return_type":"bool","function_name":"JSGPattern.matches","stripped_code":"def matches(self, txt: str):\n        \"\"\"Determine whether txt matches pattern\n\n        :param txt: text to check\n        :return: True if match\n        \"\"\"\n        # rval = ref.getText()[1:-1].encode('utf-8').decode('unicode-escape')\n        if r'\\\\u' in self.pattern_re.pattern:\n            txt = txt.encode('utf-8').decode('unicode-escape')\n        match = self.pattern_re.match(txt)\n        return match is not None and match.end() == len(txt)"}
{"code":"def fetch_digests(self, package_name: str, package_version: str) -> dict:\n        \"\"\"Fetch digests for the given package in specified version from the given package index.\"\"\"\n        report = {}\n\n        for source in self._sources:\n            try:\n                report[source.url] = source.get_package_hashes(package_name, package_version)\n            except NotFound as exc:\n                _LOGGER.debug(\n                    f\"Package {package_name} in version {package_version} not \"\n                    f\"found on index {source.name}: {str(exc)}\"\n                )\n\n        return report","return_type":"dict","function_name":"PythonDigestsFetcher.fetch_digests","stripped_code":"def fetch_digests(self, package_name: str, package_version: str):\n        \"\"\"Fetch digests for the given package in specified version from the given package index.\"\"\"\n        report = {}\n\n        for source in self._sources:\n            try:\n                report[source.url] = source.get_package_hashes(package_name, package_version)\n            except NotFound as exc:\n                _LOGGER.debug(\n                    f\"Package {package_name} in version {package_version} not \"\n                    f\"found on index {source.name}: {str(exc)}\"\n                )\n\n        return report"}
{"code":"def select_number(self, rows: List[Row], column: NumberColumn) -> Number:\n        \"\"\"\n        Select function takes a row (as a list) and a column name and returns the number in that\n        column. If multiple rows are given, will return the first number that is not None.\n        \"\"\"\n        numbers: List[float] = []\n        for row in rows:\n            cell_value = row.values[column.name]\n            if isinstance(cell_value, float):\n                numbers.append(cell_value)\n\n        return numbers[0] if numbers else -1","return_type":"Number","function_name":"WikiTablesLanguage.select_number","stripped_code":"def select_number(self, rows: List[Row], column: NumberColumn):\n        \"\"\"\n        Select function takes a row (as a list) and a column name and returns the number in that\n        column. If multiple rows are given, will return the first number that is not None.\n        \"\"\"\n        numbers: List[float] = []\n        for row in rows:\n            cell_value = row.values[column.name]\n            if isinstance(cell_value, float):\n                numbers.append(cell_value)\n\n        return numbers[0] if numbers else -1"}
{"code":"def recode(self, table: pd.DataFrame, validate=False) -> pd.DataFrame:\n        \"\"\"Return a fully recoded dataframe.\n\n        Args:\n            table (pd.DataFrame): A dataframe on which to apply recoding logic.\n            validate (bool): If ``True``, recoded table must pass validation tests.\n        \"\"\"\n        df = pd.DataFrame(index=table.index)\n\n        for column in self.columns:\n            df = column.update_dataframe(df, table=table, validate=validate)\n\n        return df","return_type":"pd.DataFrame","function_name":"Enforcer.recode","stripped_code":"def recode(self, table: pd.DataFrame, validate=False):\n        \"\"\"Return a fully recoded dataframe.\n\n        Args:\n            table (pd.DataFrame): A dataframe on which to apply recoding logic.\n            validate (bool): If ``True``, recoded table must pass validation tests.\n        \"\"\"\n        df = pd.DataFrame(index=table.index)\n\n        for column in self.columns:\n            df = column.update_dataframe(df, table=table, validate=validate)\n\n        return df"}
{"code":"def join(self, *data: Iterable[MaybeBytes]) -> bytes:\n        \"\"\"Iterable join on a delimiter.\n\n        Args:\n            data: Iterable of items to join.\n\n        Examples:\n            ::\n\n                BytesFormat(b' ').join([b'one', b'two', b'three'])\n\n        \"\"\"\n        return self.how.join([bytes(item) for item in chain(*data)])","return_type":"bytes","function_name":"BytesFormat.join","stripped_code":"def join(self, *data: Iterable[MaybeBytes]):\n        \"\"\"Iterable join on a delimiter.\n\n        Args:\n            data: Iterable of items to join.\n\n        Examples:\n            ::\n\n                BytesFormat(b' ').join([b'one', b'two', b'three'])\n\n        \"\"\"\n        return self.how.join([bytes(item) for item in chain(*data)])"}
{"code":"def is_valid_hendecasyllables(self, scanned_line: str) -> bool:\n        \"\"\"Determine if a scansion pattern is one of the valid Hendecasyllables metrical patterns\n\n        :param scanned_line: a line containing a sequence of stressed and unstressed syllables\n        :return bool\n\n        >>> print(MetricalValidator().is_valid_hendecasyllables(\"-U-UU-U-U-U\"))\n        True\n        \"\"\"\n        line = scanned_line.replace(self.constants.FOOT_SEPARATOR, \"\")\n        line = line.replace(\" \", \"\")\n        if len(line) < 11:\n            return False\n        line = line[:-1] + self.constants.OPTIONAL_ENDING\n        return self.VALID_HENDECASYLLABLES.__contains__(line)","return_type":"bool","function_name":"MetricalValidator.is_valid_hendecasyllables","stripped_code":"def is_valid_hendecasyllables(self, scanned_line: str):\n        \"\"\"Determine if a scansion pattern is one of the valid Hendecasyllables metrical patterns\n\n        :param scanned_line: a line containing a sequence of stressed and unstressed syllables\n        :return bool\n\n        >>> print(MetricalValidator().is_valid_hendecasyllables(\"-U-UU-U-U-U\"))\n        True\n        \"\"\"\n        line = scanned_line.replace(self.constants.FOOT_SEPARATOR, \"\")\n        line = line.replace(\" \", \"\")\n        if len(line) < 11:\n            return False\n        line = line[:-1] + self.constants.OPTIONAL_ENDING\n        return self.VALID_HENDECASYLLABLES.__contains__(line)"}
{"code":"def getLatency(self, instId: int) -> float:\n        \"\"\"\n        Return a dict with client identifier as a key and calculated latency as a value\n        \"\"\"\n        if len(self.clientAvgReqLatencies) == 0:\n            return 0.0\n        return self.clientAvgReqLatencies[instId].get_avg_latency()","return_type":"float","function_name":"Monitor.getLatency","stripped_code":"def getLatency(self, instId: int):\n        \"\"\"\n        Return a dict with client identifier as a key and calculated latency as a value\n        \"\"\"\n        if len(self.clientAvgReqLatencies) == 0:\n            return 0.0\n        return self.clientAvgReqLatencies[instId].get_avg_latency()"}
{"code":"def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is an ActivityPub request.\n    \"\"\"\n    # noinspection PyBroadException\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"@context\" in data:\n            return True\n    except Exception:\n        pass\n    return False","return_type":"bool","function_name":"identify_request","stripped_code":"def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is an ActivityPub request.\n    \"\"\"\n    # noinspection PyBroadException\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"@context\" in data:\n            return True\n    except Exception:\n        pass\n    return False"}
{"code":"def has_option(self, section, option) -> bool:\n        \"\"\"Test if file has option.\n\n        Parameters\n        ----------\n        section : string\n            Section.\n        option : string\n            Option.\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n        self.config.read(self.filepath)\n        return self.config.has_option(section, option)","return_type":"bool","function_name":"INI.has_option","stripped_code":"def has_option(self, section, option):\n        \"\"\"Test if file has option.\n\n        Parameters\n        ----------\n        section : string\n            Section.\n        option : string\n            Option.\n\n        Returns\n        -------\n        boolean\n        \"\"\"\n        self.config.read(self.filepath)\n        return self.config.has_option(section, option)"}
{"code":"def heatmap(self, x: str, y: str, options: str = '', title: str = '',\n                label: str = '') -> object:\n        \"\"\"\n        Documentation link: http://support.sas.com/documentation/cdl/en/grstatproc/67909/HTML/default/viewer.htm#n0w12m4cn1j5c6n12ak64u1rys4w.htm\n\n        :param x: x variable\n        :param y: y variable\n        :param options: display options (string)\n        :param title: graph title\n        :param label:\n        :return:\n        \"\"\"\n        code = \"proc sgplot data=%s.%s %s;\" % (self.libref, self.table, self._dsopts())\n        if len(options):\n            code += \"\\n\\theatmap x=%s y=%s / %s;\" % (x, y, options)\n        else:\n            code += \"\\n\\theatmap x=%s y=%s;\" % (x, y)\n\n        if len(label) > 0:\n            code += \" LegendLABEL='\" + label + \"'\"\n        code += \";\\n\"\n        if len(title) > 0:\n            code += \"\\ttitle '%s';\\n\" % title\n        code += \"run;\\ntitle;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll","return_type":"object","function_name":"SASdata.heatmap","stripped_code":"def heatmap(self, x: str, y: str, options: str = '', title: str = '',\n                label: str = ''):\n        \"\"\"\n        Documentation link: http://support.sas.com/documentation/cdl/en/grstatproc/67909/HTML/default/viewer.htm#n0w12m4cn1j5c6n12ak64u1rys4w.htm\n\n        :param x: x variable\n        :param y: y variable\n        :param options: display options (string)\n        :param title: graph title\n        :param label:\n        :return:\n        \"\"\"\n        code = \"proc sgplot data=%s.%s %s;\" % (self.libref, self.table, self._dsopts())\n        if len(options):\n            code += \"\\n\\theatmap x=%s y=%s / %s;\" % (x, y, options)\n        else:\n            code += \"\\n\\theatmap x=%s y=%s;\" % (x, y)\n\n        if len(label) > 0:\n            code += \" LegendLABEL='\" + label + \"'\"\n        code += \";\\n\"\n        if len(title) > 0:\n            code += \"\\ttitle '%s';\\n\" % title\n        code += \"run;\\ntitle;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        ll = self._is_valid()\n        if not ll:\n            html = self.HTML\n            self.HTML = 1\n            ll = self.sas._io.submit(code)\n            self.HTML = html\n        if not self.sas.batch:\n            self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n        else:\n            return ll"}
{"code":"def signed_to_float(hex: str) -> float:\n    \"\"\"Convert signed hexadecimal to floating value.\"\"\"\n    if int(hex, 16) & 0x8000:\n        return -(int(hex, 16) & 0x7FFF) / 10\n    else:\n        return int(hex, 16) / 10","return_type":"float","function_name":"signed_to_float","stripped_code":"def signed_to_float(hex: str):\n    \"\"\"Convert signed hexadecimal to floating value.\"\"\"\n    if int(hex, 16) & 0x8000:\n        return -(int(hex, 16) & 0x7FFF) / 10\n    else:\n        return int(hex, 16) / 10"}
{"code":"def get_args_index(target) -> int:\n    \"\"\"\n    Returns the index of the \"*args\" parameter if such a parameter exists in\n    the function arguments or -1 otherwise.\n\n    :param target:\n        The target function for which the args index should be determined\n    :return:\n        The arguments index if it exists or -1 if not\n    \"\"\"\n\n    code = target.__code__\n\n    if not bool(code.co_flags & inspect.CO_VARARGS):\n        return -1\n\n    return code.co_argcount + code.co_kwonlyargcount","return_type":"int","function_name":"get_args_index","stripped_code":"def get_args_index(target):\n    \"\"\"\n    Returns the index of the \"*args\" parameter if such a parameter exists in\n    the function arguments or -1 otherwise.\n\n    :param target:\n        The target function for which the args index should be determined\n    :return:\n        The arguments index if it exists or -1 if not\n    \"\"\"\n\n    code = target.__code__\n\n    if not bool(code.co_flags & inspect.CO_VARARGS):\n        return -1\n\n    return code.co_argcount + code.co_kwonlyargcount"}
{"code":"def wrap_exception(func: Callable) -> Callable:\n    \"\"\"Decorator to wrap pygatt exceptions into BluetoothBackendException.\"\"\"\n    try:\n        # only do the wrapping if pygatt is installed.\n        # otherwise it's pointless anyway\n        from pygatt.backends.bgapi.exceptions import BGAPIError\n        from pygatt.exceptions import NotConnectedError\n    except ImportError:\n        return func\n\n    def _func_wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except BGAPIError as exception:\n            raise BluetoothBackendException() from exception\n        except NotConnectedError as exception:\n            raise BluetoothBackendException() from exception\n\n    return _func_wrapper","return_type":"Callable","function_name":"wrap_exception","stripped_code":"def wrap_exception(func: Callable):\n    \"\"\"Decorator to wrap pygatt exceptions into BluetoothBackendException.\"\"\"\n    try:\n        # only do the wrapping if pygatt is installed.\n        # otherwise it's pointless anyway\n        from pygatt.backends.bgapi.exceptions import BGAPIError\n        from pygatt.exceptions import NotConnectedError\n    except ImportError:\n        return func\n\n    def _func_wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except BGAPIError as exception:\n            raise BluetoothBackendException() from exception\n        except NotConnectedError as exception:\n            raise BluetoothBackendException() from exception\n\n    return _func_wrapper"}
{"code":"def _is_trivial_angle(rad: float, atol: float) -> bool:\n    \"\"\"Tests if a circuit for an operator exp(i*rad*XX) (or YY, or ZZ) can\n    be performed with a whole CZ.\n\n    Args:\n        rad: The angle in radians, assumed to be in the range [-pi/4, pi/4]\n    \"\"\"\n    return abs(rad) < atol or abs(abs(rad) - np.pi / 4) < atol","return_type":"bool","function_name":"_is_trivial_angle","stripped_code":"def _is_trivial_angle(rad: float, atol: float):\n    \"\"\"Tests if a circuit for an operator exp(i*rad*XX) (or YY, or ZZ) can\n    be performed with a whole CZ.\n\n    Args:\n        rad: The angle in radians, assumed to be in the range [-pi/4, pi/4]\n    \"\"\"\n    return abs(rad) < atol or abs(abs(rad) - np.pi / 4) < atol"}
{"code":"def main() -> None:\n    \"\"\"\"Execute the main routine.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--outdir\", help=\"output directory\", default=os.path.dirname(__file__))\n    args = parser.parse_args()\n\n    outdir = pathlib.Path(args.outdir)\n    if not outdir.exists():\n        raise FileNotFoundError(\"Output directory is missing: {}\".format(outdir))\n\n    for contracts in [0, 1, 5, 10]:\n        if contracts == 0:\n            pth = outdir / \"functions_100_with_no_contract.py\"\n        elif contracts == 1:\n            pth = outdir / \"functions_100_with_1_contract.py\"\n        else:\n            pth = outdir / \"functions_100_with_{}_contracts.py\".format(contracts)\n\n        text = generate_functions(functions=100, contracts=contracts, disabled=False)\n        pth.write_text(text)\n\n    for contracts in [1, 5, 10]:\n        if contracts == 1:\n            pth = outdir / \"functions_100_with_1_disabled_contract.py\"\n        else:\n            pth = outdir / \"functions_100_with_{}_disabled_contracts.py\".format(contracts)\n\n        text = generate_functions(functions=100, contracts=contracts, disabled=True)\n        pth.write_text(text)\n\n    for invariants in [0, 1, 5, 10]:\n        if invariants == 0:\n            pth = outdir / \"classes_100_with_no_invariant.py\"\n        elif invariants == 1:\n            pth = outdir / \"classes_100_with_1_invariant.py\"\n        else:\n            pth = outdir / \"classes_100_with_{}_invariants.py\".format(invariants)\n\n        text = generate_classes(classes=100, invariants=invariants, disabled=False)\n        pth.write_text(text)\n\n    for invariants in [1, 5, 10]:\n        if invariants == 1:\n            pth = outdir / \"classes_100_with_1_disabled_invariant.py\"\n        else:\n            pth = outdir / \"classes_100_with_{}_disabled_invariants.py\".format(invariants)\n\n        text = generate_classes(classes=100, invariants=invariants, disabled=True)\n        pth.write_text(text)","return_type":"None","function_name":"main","stripped_code":"def main():\n    \"\"\"\"Execute the main routine.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--outdir\", help=\"output directory\", default=os.path.dirname(__file__))\n    args = parser.parse_args()\n\n    outdir = pathlib.Path(args.outdir)\n    if not outdir.exists():\n        raise FileNotFoundError(\"Output directory is missing: {}\".format(outdir))\n\n    for contracts in [0, 1, 5, 10]:\n        if contracts == 0:\n            pth = outdir / \"functions_100_with_no_contract.py\"\n        elif contracts == 1:\n            pth = outdir / \"functions_100_with_1_contract.py\"\n        else:\n            pth = outdir / \"functions_100_with_{}_contracts.py\".format(contracts)\n\n        text = generate_functions(functions=100, contracts=contracts, disabled=False)\n        pth.write_text(text)\n\n    for contracts in [1, 5, 10]:\n        if contracts == 1:\n            pth = outdir / \"functions_100_with_1_disabled_contract.py\"\n        else:\n            pth = outdir / \"functions_100_with_{}_disabled_contracts.py\".format(contracts)\n\n        text = generate_functions(functions=100, contracts=contracts, disabled=True)\n        pth.write_text(text)\n\n    for invariants in [0, 1, 5, 10]:\n        if invariants == 0:\n            pth = outdir / \"classes_100_with_no_invariant.py\"\n        elif invariants == 1:\n            pth = outdir / \"classes_100_with_1_invariant.py\"\n        else:\n            pth = outdir / \"classes_100_with_{}_invariants.py\".format(invariants)\n\n        text = generate_classes(classes=100, invariants=invariants, disabled=False)\n        pth.write_text(text)\n\n    for invariants in [1, 5, 10]:\n        if invariants == 1:\n            pth = outdir / \"classes_100_with_1_disabled_invariant.py\"\n        else:\n            pth = outdir / \"classes_100_with_{}_disabled_invariants.py\".format(invariants)\n\n        text = generate_classes(classes=100, invariants=invariants, disabled=True)\n        pth.write_text(text)"}
{"code":"def get_docker_tag(platform: str, registry: str) -> str:\n    \"\"\":return: docker tag to be used for the container\"\"\"\n    platform = platform if any(x in platform for x in ['build.', 'publish.']) else 'build.{}'.format(platform)\n    if not registry:\n        registry = \"mxnet_local\"\n    return \"{0}/{1}\".format(registry, platform)","return_type":"str","function_name":"get_docker_tag","stripped_code":"def get_docker_tag(platform: str, registry: str):\n    \"\"\":return: docker tag to be used for the container\"\"\"\n    platform = platform if any(x in platform for x in ['build.', 'publish.']) else 'build.{}'.format(platform)\n    if not registry:\n        registry = \"mxnet_local\"\n    return \"{0}/{1}\".format(registry, platform)"}
{"code":"def class_associations(self, cn: ClassDefinitionName, must_render: bool=False) -> str:\n        \"\"\" Emit all associations for a focus class.  If none are specified, all classes are generated\n\n        @param cn: Name of class to be emitted\n        @param must_render: True means render even if this is a target (class is specifically requested)\n        @return: YUML representation of the association\n        \"\"\"\n\n        # NOTE: YUML diagrams draw in the opposite order in which they are created, so we work from bottom to top and\n        # from right to left\n        assocs: List[str] = []\n        if cn not in self.associations_generated and (not self.focus_classes or cn in self.focus_classes):\n            cls = self.schema.classes[cn]\n\n            # Slots\n            for slotname in self.filtered_cls_slots(cn, False)[::-1]:\n                slot = self.schema.slots[slotname]\n                if slot.range in self.schema.classes:\n                    assocs.append(self.class_box(cn) + (yuml_inline if slot.inlined else yuml_ref) +\n                                  self.aliased_slot_name(slot) + self.prop_modifier(cls, slot) +\n                                  self.cardinality(slot) + '>' + self.class_box(slot.range))\n\n            # Referencing slots\n            if cn in self.synopsis.rangerefs:\n                for slotname in sorted(self.synopsis.rangerefs[cn]):\n                    slot = self.schema.slots[slotname]\n                    if slot.domain in self.schema.classes and (slot.range != cls.name or must_render):\n                        assocs.append(self.class_box(slot.domain) + (yuml_inline if slot.inlined else yuml_ref) +\n                                      self.aliased_slot_name(slot) + self.prop_modifier(cls, slot) +\n                                      self.cardinality(slot) + '>' + self.class_box(cn))\n\n            # Mixins used in the class\n            for mixin in cls.mixins:\n                assocs.append(self.class_box(cn) + yuml_uses + self.class_box(mixin))\n\n            # Classes that use the class as a mixin\n            if cls.name in self.synopsis.mixinrefs:\n                for mixin in sorted(self.synopsis.mixinrefs[cls.name].classrefs, reverse=True):\n                    assocs.append(self.class_box(ClassDefinitionName(mixin)) + yuml_uses + self.class_box(cn))\n\n            # Classes that inject information\n            if cn in self.synopsis.applytos:\n                for injector in sorted(self.synopsis.applytos[cn].classrefs, reverse=True):\n                    assocs.append(self.class_box(cn) + yuml_injected + self.class_box(ClassDefinitionName(injector)))\n            self.associations_generated.add(cn)\n\n            # Children\n            if cn in self.synopsis.isarefs:\n                for is_a_cls in sorted(self.synopsis.isarefs[cn].classrefs, reverse=True):\n                    assocs.append(self.class_box(cn) + yuml_is_a + self.class_box(ClassDefinitionName(is_a_cls)))\n\n            # Parent\n            if cls.is_a:\n                assocs.append(self.class_box(cls.is_a) + yuml_is_a + self.class_box(cn))\n        return ', '.join(assocs)","return_type":"str","function_name":"YumlGenerator.class_associations","stripped_code":"def class_associations(self, cn: ClassDefinitionName, must_render: bool=False):\n        \"\"\" Emit all associations for a focus class.  If none are specified, all classes are generated\n\n        @param cn: Name of class to be emitted\n        @param must_render: True means render even if this is a target (class is specifically requested)\n        @return: YUML representation of the association\n        \"\"\"\n\n        # NOTE: YUML diagrams draw in the opposite order in which they are created, so we work from bottom to top and\n        # from right to left\n        assocs: List[str] = []\n        if cn not in self.associations_generated and (not self.focus_classes or cn in self.focus_classes):\n            cls = self.schema.classes[cn]\n\n            # Slots\n            for slotname in self.filtered_cls_slots(cn, False)[::-1]:\n                slot = self.schema.slots[slotname]\n                if slot.range in self.schema.classes:\n                    assocs.append(self.class_box(cn) + (yuml_inline if slot.inlined else yuml_ref) +\n                                  self.aliased_slot_name(slot) + self.prop_modifier(cls, slot) +\n                                  self.cardinality(slot) + '>' + self.class_box(slot.range))\n\n            # Referencing slots\n            if cn in self.synopsis.rangerefs:\n                for slotname in sorted(self.synopsis.rangerefs[cn]):\n                    slot = self.schema.slots[slotname]\n                    if slot.domain in self.schema.classes and (slot.range != cls.name or must_render):\n                        assocs.append(self.class_box(slot.domain) + (yuml_inline if slot.inlined else yuml_ref) +\n                                      self.aliased_slot_name(slot) + self.prop_modifier(cls, slot) +\n                                      self.cardinality(slot) + '>' + self.class_box(cn))\n\n            # Mixins used in the class\n            for mixin in cls.mixins:\n                assocs.append(self.class_box(cn) + yuml_uses + self.class_box(mixin))\n\n            # Classes that use the class as a mixin\n            if cls.name in self.synopsis.mixinrefs:\n                for mixin in sorted(self.synopsis.mixinrefs[cls.name].classrefs, reverse=True):\n                    assocs.append(self.class_box(ClassDefinitionName(mixin)) + yuml_uses + self.class_box(cn))\n\n            # Classes that inject information\n            if cn in self.synopsis.applytos:\n                for injector in sorted(self.synopsis.applytos[cn].classrefs, reverse=True):\n                    assocs.append(self.class_box(cn) + yuml_injected + self.class_box(ClassDefinitionName(injector)))\n            self.associations_generated.add(cn)\n\n            # Children\n            if cn in self.synopsis.isarefs:\n                for is_a_cls in sorted(self.synopsis.isarefs[cn].classrefs, reverse=True):\n                    assocs.append(self.class_box(cn) + yuml_is_a + self.class_box(ClassDefinitionName(is_a_cls)))\n\n            # Parent\n            if cls.is_a:\n                assocs.append(self.class_box(cls.is_a) + yuml_is_a + self.class_box(cn))\n        return ', '.join(assocs)"}
{"code":"def from_iso_long_date(self, date_str: str) -> datetime:\n        \"\"\" Parse ISO date string (YYYY-MM-DDTHH:mm:ss) \"\"\"\n        assert isinstance(date_str, str)\n        assert len(date_str) == 19\n\n        self.value = datetime.strptime(date_str, ISO_LONG_FORMAT)\n        return self.value","return_type":"datetime","function_name":"Datum.from_iso_long_date","stripped_code":"def from_iso_long_date(self, date_str: str):\n        \"\"\" Parse ISO date string (YYYY-MM-DDTHH:mm:ss) \"\"\"\n        assert isinstance(date_str, str)\n        assert len(date_str) == 19\n\n        self.value = datetime.strptime(date_str, ISO_LONG_FORMAT)\n        return self.value"}
{"code":"def validate_unset_command(self, line: str, position: int, annotation: str) -> None:\n        \"\"\"Raise an exception when trying to ``UNSET X`` if ``X`` is not already set.\n\n        :raises: MissingAnnotationKeyWarning\n        \"\"\"\n        if annotation not in self.annotations:\n            raise MissingAnnotationKeyWarning(self.get_line_number(), line, position, annotation)","return_type":"None","function_name":"ControlParser.validate_unset_command","stripped_code":"def validate_unset_command(self, line: str, position: int, annotation: str):\n        \"\"\"Raise an exception when trying to ``UNSET X`` if ``X`` is not already set.\n\n        :raises: MissingAnnotationKeyWarning\n        \"\"\"\n        if annotation not in self.annotations:\n            raise MissingAnnotationKeyWarning(self.get_line_number(), line, position, annotation)"}
{"code":"def validate_sdl(\n    document_ast: DocumentNode,\n    schema_to_extend: GraphQLSchema = None,\n    rules: Sequence[RuleType] = None,\n) -> List[GraphQLError]:\n    \"\"\"Validate an SDL document.\"\"\"\n    context = SDLValidationContext(document_ast, schema_to_extend)\n    if rules is None:\n        rules = specified_sdl_rules\n    visitors = [rule(context) for rule in rules]\n    visit(document_ast, ParallelVisitor(visitors))\n    return context.errors","return_type":"List[GraphQLError]","function_name":"validate_sdl","stripped_code":"def validate_sdl(\n    document_ast: DocumentNode,\n    schema_to_extend: GraphQLSchema = None,\n    rules: Sequence[RuleType] = None,\n):\n    \"\"\"Validate an SDL document.\"\"\"\n    context = SDLValidationContext(document_ast, schema_to_extend)\n    if rules is None:\n        rules = specified_sdl_rules\n    visitors = [rule(context) for rule in rules]\n    visit(document_ast, ParallelVisitor(visitors))\n    return context.errors"}
{"code":"def load_json_file_contents(path: str) -> str:\n    \"\"\" Loads contents from a json file \"\"\"\n    assert isinstance(path, str)\n    content = None\n\n    file_path = os.path.abspath(path)\n    content = fileutils.read_text_from_file(file_path)\n    json_object = json.loads(content)\n    content = json.dumps(json_object, sort_keys=True, indent=4)\n\n    return content","return_type":"str","function_name":"load_json_file_contents","stripped_code":"def load_json_file_contents(path: str):\n    \"\"\" Loads contents from a json file \"\"\"\n    assert isinstance(path, str)\n    content = None\n\n    file_path = os.path.abspath(path)\n    content = fileutils.read_text_from_file(file_path)\n    json_object = json.loads(content)\n    content = json.dumps(json_object, sort_keys=True, indent=4)\n\n    return content"}
{"code":"def validate_normalized_state(state: np.ndarray,\n                              num_qubits: int,\n                              dtype: Type[np.number] = np.complex64) -> None:\n    \"\"\"Validates that the given state is a valid wave function.\"\"\"\n    if state.size != 1 << num_qubits:\n        raise ValueError(\n            'State has incorrect size. Expected {} but was {}.'.format(\n                1 << num_qubits, state.size))\n    if state.dtype != dtype:\n        raise ValueError(\n            'State has invalid dtype. Expected {} but was {}'.format(\n                dtype, state.dtype))\n    norm = np.sum(np.abs(state) ** 2)\n    if not np.isclose(norm, 1):\n        raise ValueError('State is not normalized instead had norm %s' % norm)","return_type":"None","function_name":"validate_normalized_state","stripped_code":"def validate_normalized_state(state: np.ndarray,\n                              num_qubits: int,\n                              dtype: Type[np.number] = np.complex64):\n    \"\"\"Validates that the given state is a valid wave function.\"\"\"\n    if state.size != 1 << num_qubits:\n        raise ValueError(\n            'State has incorrect size. Expected {} but was {}.'.format(\n                1 << num_qubits, state.size))\n    if state.dtype != dtype:\n        raise ValueError(\n            'State has invalid dtype. Expected {} but was {}'.format(\n                dtype, state.dtype))\n    norm = np.sum(np.abs(state) ** 2)\n    if not np.isclose(norm, 1):\n        raise ValueError('State is not normalized instead had norm %s' % norm)"}
{"code":"def close(self) -> None:\n        \"\"\"Kill the managed Spark session.\"\"\"\n        if self.session_id is not None:\n            self.client.delete_session(self.session_id)\n        self.client.close()","return_type":"None","function_name":"LivySession.close","stripped_code":"def close(self):\n        \"\"\"Kill the managed Spark session.\"\"\"\n        if self.session_id is not None:\n            self.client.delete_session(self.session_id)\n        self.client.close()"}
{"code":"def query_nvidia_smi(device_ids: List[int], result_queue: multiprocessing.Queue) -> None:\n    \"\"\"\n    Runs nvidia-smi to determine the memory usage.\n\n    :param device_ids: A list of devices for which the the memory usage will be queried.\n    :param result_queue: The queue to which the result dictionary of device id mapping to a tuple of\n    (memory used, memory total) is added.\n    \"\"\"\n    device_id_strs = [str(device_id) for device_id in device_ids]\n    query = \"--query-gpu=index,memory.used,memory.total\"\n    format_arg = \"--format=csv,noheader,nounits\"\n    try:\n        sp = subprocess.Popen(['nvidia-smi', query, format_arg, \"-i\", \",\".join(device_id_strs)],\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        result = sp.communicate()[0].decode(\"utf-8\").rstrip().split(\"\\n\")\n    except OSError:\n        logger.exception(\"Failed calling nvidia-smi to query memory usage.\")\n        result_queue.put({})\n        return\n    try:\n        memory_data = {}\n        for line in result:\n            gpu_id, mem_used, mem_total = line.split(\",\")\n            memory_data[int(gpu_id)] = (int(mem_used), int(mem_total))\n\n        result_queue.put(memory_data)\n    except:\n        logger.exception(\"Failed parsing nvidia-smi output %s\", \"\\n\".join(result))\n        result_queue.put({})","return_type":"None","function_name":"query_nvidia_smi","stripped_code":"def query_nvidia_smi(device_ids: List[int], result_queue: multiprocessing.Queue):\n    \"\"\"\n    Runs nvidia-smi to determine the memory usage.\n\n    :param device_ids: A list of devices for which the the memory usage will be queried.\n    :param result_queue: The queue to which the result dictionary of device id mapping to a tuple of\n    (memory used, memory total) is added.\n    \"\"\"\n    device_id_strs = [str(device_id) for device_id in device_ids]\n    query = \"--query-gpu=index,memory.used,memory.total\"\n    format_arg = \"--format=csv,noheader,nounits\"\n    try:\n        sp = subprocess.Popen(['nvidia-smi', query, format_arg, \"-i\", \",\".join(device_id_strs)],\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        result = sp.communicate()[0].decode(\"utf-8\").rstrip().split(\"\\n\")\n    except OSError:\n        logger.exception(\"Failed calling nvidia-smi to query memory usage.\")\n        result_queue.put({})\n        return\n    try:\n        memory_data = {}\n        for line in result:\n            gpu_id, mem_used, mem_total = line.split(\",\")\n            memory_data[int(gpu_id)] = (int(mem_used), int(mem_total))\n\n        result_queue.put(memory_data)\n    except:\n        logger.exception(\"Failed parsing nvidia-smi output %s\", \"\\n\".join(result))\n        result_queue.put({})"}
{"code":"def list_abundance_expansion(graph: BELGraph) -> None:\n    \"\"\"Flatten list abundances.\"\"\"\n    mapping = {\n        node: flatten_list_abundance(node)\n        for node in graph\n        if isinstance(node, ListAbundance)\n    }\n    relabel_nodes(graph, mapping, copy=False)","return_type":"None","function_name":"list_abundance_expansion","stripped_code":"def list_abundance_expansion(graph: BELGraph):\n    \"\"\"Flatten list abundances.\"\"\"\n    mapping = {\n        node: flatten_list_abundance(node)\n        for node in graph\n        if isinstance(node, ListAbundance)\n    }\n    relabel_nodes(graph, mapping, copy=False)"}
{"code":"def file_list_for_request(\n    list_of_paths: list, key_name: str, mimetype: str = None\n) -> list:\n    \"\"\"\n    Convenience function to construct a list of files for multiple files upload by :mod:`requests`\n\n    :param list_of_paths: Lists of strings to include in files. Should be pre validated for correctness\n    :param key_name: The key name to use for the file list in the request\n    :param mimetype: If specified, will be included in the requests\n    :return: List of open files ready to be used in a request\n    \"\"\"\n    if mimetype:\n        return [\n            (key_name, (file, open(file, mode=\"rb\"), mimetype))\n            for file in list_of_paths\n        ]\n    return [(key_name, (file, open(file, mode=\"rb\"))) for file in list_of_paths]","return_type":"list","function_name":"file_list_for_request","stripped_code":"def file_list_for_request(\n    list_of_paths: list, key_name: str, mimetype: str = None\n):\n    \"\"\"\n    Convenience function to construct a list of files for multiple files upload by :mod:`requests`\n\n    :param list_of_paths: Lists of strings to include in files. Should be pre validated for correctness\n    :param key_name: The key name to use for the file list in the request\n    :param mimetype: If specified, will be included in the requests\n    :return: List of open files ready to be used in a request\n    \"\"\"\n    if mimetype:\n        return [\n            (key_name, (file, open(file, mode=\"rb\"), mimetype))\n            for file in list_of_paths\n        ]\n    return [(key_name, (file, open(file, mode=\"rb\"))) for file in list_of_paths]"}
{"code":"def validate_resource(resource) -> None:\n    \"\"\"Validates a resource\"\"\"\n    if resource is not None and not isinstance(resource, dict):\n        raise ValidationError('The resource is not valid.')\n\n    if isinstance(resource, dict) and set(resource.keys()) <= {'requests', 'limits'}:\n        raise ValidationError(\n            'The keys `{}` for the resource are not valid.'.format(set(resource.keys())))","return_type":"None","function_name":"validate_resource","stripped_code":"def validate_resource(resource):\n    \"\"\"Validates a resource\"\"\"\n    if resource is not None and not isinstance(resource, dict):\n        raise ValidationError('The resource is not valid.')\n\n    if isinstance(resource, dict) and set(resource.keys()) <= {'requests', 'limits'}:\n        raise ValidationError(\n            'The keys `{}` for the resource are not valid.'.format(set(resource.keys())))"}
{"code":"def read_token(self, prev: Token) -> Token:\n        \"\"\"Get the next token from the source starting at the given position.\n\n        This skips over whitespace until it finds the next lexable token, then lexes\n        punctuators immediately or calls the appropriate helper function for more\n        complicated tokens.\n        \"\"\"\n        source = self.source\n        body = source.body\n        body_length = len(body)\n\n        pos = self.position_after_whitespace(body, prev.end)\n        line = self.line\n        col = 1 + pos - self.line_start\n\n        if pos >= body_length:\n            return Token(TokenKind.EOF, body_length, body_length, line, col, prev)\n\n        char = body[pos]\n        kind = _KIND_FOR_PUNCT.get(char)\n        if kind:\n            return Token(kind, pos, pos + 1, line, col, prev)\n        if char == \"#\":\n            return self.read_comment(pos, line, col, prev)\n        elif char == \".\":\n            if body[pos + 1 : pos + 3] == \"..\":\n                return Token(TokenKind.SPREAD, pos, pos + 3, line, col, prev)\n        elif \"A\" <= char <= \"Z\" or \"a\" <= char <= \"z\" or char == \"_\":\n            return self.read_name(pos, line, col, prev)\n        elif \"0\" <= char <= \"9\" or char == \"-\":\n            return self.read_number(pos, char, line, col, prev)\n        elif char == '\"':\n            if body[pos + 1 : pos + 3] == '\"\"':\n                return self.read_block_string(pos, line, col, prev)\n            return self.read_string(pos, line, col, prev)\n\n        raise GraphQLSyntaxError(source, pos, unexpected_character_message(char))","return_type":"Token","function_name":"Lexer.read_token","stripped_code":"def read_token(self, prev: Token):\n        \"\"\"Get the next token from the source starting at the given position.\n\n        This skips over whitespace until it finds the next lexable token, then lexes\n        punctuators immediately or calls the appropriate helper function for more\n        complicated tokens.\n        \"\"\"\n        source = self.source\n        body = source.body\n        body_length = len(body)\n\n        pos = self.position_after_whitespace(body, prev.end)\n        line = self.line\n        col = 1 + pos - self.line_start\n\n        if pos >= body_length:\n            return Token(TokenKind.EOF, body_length, body_length, line, col, prev)\n\n        char = body[pos]\n        kind = _KIND_FOR_PUNCT.get(char)\n        if kind:\n            return Token(kind, pos, pos + 1, line, col, prev)\n        if char == \"#\":\n            return self.read_comment(pos, line, col, prev)\n        elif char == \".\":\n            if body[pos + 1 : pos + 3] == \"..\":\n                return Token(TokenKind.SPREAD, pos, pos + 3, line, col, prev)\n        elif \"A\" <= char <= \"Z\" or \"a\" <= char <= \"z\" or char == \"_\":\n            return self.read_name(pos, line, col, prev)\n        elif \"0\" <= char <= \"9\" or char == \"-\":\n            return self.read_number(pos, char, line, col, prev)\n        elif char == '\"':\n            if body[pos + 1 : pos + 3] == '\"\"':\n                return self.read_block_string(pos, line, col, prev)\n            return self.read_string(pos, line, col, prev)\n\n        raise GraphQLSyntaxError(source, pos, unexpected_character_message(char))"}
{"code":"def is_satisfied_by(self, candidate: Any, **kwds: Any) -> bool:\n        \"\"\"Return True if `candidate` satisfies the specification.\"\"\"\n        candidate_name = self._candidate_name\n        context = self._context\n        if context:\n            if candidate_name in kwds:\n                raise ValueError(f\"Candidate name '{candidate_name}' must \"\n                                 \"not be given as keyword.\")\n            context.update(kwds)\n        context[candidate_name] = candidate\n        try:\n            code = self._code\n        except AttributeError:\n            self._code = code = compile(self._ast_expr, '<str>', mode='eval')\n        return eval(code, context)","return_type":"bool","function_name":"Specification.is_satisfied_by","stripped_code":"def is_satisfied_by(self, candidate: Any, **kwds: Any):\n        \"\"\"Return True if `candidate` satisfies the specification.\"\"\"\n        candidate_name = self._candidate_name\n        context = self._context\n        if context:\n            if candidate_name in kwds:\n                raise ValueError(f\"Candidate name '{candidate_name}' must \"\n                                 \"not be given as keyword.\")\n            context.update(kwds)\n        context[candidate_name] = candidate\n        try:\n            code = self._code\n        except AttributeError:\n            self._code = code = compile(self._ast_expr, '<str>', mode='eval')\n        return eval(code, context)"}
{"code":"def get_profile_ic(self, profile: List) -> Dict:\n        \"\"\"\n        Given a list of individuals, return their information content\n        \"\"\"\n        sim_response = get_attribute_information_profile(self.url, tuple(profile))\n\n        profile_ic = {}\n        try:\n            for cls in sim_response['input']:\n                profile_ic[cls['id']] = cls['IC']\n        except JSONDecodeError as json_exc:\n            raise JSONDecodeError(\n                \"Cannot parse owlsim2 response: {}\".format(json_exc.msg),\n                json_exc.doc,\n                json_exc.pos\n            )\n\n        return profile_ic","return_type":"Dict","function_name":"OwlSim2Api.get_profile_ic","stripped_code":"def get_profile_ic(self, profile: List):\n        \"\"\"\n        Given a list of individuals, return their information content\n        \"\"\"\n        sim_response = get_attribute_information_profile(self.url, tuple(profile))\n\n        profile_ic = {}\n        try:\n            for cls in sim_response['input']:\n                profile_ic[cls['id']] = cls['IC']\n        except JSONDecodeError as json_exc:\n            raise JSONDecodeError(\n                \"Cannot parse owlsim2 response: {}\".format(json_exc.msg),\n                json_exc.doc,\n                json_exc.pos\n            )\n\n        return profile_ic"}
{"code":"def deck_issue_mode(proto: DeckSpawnProto) -> Iterable[str]:\n    '''interpret issue mode bitfeg'''\n\n    if proto.issue_mode == 0:\n        yield \"NONE\"\n        return\n\n    for mode, value in proto.MODE.items():\n        if value > proto.issue_mode:\n            continue\n        if value & proto.issue_mode:\n            yield mode","return_type":"Iterable[str]","function_name":"deck_issue_mode","stripped_code":"def deck_issue_mode(proto: DeckSpawnProto):\n    '''interpret issue mode bitfeg'''\n\n    if proto.issue_mode == 0:\n        yield \"NONE\"\n        return\n\n    for mode, value in proto.MODE.items():\n        if value > proto.issue_mode:\n            continue\n        if value & proto.issue_mode:\n            yield mode"}
{"code":"def read_handle(self, handle: int) -> bytes:\n        \"\"\"Read a handle from the device.\"\"\"\n        if not self.is_connected():\n            raise BluetoothBackendException('Not connected to device!')\n        return self._device.char_read_handle(handle)","return_type":"bytes","function_name":"PygattBackend.read_handle","stripped_code":"def read_handle(self, handle: int):\n        \"\"\"Read a handle from the device.\"\"\"\n        if not self.is_connected():\n            raise BluetoothBackendException('Not connected to device!')\n        return self._device.char_read_handle(handle)"}
{"code":"def __release_location(self) -> None:\n        \"\"\"\n        Releases the lock on the location used by this writer. Will do nothing if the lock is\n        already released.\n        \"\"\"\n        if self._is_active:\n            LogdirWriter._locked_locations.remove(self._location)\n            self._is_active = False","return_type":"None","function_name":"LogdirWriter.__release_location","stripped_code":"def __release_location(self):\n        \"\"\"\n        Releases the lock on the location used by this writer. Will do nothing if the lock is\n        already released.\n        \"\"\"\n        if self._is_active:\n            LogdirWriter._locked_locations.remove(self._location)\n            self._is_active = False"}
{"code":"def macontrol(self, data: ['SASdata', str] = None,\n                  ewmachart: str = None,\n                  machart: str = None,\n                  procopts: str = None,\n                  stmtpassthrough: str = None,\n                  **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the MACONTROL procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_macontrol_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm ewmachart: The ewmachart variable can only be a string type.\n        :parm machart: The machart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"","return_type":"'SASresults'","function_name":"SASqc.macontrol","stripped_code":"def macontrol(self, data: ['SASdata', str] = None,\n                  ewmachart: str = None,\n                  machart: str = None,\n                  procopts: str = None,\n                  stmtpassthrough: str = None,\n                  **kwargs: dict):\n        \"\"\"\n        Python method to call the MACONTROL procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?cdcId=pgmsascdc&cdcVersion=9.4_3.4&docsetId=qcug&docsetTarget=qcug_macontrol_toc.htm&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm ewmachart: The ewmachart variable can only be a string type.\n        :parm machart: The machart variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\""}
{"code":"def do_list(self, arglist: List[str]) -> None:\n        \"\"\"Generate a list of 10 numbers.\"\"\"\n        if arglist:\n            first = arglist[0]\n            try:\n                first = int(first)\n            except ValueError:\n                first = 1\n        else:\n            first = 1\n        last = first + 10\n\n        for x in range(first, last):\n            self.poutput(str(x))","return_type":"None","function_name":"CmdLineApp.do_list","stripped_code":"def do_list(self, arglist: List[str]):\n        \"\"\"Generate a list of 10 numbers.\"\"\"\n        if arglist:\n            first = arglist[0]\n            try:\n                first = int(first)\n            except ValueError:\n                first = 1\n        else:\n            first = 1\n        last = first + 10\n\n        for x in range(first, last):\n            self.poutput(str(x))"}
{"code":"def primes(start: int = 1, end: int = 999) -> List[int]:\n        \"\"\"Generate a list of prime numbers.\n\n        :param start: First value of range.\n        :param end: Last value of range.\n        :return: A list of prime numbers from start to end.\n        \"\"\"\n        # TODO: It should generate random primes with passed length.\n        sieve_size = (end // 2 - 1) if end % 2 == 0 else (end // 2)\n        sieve = [True] * sieve_size\n\n        primes = []  # list of primes\n        # add 2 to the list if it's in the given range\n        if end >= 2:\n            primes.append(2)\n        for i in range(sieve_size):\n            if sieve[i]:\n                value_at_i = i * 2 + 3\n                primes.append(value_at_i)\n                for j in range(i, sieve_size, value_at_i):\n                    sieve[j] = False\n\n        chop_index = 0\n        for i in range(len(primes)):\n            if primes[i] >= start:\n                chop_index = i\n                break\n        return primes[chop_index:]","return_type":"List[int]","function_name":"Numbers.primes","stripped_code":"def primes(start: int = 1, end: int = 999):\n        \"\"\"Generate a list of prime numbers.\n\n        :param start: First value of range.\n        :param end: Last value of range.\n        :return: A list of prime numbers from start to end.\n        \"\"\"\n        # TODO: It should generate random primes with passed length.\n        sieve_size = (end // 2 - 1) if end % 2 == 0 else (end // 2)\n        sieve = [True] * sieve_size\n\n        primes = []  # list of primes\n        # add 2 to the list if it's in the given range\n        if end >= 2:\n            primes.append(2)\n        for i in range(sieve_size):\n            if sieve[i]:\n                value_at_i = i * 2 + 3\n                primes.append(value_at_i)\n                for j in range(i, sieve_size, value_at_i):\n                    sieve[j] = False\n\n        chop_index = 0\n        for i in range(len(primes)):\n            if primes[i] >= start:\n                chop_index = i\n                break\n        return primes[chop_index:]"}
{"code":"def samefile(a: str, b: str) -> bool:\n    \"\"\"Check if two pathes represent the same file.\"\"\"\n    try:\n        return os.path.samefile(a, b)\n    except OSError:\n        return os.path.normpath(a) == os.path.normpath(b)","return_type":"bool","function_name":"samefile","stripped_code":"def samefile(a: str, b: str):\n    \"\"\"Check if two pathes represent the same file.\"\"\"\n    try:\n        return os.path.samefile(a, b)\n    except OSError:\n        return os.path.normpath(a) == os.path.normpath(b)"}
{"code":"def custom_resource(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the CustomResource class. Registers the decorated class\n    as the CustomResource known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _custom_resource_type\n    _custom_resource_type = class_obj\n    return class_obj","return_type":"type","function_name":"custom_resource","stripped_code":"def custom_resource(class_obj: type):\n    \"\"\"\n    Decorator to annotate the CustomResource class. Registers the decorated class\n    as the CustomResource known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _custom_resource_type\n    _custom_resource_type = class_obj\n    return class_obj"}
{"code":"def satisifesShapeOr(cntxt: Context, n: Node, se: ShExJ.ShapeOr, _: DebugContext) -> bool:\n    \"\"\" Se is a ShapeOr and there is some shape expression se2 in shapeExprs such that satisfies(n, se2, G, m). \"\"\"\n    return any(satisfies(cntxt, n, se2) for se2 in se.shapeExprs)","return_type":"bool","function_name":"satisifesShapeOr","stripped_code":"def satisifesShapeOr(cntxt: Context, n: Node, se: ShExJ.ShapeOr, _: DebugContext):\n    \"\"\" Se is a ShapeOr and there is some shape expression se2 in shapeExprs such that satisfies(n, se2, G, m). \"\"\"\n    return any(satisfies(cntxt, n, se2) for se2 in se.shapeExprs)"}
{"code":"def mime_type(self, type_: Optional[MimeType] = None) -> str:\n        \"\"\"Get a random mime type from list.\n\n        :param type_: Enum object MimeType.\n        :return: Mime type.\n        \"\"\"\n        key = self._validate_enum(item=type_, enum=MimeType)\n        types = MIME_TYPES[key]\n        return self.random.choice(types)","return_type":"str","function_name":"File.mime_type","stripped_code":"def mime_type(self, type_: Optional[MimeType] = None):\n        \"\"\"Get a random mime type from list.\n\n        :param type_: Enum object MimeType.\n        :return: Mime type.\n        \"\"\"\n        key = self._validate_enum(item=type_, enum=MimeType)\n        types = MIME_TYPES[key]\n        return self.random.choice(types)"}
{"code":"def event_first_of(*events: _AbstractLinkable) -> Event:\n    \"\"\" Waits until one of `events` is set.\n\n    The event returned is /not/ cleared with any of the `events`, this value\n    must not be reused if the clearing behavior is used.\n    \"\"\"\n    first_finished = Event()\n\n    if not all(isinstance(e, _AbstractLinkable) for e in events):\n        raise ValueError('all events must be linkable')\n\n    for event in events:\n        event.rawlink(lambda _: first_finished.set())\n\n    return first_finished","return_type":"Event","function_name":"event_first_of","stripped_code":"def event_first_of(*events: _AbstractLinkable):\n    \"\"\" Waits until one of `events` is set.\n\n    The event returned is /not/ cleared with any of the `events`, this value\n    must not be reused if the clearing behavior is used.\n    \"\"\"\n    first_finished = Event()\n\n    if not all(isinstance(e, _AbstractLinkable) for e in events):\n        raise ValueError('all events must be linkable')\n\n    for event in events:\n        event.rawlink(lambda _: first_finished.set())\n\n    return first_finished"}
{"code":"def semActsSatisfied(acts: Optional[List[ShExJ.SemAct]], cntxt: Context) -> bool:\n    \"\"\" `5.7.1 Semantic Actions Semantics <http://shex.io/shex-semantics/#semantic-actions-semantics>`_\n\n    The evaluation semActsSatisfied on a list of SemActs returns success or failure. The evaluation of an individual\n    SemAct is implementation-dependent.\n    \"\"\"\n    return True","return_type":"bool","function_name":"semActsSatisfied","stripped_code":"def semActsSatisfied(acts: Optional[List[ShExJ.SemAct]], cntxt: Context):\n    \"\"\" `5.7.1 Semantic Actions Semantics <http://shex.io/shex-semantics/#semantic-actions-semantics>`_\n\n    The evaluation semActsSatisfied on a list of SemActs returns success or failure. The evaluation of an individual\n    SemAct is implementation-dependent.\n    \"\"\"\n    return True"}
{"code":"def build_pmid_inclusion_filter(pmids: Strings) -> EdgePredicate:\n    \"\"\"Build an edge predicate that passes for edges with citations from the given PubMed identifier(s).\n\n    :param pmids: A PubMed identifier or list of PubMed identifiers to filter for\n    \"\"\"\n    if isinstance(pmids, str):\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching the contained PubMed identifier.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] == pmids\n\n    elif isinstance(pmids, Iterable):\n        pmids = set(pmids)\n\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching one of the contained PubMed identifiers.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] in pmids\n\n    else:\n        raise TypeError\n\n    return pmid_inclusion_filter","return_type":"EdgePredicate","function_name":"build_pmid_inclusion_filter","stripped_code":"def build_pmid_inclusion_filter(pmids: Strings):\n    \"\"\"Build an edge predicate that passes for edges with citations from the given PubMed identifier(s).\n\n    :param pmids: A PubMed identifier or list of PubMed identifiers to filter for\n    \"\"\"\n    if isinstance(pmids, str):\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching the contained PubMed identifier.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] == pmids\n\n    elif isinstance(pmids, Iterable):\n        pmids = set(pmids)\n\n        @edge_predicate\n        def pmid_inclusion_filter(edge_data: EdgeData) -> bool:\n            \"\"\"Pass for edges with PubMed citations matching one of the contained PubMed identifiers.\"\"\"\n            return has_pubmed(edge_data) and edge_data[CITATION][CITATION_REFERENCE] in pmids\n\n    else:\n        raise TypeError\n\n    return pmid_inclusion_filter"}
{"code":"def is_capable_terminal(file=sys.stderr) -> bool:\n    \"\"\"\n    Determine whether we are connected to a capable terminal.\n    \"\"\"\n    if not os.isatty(file.fileno()):\n        return False\n    terminal = os.getenv(\"TERM\", \"dumb\")\n    # Hardcoded list of non-capable terminals.\n    return terminal not in [\"dumb\", \"emacs\"]","return_type":"bool","function_name":"is_capable_terminal","stripped_code":"def is_capable_terminal(file=sys.stderr):\n    \"\"\"\n    Determine whether we are connected to a capable terminal.\n    \"\"\"\n    if not os.isatty(file.fileno()):\n        return False\n    terminal = os.getenv(\"TERM\", \"dumb\")\n    # Hardcoded list of non-capable terminals.\n    return terminal not in [\"dumb\", \"emacs\"]"}
{"code":"def parse_tibiadata_datetime(date_dict) -> Optional[datetime.datetime]:\n    \"\"\"Parses time objects from the TibiaData API.\n\n    Time objects are made of a dictionary with three keys:\n        date: contains a string representation of the time\n        timezone: a string representation of the timezone the date time is based on\n        timezone_type: the type of representation used in the timezone key\n\n\n    Parameters\n    ----------\n    date_dict: :class:`dict`\n        Dictionary representing the time object.\n\n    Returns\n    -------\n    :class:`datetime.date`, optional\n        The represented datetime, in UTC.\n    \"\"\"\n    try:\n        t = datetime.datetime.strptime(date_dict[\"date\"], \"%Y-%m-%d %H:%M:%S.%f\")\n    except (KeyError, ValueError, TypeError):\n        return None\n\n    if date_dict[\"timezone\"] == \"CET\":\n        timezone_offset = 1\n    elif date_dict[\"timezone\"] == \"CEST\":\n        timezone_offset = 2\n    else:\n        return None\n    # We subtract the offset to convert the time to UTC\n    t = t - datetime.timedelta(hours=timezone_offset)\n    return t.replace(tzinfo=datetime.timezone.utc)","return_type":"Optional[datetime.datetime]","function_name":"parse_tibiadata_datetime","stripped_code":"def parse_tibiadata_datetime(date_dict):\n    \"\"\"Parses time objects from the TibiaData API.\n\n    Time objects are made of a dictionary with three keys:\n        date: contains a string representation of the time\n        timezone: a string representation of the timezone the date time is based on\n        timezone_type: the type of representation used in the timezone key\n\n\n    Parameters\n    ----------\n    date_dict: :class:`dict`\n        Dictionary representing the time object.\n\n    Returns\n    -------\n    :class:`datetime.date`, optional\n        The represented datetime, in UTC.\n    \"\"\"\n    try:\n        t = datetime.datetime.strptime(date_dict[\"date\"], \"%Y-%m-%d %H:%M:%S.%f\")\n    except (KeyError, ValueError, TypeError):\n        return None\n\n    if date_dict[\"timezone\"] == \"CET\":\n        timezone_offset = 1\n    elif date_dict[\"timezone\"] == \"CEST\":\n        timezone_offset = 2\n    else:\n        return None\n    # We subtract the offset to convert the time to UTC\n    t = t - datetime.timedelta(hours=timezone_offset)\n    return t.replace(tzinfo=datetime.timezone.utc)"}
{"code":"def main(args: Sequence[str] = None) -> int:\n    \"\"\"\n    Main line for script: check arguments and dispatch operation to set nym.\n\n    :param args: command-line arguments\n    :return: 0 for OK, 1 for failure\n    \"\"\"\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)-15s | %(levelname)-8s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S')\n    logging.getLogger('von_anchor').setLevel(logging.WARNING)\n    logging.getLogger('indy').setLevel(logging.ERROR)\n\n    if args is None:\n        args = sys.argv[1:]\n\n    if len(sys.argv) == 2:\n        try:\n            return do_wait(setnym(sys.argv[1]))\n        except VonAnchorError as vax:\n            print(str(vax))\n            return 1\n    else:\n        usage()\n        return 1","return_type":"int","function_name":"main","stripped_code":"def main(args: Sequence[str] = None):\n    \"\"\"\n    Main line for script: check arguments and dispatch operation to set nym.\n\n    :param args: command-line arguments\n    :return: 0 for OK, 1 for failure\n    \"\"\"\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)-15s | %(levelname)-8s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S')\n    logging.getLogger('von_anchor').setLevel(logging.WARNING)\n    logging.getLogger('indy').setLevel(logging.ERROR)\n\n    if args is None:\n        args = sys.argv[1:]\n\n    if len(sys.argv) == 2:\n        try:\n            return do_wait(setnym(sys.argv[1]))\n        except VonAnchorError as vax:\n            print(str(vax))\n            return 1\n    else:\n        usage()\n        return 1"}
{"code":"def generator_chain(initial_data: T, *factories: Callable[[T], Iterator[T]]) -> Iterator[T]:\n    \"\"\"Chain multiple generators together by passing results from one to the next.\n\n    This helper function allows to create a chain of generator where each generator is constructed by a factory that\n    gets the data yielded by the previous generator. So each generator can generate new data dependant on the data\n    yielded by the previous one. For each data item yielded by a generator, a new generator is constructed by the\n    next factory.\n\n    Example:\n\n        Lets say for every number from 0 to 4, we want to count up to that number. Then we can do\n        something like this using list comprehensions:\n\n        >>> [i for n in range(1, 5) for i in range(1, n + 1)]\n        [1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n        You can use this function to achieve the same thing:\n\n        >>> list(generator_chain(5, lambda n: iter(range(1, n)), lambda i: iter(range(1, i + 1))))\n        [1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n        The advantage is, that this is independent of the number of dependant generators you have.\n        Also, this function does not use recursion so it is safe to use even with large generator counts.\n\n    Args:\n        initial_data:\n            The initial data that is passed to the first generator factory.\n        *factories:\n            The generator factories. Each of them gets passed its predecessors data and has to return an iterable.\n            The data from this iterable is passed to the next factory.\n\n    Yields:\n        Every data item yielded by the generators of the final factory.\n\n    \"\"\"\n    generator_count = len(factories)\n    if generator_count == 0:\n        yield initial_data\n        return\n    generators = [None] * generator_count  # type: List[Optional[Iterator[T]]]\n    next_data = initial_data\n    generator_index = 0\n    while True:\n        try:\n            while generator_index < generator_count:\n                if generators[generator_index] is None:\n                    generators[generator_index] = factories[generator_index](next_data)\n                next_data = next(generators[generator_index])\n                generator_index += 1\n            yield next_data\n            generator_index -= 1\n        except StopIteration:\n            generators[generator_index] = None\n            generator_index -= 1\n            if generator_index < 0:\n                break","return_type":"Iterator[T]","function_name":"generator_chain","stripped_code":"def generator_chain(initial_data: T, *factories: Callable[[T], Iterator[T]]):\n    \"\"\"Chain multiple generators together by passing results from one to the next.\n\n    This helper function allows to create a chain of generator where each generator is constructed by a factory that\n    gets the data yielded by the previous generator. So each generator can generate new data dependant on the data\n    yielded by the previous one. For each data item yielded by a generator, a new generator is constructed by the\n    next factory.\n\n    Example:\n\n        Lets say for every number from 0 to 4, we want to count up to that number. Then we can do\n        something like this using list comprehensions:\n\n        >>> [i for n in range(1, 5) for i in range(1, n + 1)]\n        [1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n        You can use this function to achieve the same thing:\n\n        >>> list(generator_chain(5, lambda n: iter(range(1, n)), lambda i: iter(range(1, i + 1))))\n        [1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n        The advantage is, that this is independent of the number of dependant generators you have.\n        Also, this function does not use recursion so it is safe to use even with large generator counts.\n\n    Args:\n        initial_data:\n            The initial data that is passed to the first generator factory.\n        *factories:\n            The generator factories. Each of them gets passed its predecessors data and has to return an iterable.\n            The data from this iterable is passed to the next factory.\n\n    Yields:\n        Every data item yielded by the generators of the final factory.\n\n    \"\"\"\n    generator_count = len(factories)\n    if generator_count == 0:\n        yield initial_data\n        return\n    generators = [None] * generator_count  # type: List[Optional[Iterator[T]]]\n    next_data = initial_data\n    generator_index = 0\n    while True:\n        try:\n            while generator_index < generator_count:\n                if generators[generator_index] is None:\n                    generators[generator_index] = factories[generator_index](next_data)\n                next_data = next(generators[generator_index])\n                generator_index += 1\n            yield next_data\n            generator_index -= 1\n        except StopIteration:\n            generators[generator_index] = None\n            generator_index -= 1\n            if generator_index < 0:\n                break"}
{"code":"def _html_to_img_tuples(html:str, format:str='jpg', n_images:int=10) -> list:    \n    \"Parse the google images html to img tuples containining `(fname, url)`\"\n    bs = BeautifulSoup(html, 'html.parser')\n    img_tags = bs.find_all('div', {'class': 'rg_meta'})\n    metadata_dicts = (json.loads(e.text) for e in img_tags)\n    img_tuples = ((_img_fname(d['ou']), d['ou']) for d in metadata_dicts if d['ity'] == format)\n    return list(itertools.islice(img_tuples, n_images))","return_type":"list","function_name":"_html_to_img_tuples","stripped_code":"def _html_to_img_tuples(html:str, format:str='jpg', n_images:int=10):    \n    \"Parse the google images html to img tuples containining `(fname, url)`\"\n    bs = BeautifulSoup(html, 'html.parser')\n    img_tags = bs.find_all('div', {'class': 'rg_meta'})\n    metadata_dicts = (json.loads(e.text) for e in img_tags)\n    img_tuples = ((_img_fname(d['ou']), d['ou']) for d in metadata_dicts if d['ity'] == format)\n    return list(itertools.islice(img_tuples, n_images))"}
{"code":"def parse_querystring(self, req: Request, name: str, field: Field) -> typing.Any:\n        \"\"\"Pull a querystring value from the request.\"\"\"\n        return core.get_value(req.query, name, field)","return_type":"typing.Any","function_name":"AIOHTTPParser.parse_querystring","stripped_code":"def parse_querystring(self, req: Request, name: str, field: Field):\n        \"\"\"Pull a querystring value from the request.\"\"\"\n        return core.get_value(req.query, name, field)"}
{"code":"def left_outer_join(g, h) -> None:\n    \"\"\"Only add components from the ``h`` that are touching ``g``.\n\n    Algorithm:\n\n    1. Identify all weakly connected components in ``h``\n    2. Add those that have an intersection with the ``g``\n\n    :param BELGraph g: A BEL graph\n    :param BELGraph h: A BEL graph\n\n    Example usage:\n\n    >>> import pybel\n    >>> g = pybel.from_path('...')\n    >>> h = pybel.from_path('...')\n    >>> left_outer_join(g, h)\n    \"\"\"\n    g_nodes = set(g)\n    for comp in nx.weakly_connected_components(h):\n        if g_nodes.intersection(comp):\n            h_subgraph = subgraph(h, comp)\n            left_full_join(g, h_subgraph)","return_type":"None","function_name":"left_outer_join","stripped_code":"def left_outer_join(g, h):\n    \"\"\"Only add components from the ``h`` that are touching ``g``.\n\n    Algorithm:\n\n    1. Identify all weakly connected components in ``h``\n    2. Add those that have an intersection with the ``g``\n\n    :param BELGraph g: A BEL graph\n    :param BELGraph h: A BEL graph\n\n    Example usage:\n\n    >>> import pybel\n    >>> g = pybel.from_path('...')\n    >>> h = pybel.from_path('...')\n    >>> left_outer_join(g, h)\n    \"\"\"\n    g_nodes = set(g)\n    for comp in nx.weakly_connected_components(h):\n        if g_nodes.intersection(comp):\n            h_subgraph = subgraph(h, comp)\n            left_full_join(g, h_subgraph)"}
{"code":"def sin(cls, x: 'TensorFluent') -> 'TensorFluent':\n        '''Returns a TensorFluent for the sin function.\n\n        Args:\n            x: The input fluent.\n\n        Returns:\n            A TensorFluent wrapping the sin function.\n        '''\n        return cls._unary_op(x, tf.sin, tf.float32)","return_type":"'TensorFluent'","function_name":"TensorFluent.sin","stripped_code":"def sin(cls, x: 'TensorFluent'):\n        '''Returns a TensorFluent for the sin function.\n\n        Args:\n            x: The input fluent.\n\n        Returns:\n            A TensorFluent wrapping the sin function.\n        '''\n        return cls._unary_op(x, tf.sin, tf.float32)"}
{"code":"def text_width(self, text: str) -> float:\n        \"\"\"Returns the width, in pixels, of a string in DejaVu Sans 110pt.\"\"\"\n        width = 0\n        for index, c in enumerate(text):\n            width += self._char_to_width.get(c, self._default_character_width)\n            width -= self._pair_to_kern.get(text[index:index + 2], 0)\n\n        return width","return_type":"float","function_name":"PrecalculatedTextMeasurer.text_width","stripped_code":"def text_width(self, text: str):\n        \"\"\"Returns the width, in pixels, of a string in DejaVu Sans 110pt.\"\"\"\n        width = 0\n        for index, c in enumerate(text):\n            width += self._char_to_width.get(c, self._default_character_width)\n            width -= self._pair_to_kern.get(text[index:index + 2], 0)\n\n        return width"}
{"code":"def satisfiesShapeNot(cntxt: Context, n: Node, se: ShExJ.ShapeNot, _: DebugContext) -> bool:\n    \"\"\" Se is a ShapeNot and for the shape expression se2 at shapeExpr, notSatisfies(n, se2, G, m) \"\"\"\n    return not satisfies(cntxt, n, se.shapeExpr)","return_type":"bool","function_name":"satisfiesShapeNot","stripped_code":"def satisfiesShapeNot(cntxt: Context, n: Node, se: ShExJ.ShapeNot, _: DebugContext):\n    \"\"\" Se is a ShapeNot and for the shape expression se2 at shapeExpr, notSatisfies(n, se2, G, m) \"\"\"\n    return not satisfies(cntxt, n, se.shapeExpr)"}
{"code":"def assert_or_raise(stmt: bool, exception: Exception,\n                    *exception_args, **exception_kwargs) -> None:\n  \"\"\"\n  If the statement is false, raise the given exception.\n  \"\"\"\n  if not stmt:\n    raise exception(*exception_args, **exception_kwargs)","return_type":"None","function_name":"assert_or_raise","stripped_code":"def assert_or_raise(stmt: bool, exception: Exception,\n                    *exception_args, **exception_kwargs):\n  \"\"\"\n  If the statement is false, raise the given exception.\n  \"\"\"\n  if not stmt:\n    raise exception(*exception_args, **exception_kwargs)"}
{"code":"def _set_rpc(self, rpc_type: str) -> None:\n        \"\"\"\n        Sets rpc based on the type\n        :param rpc_type: The type of connection: like infura, ganache, localhost\n        :return:\n        \"\"\"\n        if rpc_type == \"infura\":\n            self.set_api_rpc_infura()\n        elif rpc_type == \"localhost\":\n            self.set_api_rpc_localhost()\n        else:\n            self.set_api_rpc(rpc_type)","return_type":"None","function_name":"MythrilConfig._set_rpc","stripped_code":"def _set_rpc(self, rpc_type: str):\n        \"\"\"\n        Sets rpc based on the type\n        :param rpc_type: The type of connection: like infura, ganache, localhost\n        :return:\n        \"\"\"\n        if rpc_type == \"infura\":\n            self.set_api_rpc_infura()\n        elif rpc_type == \"localhost\":\n            self.set_api_rpc_localhost()\n        else:\n            self.set_api_rpc(rpc_type)"}
{"code":"def create_element(tag: str, name: str = None, base: type = None,\n                   attr: dict = None) -> Node:\n    \"\"\"Create element with a tag of ``name``.\n\n    :arg str name: html tag.\n    :arg type base: Base class of the created element\n                       (defatlt: ``WdomElement``)\n    :arg dict attr: Attributes (key-value pairs dict) of the new element.\n    \"\"\"\n    from wdom.web_node import WdomElement\n    from wdom.tag import Tag\n    from wdom.window import customElements\n    if attr is None:\n        attr = {}\n    if name:\n        base_class = customElements.get((name, tag))\n    else:\n        base_class = customElements.get((tag, None))\n    if base_class is None:\n        attr['_registered'] = False\n        base_class = base or WdomElement\n    if issubclass(base_class, Tag):\n        return base_class(**attr)\n    return base_class(tag, **attr)","return_type":"Node","function_name":"create_element","stripped_code":"def create_element(tag: str, name: str = None, base: type = None,\n                   attr: dict = None):\n    \"\"\"Create element with a tag of ``name``.\n\n    :arg str name: html tag.\n    :arg type base: Base class of the created element\n                       (defatlt: ``WdomElement``)\n    :arg dict attr: Attributes (key-value pairs dict) of the new element.\n    \"\"\"\n    from wdom.web_node import WdomElement\n    from wdom.tag import Tag\n    from wdom.window import customElements\n    if attr is None:\n        attr = {}\n    if name:\n        base_class = customElements.get((name, tag))\n    else:\n        base_class = customElements.get((tag, None))\n    if base_class is None:\n        attr['_registered'] = False\n        base_class = base or WdomElement\n    if issubclass(base_class, Tag):\n        return base_class(**attr)\n    return base_class(tag, **attr)"}
{"code":"def console_set_char(\n    con: tcod.console.Console, x: int, y: int, c: Union[int, str]\n) -> None:\n    \"\"\"Change the character at x,y to c, keeping the current colors.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        c (Union[int, AnyStr]): Character to draw, can be an integer or string.\n\n    .. deprecated:: 8.4\n        Array access performs significantly faster than using this function.\n        See :any:`Console.ch`.\n    \"\"\"\n    lib.TCOD_console_set_char(_console(con), x, y, _int(c))","return_type":"None","function_name":"console_set_char","stripped_code":"def console_set_char(\n    con: tcod.console.Console, x: int, y: int, c: Union[int, str]\n):\n    \"\"\"Change the character at x,y to c, keeping the current colors.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        c (Union[int, AnyStr]): Character to draw, can be an integer or string.\n\n    .. deprecated:: 8.4\n        Array access performs significantly faster than using this function.\n        See :any:`Console.ch`.\n    \"\"\"\n    lib.TCOD_console_set_char(_console(con), x, y, _int(c))"}
{"code":"def _repr_pretty_(self, p: Any, cycle: bool) -> None:\n        \"\"\"Print ASCII diagram in Jupyter.\"\"\"\n        if cycle:\n            # There should never be a cycle.  This is just in case.\n            p.text('Circuit(...)')\n        else:\n            p.text(self.to_text_diagram())","return_type":"None","function_name":"Circuit._repr_pretty_","stripped_code":"def _repr_pretty_(self, p: Any, cycle: bool):\n        \"\"\"Print ASCII diagram in Jupyter.\"\"\"\n        if cycle:\n            # There should never be a cycle.  This is just in case.\n            p.text('Circuit(...)')\n        else:\n            p.text(self.to_text_diagram())"}
{"code":"def upgrade_database(\n        alembic_config_filename: str,\n        alembic_base_dir: str = None,\n        starting_revision: str = None,\n        destination_revision: str = \"head\",\n        version_table: str = DEFAULT_ALEMBIC_VERSION_TABLE,\n        as_sql: bool = False) -> None:\n    \"\"\"\n    Use Alembic to upgrade our database.\n\n    See http://alembic.readthedocs.org/en/latest/api/runtime.html\n    but also, in particular, ``site-packages/alembic/command.py``\n\n    Arguments:\n        alembic_config_filename:\n            config filename\n\n        alembic_base_dir:\n            directory to start in, so relative paths in the config file work\n\n        starting_revision:\n            revision to start at (typically ``None`` to ask the database)\n\n        destination_revision:\n            revision to aim for (typically ``\"head\"`` to migrate to the latest\n            structure)\n\n        version_table: table name for Alembic versions\n\n        as_sql:\n            run in \"offline\" mode: print the migration SQL, rather than\n            modifying the database. See\n            http://alembic.zzzcomputing.com/en/latest/offline.html\n\n    \"\"\"\n\n    if alembic_base_dir is None:\n        alembic_base_dir = os.path.dirname(alembic_config_filename)\n    os.chdir(alembic_base_dir)  # so the directory in the config file works\n    config = Config(alembic_config_filename)\n    script = ScriptDirectory.from_config(config)\n\n    # noinspection PyUnusedLocal,PyProtectedMember\n    def upgrade(rev, context):\n        return script._upgrade_revs(destination_revision, rev)\n\n    log.info(\"Upgrading database to revision {!r} using Alembic\",\n             destination_revision)\n\n    with EnvironmentContext(config,\n                            script,\n                            fn=upgrade,\n                            as_sql=as_sql,\n                            starting_rev=starting_revision,\n                            destination_rev=destination_revision,\n                            tag=None,\n                            version_table=version_table):\n        script.run_env()\n\n    log.info(\"Database upgrade completed\")","return_type":"None","function_name":"upgrade_database","stripped_code":"def upgrade_database(\n        alembic_config_filename: str,\n        alembic_base_dir: str = None,\n        starting_revision: str = None,\n        destination_revision: str = \"head\",\n        version_table: str = DEFAULT_ALEMBIC_VERSION_TABLE,\n        as_sql: bool = False):\n    \"\"\"\n    Use Alembic to upgrade our database.\n\n    See http://alembic.readthedocs.org/en/latest/api/runtime.html\n    but also, in particular, ``site-packages/alembic/command.py``\n\n    Arguments:\n        alembic_config_filename:\n            config filename\n\n        alembic_base_dir:\n            directory to start in, so relative paths in the config file work\n\n        starting_revision:\n            revision to start at (typically ``None`` to ask the database)\n\n        destination_revision:\n            revision to aim for (typically ``\"head\"`` to migrate to the latest\n            structure)\n\n        version_table: table name for Alembic versions\n\n        as_sql:\n            run in \"offline\" mode: print the migration SQL, rather than\n            modifying the database. See\n            http://alembic.zzzcomputing.com/en/latest/offline.html\n\n    \"\"\"\n\n    if alembic_base_dir is None:\n        alembic_base_dir = os.path.dirname(alembic_config_filename)\n    os.chdir(alembic_base_dir)  # so the directory in the config file works\n    config = Config(alembic_config_filename)\n    script = ScriptDirectory.from_config(config)\n\n    # noinspection PyUnusedLocal,PyProtectedMember\n    def upgrade(rev, context):\n        return script._upgrade_revs(destination_revision, rev)\n\n    log.info(\"Upgrading database to revision {!r} using Alembic\",\n             destination_revision)\n\n    with EnvironmentContext(config,\n                            script,\n                            fn=upgrade,\n                            as_sql=as_sql,\n                            starting_rev=starting_revision,\n                            destination_rev=destination_revision,\n                            tag=None,\n                            version_table=version_table):\n        script.run_env()\n\n    log.info(\"Database upgrade completed\")"}
{"code":"def romanize(text: str, engine: str = \"royin\") -> str:\n    \"\"\"\n    Rendering Thai words in the Latin alphabet or \"romanization\",\n    using the Royal Thai General System of Transcription (RTGS),\n    which is the official system published by the Royal Institute of Thailand.\n    \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19\n    :param str text: Thai text to be romanized\n    :param str engine: 'royin' (default) or 'thai2rom'. 'royin' uses the Royal Thai General System of Transcription issued by Royal Institute of Thailand. 'thai2rom' is deep learning Thai romanization (require keras).\n    :return: A string of Thai words rendered in the Latin alphabet.\n    \"\"\"\n\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    if engine == \"thai2rom\":\n        from .thai2rom import romanize\n    else:  # use default engine \"royin\"\n        from .royin import romanize\n\n    return romanize(text)","return_type":"str","function_name":"romanize","stripped_code":"def romanize(text: str, engine: str = \"royin\"):\n    \"\"\"\n    Rendering Thai words in the Latin alphabet or \"romanization\",\n    using the Royal Thai General System of Transcription (RTGS),\n    which is the official system published by the Royal Institute of Thailand.\n    \u0e16\u0e2d\u0e14\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e25\u0e30\u0e15\u0e34\u0e19\n    :param str text: Thai text to be romanized\n    :param str engine: 'royin' (default) or 'thai2rom'. 'royin' uses the Royal Thai General System of Transcription issued by Royal Institute of Thailand. 'thai2rom' is deep learning Thai romanization (require keras).\n    :return: A string of Thai words rendered in the Latin alphabet.\n    \"\"\"\n\n    if not text or not isinstance(text, str):\n        return \"\"\n\n    if engine == \"thai2rom\":\n        from .thai2rom import romanize\n    else:  # use default engine \"royin\"\n        from .royin import romanize\n\n    return romanize(text)"}
{"code":"def read_all(self) -> str:\n        \"\"\"\n        Reads the current state of the buffer and returns a string those\n        contents\n\n        :return:\n            A string for the current state of the print buffer contents\n        \"\"\"\n        try:\n            buffered_bytes = self.bytes_buffer.getvalue()\n            if buffered_bytes is None:\n                return ''\n\n            return buffered_bytes.decode(self.source_encoding)\n        except Exception as err:\n            return 'Redirect Buffer Error: {}'.format(err)","return_type":"str","function_name":"RedirectBuffer.read_all","stripped_code":"def read_all(self):\n        \"\"\"\n        Reads the current state of the buffer and returns a string those\n        contents\n\n        :return:\n            A string for the current state of the print buffer contents\n        \"\"\"\n        try:\n            buffered_bytes = self.bytes_buffer.getvalue()\n            if buffered_bytes is None:\n                return ''\n\n            return buffered_bytes.decode(self.source_encoding)\n        except Exception as err:\n            return 'Redirect Buffer Error: {}'.format(err)"}
{"code":"def add_sidebar(self, component: Component) -> None:\n        \"\"\"Add a widget to the sidebar.\n\n        Parameters\n        ----------\n        component : bowtie._Component\n            Add this component to the sidebar, it will be appended to the end.\n\n        \"\"\"\n        if not self.sidebar:\n            raise NoSidebarError('Set `sidebar=True` if you want to use the sidebar.')\n\n        if not isinstance(component, Component):\n            raise ValueError('component must be Component type, found {}'.format(component))\n        # self._track_widget(widget)\n        self._controllers.append(component)","return_type":"None","function_name":"View.add_sidebar","stripped_code":"def add_sidebar(self, component: Component):\n        \"\"\"Add a widget to the sidebar.\n\n        Parameters\n        ----------\n        component : bowtie._Component\n            Add this component to the sidebar, it will be appended to the end.\n\n        \"\"\"\n        if not self.sidebar:\n            raise NoSidebarError('Set `sidebar=True` if you want to use the sidebar.')\n\n        if not isinstance(component, Component):\n            raise ValueError('component must be Component type, found {}'.format(component))\n        # self._track_widget(widget)\n        self._controllers.append(component)"}
{"code":"def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState) -> None:\n        \"\"\"Handles restoring state after output redirection as well as\n        the actual pipe operation if present.\n\n        :param statement: Statement object which contains the parsed input from the user\n        :param saved_state: contains information needed to restore state data\n        \"\"\"\n        if saved_state.redirecting:\n            # If we redirected output to the clipboard\n            if statement.output and not statement.output_to:\n                self.stdout.seek(0)\n                write_to_paste_buffer(self.stdout.read())\n\n            try:\n                # Close the file or pipe that stdout was redirected to\n                self.stdout.close()\n            except BrokenPipeError:\n                pass\n\n            # Restore the stdout values\n            self.stdout = saved_state.saved_self_stdout\n            sys.stdout = saved_state.saved_sys_stdout\n\n            # Check if we need to wait for the process being piped to\n            if self.cur_pipe_proc_reader is not None:\n                self.cur_pipe_proc_reader.wait()\n\n        # Restore cur_pipe_proc_reader. This always is done, regardless of whether this command redirected.\n        self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader","return_type":"None","function_name":"Cmd._restore_output","stripped_code":"def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState):\n        \"\"\"Handles restoring state after output redirection as well as\n        the actual pipe operation if present.\n\n        :param statement: Statement object which contains the parsed input from the user\n        :param saved_state: contains information needed to restore state data\n        \"\"\"\n        if saved_state.redirecting:\n            # If we redirected output to the clipboard\n            if statement.output and not statement.output_to:\n                self.stdout.seek(0)\n                write_to_paste_buffer(self.stdout.read())\n\n            try:\n                # Close the file or pipe that stdout was redirected to\n                self.stdout.close()\n            except BrokenPipeError:\n                pass\n\n            # Restore the stdout values\n            self.stdout = saved_state.saved_self_stdout\n            sys.stdout = saved_state.saved_sys_stdout\n\n            # Check if we need to wait for the process being piped to\n            if self.cur_pipe_proc_reader is not None:\n                self.cur_pipe_proc_reader.wait()\n\n        # Restore cur_pipe_proc_reader. This always is done, regardless of whether this command redirected.\n        self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader"}
{"code":"def valid_date(x: str) -> bool:\n    \"\"\"\n    Retrun ``True`` if ``x`` is a valid YYYYMMDD date;\n    otherwise return ``False``.\n    \"\"\"\n    try:\n        if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT):\n            raise ValueError\n        return True\n    except ValueError:\n        return False","return_type":"bool","function_name":"valid_date","stripped_code":"def valid_date(x: str):\n    \"\"\"\n    Retrun ``True`` if ``x`` is a valid YYYYMMDD date;\n    otherwise return ``False``.\n    \"\"\"\n    try:\n        if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT):\n            raise ValueError\n        return True\n    except ValueError:\n        return False"}
{"code":"def _set_bang_to_py_ast(ctx: GeneratorContext, node: SetBang) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `set!` expression.\"\"\"\n    assert node.op == NodeOp.SET_BANG\n\n    val_temp_name = genname(\"set_bang_val\")\n    val_ast = gen_py_ast(ctx, node.val)\n\n    target = node.target\n    assert isinstance(\n        target, (HostField, Local, VarRef)\n    ), f\"invalid set! target type {type(target)}\"\n\n    if isinstance(target, HostField):\n        target_ast = _interop_prop_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, VarRef):\n        target_ast = _var_sym_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, Local):\n        target_ast = _local_sym_to_py_ast(ctx, target, is_assigning=True)\n    else:  # pragma: no cover\n        raise GeneratorException(\n            f\"invalid set! target type {type(target)}\", lisp_ast=target\n        )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=val_temp_name, ctx=ast.Load()),\n        dependencies=list(\n            chain(\n                val_ast.dependencies,\n                [\n                    ast.Assign(\n                        targets=[ast.Name(id=val_temp_name, ctx=ast.Store())],\n                        value=val_ast.node,\n                    )\n                ],\n                target_ast.dependencies,\n                [ast.Assign(targets=[target_ast.node], value=val_ast.node)],\n            )\n        ),\n    )","return_type":"GeneratedPyAST","function_name":"_set_bang_to_py_ast","stripped_code":"def _set_bang_to_py_ast(ctx: GeneratorContext, node: SetBang):\n    \"\"\"Return a Python AST Node for a `set!` expression.\"\"\"\n    assert node.op == NodeOp.SET_BANG\n\n    val_temp_name = genname(\"set_bang_val\")\n    val_ast = gen_py_ast(ctx, node.val)\n\n    target = node.target\n    assert isinstance(\n        target, (HostField, Local, VarRef)\n    ), f\"invalid set! target type {type(target)}\"\n\n    if isinstance(target, HostField):\n        target_ast = _interop_prop_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, VarRef):\n        target_ast = _var_sym_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, Local):\n        target_ast = _local_sym_to_py_ast(ctx, target, is_assigning=True)\n    else:  # pragma: no cover\n        raise GeneratorException(\n            f\"invalid set! target type {type(target)}\", lisp_ast=target\n        )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=val_temp_name, ctx=ast.Load()),\n        dependencies=list(\n            chain(\n                val_ast.dependencies,\n                [\n                    ast.Assign(\n                        targets=[ast.Name(id=val_temp_name, ctx=ast.Store())],\n                        value=val_ast.node,\n                    )\n                ],\n                target_ast.dependencies,\n                [ast.Assign(targets=[target_ast.node], value=val_ast.node)],\n            )\n        ),\n    )"}
{"code":"def mine_block(chain: MiningChain, **kwargs: Any) -> MiningChain:\n    \"\"\"\n    Mine a new block on the chain.  Header parameters for the new block can be\n    overridden using keyword arguments.\n\n    \"\"\"\n    if not isinstance(chain, MiningChain):\n        raise ValidationError('`mine_block` may only be used on MiningChain instances')\n    chain.mine_block(**kwargs)\n    return chain","return_type":"MiningChain","function_name":"mine_block","stripped_code":"def mine_block(chain: MiningChain, **kwargs: Any):\n    \"\"\"\n    Mine a new block on the chain.  Header parameters for the new block can be\n    overridden using keyword arguments.\n\n    \"\"\"\n    if not isinstance(chain, MiningChain):\n        raise ValidationError('`mine_block` may only be used on MiningChain instances')\n    chain.mine_block(**kwargs)\n    return chain"}
{"code":"def block_orthogonal(tensor: torch.Tensor,\n                     split_sizes: List[int],\n                     gain: float = 1.0) -> None:\n    \"\"\"\n    An initializer which allows initializing model parameters in \"blocks\". This is helpful\n    in the case of recurrent models which use multiple gates applied to linear projections,\n    which can be computed efficiently if they are concatenated together. However, they are\n    separate parameters which should be initialized independently.\n\n    Parameters\n    ----------\n    tensor : ``torch.Tensor``, required.\n        A tensor to initialize.\n    split_sizes : List[int], required.\n        A list of length ``tensor.ndim()`` specifying the size of the\n        blocks along that particular dimension. E.g. ``[10, 20]`` would\n        result in the tensor being split into chunks of size 10 along the\n        first dimension and 20 along the second.\n    gain : float, optional (default = 1.0)\n        The gain (scaling) applied to the orthogonal initialization.\n    \"\"\"\n    data = tensor.data\n    sizes = list(tensor.size())\n    if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n        raise ConfigurationError(\"tensor dimensions must be divisible by their respective \"\n                                 \"split_sizes. Found size: {} and split_sizes: {}\".format(sizes, split_sizes))\n    indexes = [list(range(0, max_size, split))\n               for max_size, split in zip(sizes, split_sizes)]\n    # Iterate over all possible blocks within the tensor.\n    for block_start_indices in itertools.product(*indexes):\n        # A list of tuples containing the index to start at for this block\n        # and the appropriate step size (i.e split_size[i] for dimension i).\n        index_and_step_tuples = zip(block_start_indices, split_sizes)\n        # This is a tuple of slices corresponding to:\n        # tensor[index: index + step_size, ...]. This is\n        # required because we could have an arbitrary number\n        # of dimensions. The actual slices we need are the\n        # start_index: start_index + step for each dimension in the tensor.\n        block_slice = tuple([slice(start_index, start_index + step)\n                             for start_index, step in index_and_step_tuples])\n        data[block_slice] = torch.nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)","return_type":"None","function_name":"block_orthogonal","stripped_code":"def block_orthogonal(tensor: torch.Tensor,\n                     split_sizes: List[int],\n                     gain: float = 1.0):\n    \"\"\"\n    An initializer which allows initializing model parameters in \"blocks\". This is helpful\n    in the case of recurrent models which use multiple gates applied to linear projections,\n    which can be computed efficiently if they are concatenated together. However, they are\n    separate parameters which should be initialized independently.\n\n    Parameters\n    ----------\n    tensor : ``torch.Tensor``, required.\n        A tensor to initialize.\n    split_sizes : List[int], required.\n        A list of length ``tensor.ndim()`` specifying the size of the\n        blocks along that particular dimension. E.g. ``[10, 20]`` would\n        result in the tensor being split into chunks of size 10 along the\n        first dimension and 20 along the second.\n    gain : float, optional (default = 1.0)\n        The gain (scaling) applied to the orthogonal initialization.\n    \"\"\"\n    data = tensor.data\n    sizes = list(tensor.size())\n    if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n        raise ConfigurationError(\"tensor dimensions must be divisible by their respective \"\n                                 \"split_sizes. Found size: {} and split_sizes: {}\".format(sizes, split_sizes))\n    indexes = [list(range(0, max_size, split))\n               for max_size, split in zip(sizes, split_sizes)]\n    # Iterate over all possible blocks within the tensor.\n    for block_start_indices in itertools.product(*indexes):\n        # A list of tuples containing the index to start at for this block\n        # and the appropriate step size (i.e split_size[i] for dimension i).\n        index_and_step_tuples = zip(block_start_indices, split_sizes)\n        # This is a tuple of slices corresponding to:\n        # tensor[index: index + step_size, ...]. This is\n        # required because we could have an arbitrary number\n        # of dimensions. The actual slices we need are the\n        # start_index: start_index + step for each dimension in the tensor.\n        block_slice = tuple([slice(start_index, start_index + step)\n                             for start_index, step in index_and_step_tuples])\n        data[block_slice] = torch.nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)"}
{"code":"def get_error_codes(cls) -> Iterable[str]:\n        \"\"\"Yield all registered codes.\"\"\"\n        for group in cls.groups:\n            for error in group.errors:\n                yield error.code","return_type":"Iterable[str]","function_name":"ErrorRegistry.get_error_codes","stripped_code":"def get_error_codes(cls):\n        \"\"\"Yield all registered codes.\"\"\"\n        for group in cls.groups:\n            for error in group.errors:\n                yield error.code"}
{"code":"def get_devices(device_type: DeviceType) -> Iterator[str]:\n    \"\"\"Gets names of power devices of the specified type.\n\n    :param str device_type: the type of the devices to retrieve\n    :return: the device names\n    :rtype: Iterator[str]\n    \"\"\"\n    for device in BASE_PATH.iterdir():\n        with open(str(Path(device, 'type'))) as type_file:\n            if type_file.readline().strip() == device_type.value:\n                yield device.name","return_type":"Iterator[str]","function_name":"get_devices","stripped_code":"def get_devices(device_type: DeviceType):\n    \"\"\"Gets names of power devices of the specified type.\n\n    :param str device_type: the type of the devices to retrieve\n    :return: the device names\n    :rtype: Iterator[str]\n    \"\"\"\n    for device in BASE_PATH.iterdir():\n        with open(str(Path(device, 'type'))) as type_file:\n            if type_file.readline().strip() == device_type.value:\n                yield device.name"}
{"code":"def dfa_co_reachable(dfa: dict) -> dict:\n    \"\"\" Side effects on input! Removes from the DFA all states that\n    do not reach a final state and returns the pruned DFA.\n\n    It is possible to remove from a DFA A all states that do not\n    reach a final state without altering the language.\n    The co-reachable dfa :math:`A_F` corresponding to A is\n    defined as:\n\n    :math:`A_F = (\u03a3, S_F , s_0 , \u03c1|S_F , F )`\n\n    where\n\n    \u2022 :math:`S_F` is the set of states that reach a final state\n    \u2022 :math:`\u03c1|S_F` is the restriction on :math:`S_F \u00d7 \u03a3` of \u03c1.\n\n    :param dict dfa: input DFA.\n    :return: *(dict)* representing the pruned DFA.\n    \"\"\"\n\n    co_reachable_states = dfa['accepting_states'].copy()\n    boundary = co_reachable_states.copy()\n\n    # inverse transition function\n    inverse_transitions = dict()\n    for key, value in dfa['transitions'].items():\n        inverse_transitions.setdefault(value, set()).add(key)\n\n    while boundary:\n        s = boundary.pop()\n        if s in inverse_transitions:\n            for (state, action) in inverse_transitions[s]:\n                if state not in co_reachable_states:\n                    boundary.add(state)\n                    co_reachable_states.add(state)\n\n    dfa['states'] = co_reachable_states\n\n    # If not s_0 \u2208 S_F the resulting dfa is empty\n    if dfa['initial_state'] not in dfa['states']:\n        dfa = {\n            'alphabet': set(),\n            'states': set(),\n            'initial_state': None,\n            'accepting_states': set(),\n            'transitions': dict()\n        }\n        return dfa\n\n    transitions = dfa['transitions'].copy()\n    for t in transitions:\n        if t[0] not in dfa['states']:\n            dfa['transitions'].pop(t)\n        elif dfa['transitions'][t] not in dfa['states']:\n            dfa['transitions'].pop(t)\n\n    return dfa","return_type":"dict","function_name":"dfa_co_reachable","stripped_code":"def dfa_co_reachable(dfa: dict):\n    \"\"\" Side effects on input! Removes from the DFA all states that\n    do not reach a final state and returns the pruned DFA.\n\n    It is possible to remove from a DFA A all states that do not\n    reach a final state without altering the language.\n    The co-reachable dfa :math:`A_F` corresponding to A is\n    defined as:\n\n    :math:`A_F = (\u03a3, S_F , s_0 , \u03c1|S_F , F )`\n\n    where\n\n    \u2022 :math:`S_F` is the set of states that reach a final state\n    \u2022 :math:`\u03c1|S_F` is the restriction on :math:`S_F \u00d7 \u03a3` of \u03c1.\n\n    :param dict dfa: input DFA.\n    :return: *(dict)* representing the pruned DFA.\n    \"\"\"\n\n    co_reachable_states = dfa['accepting_states'].copy()\n    boundary = co_reachable_states.copy()\n\n    # inverse transition function\n    inverse_transitions = dict()\n    for key, value in dfa['transitions'].items():\n        inverse_transitions.setdefault(value, set()).add(key)\n\n    while boundary:\n        s = boundary.pop()\n        if s in inverse_transitions:\n            for (state, action) in inverse_transitions[s]:\n                if state not in co_reachable_states:\n                    boundary.add(state)\n                    co_reachable_states.add(state)\n\n    dfa['states'] = co_reachable_states\n\n    # If not s_0 \u2208 S_F the resulting dfa is empty\n    if dfa['initial_state'] not in dfa['states']:\n        dfa = {\n            'alphabet': set(),\n            'states': set(),\n            'initial_state': None,\n            'accepting_states': set(),\n            'transitions': dict()\n        }\n        return dfa\n\n    transitions = dfa['transitions'].copy()\n    for t in transitions:\n        if t[0] not in dfa['states']:\n            dfa['transitions'].pop(t)\n        elif dfa['transitions'][t] not in dfa['states']:\n            dfa['transitions'].pop(t)\n\n    return dfa"}
{"code":"def run_sync(self, func: Callable, timeout: float = None) -> Any:\n        \"\"\"Starts the `IOLoop`, runs the given function, and stops the loop.\n\n        The function must return either an awaitable object or\n        ``None``. If the function returns an awaitable object, the\n        `IOLoop` will run until the awaitable is resolved (and\n        `run_sync()` will return the awaitable's result). If it raises\n        an exception, the `IOLoop` will stop and the exception will be\n        re-raised to the caller.\n\n        The keyword-only argument ``timeout`` may be used to set\n        a maximum duration for the function.  If the timeout expires,\n        a `tornado.util.TimeoutError` is raised.\n\n        This method is useful to allow asynchronous calls in a\n        ``main()`` function::\n\n            async def main():\n                # do stuff...\n\n            if __name__ == '__main__':\n                IOLoop.current().run_sync(main)\n\n        .. versionchanged:: 4.3\n           Returning a non-``None``, non-awaitable value is now an error.\n\n        .. versionchanged:: 5.0\n           If a timeout occurs, the ``func`` coroutine will be cancelled.\n\n        \"\"\"\n        future_cell = [None]  # type: List[Optional[Future]]\n\n        def run() -> None:\n            try:\n                result = func()\n                if result is not None:\n                    from tornado.gen import convert_yielded\n\n                    result = convert_yielded(result)\n            except Exception:\n                fut = Future()  # type: Future[Any]\n                future_cell[0] = fut\n                future_set_exc_info(fut, sys.exc_info())\n            else:\n                if is_future(result):\n                    future_cell[0] = result\n                else:\n                    fut = Future()\n                    future_cell[0] = fut\n                    fut.set_result(result)\n            assert future_cell[0] is not None\n            self.add_future(future_cell[0], lambda future: self.stop())\n\n        self.add_callback(run)\n        if timeout is not None:\n\n            def timeout_callback() -> None:\n                # If we can cancel the future, do so and wait on it. If not,\n                # Just stop the loop and return with the task still pending.\n                # (If we neither cancel nor wait for the task, a warning\n                # will be logged).\n                assert future_cell[0] is not None\n                if not future_cell[0].cancel():\n                    self.stop()\n\n            timeout_handle = self.add_timeout(self.time() + timeout, timeout_callback)\n        self.start()\n        if timeout is not None:\n            self.remove_timeout(timeout_handle)\n        assert future_cell[0] is not None\n        if future_cell[0].cancelled() or not future_cell[0].done():\n            raise TimeoutError(\"Operation timed out after %s seconds\" % timeout)\n        return future_cell[0].result()","return_type":"Any","function_name":"IOLoop.run_sync","stripped_code":"def run_sync(self, func: Callable, timeout: float = None):\n        \"\"\"Starts the `IOLoop`, runs the given function, and stops the loop.\n\n        The function must return either an awaitable object or\n        ``None``. If the function returns an awaitable object, the\n        `IOLoop` will run until the awaitable is resolved (and\n        `run_sync()` will return the awaitable's result). If it raises\n        an exception, the `IOLoop` will stop and the exception will be\n        re-raised to the caller.\n\n        The keyword-only argument ``timeout`` may be used to set\n        a maximum duration for the function.  If the timeout expires,\n        a `tornado.util.TimeoutError` is raised.\n\n        This method is useful to allow asynchronous calls in a\n        ``main()`` function::\n\n            async def main():\n                # do stuff...\n\n            if __name__ == '__main__':\n                IOLoop.current().run_sync(main)\n\n        .. versionchanged:: 4.3\n           Returning a non-``None``, non-awaitable value is now an error.\n\n        .. versionchanged:: 5.0\n           If a timeout occurs, the ``func`` coroutine will be cancelled.\n\n        \"\"\"\n        future_cell = [None]  # type: List[Optional[Future]]\n\n        def run() -> None:\n            try:\n                result = func()\n                if result is not None:\n                    from tornado.gen import convert_yielded\n\n                    result = convert_yielded(result)\n            except Exception:\n                fut = Future()  # type: Future[Any]\n                future_cell[0] = fut\n                future_set_exc_info(fut, sys.exc_info())\n            else:\n                if is_future(result):\n                    future_cell[0] = result\n                else:\n                    fut = Future()\n                    future_cell[0] = fut\n                    fut.set_result(result)\n            assert future_cell[0] is not None\n            self.add_future(future_cell[0], lambda future: self.stop())\n\n        self.add_callback(run)\n        if timeout is not None:\n\n            def timeout_callback() -> None:\n                # If we can cancel the future, do so and wait on it. If not,\n                # Just stop the loop and return with the task still pending.\n                # (If we neither cancel nor wait for the task, a warning\n                # will be logged).\n                assert future_cell[0] is not None\n                if not future_cell[0].cancel():\n                    self.stop()\n\n            timeout_handle = self.add_timeout(self.time() + timeout, timeout_callback)\n        self.start()\n        if timeout is not None:\n            self.remove_timeout(timeout_handle)\n        assert future_cell[0] is not None\n        if future_cell[0].cancelled() or not future_cell[0].done():\n            raise TimeoutError(\"Operation timed out after %s seconds\" % timeout)\n        return future_cell[0].result()"}
{"code":"def get(self, bucket: str, key: str) -> bytes:\n        \"\"\"\n        Retrieves the data for a given object in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which metadata is being\n        retrieved.\n        :return: the data\n        \"\"\"\n        try:\n            response = self.s3_client.get_object(\n                Bucket=bucket,\n                Key=key\n            )\n            return response['Body'].read()\n        except botocore.exceptions.ClientError as ex:\n            if ex.response['Error']['Code'] == \"NoSuchKey\":\n                raise BlobNotFoundError(f\"Could not find s3://{bucket}/{key}\") from ex\n            raise BlobStoreUnknownError(ex)","return_type":"bytes","function_name":"S3BlobStore.get","stripped_code":"def get(self, bucket: str, key: str):\n        \"\"\"\n        Retrieves the data for a given object in a given bucket.\n        :param bucket: the bucket the object resides in.\n        :param key: the key of the object for which metadata is being\n        retrieved.\n        :return: the data\n        \"\"\"\n        try:\n            response = self.s3_client.get_object(\n                Bucket=bucket,\n                Key=key\n            )\n            return response['Body'].read()\n        except botocore.exceptions.ClientError as ex:\n            if ex.response['Error']['Code'] == \"NoSuchKey\":\n                raise BlobNotFoundError(f\"Could not find s3://{bucket}/{key}\") from ex\n            raise BlobStoreUnknownError(ex)"}
{"code":"def get_templates(path: Path) -> List[str]:\n    '''List all files in ``templates`` directory, including all subdirectories.\n\n    The resulting list contains UNIX-like relative paths starting with ``templates``.\n    '''\n\n    result = []\n\n    for item in path.glob('**/*'):\n        if item.is_file() and not item.name.startswith('_'):\n            result.append(item.relative_to(path.parent).as_posix())\n\n    return result","return_type":"List[str]","function_name":"get_templates","stripped_code":"def get_templates(path: Path):\n    '''List all files in ``templates`` directory, including all subdirectories.\n\n    The resulting list contains UNIX-like relative paths starting with ``templates``.\n    '''\n\n    result = []\n\n    for item in path.glob('**/*'):\n        if item.is_file() and not item.name.startswith('_'):\n            result.append(item.relative_to(path.parent).as_posix())\n\n    return result"}
{"code":"def air_gap(self,\n                volume: float = None,\n                height: float = None) -> 'InstrumentContext':\n        \"\"\"\n        Pull air into the pipette current tip at the current location\n\n        :param volume: The amount in uL to aspirate air into the tube.\n                       (Default will use all remaining volume in tip)\n        :type volume: float\n\n        :param height: The number of millimiters to move above the current Well\n                       to air-gap aspirate. (Default: 5mm above current Well)\n        :type height: float\n\n        :raises NoTipAttachedError: If no tip is attached to the pipette\n\n        :raises RuntimeError: If location cache is None.\n                              This should happen if `touch_tip` is called\n                              without first calling a method that takes a\n                              location (eg, :py:meth:`.aspirate`,\n                              :py:meth:`dispense`)\n\n        :returns: This instance\n        \"\"\"\n        if not self.hw_pipette['has_tip']:\n            raise hc.NoTipAttachedError('Pipette has no tip. Aborting air_gap')\n\n        if height is None:\n            height = 5\n        loc = self._ctx.location_cache\n        if not loc or not isinstance(loc.labware, Well):\n            raise RuntimeError('No previous Well cached to perform air gap')\n        target = loc.labware.top(height)\n        self.move_to(target)\n        self.aspirate(volume)\n        return self","return_type":"'InstrumentContext'","function_name":"InstrumentContext.air_gap","stripped_code":"def air_gap(self,\n                volume: float = None,\n                height: float = None):\n        \"\"\"\n        Pull air into the pipette current tip at the current location\n\n        :param volume: The amount in uL to aspirate air into the tube.\n                       (Default will use all remaining volume in tip)\n        :type volume: float\n\n        :param height: The number of millimiters to move above the current Well\n                       to air-gap aspirate. (Default: 5mm above current Well)\n        :type height: float\n\n        :raises NoTipAttachedError: If no tip is attached to the pipette\n\n        :raises RuntimeError: If location cache is None.\n                              This should happen if `touch_tip` is called\n                              without first calling a method that takes a\n                              location (eg, :py:meth:`.aspirate`,\n                              :py:meth:`dispense`)\n\n        :returns: This instance\n        \"\"\"\n        if not self.hw_pipette['has_tip']:\n            raise hc.NoTipAttachedError('Pipette has no tip. Aborting air_gap')\n\n        if height is None:\n            height = 5\n        loc = self._ctx.location_cache\n        if not loc or not isinstance(loc.labware, Well):\n            raise RuntimeError('No previous Well cached to perform air gap')\n        target = loc.labware.top(height)\n        self.move_to(target)\n        self.aspirate(volume)\n        return self"}
{"code":"def update_mapping(mapping: Dict[ops.Qid, LogicalIndex],\n                   operations: ops.OP_TREE\n                   ) -> None:\n    \"\"\"Updates a mapping (in place) from qubits to logical indices according to\n    a set of permutation gates. Any gates other than permutation gates are\n    ignored.\n\n    Args:\n        mapping: The mapping to update.\n        operations: The operations to update according to.\n    \"\"\"\n    for op in ops.flatten_op_tree(operations):\n        if (isinstance(op, ops.GateOperation) and\n            isinstance(op.gate, PermutationGate)):\n            op.gate.update_mapping(mapping, op.qubits)","return_type":"None","function_name":"update_mapping","stripped_code":"def update_mapping(mapping: Dict[ops.Qid, LogicalIndex],\n                   operations: ops.OP_TREE\n                   ):\n    \"\"\"Updates a mapping (in place) from qubits to logical indices according to\n    a set of permutation gates. Any gates other than permutation gates are\n    ignored.\n\n    Args:\n        mapping: The mapping to update.\n        operations: The operations to update according to.\n    \"\"\"\n    for op in ops.flatten_op_tree(operations):\n        if (isinstance(op, ops.GateOperation) and\n            isinstance(op.gate, PermutationGate)):\n            op.gate.update_mapping(mapping, op.qubits)"}
{"code":"def set(self, name: str, value: Union[str, List[str]]) -> None:\n        \"\"\"\n        \u8bbe\u7f6e header\n        \"\"\"\n        self._headers[name] = value","return_type":"None","function_name":"Response.set","stripped_code":"def set(self, name: str, value: Union[str, List[str]]):\n        \"\"\"\n        \u8bbe\u7f6e header\n        \"\"\"\n        self._headers[name] = value"}
{"code":"def ordered_repr(obj: object, attrlist: Iterable[str],\n                 joiner: str = COMMA_SPACE) -> str:\n    \"\"\"\n    Shortcut to make :func:`repr` functions ordered.\n    Define your :func:`__repr__` like this:\n\n    .. code-block:: python\n\n        def __repr__(self):\n            return ordered_repr(self, [\"field1\", \"field2\", \"field3\"])\n\n    Args:\n        obj: object to display\n        attrlist: iterable of attribute names\n        joiner: string with which to join the elements\n\n    Returns:\n        string: :func:`repr`-style representation\n    \"\"\"\n    return \"<{classname}({kvp})>\".format(\n        classname=type(obj).__name__,\n        kvp=joiner.join(\"{}={}\".format(a, repr(getattr(obj, a)))\n                        for a in attrlist)\n    )","return_type":"str","function_name":"ordered_repr","stripped_code":"def ordered_repr(obj: object, attrlist: Iterable[str],\n                 joiner: str = COMMA_SPACE):\n    \"\"\"\n    Shortcut to make :func:`repr` functions ordered.\n    Define your :func:`__repr__` like this:\n\n    .. code-block:: python\n\n        def __repr__(self):\n            return ordered_repr(self, [\"field1\", \"field2\", \"field3\"])\n\n    Args:\n        obj: object to display\n        attrlist: iterable of attribute names\n        joiner: string with which to join the elements\n\n    Returns:\n        string: :func:`repr`-style representation\n    \"\"\"\n    return \"<{classname}({kvp})>\".format(\n        classname=type(obj).__name__,\n        kvp=joiner.join(\"{}={}\".format(a, repr(getattr(obj, a)))\n                        for a in attrlist)\n    )"}
{"code":"def copy(self, name: str) -> 'Selection':\n        \"\"\"Return a new |Selection| object with the given name and copies\n        of the handles |Nodes| and |Elements| objects based on method\n        |Devices.copy|.\"\"\"\n        return type(self)(name, copy.copy(self.nodes), copy.copy(self.elements))","return_type":"'Selection'","function_name":"Selection.copy","stripped_code":"def copy(self, name: str):\n        \"\"\"Return a new |Selection| object with the given name and copies\n        of the handles |Nodes| and |Elements| objects based on method\n        |Devices.copy|.\"\"\"\n        return type(self)(name, copy.copy(self.nodes), copy.copy(self.elements))"}
{"code":"def cubic_acquaintance_strategy(\n        qubits: Iterable[ops.Qid],\n        swap_gate: ops.Gate=ops.SWAP\n        ) -> circuits.Circuit:\n    \"\"\"Acquaints every triple of qubits.\n\n    Exploits the fact that in a simple linear swap network every pair of\n    logical qubits that starts at distance two remains so (except temporarily\n    near the edge), and that every third one `goes through` the pair at some\n    point in the network. The strategy then iterates through a series of\n    mappings in which qubits i and i + k are placed at distance two, for k = 1\n    through n / 2. Linear swap networks are used in between to effect the\n    permutation.\n    \"\"\"\n\n    qubits = tuple(qubits)\n    n_qubits = len(qubits)\n\n    swap_gate = SwapPermutationGate(swap_gate)\n\n    moments = []\n    index_order = tuple(range(n_qubits))\n    max_separation = max(((n_qubits - 1) // 2) + 1, 2)\n    for separation in range(1, max_separation):\n        stepped_indices_concatenated = tuple(itertools.chain(*(\n                range(offset, n_qubits, separation)\n                for offset in range(separation))))\n        new_index_order = skip_and_wrap_around(stepped_indices_concatenated)\n        permutation = {i: new_index_order.index(j)\n            for i, j in enumerate(index_order)}\n        permutation_gate = LinearPermutationGate(\n                n_qubits, permutation, swap_gate)\n        moments.append(ops.Moment([permutation_gate(*qubits)]))\n        for i in range(n_qubits + 1):\n            for offset in range(3):\n                moment = ops.Moment(acquaint(*qubits[j:j+3])\n                        for j in range(offset, n_qubits - 2, 3))\n                moments.append(moment)\n            if i < n_qubits:\n                moment = ops.Moment(swap_gate(*qubits[j:j+2])\n                        for j in range(i % 2, n_qubits - 1, 2))\n                moments.append(moment)\n        index_order = new_index_order[::-1]\n    return circuits.Circuit(moments, device=UnconstrainedAcquaintanceDevice)","return_type":"circuits.Circuit","function_name":"cubic_acquaintance_strategy","stripped_code":"def cubic_acquaintance_strategy(\n        qubits: Iterable[ops.Qid],\n        swap_gate: ops.Gate=ops.SWAP\n        ):\n    \"\"\"Acquaints every triple of qubits.\n\n    Exploits the fact that in a simple linear swap network every pair of\n    logical qubits that starts at distance two remains so (except temporarily\n    near the edge), and that every third one `goes through` the pair at some\n    point in the network. The strategy then iterates through a series of\n    mappings in which qubits i and i + k are placed at distance two, for k = 1\n    through n / 2. Linear swap networks are used in between to effect the\n    permutation.\n    \"\"\"\n\n    qubits = tuple(qubits)\n    n_qubits = len(qubits)\n\n    swap_gate = SwapPermutationGate(swap_gate)\n\n    moments = []\n    index_order = tuple(range(n_qubits))\n    max_separation = max(((n_qubits - 1) // 2) + 1, 2)\n    for separation in range(1, max_separation):\n        stepped_indices_concatenated = tuple(itertools.chain(*(\n                range(offset, n_qubits, separation)\n                for offset in range(separation))))\n        new_index_order = skip_and_wrap_around(stepped_indices_concatenated)\n        permutation = {i: new_index_order.index(j)\n            for i, j in enumerate(index_order)}\n        permutation_gate = LinearPermutationGate(\n                n_qubits, permutation, swap_gate)\n        moments.append(ops.Moment([permutation_gate(*qubits)]))\n        for i in range(n_qubits + 1):\n            for offset in range(3):\n                moment = ops.Moment(acquaint(*qubits[j:j+3])\n                        for j in range(offset, n_qubits - 2, 3))\n                moments.append(moment)\n            if i < n_qubits:\n                moment = ops.Moment(swap_gate(*qubits[j:j+2])\n                        for j in range(i % 2, n_qubits - 1, 2))\n                moments.append(moment)\n        index_order = new_index_order[::-1]\n    return circuits.Circuit(moments, device=UnconstrainedAcquaintanceDevice)"}
{"code":"def split_text(text: str, length: int = MAX_MESSAGE_LENGTH) -> typing.List[str]:\n    \"\"\"\n    Split long text\n\n    :param text:\n    :param length:\n    :return: list of parts\n    :rtype: :obj:`typing.List[str]`\n    \"\"\"\n    return [text[i:i + length] for i in range(0, len(text), length)]","return_type":"typing.List[str]","function_name":"split_text","stripped_code":"def split_text(text: str, length: int = MAX_MESSAGE_LENGTH):\n    \"\"\"\n    Split long text\n\n    :param text:\n    :param length:\n    :return: list of parts\n    :rtype: :obj:`typing.List[str]`\n    \"\"\"\n    return [text[i:i + length] for i in range(0, len(text), length)]"}
{"code":"def from_int(value: int) -> ulid.ULID:\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance from the given :class:`~int` value.\n\n    :param value: 128 bit integer\n    :type value: :class:`~int`\n    :return: ULID from integer value\n    :rtype: :class:`~ulid.ulid.ULID`\n    :raises ValueError: when the value is not a 128 bit integer\n    \"\"\"\n    if value < 0:\n        raise ValueError('Expects positive integer')\n\n    length = (value.bit_length() + 7) // 8\n    if length > 16:\n        raise ValueError('Expects integer to be 128 bits; got {} bytes'.format(length))\n\n    return ulid.ULID(value.to_bytes(16, byteorder='big'))","return_type":"ulid.ULID","function_name":"from_int","stripped_code":"def from_int(value: int):\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance from the given :class:`~int` value.\n\n    :param value: 128 bit integer\n    :type value: :class:`~int`\n    :return: ULID from integer value\n    :rtype: :class:`~ulid.ulid.ULID`\n    :raises ValueError: when the value is not a 128 bit integer\n    \"\"\"\n    if value < 0:\n        raise ValueError('Expects positive integer')\n\n    length = (value.bit_length() + 7) // 8\n    if length > 16:\n        raise ValueError('Expects integer to be 128 bits; got {} bytes'.format(length))\n\n    return ulid.ULID(value.to_bytes(16, byteorder='big'))"}
{"code":"def add(self, selected: 'SelectedMailbox', *,\n            replace: 'SelectedMailbox' = None) -> None:\n        \"\"\"Add a new selected mailbox object to the set, which may then be\n        returned by :meth:`.any_selected`.\n\n        Args:\n            selected: The new selected mailbox object.\n            replace: An existing selected mailbox object that should be removed\n                from the weak set.\n\n        \"\"\"\n        if replace is not None:\n            self._set.discard(replace)\n        self._set.add(selected)","return_type":"None","function_name":"SelectedSet.add","stripped_code":"def add(self, selected: 'SelectedMailbox', *,\n            replace: 'SelectedMailbox' = None):\n        \"\"\"Add a new selected mailbox object to the set, which may then be\n        returned by :meth:`.any_selected`.\n\n        Args:\n            selected: The new selected mailbox object.\n            replace: An existing selected mailbox object that should be removed\n                from the weak set.\n\n        \"\"\"\n        if replace is not None:\n            self._set.discard(replace)\n        self._set.add(selected)"}
{"code":"def get_config_multiline_option(parser: ConfigParser,\n                                section: str,\n                                option: str,\n                                default: List[str] = None) -> List[str]:\n    \"\"\"\n    Retrieves a multi-line string value from a parser as a list of strings\n    (one per line, ignoring blank lines).\n\n    Args:\n        parser: instance of :class:`ConfigParser`\n        section: section name within config file\n        option: option (variable) name within that section\n        default: value to return if option is absent (``None`` is mapped to\n            ``[]``)\n\n    Returns:\n        list of strings\n\n    Raises:\n        ValueError: if the section is absent\n\n    \"\"\"\n    default = default or []\n    if not parser.has_section(section):\n        raise ValueError(\"config missing section: \" + section)\n    try:\n        multiline = parser.get(section, option)\n        values = [x.strip() for x in multiline.splitlines() if x.strip()]\n        return values\n    except NoOptionError:\n        return default","return_type":"List[str]","function_name":"get_config_multiline_option","stripped_code":"def get_config_multiline_option(parser: ConfigParser,\n                                section: str,\n                                option: str,\n                                default: List[str] = None):\n    \"\"\"\n    Retrieves a multi-line string value from a parser as a list of strings\n    (one per line, ignoring blank lines).\n\n    Args:\n        parser: instance of :class:`ConfigParser`\n        section: section name within config file\n        option: option (variable) name within that section\n        default: value to return if option is absent (``None`` is mapped to\n            ``[]``)\n\n    Returns:\n        list of strings\n\n    Raises:\n        ValueError: if the section is absent\n\n    \"\"\"\n    default = default or []\n    if not parser.has_section(section):\n        raise ValueError(\"config missing section: \" + section)\n    try:\n        multiline = parser.get(section, option)\n        values = [x.strip() for x in multiline.splitlines() if x.strip()]\n        return values\n    except NoOptionError:\n        return default"}
{"code":"def get_undefined_namespace_names(graph: BELGraph, namespace: str) -> Set[str]:\n    \"\"\"Get the names from a namespace that wasn't actually defined.\n\n    :return: The set of all names from the undefined namespace\n    \"\"\"\n    return {\n        exc.name\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, UndefinedNamespaceWarning) and exc.namespace == namespace\n    }","return_type":"Set[str]","function_name":"get_undefined_namespace_names","stripped_code":"def get_undefined_namespace_names(graph: BELGraph, namespace: str):\n    \"\"\"Get the names from a namespace that wasn't actually defined.\n\n    :return: The set of all names from the undefined namespace\n    \"\"\"\n    return {\n        exc.name\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, UndefinedNamespaceWarning) and exc.namespace == namespace\n    }"}
{"code":"def get_linked_agenda_items(self) -> List[str]:\n        \"\"\"\n        Returns entities that can be linked to spans in the question, that should be in the agenda,\n        for training a coverage based semantic parser. This method essentially does a heuristic\n        entity linking, to provide weak supervision for a learning to search parser.\n        \"\"\"\n        agenda_items: List[str] = []\n        for entity in self._get_longest_span_matching_entities():\n            agenda_items.append(entity)\n            # If the entity is a cell, we need to add the column to the agenda as well,\n            # because the answer most likely involves getting the row with the cell.\n            if 'fb:cell' in entity:\n                agenda_items.append(self.neighbors[entity][0])\n        return agenda_items","return_type":"List[str]","function_name":"TableQuestionKnowledgeGraph.get_linked_agenda_items","stripped_code":"def get_linked_agenda_items(self):\n        \"\"\"\n        Returns entities that can be linked to spans in the question, that should be in the agenda,\n        for training a coverage based semantic parser. This method essentially does a heuristic\n        entity linking, to provide weak supervision for a learning to search parser.\n        \"\"\"\n        agenda_items: List[str] = []\n        for entity in self._get_longest_span_matching_entities():\n            agenda_items.append(entity)\n            # If the entity is a cell, we need to add the column to the agenda as well,\n            # because the answer most likely involves getting the row with the cell.\n            if 'fb:cell' in entity:\n                agenda_items.append(self.neighbors[entity][0])\n        return agenda_items"}
{"code":"def relabel(self, qubits: Qubits) -> 'State':\n        \"\"\"Return a copy of this state with new qubits\"\"\"\n        return State(self.vec.tensor, qubits, self._memory)","return_type":"'State'","function_name":"State.relabel","stripped_code":"def relabel(self, qubits: Qubits):\n        \"\"\"Return a copy of this state with new qubits\"\"\"\n        return State(self.vec.tensor, qubits, self._memory)"}
{"code":"def usergroups_create(self, *, name: str, **kwargs) -> SlackResponse:\n        \"\"\"Create a User Group\n\n        Args:\n            name (str): A name for the User Group. Must be unique among User Groups.\n                e.g. 'My Test Team'\n        \"\"\"\n        self._validate_xoxp_token()\n        kwargs.update({\"name\": name})\n        return self.api_call(\"usergroups.create\", json=kwargs)","return_type":"SlackResponse","function_name":"WebClient.usergroups_create","stripped_code":"def usergroups_create(self, *, name: str, **kwargs):\n        \"\"\"Create a User Group\n\n        Args:\n            name (str): A name for the User Group. Must be unique among User Groups.\n                e.g. 'My Test Team'\n        \"\"\"\n        self._validate_xoxp_token()\n        kwargs.update({\"name\": name})\n        return self.api_call(\"usergroups.create\", json=kwargs)"}
{"code":"def calculate_all_information_content(self) -> pd.Series:\n        \"\"\"\n        Calculate the Information Content (IC) value of every class\n\n        Sets the internal icmap cache and returns an array \n\n        Return\n        ------\n        Series\n            a pandas Series indexed by class id and with IC as value\n        \"\"\"\n        logging.info(\"Calculating all class ICs\")\n        df = self.assoc_df\n        freqs = df.sum(axis=0)\n        n_subjects, _ = df.shape\n        ics = freqs.apply(lambda x: -math.log(x / n_subjects)/math.log(2))\n        self.ics = ics\n        logging.info(\"DONE calculating all class ICs\")\n        return ics","return_type":"pd.Series","function_name":"SemSearchEngine.calculate_all_information_content","stripped_code":"def calculate_all_information_content(self):\n        \"\"\"\n        Calculate the Information Content (IC) value of every class\n\n        Sets the internal icmap cache and returns an array \n\n        Return\n        ------\n        Series\n            a pandas Series indexed by class id and with IC as value\n        \"\"\"\n        logging.info(\"Calculating all class ICs\")\n        df = self.assoc_df\n        freqs = df.sum(axis=0)\n        n_subjects, _ = df.shape\n        ics = freqs.apply(lambda x: -math.log(x / n_subjects)/math.log(2))\n        self.ics = ics\n        logging.info(\"DONE calculating all class ICs\")\n        return ics"}
{"code":"def get_cache_time(\n        self, path: str, modified: Optional[datetime.datetime], mime_type: str\n    ) -> int:\n        \"\"\"Override to customize cache control behavior.\n\n        Return a positive number of seconds to make the result\n        cacheable for that amount of time or 0 to mark resource as\n        cacheable for an unspecified amount of time (subject to\n        browser heuristics).\n\n        By default returns cache expiry of 10 years for resources requested\n        with ``v`` argument.\n        \"\"\"\n        return self.CACHE_MAX_AGE if \"v\" in self.request.arguments else 0","return_type":"int","function_name":"StaticFileHandler.get_cache_time","stripped_code":"def get_cache_time(\n        self, path: str, modified: Optional[datetime.datetime], mime_type: str\n    ):\n        \"\"\"Override to customize cache control behavior.\n\n        Return a positive number of seconds to make the result\n        cacheable for that amount of time or 0 to mark resource as\n        cacheable for an unspecified amount of time (subject to\n        browser heuristics).\n\n        By default returns cache expiry of 10 years for resources requested\n        with ``v`` argument.\n        \"\"\"\n        return self.CACHE_MAX_AGE if \"v\" in self.request.arguments else 0"}
{"code":"def df(self, qname_predicates:bool=False, keep_variable_type:bool=True) -> pd.DataFrame:\n        ''' Multi funcitonal DataFrame with settings '''\n        local_df = self.df.copy()\n        if qname_predicates:\n            for col in self.columns:\n                local_df.rename({col: self.g.qname(col)})\n        if not keep_variable_type:\n            pass\n            # convert all to strings, watch out for lists\n        return local_df","return_type":"pd.DataFrame","function_name":"OntoPandas.df","stripped_code":"def df(self, qname_predicates:bool=False, keep_variable_type:bool=True):\n        ''' Multi funcitonal DataFrame with settings '''\n        local_df = self.df.copy()\n        if qname_predicates:\n            for col in self.columns:\n                local_df.rename({col: self.g.qname(col)})\n        if not keep_variable_type:\n            pass\n            # convert all to strings, watch out for lists\n        return local_df"}
{"code":"def console_print(con: tcod.console.Console, x: int, y: int, fmt: str) -> None:\n    \"\"\"Print a color formatted string on a console.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        fmt (AnyStr): A unicode or bytes string optionaly using color codes.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.print_` instead.\n    \"\"\"\n    lib.TCOD_console_printf(_console(con), x, y, _fmt(fmt))","return_type":"None","function_name":"console_print","stripped_code":"def console_print(con: tcod.console.Console, x: int, y: int, fmt: str):\n    \"\"\"Print a color formatted string on a console.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        fmt (AnyStr): A unicode or bytes string optionaly using color codes.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.print_` instead.\n    \"\"\"\n    lib.TCOD_console_printf(_console(con), x, y, _fmt(fmt))"}
{"code":"def _build_cmd(self, args: Union[list, tuple]) -> str:\n        '''Build command.'''\n        cmd = [self.path]\n        cmd.extend(args)\n        return cmd","return_type":"str","function_name":"Commands._build_cmd","stripped_code":"def _build_cmd(self, args: Union[list, tuple]):\n        '''Build command.'''\n        cmd = [self.path]\n        cmd.extend(args)\n        return cmd"}
{"code":"def verify_http_auth_refresh_token(self) -> bool:\r\n        \"\"\" Use expired token to check refresh token information \"\"\"\r\n        authorization_token = self.get_http_token()\r\n        if authorization_token is not None:\r\n            if self.verify_refresh_token(authorization_token):\r\n                if self.data is not None:\r\n                    self.data = self.data['data']\r\n                    return True\r\n                return False\r\n            else:\r\n                return False\r\n        return False","return_type":"bool","function_name":"JWT.verify_http_auth_refresh_token","stripped_code":"def verify_http_auth_refresh_token(self):\r\n        \"\"\" Use expired token to check refresh token information \"\"\"\r\n        authorization_token = self.get_http_token()\r\n        if authorization_token is not None:\r\n            if self.verify_refresh_token(authorization_token):\r\n                if self.data is not None:\r\n                    self.data = self.data['data']\r\n                    return True\r\n                return False\r\n            else:\r\n                return False\r\n        return False"}
{"code":"def draw_polygon(\n            self,\n            *pts,\n            close_path=True,\n            stroke=None,\n            stroke_width=1,\n            stroke_dash=None,\n            fill=None\n            ) -> None:\n        \"\"\"Draws the given polygon.\"\"\"\n        c = self.c\n        c.save()\n        c.new_path()\n        for x,y in zip(*[iter(pts)]*2):\n            c.line_to(x, y)\n        if close_path:\n            c.close_path()\n        self._fill_and_stroke(stroke, stroke_width, stroke_dash, fill)\n        c.restore()","return_type":"None","function_name":"CairoOutput.draw_polygon","stripped_code":"def draw_polygon(\n            self,\n            *pts,\n            close_path=True,\n            stroke=None,\n            stroke_width=1,\n            stroke_dash=None,\n            fill=None\n            ):\n        \"\"\"Draws the given polygon.\"\"\"\n        c = self.c\n        c.save()\n        c.new_path()\n        for x,y in zip(*[iter(pts)]*2):\n            c.line_to(x, y)\n        if close_path:\n            c.close_path()\n        self._fill_and_stroke(stroke, stroke_width, stroke_dash, fill)\n        c.restore()"}
{"code":"def combine_tensors(combination: str, tensors: List[torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Combines a list of tensors using element-wise operations and concatenation, specified by a\n    ``combination`` string.  The string refers to (1-indexed) positions in the input tensor list,\n    and looks like ``\"1,2,1+2,3-1\"``.\n\n    We allow the following kinds of combinations: ``x``, ``x*y``, ``x+y``, ``x-y``, and ``x/y``,\n    where ``x`` and ``y`` are positive integers less than or equal to ``len(tensors)``.  Each of\n    the binary operations is performed elementwise.  You can give as many combinations as you want\n    in the ``combination`` string.  For example, for the input string ``\"1,2,1*2\"``, the result\n    would be ``[1;2;1*2]``, as you would expect, where ``[;]`` is concatenation along the last\n    dimension.\n\n    If you have a fixed, known way to combine tensors that you use in a model, you should probably\n    just use something like ``torch.cat([x_tensor, y_tensor, x_tensor * y_tensor])``.  This\n    function adds some complexity that is only necessary if you want the specific combination used\n    to be `configurable`.\n\n    If you want to do any element-wise operations, the tensors involved in each element-wise\n    operation must have the same shape.\n\n    This function also accepts ``x`` and ``y`` in place of ``1`` and ``2`` in the combination\n    string.\n    \"\"\"\n    if len(tensors) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    to_concatenate = [_get_combination(piece, tensors) for piece in combination.split(',')]\n    return torch.cat(to_concatenate, dim=-1)","return_type":"torch.Tensor","function_name":"combine_tensors","stripped_code":"def combine_tensors(combination: str, tensors: List[torch.Tensor]):\n    \"\"\"\n    Combines a list of tensors using element-wise operations and concatenation, specified by a\n    ``combination`` string.  The string refers to (1-indexed) positions in the input tensor list,\n    and looks like ``\"1,2,1+2,3-1\"``.\n\n    We allow the following kinds of combinations: ``x``, ``x*y``, ``x+y``, ``x-y``, and ``x/y``,\n    where ``x`` and ``y`` are positive integers less than or equal to ``len(tensors)``.  Each of\n    the binary operations is performed elementwise.  You can give as many combinations as you want\n    in the ``combination`` string.  For example, for the input string ``\"1,2,1*2\"``, the result\n    would be ``[1;2;1*2]``, as you would expect, where ``[;]`` is concatenation along the last\n    dimension.\n\n    If you have a fixed, known way to combine tensors that you use in a model, you should probably\n    just use something like ``torch.cat([x_tensor, y_tensor, x_tensor * y_tensor])``.  This\n    function adds some complexity that is only necessary if you want the specific combination used\n    to be `configurable`.\n\n    If you want to do any element-wise operations, the tensors involved in each element-wise\n    operation must have the same shape.\n\n    This function also accepts ``x`` and ``y`` in place of ``1`` and ``2`` in the combination\n    string.\n    \"\"\"\n    if len(tensors) > 9:\n        raise ConfigurationError(\"Double-digit tensor lists not currently supported\")\n    combination = combination.replace('x', '1').replace('y', '2')\n    to_concatenate = [_get_combination(piece, tensors) for piece in combination.split(',')]\n    return torch.cat(to_concatenate, dim=-1)"}
{"code":"def index_in_block(self, channel_index: int) -> int:\n        \"\"\"Return the index a channel has within the subblock it belongs to\n\n        I.e., only for reducible circuits, this gives a result different from\n        the argument itself.\n\n        Args:\n            channel_index (int): The index of the external channel\n\n        Raises:\n            ValueError: for an invalid `channel_index`\n\n        \"\"\"\n        if channel_index < 0 or channel_index >= self.cdim:\n            raise ValueError()\n\n        struct = self.block_structure\n\n        if len(struct) == 1:\n            return channel_index, 0\n        i = 1\n        while sum(struct[:i]) <= channel_index and i < self.cdim:\n            i += 1\n        block_index = i - 1\n        index_in_block = channel_index - sum(struct[:block_index])\n\n        return index_in_block, block_index","return_type":"int","function_name":"Circuit.index_in_block","stripped_code":"def index_in_block(self, channel_index: int):\n        \"\"\"Return the index a channel has within the subblock it belongs to\n\n        I.e., only for reducible circuits, this gives a result different from\n        the argument itself.\n\n        Args:\n            channel_index (int): The index of the external channel\n\n        Raises:\n            ValueError: for an invalid `channel_index`\n\n        \"\"\"\n        if channel_index < 0 or channel_index >= self.cdim:\n            raise ValueError()\n\n        struct = self.block_structure\n\n        if len(struct) == 1:\n            return channel_index, 0\n        i = 1\n        while sum(struct[:i]) <= channel_index and i < self.cdim:\n            i += 1\n        block_index = i - 1\n        index_in_block = channel_index - sum(struct[:block_index])\n\n        return index_in_block, block_index"}
{"code":"def get_events(self) -> List[Event]:\n        \"\"\"Get events associated with the scheduling object.\n\n        Returns:\n            list of Event objects\n\n        \"\"\"\n        LOG.debug('Getting events for %s', self.key)\n        return get_events(self.key)","return_type":"List[Event]","function_name":"SchedulingObject.get_events","stripped_code":"def get_events(self):\n        \"\"\"Get events associated with the scheduling object.\n\n        Returns:\n            list of Event objects\n\n        \"\"\"\n        LOG.debug('Getting events for %s', self.key)\n        return get_events(self.key)"}
{"code":"def run_main(args: argparse.Namespace, do_exit=True) -> None:\n    \"\"\"Runs the checks and exits.\n\n    To extend this tool, use this function and set do_exit to False\n    to get returned the status code.\n    \"\"\"\n    if args.init:\n        generate()\n        return None  # exit after generate instead of starting to lint\n\n    handler = CheckHandler(\n        file=args.config_file, out_json=args.json, files=args.files)\n\n    for style in get_stylers():\n        handler.run_linter(style())\n\n    for linter in get_linters():\n        handler.run_linter(linter())\n\n    for security in get_security():\n        handler.run_linter(security())\n\n    for tool in get_tools():\n        tool = tool()\n\n        # Only run pypi if everything else passed\n        if tool.name == \"pypi\" and handler.status_code != 0:\n            continue\n\n        handler.run_linter(tool)\n\n    if do_exit:\n        handler.exit()\n    return handler.status_code","return_type":"None","function_name":"run_main","stripped_code":"def run_main(args: argparse.Namespace, do_exit=True):\n    \"\"\"Runs the checks and exits.\n\n    To extend this tool, use this function and set do_exit to False\n    to get returned the status code.\n    \"\"\"\n    if args.init:\n        generate()\n        return None  # exit after generate instead of starting to lint\n\n    handler = CheckHandler(\n        file=args.config_file, out_json=args.json, files=args.files)\n\n    for style in get_stylers():\n        handler.run_linter(style())\n\n    for linter in get_linters():\n        handler.run_linter(linter())\n\n    for security in get_security():\n        handler.run_linter(security())\n\n    for tool in get_tools():\n        tool = tool()\n\n        # Only run pypi if everything else passed\n        if tool.name == \"pypi\" and handler.status_code != 0:\n            continue\n\n        handler.run_linter(tool)\n\n    if do_exit:\n        handler.exit()\n    return handler.status_code"}
{"code":"def remove_library_path(path: str) -> bool:\n    \"\"\"\n    Removes the path from the Python system path if it is found in the system\n    paths.\n\n    :param path:\n        The path to remove from the system paths\n    :return:\n        Whether or not the path was removed.\n    \"\"\"\n\n    if path in sys.path:\n        sys.path.remove(path)\n        return True\n\n    return False","return_type":"bool","function_name":"remove_library_path","stripped_code":"def remove_library_path(path: str):\n    \"\"\"\n    Removes the path from the Python system path if it is found in the system\n    paths.\n\n    :param path:\n        The path to remove from the system paths\n    :return:\n        Whether or not the path was removed.\n    \"\"\"\n\n    if path in sys.path:\n        sys.path.remove(path)\n        return True\n\n    return False"}
{"code":"def dump_statespace(self, contract: EVMContract = None) -> str:\n        \"\"\"\n        Returns serializable statespace of the contract\n        :param contract: The Contract on which the analysis should be done\n        :return: The serialized state space\n        \"\"\"\n        sym = SymExecWrapper(\n            contract or self.contracts[0],\n            self.address,\n            self.strategy,\n            dynloader=DynLoader(\n                self.eth,\n                storage_loading=self.onchain_storage_access,\n                contract_loading=self.dynld,\n            ),\n            max_depth=self.max_depth,\n            execution_timeout=self.execution_timeout,\n            create_timeout=self.create_timeout,\n            enable_iprof=self.enable_iprof,\n        )\n\n        return get_serializable_statespace(sym)","return_type":"str","function_name":"MythrilAnalyzer.dump_statespace","stripped_code":"def dump_statespace(self, contract: EVMContract = None):\n        \"\"\"\n        Returns serializable statespace of the contract\n        :param contract: The Contract on which the analysis should be done\n        :return: The serialized state space\n        \"\"\"\n        sym = SymExecWrapper(\n            contract or self.contracts[0],\n            self.address,\n            self.strategy,\n            dynloader=DynLoader(\n                self.eth,\n                storage_loading=self.onchain_storage_access,\n                contract_loading=self.dynld,\n            ),\n            max_depth=self.max_depth,\n            execution_timeout=self.execution_timeout,\n            create_timeout=self.create_timeout,\n            enable_iprof=self.enable_iprof,\n        )\n\n        return get_serializable_statespace(sym)"}
{"code":"def asset_class(self) -> str:\n        \"\"\" Returns the full asset class path for this stock \"\"\"\n        result = self.parent.name if self.parent else \"\"\n        # Iterate to the top asset class and add names.\n        cursor = self.parent\n        while cursor:\n            result = cursor.name + \":\" + result\n            cursor = cursor.parent\n        return result","return_type":"str","function_name":"Stock.asset_class","stripped_code":"def asset_class(self):\n        \"\"\" Returns the full asset class path for this stock \"\"\"\n        result = self.parent.name if self.parent else \"\"\n        # Iterate to the top asset class and add names.\n        cursor = self.parent\n        while cursor:\n            result = cursor.name + \":\" + result\n            cursor = cursor.parent\n        return result"}
{"code":"def Refind(self, maxSearchSeconds: float = TIME_OUT_SECOND, searchIntervalSeconds: float = SEARCH_INTERVAL, raiseException: bool = True) -> bool:\n        \"\"\"\n        Refind the control every searchIntervalSeconds seconds in maxSearchSeconds seconds.\n        maxSearchSeconds: float.\n        searchIntervalSeconds: float.\n        raiseException: bool, if True, raise a LookupError if timeout.\n        Return bool, True if find.\n        \"\"\"\n        if not self.Exists(maxSearchSeconds, searchIntervalSeconds, False if raiseException else DEBUG_EXIST_DISAPPEAR):\n            if raiseException:\n                Logger.ColorfullyWriteLine('<Color=Red>Find Control Timeout: </Color>' + self.GetColorfulSearchPropertiesStr())\n                raise LookupError('Find Control Timeout: ' + self.GetSearchPropertiesStr())\n            else:\n                return False\n        return True","return_type":"bool","function_name":"Control.Refind","stripped_code":"def Refind(self, maxSearchSeconds: float = TIME_OUT_SECOND, searchIntervalSeconds: float = SEARCH_INTERVAL, raiseException: bool = True):\n        \"\"\"\n        Refind the control every searchIntervalSeconds seconds in maxSearchSeconds seconds.\n        maxSearchSeconds: float.\n        searchIntervalSeconds: float.\n        raiseException: bool, if True, raise a LookupError if timeout.\n        Return bool, True if find.\n        \"\"\"\n        if not self.Exists(maxSearchSeconds, searchIntervalSeconds, False if raiseException else DEBUG_EXIST_DISAPPEAR):\n            if raiseException:\n                Logger.ColorfullyWriteLine('<Color=Red>Find Control Timeout: </Color>' + self.GetColorfulSearchPropertiesStr())\n                raise LookupError('Find Control Timeout: ' + self.GetSearchPropertiesStr())\n            else:\n                return False\n        return True"}
{"code":"def get(self, key: K) -> TOption[T]:\n        \"\"\"\n        :param key:\n\n        Usage:\n\n            >>> TDict(k1=1, k2=2, k3=3).get(\"k2\")\n            Option --> 2\n            >>> TDict(k1=1, k2=2, k3=3).get(\"unknown\")\n            Option --> None\n        \"\"\"\n        return TOption(self[key]) if key in self else TOption(None)","return_type":"TOption[T]","function_name":"TDict.get","stripped_code":"def get(self, key: K):\n        \"\"\"\n        :param key:\n\n        Usage:\n\n            >>> TDict(k1=1, k2=2, k3=3).get(\"k2\")\n            Option --> 2\n            >>> TDict(k1=1, k2=2, k3=3).get(\"unknown\")\n            Option --> None\n        \"\"\"\n        return TOption(self[key]) if key in self else TOption(None)"}
{"code":"def on_train_begin(self, pbar, **kwargs:Any)->None:\n        \"Initialize optimizer and learner hyperparameters.\"\n        setattr(pbar, 'clean_on_interrupt', True)\n        self.learn.save('tmp')\n        self.opt = self.learn.opt\n        self.opt.lr = self.sched.start\n        self.stop,self.best_loss = False,0.\n        return {'skip_validate': True}","return_type":"None","function_name":"LRFinder.on_train_begin","stripped_code":"def on_train_begin(self, pbar, **kwargs:Any):\n        \"Initialize optimizer and learner hyperparameters.\"\n        setattr(pbar, 'clean_on_interrupt', True)\n        self.learn.save('tmp')\n        self.opt = self.learn.opt\n        self.opt.lr = self.sched.start\n        self.stop,self.best_loss = False,0.\n        return {'skip_validate': True}"}
{"code":"def is_isolated_list_abundance(graph: BELGraph, node: BaseEntity, cls: Type[ListAbundance] = ListAbundance) -> bool:\n    \"\"\"Return if the node is a list abundance but has no qualified edges.\"\"\"\n    return (\n        isinstance(node, cls) and\n        0 == graph.in_degree(node) and\n        all(\n            data[RELATION] == HAS_COMPONENT\n            for _, __, data in graph.out_edges(node, data=True)\n        )\n    )","return_type":"bool","function_name":"is_isolated_list_abundance","stripped_code":"def is_isolated_list_abundance(graph: BELGraph, node: BaseEntity, cls: Type[ListAbundance] = ListAbundance):\n    \"\"\"Return if the node is a list abundance but has no qualified edges.\"\"\"\n    return (\n        isinstance(node, cls) and\n        0 == graph.in_degree(node) and\n        all(\n            data[RELATION] == HAS_COMPONENT\n            for _, __, data in graph.out_edges(node, data=True)\n        )\n    )"}
{"code":"def compute_route_time_series_base(\n    trip_stats_subset: DataFrame,\n    date_label: str = \"20010101\",\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n) -> DataFrame:\n    \"\"\"\n    Compute stats in a 24-hour time series form for the given subset of trips.\n\n    Parameters\n    ----------\n    trip_stats_subset : DataFrame\n        A subset of the output of :func:`.trips.compute_trip_stats`\n    split_directions : boolean\n        If ``True``, then separate each routes's stats by trip direction;\n        otherwise aggregate trips in both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n    date_label : string\n        YYYYMMDD date string used as the date in the time series index\n\n    Returns\n    -------\n    DataFrame\n        A time series version of the following route stats for each\n        route.\n\n        - ``num_trips``: number of trips in service on the route\n          at any time within the time bin\n        - ``num_trip_starts``: number of trips that start within\n          the time bin\n        - ``num_trip_ends``: number of trips that end within the\n          time bin, ignoring trips that end past midnight\n        - ``service_distance``: sum of the service duration accrued\n          during the time bin across all trips on the route;\n          measured in hours\n        - ``service_distance``: sum of the service distance accrued\n          during the time bin across all trips on the route; measured\n          in kilometers\n        - ``service_speed``: ``service_distance/service_duration``\n          for the route\n\n        The columns are hierarchical (multi-indexed) with\n\n        - top level: name is ``'indicator'``; values are\n          ``'num_trip_starts'``, ``'num_trip_ends'``, ``'num_trips'``,\n          ``'service_distance'``, ``'service_duration'``, and\n          ``'service_speed'``\n        - middle level: name is ``'route_id'``;\n          values are the active routes\n        - bottom level: name is ``'direction_id'``; values are 0s and 1s\n\n        If not ``split_directions``, then don't include the bottom level.\n\n        The time series has a timestamp index for a 24-hour period\n        sampled at the given frequency.\n        The maximum allowable frequency is 1 minute.\n        If ``trip_stats_subset`` is empty, then return an empty\n        DataFrame with the columns ``'num_trip_starts'``,\n        ``'num_trip_ends'``, ``'num_trips'``, ``'service_distance'``,\n        ``'service_duration'``, and ``'service_speed'``.\n\n    Notes\n    -----\n    - The time series is computed at a one-minute frequency, then\n      resampled at the end to the given frequency\n    - Trips that lack start or end times are ignored, so the the\n      aggregate ``num_trips`` across the day could be less than the\n      ``num_trips`` column of :func:`compute_route_stats_base`\n    - All trip departure times are taken modulo 24 hours.\n      So routes with trips that end past 23:59:59 will have all\n      their stats wrap around to the early morning of the time series,\n      except for their ``num_trip_ends`` indicator.\n      Trip endings past 23:59:59 not binned so that resampling the\n      ``num_trips`` indicator works efficiently.\n    - Note that the total number of trips for two consecutive time bins\n      t1 < t2 is the sum of the number of trips in bin t2 plus the\n      number of trip endings in bin t1.\n      Thus we can downsample the ``num_trips`` indicator by keeping\n      track of only one extra count, ``num_trip_ends``, and can avoid\n      recording individual trip IDs.\n    - All other indicators are downsampled by summing.\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    if trip_stats_subset.empty:\n        return pd.DataFrame()\n\n    tss = trip_stats_subset.copy()\n    if split_directions:\n        tss = tss.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if tss.empty:\n            raise ValueError(\n                \"At least one trip stats direction ID value \"\n                \"must be non-NaN.\"\n            )\n\n        # Alter route IDs to encode direction:\n        # <route ID>-0 and <route ID>-1 or <route ID>-NA\n        tss[\"route_id\"] = (\n            tss[\"route_id\"]\n            + \"-\"\n            + tss[\"direction_id\"].map(lambda x: str(int(x)))\n        )\n\n    routes = tss[\"route_id\"].unique()\n    # Build a dictionary of time series and then merge them all\n    # at the end.\n    # Assign a uniform generic date for the index\n    date_str = date_label\n    day_start = pd.to_datetime(date_str + \" 00:00:00\")\n    day_end = pd.to_datetime(date_str + \" 23:59:00\")\n    rng = pd.period_range(day_start, day_end, freq=\"Min\")\n    indicators = [\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"num_trips\",\n        \"service_duration\",\n        \"service_distance\",\n    ]\n\n    bins = [i for i in range(24 * 60)]  # One bin for each minute\n    num_bins = len(bins)\n\n    # Bin start and end times\n    def F(x):\n        return (hp.timestr_to_seconds(x) // 60) % (24 * 60)\n\n    tss[[\"start_index\", \"end_index\"]] = tss[\n        [\"start_time\", \"end_time\"]\n    ].applymap(F)\n    routes = sorted(set(tss[\"route_id\"].values))\n\n    # Bin each trip according to its start and end time and weight\n    series_by_route_by_indicator = {\n        indicator: {route: [0 for i in range(num_bins)] for route in routes}\n        for indicator in indicators\n    }\n    for index, row in tss.iterrows():\n        route = row[\"route_id\"]\n        start = row[\"start_index\"]\n        end = row[\"end_index\"]\n        distance = row[\"distance\"]\n\n        if start is None or np.isnan(start) or start == end:\n            continue\n\n        # Get bins to fill\n        if start <= end:\n            bins_to_fill = bins[start:end]\n        else:\n            bins_to_fill = bins[start:] + bins[:end]\n\n        # Bin trip\n        # Do num trip starts\n        series_by_route_by_indicator[\"num_trip_starts\"][route][start] += 1\n        # Don't mark trip ends for trips that run past midnight;\n        # allows for easy resampling of num_trips later\n        if start <= end:\n            series_by_route_by_indicator[\"num_trip_ends\"][route][end] += 1\n        # Do rest of indicators\n        for indicator in indicators[2:]:\n            if indicator == \"num_trips\":\n                weight = 1\n            elif indicator == \"service_duration\":\n                weight = 1 / 60\n            else:\n                weight = distance / len(bins_to_fill)\n            for bin in bins_to_fill:\n                series_by_route_by_indicator[indicator][route][bin] += weight\n\n    # Create one time series per indicator\n    rng = pd.date_range(date_str, periods=24 * 60, freq=\"Min\")\n    series_by_indicator = {\n        indicator: pd.DataFrame(\n            series_by_route_by_indicator[indicator], index=rng\n        ).fillna(0)\n        for indicator in indicators\n    }\n\n    # Combine all time series into one time series\n    g = hp.combine_time_series(\n        series_by_indicator, kind=\"route\", split_directions=split_directions\n    )\n\n    return hp.downsample(g, freq=freq)","return_type":"DataFrame","function_name":"compute_route_time_series_base","stripped_code":"def compute_route_time_series_base(\n    trip_stats_subset: DataFrame,\n    date_label: str = \"20010101\",\n    freq: str = \"5Min\",\n    *,\n    split_directions: bool = False,\n):\n    \"\"\"\n    Compute stats in a 24-hour time series form for the given subset of trips.\n\n    Parameters\n    ----------\n    trip_stats_subset : DataFrame\n        A subset of the output of :func:`.trips.compute_trip_stats`\n    split_directions : boolean\n        If ``True``, then separate each routes's stats by trip direction;\n        otherwise aggregate trips in both directions\n    freq : Pandas frequency string\n        Specifices the frequency with which to resample the time series;\n        max frequency is one minute ('Min')\n    date_label : string\n        YYYYMMDD date string used as the date in the time series index\n\n    Returns\n    -------\n    DataFrame\n        A time series version of the following route stats for each\n        route.\n\n        - ``num_trips``: number of trips in service on the route\n          at any time within the time bin\n        - ``num_trip_starts``: number of trips that start within\n          the time bin\n        - ``num_trip_ends``: number of trips that end within the\n          time bin, ignoring trips that end past midnight\n        - ``service_distance``: sum of the service duration accrued\n          during the time bin across all trips on the route;\n          measured in hours\n        - ``service_distance``: sum of the service distance accrued\n          during the time bin across all trips on the route; measured\n          in kilometers\n        - ``service_speed``: ``service_distance/service_duration``\n          for the route\n\n        The columns are hierarchical (multi-indexed) with\n\n        - top level: name is ``'indicator'``; values are\n          ``'num_trip_starts'``, ``'num_trip_ends'``, ``'num_trips'``,\n          ``'service_distance'``, ``'service_duration'``, and\n          ``'service_speed'``\n        - middle level: name is ``'route_id'``;\n          values are the active routes\n        - bottom level: name is ``'direction_id'``; values are 0s and 1s\n\n        If not ``split_directions``, then don't include the bottom level.\n\n        The time series has a timestamp index for a 24-hour period\n        sampled at the given frequency.\n        The maximum allowable frequency is 1 minute.\n        If ``trip_stats_subset`` is empty, then return an empty\n        DataFrame with the columns ``'num_trip_starts'``,\n        ``'num_trip_ends'``, ``'num_trips'``, ``'service_distance'``,\n        ``'service_duration'``, and ``'service_speed'``.\n\n    Notes\n    -----\n    - The time series is computed at a one-minute frequency, then\n      resampled at the end to the given frequency\n    - Trips that lack start or end times are ignored, so the the\n      aggregate ``num_trips`` across the day could be less than the\n      ``num_trips`` column of :func:`compute_route_stats_base`\n    - All trip departure times are taken modulo 24 hours.\n      So routes with trips that end past 23:59:59 will have all\n      their stats wrap around to the early morning of the time series,\n      except for their ``num_trip_ends`` indicator.\n      Trip endings past 23:59:59 not binned so that resampling the\n      ``num_trips`` indicator works efficiently.\n    - Note that the total number of trips for two consecutive time bins\n      t1 < t2 is the sum of the number of trips in bin t2 plus the\n      number of trip endings in bin t1.\n      Thus we can downsample the ``num_trips`` indicator by keeping\n      track of only one extra count, ``num_trip_ends``, and can avoid\n      recording individual trip IDs.\n    - All other indicators are downsampled by summing.\n    - Raise a ValueError if ``split_directions`` and no non-NaN\n      direction ID values present\n\n    \"\"\"\n    if trip_stats_subset.empty:\n        return pd.DataFrame()\n\n    tss = trip_stats_subset.copy()\n    if split_directions:\n        tss = tss.loc[lambda x: x.direction_id.notnull()].assign(\n            direction_id=lambda x: x.direction_id.astype(int)\n        )\n        if tss.empty:\n            raise ValueError(\n                \"At least one trip stats direction ID value \"\n                \"must be non-NaN.\"\n            )\n\n        # Alter route IDs to encode direction:\n        # <route ID>-0 and <route ID>-1 or <route ID>-NA\n        tss[\"route_id\"] = (\n            tss[\"route_id\"]\n            + \"-\"\n            + tss[\"direction_id\"].map(lambda x: str(int(x)))\n        )\n\n    routes = tss[\"route_id\"].unique()\n    # Build a dictionary of time series and then merge them all\n    # at the end.\n    # Assign a uniform generic date for the index\n    date_str = date_label\n    day_start = pd.to_datetime(date_str + \" 00:00:00\")\n    day_end = pd.to_datetime(date_str + \" 23:59:00\")\n    rng = pd.period_range(day_start, day_end, freq=\"Min\")\n    indicators = [\n        \"num_trip_starts\",\n        \"num_trip_ends\",\n        \"num_trips\",\n        \"service_duration\",\n        \"service_distance\",\n    ]\n\n    bins = [i for i in range(24 * 60)]  # One bin for each minute\n    num_bins = len(bins)\n\n    # Bin start and end times\n    def F(x):\n        return (hp.timestr_to_seconds(x) // 60) % (24 * 60)\n\n    tss[[\"start_index\", \"end_index\"]] = tss[\n        [\"start_time\", \"end_time\"]\n    ].applymap(F)\n    routes = sorted(set(tss[\"route_id\"].values))\n\n    # Bin each trip according to its start and end time and weight\n    series_by_route_by_indicator = {\n        indicator: {route: [0 for i in range(num_bins)] for route in routes}\n        for indicator in indicators\n    }\n    for index, row in tss.iterrows():\n        route = row[\"route_id\"]\n        start = row[\"start_index\"]\n        end = row[\"end_index\"]\n        distance = row[\"distance\"]\n\n        if start is None or np.isnan(start) or start == end:\n            continue\n\n        # Get bins to fill\n        if start <= end:\n            bins_to_fill = bins[start:end]\n        else:\n            bins_to_fill = bins[start:] + bins[:end]\n\n        # Bin trip\n        # Do num trip starts\n        series_by_route_by_indicator[\"num_trip_starts\"][route][start] += 1\n        # Don't mark trip ends for trips that run past midnight;\n        # allows for easy resampling of num_trips later\n        if start <= end:\n            series_by_route_by_indicator[\"num_trip_ends\"][route][end] += 1\n        # Do rest of indicators\n        for indicator in indicators[2:]:\n            if indicator == \"num_trips\":\n                weight = 1\n            elif indicator == \"service_duration\":\n                weight = 1 / 60\n            else:\n                weight = distance / len(bins_to_fill)\n            for bin in bins_to_fill:\n                series_by_route_by_indicator[indicator][route][bin] += weight\n\n    # Create one time series per indicator\n    rng = pd.date_range(date_str, periods=24 * 60, freq=\"Min\")\n    series_by_indicator = {\n        indicator: pd.DataFrame(\n            series_by_route_by_indicator[indicator], index=rng\n        ).fillna(0)\n        for indicator in indicators\n    }\n\n    # Combine all time series into one time series\n    g = hp.combine_time_series(\n        series_by_indicator, kind=\"route\", split_directions=split_directions\n    )\n\n    return hp.downsample(g, freq=freq)"}
{"code":"def known_pipettes() -> Sequence[str]:\n    \"\"\" List pipette IDs for which we have known overrides \"\"\"\n    return [fi.stem\n            for fi in CONFIG['pipette_config_overrides_dir'].iterdir()\n            if fi.is_file() and '.json' in fi.suffixes]","return_type":"Sequence[str]","function_name":"known_pipettes","stripped_code":"def known_pipettes():\n    \"\"\" List pipette IDs for which we have known overrides \"\"\"\n    return [fi.stem\n            for fi in CONFIG['pipette_config_overrides_dir'].iterdir()\n            if fi.is_file() and '.json' in fi.suffixes]"}
{"code":"def add_options(cls, parser: OptionManager) -> None:\n        \"\"\"\n        ``flake8`` api method to register new plugin options.\n\n        See :class:`.Configuration` docs for detailed options reference.\n\n        Arguments:\n            parser: ``flake8`` option parser instance.\n\n        \"\"\"\n        parser.add_option(\n            '--eradicate-aggressive',\n            default=False,\n            help=(\n                'Enables aggressive mode for eradicate; '\n                'this may result in false positives'\n            ),\n            action='store_true',\n            type=None,\n        )","return_type":"None","function_name":"Checker.add_options","stripped_code":"def add_options(cls, parser: OptionManager):\n        \"\"\"\n        ``flake8`` api method to register new plugin options.\n\n        See :class:`.Configuration` docs for detailed options reference.\n\n        Arguments:\n            parser: ``flake8`` option parser instance.\n\n        \"\"\"\n        parser.add_option(\n            '--eradicate-aggressive',\n            default=False,\n            help=(\n                'Enables aggressive mode for eradicate; '\n                'this may result in false positives'\n            ),\n            action='store_true',\n            type=None,\n        )"}
{"code":"def w_state(qubits: Union[int, Qubits]) -> State:\n    \"\"\"Return a W state on N qubits\"\"\"\n    N, qubits = qubits_count_tuple(qubits)\n    ket = np.zeros(shape=[2] * N)\n    for n in range(N):\n        idx = np.zeros(shape=N, dtype=int)\n        idx[n] += 1\n        ket[tuple(idx)] = 1 / sqrt(N)\n    return State(ket, qubits)","return_type":"State","function_name":"w_state","stripped_code":"def w_state(qubits: Union[int, Qubits]):\n    \"\"\"Return a W state on N qubits\"\"\"\n    N, qubits = qubits_count_tuple(qubits)\n    ket = np.zeros(shape=[2] * N)\n    for n in range(N):\n        idx = np.zeros(shape=N, dtype=int)\n        idx[n] += 1\n        ket[tuple(idx)] = 1 / sqrt(N)\n    return State(ket, qubits)"}
{"code":"def _set_type(self, other_type: Type = ANY_TYPE, signature=None) -> None:\n        \"\"\"\n        We override this method to do just one thing on top of ``ApplicationExpression._set_type``.\n        In lambda expressions of the form /x F(x), where the function is F and the argument is x,\n        we can use the type of F to infer the type of x. That is, if F is of type <a, b>, we can\n        resolve the type of x against a. We do this as the additional step after setting the type\n        of F(x).\n\n        So why does NLTK not already do this? NLTK assumes all variables (x) are of type entity\n        (e).  So it does not have to resolve the type of x anymore. However, this would cause type\n        inference failures in our case since x can bind to rows, numbers or cells, each of which\n        has a different type. To deal with this issue, we made X of type ANY_TYPE. Also, LambdaDCS\n        (and some other languages) contain a var function that indicate the usage of variables\n        within lambda functions. We map var to V, and made it of type <#1, #1>. We cannot leave X\n        as ANY_TYPE because that would propagate up the tree. We need to set its type when we have\n        the information about F. Hence this method. Note that the language may or may not contain\n        the var function. We deal with both cases below.\n        \"\"\"\n        super(DynamicTypeApplicationExpression, self)._set_type(other_type, signature)\n        # TODO(pradeep): Assuming the mapping of \"var\" function is \"V\". Do something better.\n        if isinstance(self.argument, ApplicationExpression) and str(self.argument.function) == \"V\":\n            # pylint: disable=protected-access\n            self.argument.argument._set_type(self.function.type.first)\n        if str(self.argument) == \"X\" and str(self.function) != \"V\":\n            # pylint: disable=protected-access\n            self.argument._set_type(self.function.type.first)","return_type":"None","function_name":"DynamicTypeApplicationExpression._set_type","stripped_code":"def _set_type(self, other_type: Type = ANY_TYPE, signature=None):\n        \"\"\"\n        We override this method to do just one thing on top of ``ApplicationExpression._set_type``.\n        In lambda expressions of the form /x F(x), where the function is F and the argument is x,\n        we can use the type of F to infer the type of x. That is, if F is of type <a, b>, we can\n        resolve the type of x against a. We do this as the additional step after setting the type\n        of F(x).\n\n        So why does NLTK not already do this? NLTK assumes all variables (x) are of type entity\n        (e).  So it does not have to resolve the type of x anymore. However, this would cause type\n        inference failures in our case since x can bind to rows, numbers or cells, each of which\n        has a different type. To deal with this issue, we made X of type ANY_TYPE. Also, LambdaDCS\n        (and some other languages) contain a var function that indicate the usage of variables\n        within lambda functions. We map var to V, and made it of type <#1, #1>. We cannot leave X\n        as ANY_TYPE because that would propagate up the tree. We need to set its type when we have\n        the information about F. Hence this method. Note that the language may or may not contain\n        the var function. We deal with both cases below.\n        \"\"\"\n        super(DynamicTypeApplicationExpression, self)._set_type(other_type, signature)\n        # TODO(pradeep): Assuming the mapping of \"var\" function is \"V\". Do something better.\n        if isinstance(self.argument, ApplicationExpression) and str(self.argument.function) == \"V\":\n            # pylint: disable=protected-access\n            self.argument.argument._set_type(self.function.type.first)\n        if str(self.argument) == \"X\" and str(self.function) != \"V\":\n            # pylint: disable=protected-access\n            self.argument._set_type(self.function.type.first)"}
{"code":"def build_toc_line(toc_line_no_indent: str,\n                   no_of_indentation_spaces: int = 0) -> str:\n    r\"\"\"Build the TOC line.\n\n    :parameter toc_line_no_indent: the TOC line without indentation.\n    :parameter no_of_indentation_spaces: the number of indentation spaces.\n         Defaults to ``0``.\n    :type toc_line_no_indent: str\n    :type no_of_indentation_spaces: int\n    :returns: toc_line, a single line of the table of contents.\n    :rtype: str\n    :raises: a built-in exception.\n    \"\"\"\n    assert no_of_indentation_spaces >= 0\n\n    indentation = no_of_indentation_spaces * ' '\n    toc_line = indentation + toc_line_no_indent\n\n    return toc_line","return_type":"str","function_name":"build_toc_line","stripped_code":"def build_toc_line(toc_line_no_indent: str,\n                   no_of_indentation_spaces: int = 0):\n    r\"\"\"Build the TOC line.\n\n    :parameter toc_line_no_indent: the TOC line without indentation.\n    :parameter no_of_indentation_spaces: the number of indentation spaces.\n         Defaults to ``0``.\n    :type toc_line_no_indent: str\n    :type no_of_indentation_spaces: int\n    :returns: toc_line, a single line of the table of contents.\n    :rtype: str\n    :raises: a built-in exception.\n    \"\"\"\n    assert no_of_indentation_spaces >= 0\n\n    indentation = no_of_indentation_spaces * ' '\n    toc_line = indentation + toc_line_no_indent\n\n    return toc_line"}
{"code":"def to_triple(self, fmt: str = \"medium\") -> dict:\n        \"\"\"Convert AST object to BEL triple\n\n        Args:\n            fmt (str): short, medium, long formatted BEL statements\n                short = short function and short relation format\n                medium = short function and long relation format\n                long = long function and long relation format\n\n        Returns:\n            dict: {'subject': <subject>, 'relation': <relations>, 'object': <object>}\n        \"\"\"\n\n        if self.ast:\n            return self.ast.to_triple(ast_obj=self.ast, fmt=fmt)\n        else:\n            return {}","return_type":"dict","function_name":"BEL.to_triple","stripped_code":"def to_triple(self, fmt: str = \"medium\"):\n        \"\"\"Convert AST object to BEL triple\n\n        Args:\n            fmt (str): short, medium, long formatted BEL statements\n                short = short function and short relation format\n                medium = short function and long relation format\n                long = long function and long relation format\n\n        Returns:\n            dict: {'subject': <subject>, 'relation': <relations>, 'object': <object>}\n        \"\"\"\n\n        if self.ast:\n            return self.ast.to_triple(ast_obj=self.ast, fmt=fmt)\n        else:\n            return {}"}
{"code":"def reply_poll(\n        self,\n        question: str,\n        options: List[str],\n        quote: bool = None,\n        disable_notification: bool = None,\n        reply_to_message_id: int = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *reply_poll* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_poll(\n                chat_id=message.chat.id,\n                question=\"Is Pyrogram the best?\",\n                options=[\"Yes\", \"Yes\"]\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_poll(\"Is Pyrogram the best?\", [\"Yes\", \"Yes\"])\n\n        Args:\n            question (``str``):\n                The poll question, as string.\n\n            options (List of ``str``):\n                The poll options, as list of strings (2 to 10 options are allowed).\n\n            quote (``bool``, *optional*):\n                If ``True``, the message will be sent as a reply to this message.\n                If *reply_to_message_id* is passed, this parameter will be ignored.\n                Defaults to ``True`` in group chats and ``False`` in private chats.\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message.\n\n            reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*):\n                Additional interface options. An object for an inline keyboard, custom reply keyboard,\n                instructions to remove reply keyboard or to force a reply from the user.\n\n        Returns:\n            On success, the sent :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        if quote is None:\n            quote = self.chat.type != \"private\"\n\n        if reply_to_message_id is None and quote:\n            reply_to_message_id = self.message_id\n\n        return self._client.send_poll(\n            chat_id=self.chat.id,\n            question=question,\n            options=options,\n            disable_notification=disable_notification,\n            reply_to_message_id=reply_to_message_id,\n            reply_markup=reply_markup\n        )","return_type":"\"Message\"","function_name":"Message.reply_poll","stripped_code":"def reply_poll(\n        self,\n        question: str,\n        options: List[str],\n        quote: bool = None,\n        disable_notification: bool = None,\n        reply_to_message_id: int = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ):\n        \"\"\"Bound method *reply_poll* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_poll(\n                chat_id=message.chat.id,\n                question=\"Is Pyrogram the best?\",\n                options=[\"Yes\", \"Yes\"]\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_poll(\"Is Pyrogram the best?\", [\"Yes\", \"Yes\"])\n\n        Args:\n            question (``str``):\n                The poll question, as string.\n\n            options (List of ``str``):\n                The poll options, as list of strings (2 to 10 options are allowed).\n\n            quote (``bool``, *optional*):\n                If ``True``, the message will be sent as a reply to this message.\n                If *reply_to_message_id* is passed, this parameter will be ignored.\n                Defaults to ``True`` in group chats and ``False`` in private chats.\n\n            disable_notification (``bool``, *optional*):\n                Sends the message silently.\n                Users will receive a notification with no sound.\n\n            reply_to_message_id (``int``, *optional*):\n                If the message is a reply, ID of the original message.\n\n            reply_markup (:obj:`InlineKeyboardMarkup` | :obj:`ReplyKeyboardMarkup` | :obj:`ReplyKeyboardRemove` | :obj:`ForceReply`, *optional*):\n                Additional interface options. An object for an inline keyboard, custom reply keyboard,\n                instructions to remove reply keyboard or to force a reply from the user.\n\n        Returns:\n            On success, the sent :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        if quote is None:\n            quote = self.chat.type != \"private\"\n\n        if reply_to_message_id is None and quote:\n            reply_to_message_id = self.message_id\n\n        return self._client.send_poll(\n            chat_id=self.chat.id,\n            question=question,\n            options=options,\n            disable_notification=disable_notification,\n            reply_to_message_id=reply_to_message_id,\n            reply_markup=reply_markup\n        )"}
{"code":"def get_untranscribed_prefixes_from_file(target_directory: Path) -> List[str]:\n    \"\"\"\n    The file \"untranscribed_prefixes.txt\" will specify prefixes which\n    do not have an associated transcription file if placed in the target directory.\n\n    This will fetch those prefixes from that file and will return an empty\n    list if that file does not exist.\n\n    See find_untranscribed_wavs function for finding untranscribed prefixes in an\n    experiment directory.\n\n    Returns:\n        A list of all untranscribed prefixes as specified in the file\n    \"\"\"\n\n    untranscribed_prefix_fn = target_directory / \"untranscribed_prefixes.txt\"\n    if untranscribed_prefix_fn.exists():\n        with untranscribed_prefix_fn.open() as f:\n            prefixes = f.readlines()\n\n        return [prefix.strip() for prefix in prefixes]\n    else:\n        #logger.warning(\"Attempting to get untranscribed prefixes but the file ({})\"\n        #                \" that should specify these does not exist\".format(untranscribed_prefix_fn))\n        pass\n    return []","return_type":"List[str]","function_name":"get_untranscribed_prefixes_from_file","stripped_code":"def get_untranscribed_prefixes_from_file(target_directory: Path):\n    \"\"\"\n    The file \"untranscribed_prefixes.txt\" will specify prefixes which\n    do not have an associated transcription file if placed in the target directory.\n\n    This will fetch those prefixes from that file and will return an empty\n    list if that file does not exist.\n\n    See find_untranscribed_wavs function for finding untranscribed prefixes in an\n    experiment directory.\n\n    Returns:\n        A list of all untranscribed prefixes as specified in the file\n    \"\"\"\n\n    untranscribed_prefix_fn = target_directory / \"untranscribed_prefixes.txt\"\n    if untranscribed_prefix_fn.exists():\n        with untranscribed_prefix_fn.open() as f:\n            prefixes = f.readlines()\n\n        return [prefix.strip() for prefix in prefixes]\n    else:\n        #logger.warning(\"Attempting to get untranscribed prefixes but the file ({})\"\n        #                \" that should specify these does not exist\".format(untranscribed_prefix_fn))\n        pass\n    return []"}
{"code":"def is_super(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return True if the node is referencing the \"super\" builtin function\n    \"\"\"\n    if getattr(node, \"name\", None) == \"super\" and node.root().name == BUILTINS_NAME:\n        return True\n    return False","return_type":"bool","function_name":"is_super","stripped_code":"def is_super(node: astroid.node_classes.NodeNG):\n    \"\"\"return True if the node is referencing the \"super\" builtin function\n    \"\"\"\n    if getattr(node, \"name\", None) == \"super\" and node.root().name == BUILTINS_NAME:\n        return True\n    return False"}
{"code":"def advance(delay: float) -> None:\n    \"\"\"\n    Pauses the current process for the given delay (in simulated time). The process will be resumed when the simulation\n    has advanced to the moment corresponding to `now() + delay`.\n    \"\"\"\n    if _logger is not None:\n        _log(INFO, \"Process\", local.name, \"advance\", delay=delay)\n    curr = Process.current()\n    rsim = curr.rsim\n    id_wakeup = rsim()._schedule(delay, curr.switch)  # type: ignore\n\n    try:\n        rsim()._gr.switch()                   # type: ignore\n    except Interrupt:\n        rsim()._cancel(id_wakeup)             # type: ignore\n        raise","return_type":"None","function_name":"advance","stripped_code":"def advance(delay: float):\n    \"\"\"\n    Pauses the current process for the given delay (in simulated time). The process will be resumed when the simulation\n    has advanced to the moment corresponding to `now() + delay`.\n    \"\"\"\n    if _logger is not None:\n        _log(INFO, \"Process\", local.name, \"advance\", delay=delay)\n    curr = Process.current()\n    rsim = curr.rsim\n    id_wakeup = rsim()._schedule(delay, curr.switch)  # type: ignore\n\n    try:\n        rsim()._gr.switch()                   # type: ignore\n    except Interrupt:\n        rsim()._cancel(id_wakeup)             # type: ignore\n        raise"}
{"code":"def softmax(x: np.ndarray,\n            b: float = 1.0) -> np.ndarray:\n    r\"\"\"\n    Standard softmax function:\n\n    .. math::\n\n        P_i = \\frac {e ^ {\\beta \\cdot x_i}} { \\sum_{i}{\\beta \\cdot x_i} }\n\n    Args:\n        x: vector (``numpy.array``) of values\n        b: exploration parameter :math:`\\beta`, or inverse temperature\n            [Daw2009], or :math:`1/t`; see below\n\n    Returns:\n        vector of probabilities corresponding to the input values\n\n    where:\n\n    - :math:`t` is temperature (towards infinity: all actions equally likely;\n      towards zero: probability of action with highest value tends to 1)\n    - Temperature is not used directly as optimizers may take it to zero,\n      giving an infinity; use inverse temperature instead.\n    - [Daw2009] Daw ND, \"Trial-by-trial data analysis using computational\n      methods\", 2009/2011; in \"Decision Making, Affect, and Learning: Attention\n      and Performance XXIII\"; Delgado MR, Phelps EA, Robbins TW (eds),\n      Oxford University Press.\n\n    \"\"\"\n    constant = np.mean(x)\n    products = x * b - constant\n    # ... softmax is invariant to addition of a constant: Daw article and\n    # http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html#b\n    # noinspection PyUnresolvedReferences\n    if products.max() > sys.float_info.max_exp:\n        # ... max_exp for base e; max_10_exp for base 10\n        log.warning(\"OVERFLOW in softmax(): x = {}, b = {}, constant = {}, \"\n                    \"x*b - constant = {}\".format(x, b, constant, products))\n        # map the maximum to 1, other things to zero\n        n = len(x)\n        index_of_max = np.argmax(products)\n        answer = np.zeros(n)\n        answer[index_of_max] = 1.0\n    else:\n        # noinspection PyUnresolvedReferences\n        exponented = np.exp(products)\n        answer = exponented / np.sum(exponented)\n    return answer","return_type":"np.ndarray","function_name":"softmax","stripped_code":"def softmax(x: np.ndarray,\n            b: float = 1.0):\n    r\"\"\"\n    Standard softmax function:\n\n    .. math::\n\n        P_i = \\frac {e ^ {\\beta \\cdot x_i}} { \\sum_{i}{\\beta \\cdot x_i} }\n\n    Args:\n        x: vector (``numpy.array``) of values\n        b: exploration parameter :math:`\\beta`, or inverse temperature\n            [Daw2009], or :math:`1/t`; see below\n\n    Returns:\n        vector of probabilities corresponding to the input values\n\n    where:\n\n    - :math:`t` is temperature (towards infinity: all actions equally likely;\n      towards zero: probability of action with highest value tends to 1)\n    - Temperature is not used directly as optimizers may take it to zero,\n      giving an infinity; use inverse temperature instead.\n    - [Daw2009] Daw ND, \"Trial-by-trial data analysis using computational\n      methods\", 2009/2011; in \"Decision Making, Affect, and Learning: Attention\n      and Performance XXIII\"; Delgado MR, Phelps EA, Robbins TW (eds),\n      Oxford University Press.\n\n    \"\"\"\n    constant = np.mean(x)\n    products = x * b - constant\n    # ... softmax is invariant to addition of a constant: Daw article and\n    # http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html#b\n    # noinspection PyUnresolvedReferences\n    if products.max() > sys.float_info.max_exp:\n        # ... max_exp for base e; max_10_exp for base 10\n        log.warning(\"OVERFLOW in softmax(): x = {}, b = {}, constant = {}, \"\n                    \"x*b - constant = {}\".format(x, b, constant, products))\n        # map the maximum to 1, other things to zero\n        n = len(x)\n        index_of_max = np.argmax(products)\n        answer = np.zeros(n)\n        answer[index_of_max] = 1.0\n    else:\n        # noinspection PyUnresolvedReferences\n        exponented = np.exp(products)\n        answer = exponented / np.sum(exponented)\n    return answer"}
{"code":"def is_not_blocked(self, item: str) -> bool:\n        \"\"\"\n        Check if an item is _not_ already on the blacklist\n\n        :param str item: The item to check\n        :return: True, when the item is _not_ on the blacklist\n        :rtype: bool\n        \"\"\"\n        assert item is not None\n        item = self._encode_item(item)\n        connection = self.__get_connection()\n        key = self.__redis_conf['blacklist_template'].format(item)\n        value = connection.get(key)\n        if value is None:\n            BlackRed.__release_connection(connection)\n            return True\n        if self.__redis_conf['blacklist_refresh_ttl']:\n            connection.expire(key, self.__redis_conf['blacklist_ttl'])\n        BlackRed.__release_connection(connection)\n        return False","return_type":"bool","function_name":"BlackRed.is_not_blocked","stripped_code":"def is_not_blocked(self, item: str):\n        \"\"\"\n        Check if an item is _not_ already on the blacklist\n\n        :param str item: The item to check\n        :return: True, when the item is _not_ on the blacklist\n        :rtype: bool\n        \"\"\"\n        assert item is not None\n        item = self._encode_item(item)\n        connection = self.__get_connection()\n        key = self.__redis_conf['blacklist_template'].format(item)\n        value = connection.get(key)\n        if value is None:\n            BlackRed.__release_connection(connection)\n            return True\n        if self.__redis_conf['blacklist_refresh_ttl']:\n            connection.expire(key, self.__redis_conf['blacklist_ttl'])\n        BlackRed.__release_connection(connection)\n        return False"}
{"code":"def __can_attempt(self, namespace: str, add_attempt=True) -> bool:\n        \"\"\"\n        Checks if a namespace is rate limited or not with including/excluding the current call\n\n        :param namespace: Rate limiting namespace\n        :type namespace: str\n\n        :param add_attempt: Boolean value indicating if the current call should be considered as an attempt or not\n        :type add_attempt: bool\n\n        :return: Returns true if attempt can go ahead under current rate limiting rules, false otherwise\n        \"\"\"\n        can_attempt = False\n        if not PyRateLimit.redis_helper:\n            raise PyRateLimitException(\"redis connection information not provided\")\n        connection = PyRateLimit.redis_helper.get_atomic_connection()\n        current_time = int(round(time.time() * 1000000))\n        old_time_limit = current_time - (self.period * 1000000)\n        connection.zremrangebyscore(namespace, 0, old_time_limit)\n        connection.expire(namespace, self.period)\n        if add_attempt:\n            current_count = 0\n            connection.zadd(namespace, current_time, current_time)\n        else:\n            current_count = 1   # initialize at 1 to compensate the case that this attempt is not getting counted\n        connection.zcard(namespace)\n        redis_result = connection.execute()\n        current_count += redis_result[-1]\n        if current_count <= self.limit:\n            can_attempt = True\n        return can_attempt","return_type":"bool","function_name":"PyRateLimit.__can_attempt","stripped_code":"def __can_attempt(self, namespace: str, add_attempt=True):\n        \"\"\"\n        Checks if a namespace is rate limited or not with including/excluding the current call\n\n        :param namespace: Rate limiting namespace\n        :type namespace: str\n\n        :param add_attempt: Boolean value indicating if the current call should be considered as an attempt or not\n        :type add_attempt: bool\n\n        :return: Returns true if attempt can go ahead under current rate limiting rules, false otherwise\n        \"\"\"\n        can_attempt = False\n        if not PyRateLimit.redis_helper:\n            raise PyRateLimitException(\"redis connection information not provided\")\n        connection = PyRateLimit.redis_helper.get_atomic_connection()\n        current_time = int(round(time.time() * 1000000))\n        old_time_limit = current_time - (self.period * 1000000)\n        connection.zremrangebyscore(namespace, 0, old_time_limit)\n        connection.expire(namespace, self.period)\n        if add_attempt:\n            current_count = 0\n            connection.zadd(namespace, current_time, current_time)\n        else:\n            current_count = 1   # initialize at 1 to compensate the case that this attempt is not getting counted\n        connection.zcard(namespace)\n        redis_result = connection.execute()\n        current_count += redis_result[-1]\n        if current_count <= self.limit:\n            can_attempt = True\n        return can_attempt"}
{"code":"def get_volume_details(self, volume_name: str) -> dict:\n        \"\"\"Get details of the volume.\n\n        Args:\n            volume_name (str): Name of the volume\n\n        Returns:\n            dict, details of the volume\n\n        \"\"\"\n        if volume_name not in self.volumes:\n            raise RuntimeError('No such volume found: ', volume_name)\n\n        volume = self._client.volumes.get(volume_name)\n        return volume.attrs","return_type":"dict","function_name":"DockerSwarmClient.get_volume_details","stripped_code":"def get_volume_details(self, volume_name: str):\n        \"\"\"Get details of the volume.\n\n        Args:\n            volume_name (str): Name of the volume\n\n        Returns:\n            dict, details of the volume\n\n        \"\"\"\n        if volume_name not in self.volumes:\n            raise RuntimeError('No such volume found: ', volume_name)\n\n        volume = self._client.volumes.get(volume_name)\n        return volume.attrs"}
{"code":"def encode(self, text: str) -> str:\n        \"\"\"Encode @username into <@id> or <!alias>.\"\"\"\n\n        def callback(match: Match) -> str:\n            name = match.group(\"name\").lower()\n            if name in [\"here\", \"everyone\", \"channel\"]:\n                return f\"<!{name}>\"\n            else:\n                for user in self.users.values():\n                    if user.name == name:\n                        return f\"<@{user.id}>\"\n            return match.group(0)\n\n        return self.encode_re.sub(callback, text)","return_type":"str","function_name":"Slack.encode","stripped_code":"def encode(self, text: str):\n        \"\"\"Encode @username into <@id> or <!alias>.\"\"\"\n\n        def callback(match: Match) -> str:\n            name = match.group(\"name\").lower()\n            if name in [\"here\", \"everyone\", \"channel\"]:\n                return f\"<!{name}>\"\n            else:\n                for user in self.users.values():\n                    if user.name == name:\n                        return f\"<@{user.id}>\"\n            return match.group(0)\n\n        return self.encode_re.sub(callback, text)"}
{"code":"def getblockhash(self, index: int) -> str:\n        '''Returns the hash of the block at ; index 0 is the genesis block.'''\n\n        return cast(str, self.api_fetch('getblockhash?index=' + str(index)))","return_type":"str","function_name":"Explorer.getblockhash","stripped_code":"def getblockhash(self, index: int):\n        '''Returns the hash of the block at ; index 0 is the genesis block.'''\n\n        return cast(str, self.api_fetch('getblockhash?index=' + str(index)))"}
{"code":"def _stmt_check(self, req: set, legal: set, stmt: dict) -> dict:\n        \"\"\"\n        This method checks to make sure that the proc has all required statements and removes any statements\n        aren't valid. Missing required statements is an error. Extra statements are not.\n        :param req: set\n        :param legal: set\n        :param stmt: dict\n        :return: dictionary of verified statements\n        \"\"\"\n        # debug the argument list\n        if self.logger.level == 10:\n            for k, v in stmt.items():\n                print(\"Key: \" + k + \", Value: \" + str(v) + \", Type: \" + str(type(v)))\n\n        # required statements\n        reqSet = req\n        if len(reqSet):\n            self.logger.debug(\"reqSet: {}\".format(reqSet))\n            missing_set = reqSet.difference(set(stmt.keys()))\n            if missing_set:\n                if not stmt.get(\n                        'score'):  # till we handle either/or required. proc can be called more than one way w/ diff requirements\n                    raise SyntaxError(\n                        \"You are missing %d required statements:\\n%s\" % (len(missing_set), str(missing_set)))\n\n        # legal statements\n        legalSet = legal\n        if len(legalSet):\n            self.logger.debug(\"legalSet: {}\".format(legalSet))\n            if len(reqSet):\n                totSet = legalSet | reqSet\n            else:\n                totSet = legalSet\n            generalSet = {'ODSGraphics', 'stmtpassthrough', 'targOpts', 'procopts'}\n            extraSet = set(stmt.keys() - generalSet).difference(totSet)  # find keys not in legal or required sets\n            if extraSet:\n                self.logger.debug(\"extraSet: {}\".format(extraSet))\n                for item in extraSet:\n                    stmt.pop(item, None)\n                warnings.warn(\n                    \"The following {} statements are invalid and will be ignored:\\n{}\".format(len(extraSet), extraSet))\n        self.logger.debug(\"stmt: {}\".format(stmt))\n        return stmt","return_type":"dict","function_name":"SASProcCommons._stmt_check","stripped_code":"def _stmt_check(self, req: set, legal: set, stmt: dict):\n        \"\"\"\n        This method checks to make sure that the proc has all required statements and removes any statements\n        aren't valid. Missing required statements is an error. Extra statements are not.\n        :param req: set\n        :param legal: set\n        :param stmt: dict\n        :return: dictionary of verified statements\n        \"\"\"\n        # debug the argument list\n        if self.logger.level == 10:\n            for k, v in stmt.items():\n                print(\"Key: \" + k + \", Value: \" + str(v) + \", Type: \" + str(type(v)))\n\n        # required statements\n        reqSet = req\n        if len(reqSet):\n            self.logger.debug(\"reqSet: {}\".format(reqSet))\n            missing_set = reqSet.difference(set(stmt.keys()))\n            if missing_set:\n                if not stmt.get(\n                        'score'):  # till we handle either/or required. proc can be called more than one way w/ diff requirements\n                    raise SyntaxError(\n                        \"You are missing %d required statements:\\n%s\" % (len(missing_set), str(missing_set)))\n\n        # legal statements\n        legalSet = legal\n        if len(legalSet):\n            self.logger.debug(\"legalSet: {}\".format(legalSet))\n            if len(reqSet):\n                totSet = legalSet | reqSet\n            else:\n                totSet = legalSet\n            generalSet = {'ODSGraphics', 'stmtpassthrough', 'targOpts', 'procopts'}\n            extraSet = set(stmt.keys() - generalSet).difference(totSet)  # find keys not in legal or required sets\n            if extraSet:\n                self.logger.debug(\"extraSet: {}\".format(extraSet))\n                for item in extraSet:\n                    stmt.pop(item, None)\n                warnings.warn(\n                    \"The following {} statements are invalid and will be ignored:\\n{}\".format(len(extraSet), extraSet))\n        self.logger.debug(\"stmt: {}\".format(stmt))\n        return stmt"}
{"code":"def load(self, meta: ResourceDescription) -> Any:\n        \"\"\"\n        Loads a resource or return existing one\n\n        :param meta: The resource description\n        \"\"\"\n        self._check_meta(meta)\n        self.resolve_loader(meta)\n        return meta.loader_cls(meta).load()","return_type":"Any","function_name":"BaseRegistry.load","stripped_code":"def load(self, meta: ResourceDescription):\n        \"\"\"\n        Loads a resource or return existing one\n\n        :param meta: The resource description\n        \"\"\"\n        self._check_meta(meta)\n        self.resolve_loader(meta)\n        return meta.loader_cls(meta).load()"}
{"code":"def price_in_btc(self, minimum: float = 0, maximum: float = 2) -> str:\n        \"\"\"Generate random price in BTC.\n\n        :param minimum: Minimum value of price.\n        :param maximum: Maximum value of price.\n        :return: Price in BTC.\n        \"\"\"\n        return '{} BTC'.format(\n            self.random.uniform(\n                minimum,\n                maximum,\n                precision=7,\n            ),\n        )","return_type":"str","function_name":"Business.price_in_btc","stripped_code":"def price_in_btc(self, minimum: float = 0, maximum: float = 2):\n        \"\"\"Generate random price in BTC.\n\n        :param minimum: Minimum value of price.\n        :param maximum: Maximum value of price.\n        :return: Price in BTC.\n        \"\"\"\n        return '{} BTC'.format(\n            self.random.uniform(\n                minimum,\n                maximum,\n                precision=7,\n            ),\n        )"}
{"code":"def mpim_history(self, *, channel: str, **kwargs) -> SlackResponse:\n        \"\"\"Fetches history of messages and events from a multiparty direct message.\n\n        Args:\n            channel (str): Multiparty direct message to fetch history for. e.g. 'G1234567890'\n        \"\"\"\n        kwargs.update({\"channel\": channel})\n        return self.api_call(\"mpim.history\", http_verb=\"GET\", params=kwargs)","return_type":"SlackResponse","function_name":"WebClient.mpim_history","stripped_code":"def mpim_history(self, *, channel: str, **kwargs):\n        \"\"\"Fetches history of messages and events from a multiparty direct message.\n\n        Args:\n            channel (str): Multiparty direct message to fetch history for. e.g. 'G1234567890'\n        \"\"\"\n        kwargs.update({\"channel\": channel})\n        return self.api_call(\"mpim.history\", http_verb=\"GET\", params=kwargs)"}
{"code":"def last_modified(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        httpdate = self._headers.get(hdrs.LAST_MODIFIED)\n        if httpdate is not None:\n            timetuple = parsedate(httpdate)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None","return_type":"Optional[datetime.datetime]","function_name":"StreamResponse.last_modified","stripped_code":"def last_modified(self):\n        \"\"\"The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        httpdate = self._headers.get(hdrs.LAST_MODIFIED)\n        if httpdate is not None:\n            timetuple = parsedate(httpdate)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None"}
{"code":"def _tree_line(self, no_type: bool = False) -> str:\n        \"\"\"Return the receiver's contribution to tree diagram.\"\"\"\n        return super()._tree_line() + (\"!\" if self.presence else \"\")","return_type":"str","function_name":"ContainerNode._tree_line","stripped_code":"def _tree_line(self, no_type: bool = False):\n        \"\"\"Return the receiver's contribution to tree diagram.\"\"\"\n        return super()._tree_line() + (\"!\" if self.presence else \"\")"}
{"code":"def ensure_caches_alive(max_retries: int = 100,\n                        retry_timeout: int = 5,\n                        exit_on_failure: bool = True) -> bool:\n    \"\"\"\n    Checks every cache backend alias in ``settings.CACHES`` until it becomes available. After ``max_retries``\n    attempts to reach any backend are failed it returns ``False``. If ``exit_on_failure`` is set it shuts down with\n    ``exit(1)``.\n\n    It sets the ``django-docker-helpers:available-check`` key for every cache backend to ensure\n    it's receiving connections. If check is passed the key is deleted.\n\n    :param exit_on_failure: set to ``True`` if there's no sense to continue\n    :param int max_retries: a number of attempts to reach cache backend, default is ``100``\n    :param int retry_timeout: a timeout in seconds between attempts, default is ``5``\n    :return: ``True`` if all backends are available ``False`` if any backend check failed\n    \"\"\"\n    for cache_alias in settings.CACHES.keys():\n        cache = caches[cache_alias]\n        wf('Checking if the cache backed is accessible for the alias `%s`... ' % cache_alias, False)\n        for i in range(max_retries):\n            try:\n                cache.set('django-docker-helpers:available-check', '1')\n                assert cache.get('django-docker-helpers:available-check') == '1'\n                cache.delete('django-docker-helpers:available-check')\n                wf('[+]\\n')\n                break\n            except Exception as e:\n                wf(str(e) + '\\n')\n                sleep(retry_timeout)\n        else:\n            wf('Tried %s time(s). Shutting down.\\n' % max_retries)\n            exit_on_failure and exit(1)\n            return False\n    return True","return_type":"bool","function_name":"ensure_caches_alive","stripped_code":"def ensure_caches_alive(max_retries: int = 100,\n                        retry_timeout: int = 5,\n                        exit_on_failure: bool = True):\n    \"\"\"\n    Checks every cache backend alias in ``settings.CACHES`` until it becomes available. After ``max_retries``\n    attempts to reach any backend are failed it returns ``False``. If ``exit_on_failure`` is set it shuts down with\n    ``exit(1)``.\n\n    It sets the ``django-docker-helpers:available-check`` key for every cache backend to ensure\n    it's receiving connections. If check is passed the key is deleted.\n\n    :param exit_on_failure: set to ``True`` if there's no sense to continue\n    :param int max_retries: a number of attempts to reach cache backend, default is ``100``\n    :param int retry_timeout: a timeout in seconds between attempts, default is ``5``\n    :return: ``True`` if all backends are available ``False`` if any backend check failed\n    \"\"\"\n    for cache_alias in settings.CACHES.keys():\n        cache = caches[cache_alias]\n        wf('Checking if the cache backed is accessible for the alias `%s`... ' % cache_alias, False)\n        for i in range(max_retries):\n            try:\n                cache.set('django-docker-helpers:available-check', '1')\n                assert cache.get('django-docker-helpers:available-check') == '1'\n                cache.delete('django-docker-helpers:available-check')\n                wf('[+]\\n')\n                break\n            except Exception as e:\n                wf(str(e) + '\\n')\n                sleep(retry_timeout)\n        else:\n            wf('Tried %s time(s). Shutting down.\\n' % max_retries)\n            exit_on_failure and exit(1)\n            return False\n    return True"}
{"code":"def disable_category(self, category: str, message_to_print: str) -> None:\n        \"\"\"\n        Disable an entire category of commands\n        :param category: the category to disable\n        :param message_to_print: what to print when anything in this category is run or help is called on it\n                                 while disabled\n\n                                 The variable COMMAND_NAME can be used as a placeholder for the name of the\n                                 command being disabled.\n                                 ex: message_to_print = \"{} is currently disabled\".format(COMMAND_NAME)\n        \"\"\"\n        all_commands = self.get_all_commands()\n\n        for cmd_name in all_commands:\n            func = self.cmd_func(cmd_name)\n            if hasattr(func, HELP_CATEGORY) and getattr(func, HELP_CATEGORY) == category:\n                self.disable_command(cmd_name, message_to_print)","return_type":"None","function_name":"Cmd.disable_category","stripped_code":"def disable_category(self, category: str, message_to_print: str):\n        \"\"\"\n        Disable an entire category of commands\n        :param category: the category to disable\n        :param message_to_print: what to print when anything in this category is run or help is called on it\n                                 while disabled\n\n                                 The variable COMMAND_NAME can be used as a placeholder for the name of the\n                                 command being disabled.\n                                 ex: message_to_print = \"{} is currently disabled\".format(COMMAND_NAME)\n        \"\"\"\n        all_commands = self.get_all_commands()\n\n        for cmd_name in all_commands:\n            func = self.cmd_func(cmd_name)\n            if hasattr(func, HELP_CATEGORY) and getattr(func, HELP_CATEGORY) == category:\n                self.disable_command(cmd_name, message_to_print)"}
{"code":"def from_spcm(filepath, name=None, *, delimiter=\",\", parent=None, verbose=True) -> Data:\n    \"\"\"Create a ``Data`` object from a Becker & Hickl spcm file (ASCII-exported, ``.asc``).\n\n    If provided, setup parameters are stored in the ``attrs`` dictionary of the ``Data`` object.\n\n    See the `spcm`__ software hompage for more info.\n\n    __ http://www.becker-hickl.com/software/spcm.htm\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to SPC-xxx .asc file.\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Name to give to the created data object. If None, filename is used.\n        Default is None.\n    delimiter : string (optional)\n        The string used to separate values. Default is ','.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    WrightTools.data.Data object\n    \"\"\"\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    # check filepath\n    if not \".asc\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".asc\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create headers dictionary\n    headers = collections.OrderedDict()\n    header_lines = 0\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    while True:\n        line = f.readline().strip()\n        header_lines += 1\n        if len(line) == 0:\n            break\n        else:\n            key, value = line.split(\":\", 1)\n            if key.strip() == \"Revision\":\n                headers[\"resolution\"] = int(value.strip(\" bits ADC\"))\n            else:\n                headers[key.strip()] = value.strip()\n    line = f.readline().strip()\n    while \"_BEGIN\" in line:\n        header_lines += 1\n        section = line.split(\"_BEGIN\")[0]\n        while True:\n            line = f.readline().strip()\n            header_lines += 1\n            if section + \"_END\" in line:\n                break\n            if section == \"SYS_PARA\":\n                use_type = {\n                    \"B\": lambda b: int(b) == 1,\n                    \"C\": str,  # e.g. #SP [SP_OVERFL,C,N]\n                    \"F\": float,\n                    \"I\": int,\n                    \"L\": int,  # e.g. #DI [DI_MAXCNT,L,128]\n                    \"S\": str,\n                    \"U\": int,  # unsigned int?\n                }\n                item = line[line.find(\"[\") + 1 : line.find(\"]\")].split(\",\")\n                key = item[0]\n                value = use_type[item[1]](item[2])\n                headers[key] = value\n            else:\n                splitted = line.split()\n                value = splitted[-1][1:-1].split(\",\")\n                key = \" \".join(splitted[:-1])\n                headers[key] = value\n        line = f.readline().strip()\n        if \"END\" in line:\n            header_lines += 1\n            break\n    if \"Date\" in headers.keys() and \"Time\" in headers.keys():\n        # NOTE:  reports created in local time, no-way to calculate absolute time\n        created = \" \".join([headers[\"Date\"], headers[\"Time\"]])\n        created = time.strptime(created, \"%Y-%m-%d %H:%M:%S\")\n        created = timestamp.TimeStamp(time.mktime(created)).RFC3339\n        headers[\"created\"] = created\n\n    # initialize data object\n    kwargs = {\"name\": name, \"kind\": \"spcm\", \"source\": filestr, **headers}\n    if parent:\n        data = parent.create_data(**kwargs)\n    else:\n        data = Data(**kwargs)\n    # import data\n    f.seek(0)\n    arr = np.genfromtxt(\n        f, skip_header=(header_lines + 1), skip_footer=1, delimiter=delimiter, unpack=True\n    )\n    f.close()\n    # construct data\n    data.create_variable(name=\"time\", values=arr[0], units=\"ns\")\n    data.create_channel(name=\"counts\", values=arr[1])\n    data.transform(\"time\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  kind: {0}\".format(data.kind))\n        print(\"  range: {0} to {1} (ns)\".format(data.time[0], data.time[-1]))\n        print(\"  size: {0}\".format(data.size))\n        if \"SP_COL_T\" in data.attrs.keys():\n            print(\"  collection time:  {0} sec\".format(data.attrs[\"SP_COL_T\"]))\n    return data","return_type":"Data","function_name":"from_spcm","stripped_code":"def from_spcm(filepath, name=None, *, delimiter=\",\", parent=None, verbose=True):\n    \"\"\"Create a ``Data`` object from a Becker & Hickl spcm file (ASCII-exported, ``.asc``).\n\n    If provided, setup parameters are stored in the ``attrs`` dictionary of the ``Data`` object.\n\n    See the `spcm`__ software hompage for more info.\n\n    __ http://www.becker-hickl.com/software/spcm.htm\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to SPC-xxx .asc file.\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Name to give to the created data object. If None, filename is used.\n        Default is None.\n    delimiter : string (optional)\n        The string used to separate values. Default is ','.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    WrightTools.data.Data object\n    \"\"\"\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    # check filepath\n    if not \".asc\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".asc\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create headers dictionary\n    headers = collections.OrderedDict()\n    header_lines = 0\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    while True:\n        line = f.readline().strip()\n        header_lines += 1\n        if len(line) == 0:\n            break\n        else:\n            key, value = line.split(\":\", 1)\n            if key.strip() == \"Revision\":\n                headers[\"resolution\"] = int(value.strip(\" bits ADC\"))\n            else:\n                headers[key.strip()] = value.strip()\n    line = f.readline().strip()\n    while \"_BEGIN\" in line:\n        header_lines += 1\n        section = line.split(\"_BEGIN\")[0]\n        while True:\n            line = f.readline().strip()\n            header_lines += 1\n            if section + \"_END\" in line:\n                break\n            if section == \"SYS_PARA\":\n                use_type = {\n                    \"B\": lambda b: int(b) == 1,\n                    \"C\": str,  # e.g. #SP [SP_OVERFL,C,N]\n                    \"F\": float,\n                    \"I\": int,\n                    \"L\": int,  # e.g. #DI [DI_MAXCNT,L,128]\n                    \"S\": str,\n                    \"U\": int,  # unsigned int?\n                }\n                item = line[line.find(\"[\") + 1 : line.find(\"]\")].split(\",\")\n                key = item[0]\n                value = use_type[item[1]](item[2])\n                headers[key] = value\n            else:\n                splitted = line.split()\n                value = splitted[-1][1:-1].split(\",\")\n                key = \" \".join(splitted[:-1])\n                headers[key] = value\n        line = f.readline().strip()\n        if \"END\" in line:\n            header_lines += 1\n            break\n    if \"Date\" in headers.keys() and \"Time\" in headers.keys():\n        # NOTE:  reports created in local time, no-way to calculate absolute time\n        created = \" \".join([headers[\"Date\"], headers[\"Time\"]])\n        created = time.strptime(created, \"%Y-%m-%d %H:%M:%S\")\n        created = timestamp.TimeStamp(time.mktime(created)).RFC3339\n        headers[\"created\"] = created\n\n    # initialize data object\n    kwargs = {\"name\": name, \"kind\": \"spcm\", \"source\": filestr, **headers}\n    if parent:\n        data = parent.create_data(**kwargs)\n    else:\n        data = Data(**kwargs)\n    # import data\n    f.seek(0)\n    arr = np.genfromtxt(\n        f, skip_header=(header_lines + 1), skip_footer=1, delimiter=delimiter, unpack=True\n    )\n    f.close()\n    # construct data\n    data.create_variable(name=\"time\", values=arr[0], units=\"ns\")\n    data.create_channel(name=\"counts\", values=arr[1])\n    data.transform(\"time\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  kind: {0}\".format(data.kind))\n        print(\"  range: {0} to {1} (ns)\".format(data.time[0], data.time[-1]))\n        print(\"  size: {0}\".format(data.size))\n        if \"SP_COL_T\" in data.attrs.keys():\n            print(\"  collection time:  {0} sec\".format(data.attrs[\"SP_COL_T\"]))\n    return data"}
{"code":"def _write_cpr(self, f, cType, parameter) -> int:\n        '''\n        Write compression info to the end of the file in a CPR.\n        '''\n        f.seek(0, 2)\n        byte_loc = f.tell()\n        block_size = CDF.CPR_BASE_SIZE64 + 4\n        section_type = CDF.CPR_\n        rfuA = 0\n        pCount = 1\n\n        cpr = bytearray(block_size)\n        cpr[0:8] = struct.pack('>q', block_size)\n        cpr[8:12] = struct.pack('>i', section_type)\n        cpr[12:16] = struct.pack('>i', cType)\n        cpr[16:20] = struct.pack('>i', rfuA)\n        cpr[20:24] = struct.pack('>i', pCount)\n        cpr[24:28] = struct.pack('>i', parameter)\n        f.write(cpr)\n\n        return byte_loc","return_type":"int","function_name":"CDF._write_cpr","stripped_code":"def _write_cpr(self, f, cType, parameter):\n        '''\n        Write compression info to the end of the file in a CPR.\n        '''\n        f.seek(0, 2)\n        byte_loc = f.tell()\n        block_size = CDF.CPR_BASE_SIZE64 + 4\n        section_type = CDF.CPR_\n        rfuA = 0\n        pCount = 1\n\n        cpr = bytearray(block_size)\n        cpr[0:8] = struct.pack('>q', block_size)\n        cpr[8:12] = struct.pack('>i', section_type)\n        cpr[12:16] = struct.pack('>i', cType)\n        cpr[16:20] = struct.pack('>i', rfuA)\n        cpr[20:24] = struct.pack('>i', pCount)\n        cpr[24:28] = struct.pack('>i', parameter)\n        f.write(cpr)\n\n        return byte_loc"}
{"code":"def is_jsonable(obj) -> bool:\n    \"\"\"\n    Check if an object is jsonable.\n\n    An object is jsonable if it is json serialisable and by loading its json representation the same object is recovered.\n    Parameters\n    ----------\n    obj : \n        Python object\n\n    Returns\n    -------\n    bool\n\n    >>> is_jsonable([1,2,3])\n    True\n    >>> is_jsonable((1,2,3))\n    False\n    >>> is_jsonable({'a':True,'b':1,'c':None})\n    True\n    \"\"\"\n    try:\n        return obj==json.loads(json.dumps(obj))\n    except TypeError:\n        return False\n    except:\n        raise","return_type":"bool","function_name":"is_jsonable","stripped_code":"def is_jsonable(obj):\n    \"\"\"\n    Check if an object is jsonable.\n\n    An object is jsonable if it is json serialisable and by loading its json representation the same object is recovered.\n    Parameters\n    ----------\n    obj : \n        Python object\n\n    Returns\n    -------\n    bool\n\n    >>> is_jsonable([1,2,3])\n    True\n    >>> is_jsonable((1,2,3))\n    False\n    >>> is_jsonable({'a':True,'b':1,'c':None})\n    True\n    \"\"\"\n    try:\n        return obj==json.loads(json.dumps(obj))\n    except TypeError:\n        return False\n    except:\n        raise"}
{"code":"def create_identity_from_private_key(self, label: str, pwd: str, private_key: str) -> Identity:\n        \"\"\"\n        This interface is used to create identity based on given label, password and private key.\n\n        :param label: a label for identity.\n        :param pwd: a password which will be used to encrypt and decrypt the private key.\n        :param private_key: a private key in the form of string.\n        :return: if succeed, an Identity object will be returned.\n        \"\"\"\n        salt = get_random_hex_str(16)\n        identity = self.__create_identity(label, pwd, salt, private_key)\n        return identity","return_type":"Identity","function_name":"WalletManager.create_identity_from_private_key","stripped_code":"def create_identity_from_private_key(self, label: str, pwd: str, private_key: str):\n        \"\"\"\n        This interface is used to create identity based on given label, password and private key.\n\n        :param label: a label for identity.\n        :param pwd: a password which will be used to encrypt and decrypt the private key.\n        :param private_key: a private key in the form of string.\n        :return: if succeed, an Identity object will be returned.\n        \"\"\"\n        salt = get_random_hex_str(16)\n        identity = self.__create_identity(label, pwd, salt, private_key)\n        return identity"}
{"code":"def matchesTripleConstraint(cntxt: Context, t: RDFTriple, expr: ShExJ.TripleConstraint, c: DebugContext) -> bool:\n    \"\"\"\n    expr is a TripleConstraint and:\n\n    * t is a triple\n    * t's predicate equals expr's predicate.\n      Let value be t's subject if inverse is true, else t's object.\n    * if inverse is true, t is in arcsIn, else t is in arcsOut.\n\n    \"\"\"\n    from pyshex.shape_expressions_language.p5_3_shape_expressions import satisfies\n\n    if c.debug:\n        print(c.i(1, f\" triple: {t}\"))\n        print(c.i(1, '', expr._as_json_dumps().split('\\n')))\n\n    if uriref_matches_iriref(t.p, expr.predicate):\n        value = t.s if expr.inverse else t.o\n        return expr.valueExpr is None or satisfies(cntxt, value, expr.valueExpr)\n    else:\n        cntxt.fail_reason = f\"Predicate mismatch: {t.p} \u2260 {expr.predicate}\"\n        return False","return_type":"bool","function_name":"matchesTripleConstraint","stripped_code":"def matchesTripleConstraint(cntxt: Context, t: RDFTriple, expr: ShExJ.TripleConstraint, c: DebugContext):\n    \"\"\"\n    expr is a TripleConstraint and:\n\n    * t is a triple\n    * t's predicate equals expr's predicate.\n      Let value be t's subject if inverse is true, else t's object.\n    * if inverse is true, t is in arcsIn, else t is in arcsOut.\n\n    \"\"\"\n    from pyshex.shape_expressions_language.p5_3_shape_expressions import satisfies\n\n    if c.debug:\n        print(c.i(1, f\" triple: {t}\"))\n        print(c.i(1, '', expr._as_json_dumps().split('\\n')))\n\n    if uriref_matches_iriref(t.p, expr.predicate):\n        value = t.s if expr.inverse else t.o\n        return expr.valueExpr is None or satisfies(cntxt, value, expr.valueExpr)\n    else:\n        cntxt.fail_reason = f\"Predicate mismatch: {t.p} \u2260 {expr.predicate}\"\n        return False"}
{"code":"def execute_pool_txns(self, three_pc_batch) -> List:\n        \"\"\"\n        Execute a transaction that involves consensus pool management, like\n        adding a node, client or a steward.\n\n        :param ppTime: PrePrepare request time\n        :param reqs_keys: requests keys to be committed\n        \"\"\"\n        committed_txns = self.default_executer(three_pc_batch)\n        for txn in committed_txns:\n            self.poolManager.onPoolMembershipChange(txn)\n        return committed_txns","return_type":"List","function_name":"Node.execute_pool_txns","stripped_code":"def execute_pool_txns(self, three_pc_batch):\n        \"\"\"\n        Execute a transaction that involves consensus pool management, like\n        adding a node, client or a steward.\n\n        :param ppTime: PrePrepare request time\n        :param reqs_keys: requests keys to be committed\n        \"\"\"\n        committed_txns = self.default_executer(three_pc_batch)\n        for txn in committed_txns:\n            self.poolManager.onPoolMembershipChange(txn)\n        return committed_txns"}
{"code":"def _retrieve_object(output_dict: Dict[str, Any], obj: Any) -> None:\n    \"\"\" Function to recursively retrieve histograms from a list in a ROOT file.\n\n    ``SetDirectory(True)`` is applied to TH1 derived hists and python is explicitly given\n    ownership of the retrieved objects.\n\n    Args:\n        output_dict (dict): Dict under which hists should be stored.\n        obj (ROOT.TObject derived): Object(s) to be stored. If it is a collection,\n            it will be recursed through.\n    Returns:\n        None: Changes in the dict are reflected in the output_dict which was passed.\n    \"\"\"\n    import ROOT\n\n    # Store TH1 or THn\n    if isinstance(obj, ROOT.TH1) or isinstance(obj, ROOT.THnBase):\n        # Ensure that it is not lost after the file is closed\n        # Only works for TH1\n        if isinstance(obj, ROOT.TH1):\n            obj.SetDirectory(0)\n\n        # Explicitly note that python owns the object\n        # From more on memory management with ROOT and python, see:\n        # https://root.cern.ch/root/html/guides/users-guide/PythonRuby.html#memory-handling\n        ROOT.SetOwnership(obj, False)\n\n        # Store the object\n        output_dict[obj.GetName()] = obj\n\n    # Recurse over lists\n    if isinstance(obj, ROOT.TCollection):\n        # Keeping it in order simply makes it easier to follow\n        output_dict[obj.GetName()] = {}\n        # Iterate over the objects in the collection and recursively store them\n        for obj_temp in list(obj):\n            _retrieve_object(output_dict[obj.GetName()], obj_temp)","return_type":"None","function_name":"_retrieve_object","stripped_code":"def _retrieve_object(output_dict: Dict[str, Any], obj: Any):\n    \"\"\" Function to recursively retrieve histograms from a list in a ROOT file.\n\n    ``SetDirectory(True)`` is applied to TH1 derived hists and python is explicitly given\n    ownership of the retrieved objects.\n\n    Args:\n        output_dict (dict): Dict under which hists should be stored.\n        obj (ROOT.TObject derived): Object(s) to be stored. If it is a collection,\n            it will be recursed through.\n    Returns:\n        None: Changes in the dict are reflected in the output_dict which was passed.\n    \"\"\"\n    import ROOT\n\n    # Store TH1 or THn\n    if isinstance(obj, ROOT.TH1) or isinstance(obj, ROOT.THnBase):\n        # Ensure that it is not lost after the file is closed\n        # Only works for TH1\n        if isinstance(obj, ROOT.TH1):\n            obj.SetDirectory(0)\n\n        # Explicitly note that python owns the object\n        # From more on memory management with ROOT and python, see:\n        # https://root.cern.ch/root/html/guides/users-guide/PythonRuby.html#memory-handling\n        ROOT.SetOwnership(obj, False)\n\n        # Store the object\n        output_dict[obj.GetName()] = obj\n\n    # Recurse over lists\n    if isinstance(obj, ROOT.TCollection):\n        # Keeping it in order simply makes it easier to follow\n        output_dict[obj.GetName()] = {}\n        # Iterate over the objects in the collection and recursively store them\n        for obj_temp in list(obj):\n            _retrieve_object(output_dict[obj.GetName()], obj_temp)"}
{"code":"def get_many(self, query: Mapping[str, Any], context: PipelineContext = None, streaming: bool = False) -> Iterable[T]:\n        \"\"\"Gets a query from the data source, where the query contains multiple elements to be extracted.\n\n        1) Extracts the query from the data source.\n        2) Inserts the result into any data sinks.\n        3) Transforms the results into the requested type if it wasn't already.\n        4) Inserts the transformed result into any data sinks.\n\n        Args:\n            query: The query being requested.\n            context: The context for the extraction (mutable).\n            streaming: Specifies whether the results should be returned as a generator (default False).\n\n        Returns:\n            The requested objects or a generator of the objects if streaming is True.\n        \"\"\"\n        result = self._source.get_many(self._source_type, deepcopy(query), context)\n        LOGGER.info(\"Got results \\\"{result}\\\" from query \\\"{query}\\\" of source \\\"{source}\\\"\".format(result=result, query=query, source=self._source))\n\n        if not streaming:\n            LOGGER.info(\"Non-streaming get_many request. Ensuring results \\\"{result}\\\" are a Iterable\".format(result=result))\n            result = list(result)\n\n            LOGGER.info(\"Sending results \\\"{result}\\\" to sinks before converting\".format(result=result))\n            for sink in self._before_transform:\n                sink.put_many(result, context)\n\n            LOGGER.info(\"Converting results \\\"{result}\\\" to request type\".format(result=result))\n            result = [self._transform(data=item, context=context) for item in result]\n\n            LOGGER.info(\"Sending results \\\"{result}\\\" to sinks after converting\".format(result=result))\n            for sink in self._after_transform:\n                sink.put_many(result, context)\n\n            return result\n        else:\n            LOGGER.info(\"Streaming get_many request. Returning result generator for results \\\"{result}\\\"\".format(result=result))\n            return self._get_many_generator(result)","return_type":"Iterable[T]","function_name":"_SourceHandler.get_many","stripped_code":"def get_many(self, query: Mapping[str, Any], context: PipelineContext = None, streaming: bool = False):\n        \"\"\"Gets a query from the data source, where the query contains multiple elements to be extracted.\n\n        1) Extracts the query from the data source.\n        2) Inserts the result into any data sinks.\n        3) Transforms the results into the requested type if it wasn't already.\n        4) Inserts the transformed result into any data sinks.\n\n        Args:\n            query: The query being requested.\n            context: The context for the extraction (mutable).\n            streaming: Specifies whether the results should be returned as a generator (default False).\n\n        Returns:\n            The requested objects or a generator of the objects if streaming is True.\n        \"\"\"\n        result = self._source.get_many(self._source_type, deepcopy(query), context)\n        LOGGER.info(\"Got results \\\"{result}\\\" from query \\\"{query}\\\" of source \\\"{source}\\\"\".format(result=result, query=query, source=self._source))\n\n        if not streaming:\n            LOGGER.info(\"Non-streaming get_many request. Ensuring results \\\"{result}\\\" are a Iterable\".format(result=result))\n            result = list(result)\n\n            LOGGER.info(\"Sending results \\\"{result}\\\" to sinks before converting\".format(result=result))\n            for sink in self._before_transform:\n                sink.put_many(result, context)\n\n            LOGGER.info(\"Converting results \\\"{result}\\\" to request type\".format(result=result))\n            result = [self._transform(data=item, context=context) for item in result]\n\n            LOGGER.info(\"Sending results \\\"{result}\\\" to sinks after converting\".format(result=result))\n            for sink in self._after_transform:\n                sink.put_many(result, context)\n\n            return result\n        else:\n            LOGGER.info(\"Streaming get_many request. Returning result generator for results \\\"{result}\\\"\".format(result=result))\n            return self._get_many_generator(result)"}
{"code":"def client_connected(\n        self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter\n    ) -> None:\n        \"\"\"\n        Callback when the TCP connection is established.\n\n        Record references to the stream reader and the stream writer to avoid\n        using private attributes ``_stream_reader`` and ``_stream_writer`` of\n        :class:`~asyncio.StreamReaderProtocol`.\n\n        \"\"\"\n        self.reader = reader\n        self.writer = writer","return_type":"None","function_name":"WebSocketCommonProtocol.client_connected","stripped_code":"def client_connected(\n        self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter\n    ):\n        \"\"\"\n        Callback when the TCP connection is established.\n\n        Record references to the stream reader and the stream writer to avoid\n        using private attributes ``_stream_reader`` and ``_stream_writer`` of\n        :class:`~asyncio.StreamReaderProtocol`.\n\n        \"\"\"\n        self.reader = reader\n        self.writer = writer"}
{"code":"def get_next_version() -> str:\n    \"\"\"\n    Returns: next version for this Git repository\n    \"\"\"\n    LOGGER.info('computing next version')\n    should_be_alpha = bool(CTX.repo.get_current_branch() != 'master')\n    LOGGER.info('alpha: %s', should_be_alpha)\n    calver = _get_calver()\n    LOGGER.info('current calver: %s', calver)\n    calver_tags = _get_current_calver_tags(calver)\n    LOGGER.info('found %s matching tags for this calver', len(calver_tags))\n    next_stable_version = _next_stable_version(calver, calver_tags)\n    LOGGER.info('next stable version: %s', next_stable_version)\n    if should_be_alpha:\n        return _next_alpha_version(next_stable_version, calver_tags)\n\n    return next_stable_version","return_type":"str","function_name":"get_next_version","stripped_code":"def get_next_version():\n    \"\"\"\n    Returns: next version for this Git repository\n    \"\"\"\n    LOGGER.info('computing next version')\n    should_be_alpha = bool(CTX.repo.get_current_branch() != 'master')\n    LOGGER.info('alpha: %s', should_be_alpha)\n    calver = _get_calver()\n    LOGGER.info('current calver: %s', calver)\n    calver_tags = _get_current_calver_tags(calver)\n    LOGGER.info('found %s matching tags for this calver', len(calver_tags))\n    next_stable_version = _next_stable_version(calver, calver_tags)\n    LOGGER.info('next stable version: %s', next_stable_version)\n    if should_be_alpha:\n        return _next_alpha_version(next_stable_version, calver_tags)\n\n    return next_stable_version"}
{"code":"def rungtd1d(time: Union[datetime, str, np.ndarray],\n             altkm: np.ndarray,\n             glat: float, glon: float) -> xarray.Dataset:\n    \"\"\"\n    This is the \"atomic\" function looped by other functions\n    \"\"\"\n    time = todt64(time)\n    # %% get solar parameters for date\n    f107Ap = gi.getApF107(time, smoothdays=81)\n    f107a = f107Ap['f107s'].item()\n    f107 = f107Ap['f107'].item()\n    Ap = f107Ap['Ap'].item()\n# %% dimensions\n    altkm = np.atleast_1d(altkm)\n    assert altkm.ndim == 1\n    assert isinstance(glon, (int, float))\n    assert isinstance(glat, (int, float))\n    assert isinstance(time, np.datetime64) or (time.size == 1 and isinstance(\n        time[0], np.datetime64)), 'if you have multiple times, for loop over them'\n\n# don't check ap, too complicated\n    assert isinstance(MASS, (float, int))\n    assert len(TSELECOPS) == 25\n# %%\n    gtd7.tselec(TSELECOPS)  # like the msis_driver example\n\n    iyd, utsec, stl = datetime2gtd(time, glon)\n    altkm = np.atleast_1d(altkm)\n\n    gtd7.meters(1)  # makes output in m^-3 and kg/m^-3\n# %%\n    if isinstance(Ap, (float, int)):\n        Ap = [Ap]*7  # even if SW(9) == 1 due to f2py needs for array\n\n    dens = np.empty((altkm.size, len(species)))\n    temp = np.empty((altkm.size, len(ttypes)))\n    for i, a in enumerate(altkm):\n        dens[i, :], temp[i, :] = gtd7.gtd7(iyd, utsec, a, glat, glon, stl, f107a, f107, Ap, MASS)\n\n    dsf = {k: (('time', 'alt_km', 'lat', 'lon'), v[None, :, None, None]) for (k, v) in zip(species, dens.T)}\n    dsf.update({'Tn':  (('time', 'alt_km', 'lat', 'lon'), temp[:, 1][None, :, None, None]),\n                'Texo': (('time', 'alt_km', 'lat', 'lon'), temp[:, 0][None, :, None, None])})\n\n    atmos = xarray.Dataset(dsf,\n                           coords={'time': time.astype(datetime), 'alt_km': altkm, 'lat': [glat], 'lon': [glon], },\n                           attrs={'Ap': Ap, 'f107': f107, 'f107a': f107a,\n                                  'species': species})\n\n    return atmos","return_type":"xarray.Dataset","function_name":"rungtd1d","stripped_code":"def rungtd1d(time: Union[datetime, str, np.ndarray],\n             altkm: np.ndarray,\n             glat: float, glon: float):\n    \"\"\"\n    This is the \"atomic\" function looped by other functions\n    \"\"\"\n    time = todt64(time)\n    # %% get solar parameters for date\n    f107Ap = gi.getApF107(time, smoothdays=81)\n    f107a = f107Ap['f107s'].item()\n    f107 = f107Ap['f107'].item()\n    Ap = f107Ap['Ap'].item()\n# %% dimensions\n    altkm = np.atleast_1d(altkm)\n    assert altkm.ndim == 1\n    assert isinstance(glon, (int, float))\n    assert isinstance(glat, (int, float))\n    assert isinstance(time, np.datetime64) or (time.size == 1 and isinstance(\n        time[0], np.datetime64)), 'if you have multiple times, for loop over them'\n\n# don't check ap, too complicated\n    assert isinstance(MASS, (float, int))\n    assert len(TSELECOPS) == 25\n# %%\n    gtd7.tselec(TSELECOPS)  # like the msis_driver example\n\n    iyd, utsec, stl = datetime2gtd(time, glon)\n    altkm = np.atleast_1d(altkm)\n\n    gtd7.meters(1)  # makes output in m^-3 and kg/m^-3\n# %%\n    if isinstance(Ap, (float, int)):\n        Ap = [Ap]*7  # even if SW(9) == 1 due to f2py needs for array\n\n    dens = np.empty((altkm.size, len(species)))\n    temp = np.empty((altkm.size, len(ttypes)))\n    for i, a in enumerate(altkm):\n        dens[i, :], temp[i, :] = gtd7.gtd7(iyd, utsec, a, glat, glon, stl, f107a, f107, Ap, MASS)\n\n    dsf = {k: (('time', 'alt_km', 'lat', 'lon'), v[None, :, None, None]) for (k, v) in zip(species, dens.T)}\n    dsf.update({'Tn':  (('time', 'alt_km', 'lat', 'lon'), temp[:, 1][None, :, None, None]),\n                'Texo': (('time', 'alt_km', 'lat', 'lon'), temp[:, 0][None, :, None, None])})\n\n    atmos = xarray.Dataset(dsf,\n                           coords={'time': time.astype(datetime), 'alt_km': altkm, 'lat': [glat], 'lon': [glon], },\n                           attrs={'Ap': Ap, 'f107': f107, 'f107a': f107a,\n                                  'species': species})\n\n    return atmos"}
{"code":"def _incremental_compile_module(\n    optimizer: PythonASTOptimizer,\n    py_ast: GeneratedPyAST,\n    mod: types.ModuleType,\n    source_filename: str,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n) -> None:\n    \"\"\"Incrementally compile a stream of AST nodes in module mod.\n\n    The source_filename will be passed to Python's native compile.\n\n    Incremental compilation is an integral part of generating a Python module\n    during the same process as macro-expansion.\"\"\"\n    module_body = list(\n        map(_statementize, itertools.chain(py_ast.dependencies, [py_ast.node]))\n    )\n\n    module = ast.Module(body=list(module_body))\n    module = optimizer.visit(module)\n    ast.fix_missing_locations(module)\n\n    _emit_ast_string(module)\n\n    bytecode = compile(module, source_filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, mod.__dict__)","return_type":"None","function_name":"_incremental_compile_module","stripped_code":"def _incremental_compile_module(\n    optimizer: PythonASTOptimizer,\n    py_ast: GeneratedPyAST,\n    mod: types.ModuleType,\n    source_filename: str,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n):\n    \"\"\"Incrementally compile a stream of AST nodes in module mod.\n\n    The source_filename will be passed to Python's native compile.\n\n    Incremental compilation is an integral part of generating a Python module\n    during the same process as macro-expansion.\"\"\"\n    module_body = list(\n        map(_statementize, itertools.chain(py_ast.dependencies, [py_ast.node]))\n    )\n\n    module = ast.Module(body=list(module_body))\n    module = optimizer.visit(module)\n    ast.fix_missing_locations(module)\n\n    _emit_ast_string(module)\n\n    bytecode = compile(module, source_filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, mod.__dict__)"}
{"code":"def _cache_from_source(path: str) -> str:\n    \"\"\"Return the path to the cached file for the given path. The original path\n    does not have to exist.\"\"\"\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")","return_type":"str","function_name":"_cache_from_source","stripped_code":"def _cache_from_source(path: str):\n    \"\"\"Return the path to the cached file for the given path. The original path\n    does not have to exist.\"\"\"\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")"}
{"code":"def line_content_counts_as_uncovered_manual(content: str) -> bool:\n    \"\"\"\n    Args:\n        content: A line with indentation and tail comments/space removed.\n\n    Returns:\n        Whether the line could be included in the coverage report.\n    \"\"\"\n    # Omit empty lines.\n    if not content:\n        return False\n\n    # Omit declarations.\n    for keyword in ['def', 'class']:\n        if content.startswith(keyword) and content.endswith(':'):\n            return False\n\n    # TODO: multiline comments, multiline strings, etc, etc.\n    return True","return_type":"bool","function_name":"line_content_counts_as_uncovered_manual","stripped_code":"def line_content_counts_as_uncovered_manual(content: str):\n    \"\"\"\n    Args:\n        content: A line with indentation and tail comments/space removed.\n\n    Returns:\n        Whether the line could be included in the coverage report.\n    \"\"\"\n    # Omit empty lines.\n    if not content:\n        return False\n\n    # Omit declarations.\n    for keyword in ['def', 'class']:\n        if content.startswith(keyword) and content.endswith(':'):\n            return False\n\n    # TODO: multiline comments, multiline strings, etc, etc.\n    return True"}
{"code":"def dispatch(self, state_change: StateChange) -> List[Event]:\n        \"\"\" Apply the `state_change` in the current machine and return the\n        resulting events.\n\n        Args:\n            state_change: An object representation of a state\n            change.\n\n        Return:\n            A list of events produced by the state transition.\n            It's the upper layer's responsibility to decided how to handle\n            these events.\n        \"\"\"\n        assert isinstance(state_change, StateChange)\n\n        # the state objects must be treated as immutable, so make a copy of the\n        # current state and pass the copy to the state machine to be modified.\n        next_state = deepcopy(self.current_state)\n\n        # update the current state by applying the change\n        iteration = self.state_transition(\n            next_state,\n            state_change,\n        )\n\n        assert isinstance(iteration, TransitionResult)\n\n        self.current_state = iteration.new_state\n        events = iteration.events\n\n        assert isinstance(self.current_state, (State, type(None)))\n        assert all(isinstance(e, Event) for e in events)\n\n        return events","return_type":"List[Event]","function_name":"StateManager.dispatch","stripped_code":"def dispatch(self, state_change: StateChange):\n        \"\"\" Apply the `state_change` in the current machine and return the\n        resulting events.\n\n        Args:\n            state_change: An object representation of a state\n            change.\n\n        Return:\n            A list of events produced by the state transition.\n            It's the upper layer's responsibility to decided how to handle\n            these events.\n        \"\"\"\n        assert isinstance(state_change, StateChange)\n\n        # the state objects must be treated as immutable, so make a copy of the\n        # current state and pass the copy to the state machine to be modified.\n        next_state = deepcopy(self.current_state)\n\n        # update the current state by applying the change\n        iteration = self.state_transition(\n            next_state,\n            state_change,\n        )\n\n        assert isinstance(iteration, TransitionResult)\n\n        self.current_state = iteration.new_state\n        events = iteration.events\n\n        assert isinstance(self.current_state, (State, type(None)))\n        assert all(isinstance(e, Event) for e in events)\n\n        return events"}
{"code":"def get_dihedral(self, i: int, j: int, k: int, l: int) -> float:\n        \"\"\"\n        Returns dihedral angle specified by four sites.\n\n        Args:\n            i: Index of first site\n            j: Index of second site\n            k: Index of third site\n            l: Index of fourth site\n\n        Returns:\n            Dihedral angle in degrees.\n        \"\"\"\n        v1 = self[k].coords - self[l].coords\n        v2 = self[j].coords - self[k].coords\n        v3 = self[i].coords - self[j].coords\n        v23 = np.cross(v2, v3)\n        v12 = np.cross(v1, v2)\n        return math.degrees(math.atan2(np.linalg.norm(v2) * np.dot(v1, v23),\n                                       np.dot(v12, v23)))","return_type":"float","function_name":"SiteCollection.get_dihedral","stripped_code":"def get_dihedral(self, i: int, j: int, k: int, l: int):\n        \"\"\"\n        Returns dihedral angle specified by four sites.\n\n        Args:\n            i: Index of first site\n            j: Index of second site\n            k: Index of third site\n            l: Index of fourth site\n\n        Returns:\n            Dihedral angle in degrees.\n        \"\"\"\n        v1 = self[k].coords - self[l].coords\n        v2 = self[j].coords - self[k].coords\n        v3 = self[i].coords - self[j].coords\n        v23 = np.cross(v2, v3)\n        v12 = np.cross(v1, v2)\n        return math.degrees(math.atan2(np.linalg.norm(v2) * np.dot(v1, v23),\n                                       np.dot(v12, v23)))"}
{"code":"def node_number(self, *, count_pnode=True) -> int:\n        \"\"\"Return the number of node\"\"\"\n        return (sum(1 for n in self.nodes())\n                + (sum(1 for n in self.powernodes()) if count_pnode else 0))","return_type":"int","function_name":"BubbleTree.node_number","stripped_code":"def node_number(self, *, count_pnode=True):\n        \"\"\"Return the number of node\"\"\"\n        return (sum(1 for n in self.nodes())\n                + (sum(1 for n in self.powernodes()) if count_pnode else 0))"}
{"code":"def add(self, t: RDFTriple) -> None:\n        \"\"\"\n        Add a triple as a query result\n        :param t: triple being added\n        \"\"\"\n        if self.chained_hook is not None:\n            self.chained_hook.add(t)","return_type":"None","function_name":"QueryResultHook.add","stripped_code":"def add(self, t: RDFTriple):\n        \"\"\"\n        Add a triple as a query result\n        :param t: triple being added\n        \"\"\"\n        if self.chained_hook is not None:\n            self.chained_hook.add(t)"}
{"code":"def contains_duplicates(values: Iterable[Any]) -> bool:\n    \"\"\"\n    Does the iterable contain any duplicate values?\n    \"\"\"\n    for v in Counter(values).values():\n        if v > 1:\n            return True\n    return False","return_type":"bool","function_name":"contains_duplicates","stripped_code":"def contains_duplicates(values: Iterable[Any]):\n    \"\"\"\n    Does the iterable contain any duplicate values?\n    \"\"\"\n    for v in Counter(values).values():\n        if v > 1:\n            return True\n    return False"}
{"code":"def password_length_needed(self) -> int:\n        \"\"\"Calculate the needed password length to satisfy the entropy number.\n\n        This is for the given character set.\n\n        \"\"\"\n        characters = self._get_password_characters()\n        if (\n                self.entropy_bits_req is None\n                or not characters\n        ):\n            raise ValueError(\"Can't calculate the password length needed: \"\n                             \"entropy_bits_req isn't set or the character \"\n                             \"set is empty\")\n\n        return calc_password_length_needed(\n            self.entropy_bits_req,\n            characters\n        )","return_type":"int","function_name":"Passphrase.password_length_needed","stripped_code":"def password_length_needed(self):\n        \"\"\"Calculate the needed password length to satisfy the entropy number.\n\n        This is for the given character set.\n\n        \"\"\"\n        characters = self._get_password_characters()\n        if (\n                self.entropy_bits_req is None\n                or not characters\n        ):\n            raise ValueError(\"Can't calculate the password length needed: \"\n                             \"entropy_bits_req isn't set or the character \"\n                             \"set is empty\")\n\n        return calc_password_length_needed(\n            self.entropy_bits_req,\n            characters\n        )"}
{"code":"def add_cli_drop(main: click.Group) -> click.Group:  # noqa: D202\n    \"\"\"Add a ``drop`` command to main :mod:`click` function.\"\"\"\n\n    @main.command()\n    @click.confirmation_option(prompt='Are you sure you want to drop the db?')\n    @click.pass_obj\n    def drop(manager):\n        \"\"\"Drop the database.\"\"\"\n        manager.drop_all()\n\n    return main","return_type":"click.Group","function_name":"add_cli_drop","stripped_code":"def add_cli_drop(main: click.Group):  # noqa: D202\n    \"\"\"Add a ``drop`` command to main :mod:`click` function.\"\"\"\n\n    @main.command()\n    @click.confirmation_option(prompt='Are you sure you want to drop the db?')\n    @click.pass_obj\n    def drop(manager):\n        \"\"\"Drop the database.\"\"\"\n        manager.drop_all()\n\n    return main"}
{"code":"def _split_lines(self, original_lines: List[str]) -> List[str]:\n        \"\"\"\n        Splits the original lines list according to the current console width and group indentations.\n\n        :param original_lines: The original lines list to split.\n        :return: A list of the new width-formatted lines.\n        \"\"\"\n        console_width = get_console_width()\n        # We take indent into account only in the inner group lines.\n        max_line_length = console_width - len(self.LINE_SEP) - self._last_position - \\\n            (self.indents_sum if not self._is_first_line else self.indents_sum - self._indents[-1])\n\n        lines = []\n        for i, line in enumerate(original_lines):\n            fixed_line = []\n            colors_counter = 0\n            line_index = 0\n            while line_index < len(line):\n                c = line[line_index]\n\n                # Check if we're in a color block.\n                if self._colors and c == self._ANSI_COLOR_PREFIX and \\\n                        len(line) >= (line_index + self._ANSI_COLOR_LENGTH):\n                    current_color = line[line_index:line_index + self._ANSI_COLOR_LENGTH]\n                    # If it really is a color, skip it.\n                    if self._ANSI_REGEXP.match(current_color):\n                        line_index += self._ANSI_COLOR_LENGTH\n                        fixed_line.extend(list(current_color))\n                        colors_counter += 1\n                        continue\n                fixed_line.append(line[line_index])\n                line_index += 1\n\n                # Create a new line, if max line is reached.\n                if len(fixed_line) >= max_line_length + (colors_counter * self._ANSI_COLOR_LENGTH):\n                    # Special case in which we want to split right before the line break.\n                    if len(line) > line_index and line[line_index] == self.LINE_SEP:\n                        continue\n                    line_string = ''.join(fixed_line)\n                    if not line_string.endswith(self.LINE_SEP):\n                        line_string += self.LINE_SEP\n                    lines.append(line_string)\n                    fixed_line = []\n                    colors_counter = 0\n                    self._last_position = 0\n                    # Max line length has changed since the last position is now 0.\n                    max_line_length = console_width - len(self.LINE_SEP) - self.indents_sum\n                    self._is_first_line = False\n\n            if len(fixed_line) > 0:\n                fixed_line = ''.join(fixed_line)\n                # If this line contains only color codes, attach it to the last line instead of creating a new one.\n                if len(fixed_line) == self._ANSI_COLOR_LENGTH and self._ANSI_REGEXP.match(fixed_line) is not None and \\\n                        len(lines) > 0:\n                    lines[-1] = lines[-1][:-1] + fixed_line\n                else:\n                    lines.append(fixed_line)\n        return lines","return_type":"List[str]","function_name":"Printer._split_lines","stripped_code":"def _split_lines(self, original_lines: List[str]):\n        \"\"\"\n        Splits the original lines list according to the current console width and group indentations.\n\n        :param original_lines: The original lines list to split.\n        :return: A list of the new width-formatted lines.\n        \"\"\"\n        console_width = get_console_width()\n        # We take indent into account only in the inner group lines.\n        max_line_length = console_width - len(self.LINE_SEP) - self._last_position - \\\n            (self.indents_sum if not self._is_first_line else self.indents_sum - self._indents[-1])\n\n        lines = []\n        for i, line in enumerate(original_lines):\n            fixed_line = []\n            colors_counter = 0\n            line_index = 0\n            while line_index < len(line):\n                c = line[line_index]\n\n                # Check if we're in a color block.\n                if self._colors and c == self._ANSI_COLOR_PREFIX and \\\n                        len(line) >= (line_index + self._ANSI_COLOR_LENGTH):\n                    current_color = line[line_index:line_index + self._ANSI_COLOR_LENGTH]\n                    # If it really is a color, skip it.\n                    if self._ANSI_REGEXP.match(current_color):\n                        line_index += self._ANSI_COLOR_LENGTH\n                        fixed_line.extend(list(current_color))\n                        colors_counter += 1\n                        continue\n                fixed_line.append(line[line_index])\n                line_index += 1\n\n                # Create a new line, if max line is reached.\n                if len(fixed_line) >= max_line_length + (colors_counter * self._ANSI_COLOR_LENGTH):\n                    # Special case in which we want to split right before the line break.\n                    if len(line) > line_index and line[line_index] == self.LINE_SEP:\n                        continue\n                    line_string = ''.join(fixed_line)\n                    if not line_string.endswith(self.LINE_SEP):\n                        line_string += self.LINE_SEP\n                    lines.append(line_string)\n                    fixed_line = []\n                    colors_counter = 0\n                    self._last_position = 0\n                    # Max line length has changed since the last position is now 0.\n                    max_line_length = console_width - len(self.LINE_SEP) - self.indents_sum\n                    self._is_first_line = False\n\n            if len(fixed_line) > 0:\n                fixed_line = ''.join(fixed_line)\n                # If this line contains only color codes, attach it to the last line instead of creating a new one.\n                if len(fixed_line) == self._ANSI_COLOR_LENGTH and self._ANSI_REGEXP.match(fixed_line) is not None and \\\n                        len(lines) > 0:\n                    lines[-1] = lines[-1][:-1] + fixed_line\n                else:\n                    lines.append(fixed_line)\n        return lines"}
{"code":"def _construct_linebreak_token(self, d: Dict) -> List[Dict]:\n        \"\"\"\n        Construct a shape token\n        Args:\n            d: Dict\n\n        Returns: List[Dict]\n        \"\"\"\n\n        result = []\n        num_break = int(d[\"length\"][0]) if d[\"length\"] else 1\n        if num_break:\n            s = ''\n            for i in range(num_break):\n                s += '\\n'\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n            s += ' '\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n        result = self._add_common_constrain(result, d)\n\n        return result","return_type":"List[Dict]","function_name":"Pattern._construct_linebreak_token","stripped_code":"def _construct_linebreak_token(self, d: Dict):\n        \"\"\"\n        Construct a shape token\n        Args:\n            d: Dict\n\n        Returns: List[Dict]\n        \"\"\"\n\n        result = []\n        num_break = int(d[\"length\"][0]) if d[\"length\"] else 1\n        if num_break:\n            s = ''\n            for i in range(num_break):\n                s += '\\n'\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n            s += ' '\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n        result = self._add_common_constrain(result, d)\n\n        return result"}
{"code":"def revealed_attrs(proof: dict) -> dict:\n    \"\"\"\n    Fetch revealed attributes from input proof and return dict mapping credential definition identifiers\n    to dicts, each dict mapping attribute names to (raw) values, for processing in further creds downstream.\n\n    :param proof: indy-sdk proof as dict\n    :return: dict mapping cred-ids to dicts, each mapping revealed attribute names to (raw) values\n    \"\"\"\n\n    rv = {}\n    for sub_index in range(len(proof['identifiers'])):\n        cd_id = proof['identifiers'][sub_index]['cred_def_id']\n        rv[cd_id] = ({  # uses von_anchor convention for uuid (referent) construction: will break on foreign anchor's\n            '_'.join(uuid.split('_')[1:-1]): proof['requested_proof']['revealed_attrs'][uuid]['raw']\n            for uuid in proof['requested_proof']['revealed_attrs']\n            if proof['requested_proof']['revealed_attrs'][uuid]['sub_proof_index'] == sub_index})\n\n    return rv","return_type":"dict","function_name":"revealed_attrs","stripped_code":"def revealed_attrs(proof: dict):\n    \"\"\"\n    Fetch revealed attributes from input proof and return dict mapping credential definition identifiers\n    to dicts, each dict mapping attribute names to (raw) values, for processing in further creds downstream.\n\n    :param proof: indy-sdk proof as dict\n    :return: dict mapping cred-ids to dicts, each mapping revealed attribute names to (raw) values\n    \"\"\"\n\n    rv = {}\n    for sub_index in range(len(proof['identifiers'])):\n        cd_id = proof['identifiers'][sub_index]['cred_def_id']\n        rv[cd_id] = ({  # uses von_anchor convention for uuid (referent) construction: will break on foreign anchor's\n            '_'.join(uuid.split('_')[1:-1]): proof['requested_proof']['revealed_attrs'][uuid]['raw']\n            for uuid in proof['requested_proof']['revealed_attrs']\n            if proof['requested_proof']['revealed_attrs'][uuid]['sub_proof_index'] == sub_index})\n\n    return rv"}
{"code":"def circuit_to_latex_using_qcircuit(\n        circuit: circuits.Circuit,\n        qubit_order: ops.QubitOrderOrList = ops.QubitOrder.DEFAULT) -> str:\n    \"\"\"Returns a QCircuit-based latex diagram of the given circuit.\n\n    Args:\n        circuit: The circuit to represent in latex.\n        qubit_order: Determines the order of qubit wires in the diagram.\n\n    Returns:\n        Latex code for the diagram.\n    \"\"\"\n    diagram = circuit.to_text_diagram_drawer(\n        qubit_namer=qcircuit_qubit_namer,\n        qubit_order=qubit_order,\n        get_circuit_diagram_info=get_qcircuit_diagram_info)\n    return _render(diagram)","return_type":"str","function_name":"circuit_to_latex_using_qcircuit","stripped_code":"def circuit_to_latex_using_qcircuit(\n        circuit: circuits.Circuit,\n        qubit_order: ops.QubitOrderOrList = ops.QubitOrder.DEFAULT):\n    \"\"\"Returns a QCircuit-based latex diagram of the given circuit.\n\n    Args:\n        circuit: The circuit to represent in latex.\n        qubit_order: Determines the order of qubit wires in the diagram.\n\n    Returns:\n        Latex code for the diagram.\n    \"\"\"\n    diagram = circuit.to_text_diagram_drawer(\n        qubit_namer=qcircuit_qubit_namer,\n        qubit_order=qubit_order,\n        get_circuit_diagram_info=get_qcircuit_diagram_info)\n    return _render(diagram)"}
{"code":"def ip_v4(self, with_port: bool = False) -> str:\n        \"\"\"Generate a random IPv4 address.\n\n        :param with_port: Add port to IP.\n        :return: Random IPv4 address.\n\n        :Example:\n            19.121.223.58\n        \"\"\"\n        ip = '.'.join(str(self.random.randint(0, 255)) for _ in range(4))\n\n        if with_port:\n            ip += ':{}'.format(self.port())\n\n        return ip","return_type":"str","function_name":"Internet.ip_v4","stripped_code":"def ip_v4(self, with_port: bool = False):\n        \"\"\"Generate a random IPv4 address.\n\n        :param with_port: Add port to IP.\n        :return: Random IPv4 address.\n\n        :Example:\n            19.121.223.58\n        \"\"\"\n        ip = '.'.join(str(self.random.randint(0, 255)) for _ in range(4))\n\n        if with_port:\n            ip += ':{}'.format(self.port())\n\n        return ip"}
{"code":"def draggable(self, value: Union[bool, str]) -> None:\n        \"\"\"Set ``draggable`` property.\n\n        ``value`` is boolean or string.\n        \"\"\"\n        if value is False:\n            self.removeAttribute('draggable')\n        else:\n            self.setAttribute('draggable', value)","return_type":"None","function_name":"HTMLElement.draggable","stripped_code":"def draggable(self, value: Union[bool, str]):\n        \"\"\"Set ``draggable`` property.\n\n        ``value`` is boolean or string.\n        \"\"\"\n        if value is False:\n            self.removeAttribute('draggable')\n        else:\n            self.setAttribute('draggable', value)"}
{"code":"def get_dom(self) -> str:\n        \"\"\" Retrieves the current value of the DOM for the step \"\"\"\n\n        if self.is_running:\n            return self.dumps()\n\n        if self.dom is not None:\n            return self.dom\n\n        dom = self.dumps()\n        self.dom = dom\n        return dom","return_type":"str","function_name":"ProjectStep.get_dom","stripped_code":"def get_dom(self):\n        \"\"\" Retrieves the current value of the DOM for the step \"\"\"\n\n        if self.is_running:\n            return self.dumps()\n\n        if self.dom is not None:\n            return self.dom\n\n        dom = self.dumps()\n        self.dom = dom\n        return dom"}
{"code":"def timeout_exponential_backoff(\n        retries: int,\n        timeout: int,\n        maximum: int,\n) -> Iterator[int]:\n    \"\"\" Timeouts generator with an exponential backoff strategy.\n\n    Timeouts start spaced by `timeout`, after `retries` exponentially increase\n    the retry delays until `maximum`, then maximum is returned indefinitely.\n    \"\"\"\n    yield timeout\n\n    tries = 1\n    while tries < retries:\n        tries += 1\n        yield timeout\n\n    while timeout < maximum:\n        timeout = min(timeout * 2, maximum)\n        yield timeout\n\n    while True:\n        yield maximum","return_type":"Iterator[int]","function_name":"timeout_exponential_backoff","stripped_code":"def timeout_exponential_backoff(\n        retries: int,\n        timeout: int,\n        maximum: int,\n):\n    \"\"\" Timeouts generator with an exponential backoff strategy.\n\n    Timeouts start spaced by `timeout`, after `retries` exponentially increase\n    the retry delays until `maximum`, then maximum is returned indefinitely.\n    \"\"\"\n    yield timeout\n\n    tries = 1\n    while tries < retries:\n        tries += 1\n        yield timeout\n\n    while timeout < maximum:\n        timeout = min(timeout * 2, maximum)\n        yield timeout\n\n    while True:\n        yield maximum"}
{"code":"def _parse_resources(resource_values: dict, resource_name: str) -> dict:\n        \"\"\"Parse resources key.\n\n        Args:\n            resource_values (dict): resource configurations values\n            resource_name (string): Resource name\n\n        Returns:\n            dict, resources specification\n\n        \"\"\"\n        # Initialising empty dictionary\n        resources = {}\n\n        for r_values in resource_values[resource_name]:\n            if 'limits' in r_values:\n                for r_key, r_value in \\\n                        resource_values[resource_name][r_values].items():\n                    if 'cpu' in r_key:\n                        cpu_value = float(r_value) * 10 ** 9\n                        cpu_key = r_key[:3] + '_limit'\n                        resources[cpu_key] = int(cpu_value)\n                    if 'mem' in r_key:\n                        mem_value = re.sub('M', '', r_value)\n                        mem_key = r_key[:3] + '_limit'\n                        resources[mem_key] = int(mem_value) * 1048576\n        resources_spec = docker.types.Resources(**resources)\n\n        return resources_spec","return_type":"dict","function_name":"DockerSwarmClient._parse_resources","stripped_code":"def _parse_resources(resource_values: dict, resource_name: str):\n        \"\"\"Parse resources key.\n\n        Args:\n            resource_values (dict): resource configurations values\n            resource_name (string): Resource name\n\n        Returns:\n            dict, resources specification\n\n        \"\"\"\n        # Initialising empty dictionary\n        resources = {}\n\n        for r_values in resource_values[resource_name]:\n            if 'limits' in r_values:\n                for r_key, r_value in \\\n                        resource_values[resource_name][r_values].items():\n                    if 'cpu' in r_key:\n                        cpu_value = float(r_value) * 10 ** 9\n                        cpu_key = r_key[:3] + '_limit'\n                        resources[cpu_key] = int(cpu_value)\n                    if 'mem' in r_key:\n                        mem_value = re.sub('M', '', r_value)\n                        mem_key = r_key[:3] + '_limit'\n                        resources[mem_key] = int(mem_value) * 1048576\n        resources_spec = docker.types.Resources(**resources)\n\n        return resources_spec"}
{"code":"def compute_etag(self) -> Optional[str]:\n        \"\"\"Sets the ``Etag`` header based on static url version.\n\n        This allows efficient ``If-None-Match`` checks against cached\n        versions, and sends the correct ``Etag`` for a partial response\n        (i.e. the same ``Etag`` as the full file).\n\n        .. versionadded:: 3.1\n        \"\"\"\n        assert self.absolute_path is not None\n        version_hash = self._get_cached_version(self.absolute_path)\n        if not version_hash:\n            return None\n        return '\"%s\"' % (version_hash,)","return_type":"Optional[str]","function_name":"StaticFileHandler.compute_etag","stripped_code":"def compute_etag(self):\n        \"\"\"Sets the ``Etag`` header based on static url version.\n\n        This allows efficient ``If-None-Match`` checks against cached\n        versions, and sends the correct ``Etag`` for a partial response\n        (i.e. the same ``Etag`` as the full file).\n\n        .. versionadded:: 3.1\n        \"\"\"\n        assert self.absolute_path is not None\n        version_hash = self._get_cached_version(self.absolute_path)\n        if not version_hash:\n            return None\n        return '\"%s\"' % (version_hash,)"}
{"code":"def is_cleanly_mergable(*dicts: Dict[Any, Any]) -> bool:\n    \"\"\"Check that nothing will be overwritten when dictionaries are merged using `deep_merge`.\n\n    Examples:\n\n        >>> is_cleanly_mergable({\"a\": 1}, {\"b\": 2}, {\"c\": 3})\n        True\n        >>> is_cleanly_mergable({\"a\": 1}, {\"b\": 2}, {\"a\": 0, c\": 3})\n        False\n        >>> is_cleanly_mergable({\"a\": 1, \"b\": {\"ba\": 2}}, {\"c\": 3, {\"b\": {\"bb\": 4}})\n        True\n        >>> is_cleanly_mergable({\"a\": 1, \"b\": {\"ba\": 2}}, {\"b\": {\"ba\": 4}})\n        False\n\n    \"\"\"\n    if len(dicts) <= 1:\n        return True\n    elif len(dicts) == 2:\n        if not all(isinstance(d, Mapping) for d in dicts):\n            return False\n        else:\n            shared_keys = set(dicts[0].keys()) & set(dicts[1].keys())\n            return all(is_cleanly_mergable(dicts[0][key], dicts[1][key]) for key in shared_keys)\n    else:\n        dict_combinations = itertools.combinations(dicts, 2)\n        return all(is_cleanly_mergable(*combination) for combination in dict_combinations)","return_type":"bool","function_name":"is_cleanly_mergable","stripped_code":"def is_cleanly_mergable(*dicts: Dict[Any, Any]):\n    \"\"\"Check that nothing will be overwritten when dictionaries are merged using `deep_merge`.\n\n    Examples:\n\n        >>> is_cleanly_mergable({\"a\": 1}, {\"b\": 2}, {\"c\": 3})\n        True\n        >>> is_cleanly_mergable({\"a\": 1}, {\"b\": 2}, {\"a\": 0, c\": 3})\n        False\n        >>> is_cleanly_mergable({\"a\": 1, \"b\": {\"ba\": 2}}, {\"c\": 3, {\"b\": {\"bb\": 4}})\n        True\n        >>> is_cleanly_mergable({\"a\": 1, \"b\": {\"ba\": 2}}, {\"b\": {\"ba\": 4}})\n        False\n\n    \"\"\"\n    if len(dicts) <= 1:\n        return True\n    elif len(dicts) == 2:\n        if not all(isinstance(d, Mapping) for d in dicts):\n            return False\n        else:\n            shared_keys = set(dicts[0].keys()) & set(dicts[1].keys())\n            return all(is_cleanly_mergable(dicts[0][key], dicts[1][key]) for key in shared_keys)\n    else:\n        dict_combinations = itertools.combinations(dicts, 2)\n        return all(is_cleanly_mergable(*combination) for combination in dict_combinations)"}
{"code":"def get_file_contents_text(\n        filename: str = None, blob: bytes = None,\n        config: TextProcessingConfig = _DEFAULT_CONFIG) -> str:\n    \"\"\"\n    Returns the string contents of a file, or of a BLOB.\n    \"\"\"\n    binary_contents = get_file_contents(filename=filename, blob=blob)\n    # 1. Try the encoding the user specified\n    if config.encoding:\n        try:\n            return binary_contents.decode(config.encoding)\n        except ValueError:  # of which UnicodeDecodeError is more specific\n            # ... https://docs.python.org/3/library/codecs.html\n            pass\n    # 2. Try the system encoding\n    sysdef = sys.getdefaultencoding()\n    if sysdef != config.encoding:\n        try:\n            return binary_contents.decode(sysdef)\n        except ValueError:\n            pass\n    # 3. Try the best guess from chardet\n    #    http://chardet.readthedocs.io/en/latest/usage.html\n    if chardet:\n        guess = chardet.detect(binary_contents)\n        if guess['encoding']:\n            return binary_contents.decode(guess['encoding'])\n    raise ValueError(\"Unknown encoding ({})\".format(\n        \"filename={}\".format(repr(filename)) if filename else \"blob\"))","return_type":"str","function_name":"get_file_contents_text","stripped_code":"def get_file_contents_text(\n        filename: str = None, blob: bytes = None,\n        config: TextProcessingConfig = _DEFAULT_CONFIG):\n    \"\"\"\n    Returns the string contents of a file, or of a BLOB.\n    \"\"\"\n    binary_contents = get_file_contents(filename=filename, blob=blob)\n    # 1. Try the encoding the user specified\n    if config.encoding:\n        try:\n            return binary_contents.decode(config.encoding)\n        except ValueError:  # of which UnicodeDecodeError is more specific\n            # ... https://docs.python.org/3/library/codecs.html\n            pass\n    # 2. Try the system encoding\n    sysdef = sys.getdefaultencoding()\n    if sysdef != config.encoding:\n        try:\n            return binary_contents.decode(sysdef)\n        except ValueError:\n            pass\n    # 3. Try the best guess from chardet\n    #    http://chardet.readthedocs.io/en/latest/usage.html\n    if chardet:\n        guess = chardet.detect(binary_contents)\n        if guess['encoding']:\n            return binary_contents.decode(guess['encoding'])\n    raise ValueError(\"Unknown encoding ({})\".format(\n        \"filename={}\".format(repr(filename)) if filename else \"blob\"))"}
{"code":"def serialize_to_rsb(params: dict) -> bytes:\n    \"\"\"\u0421\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f JSON \u0445\u0435\u0434\u0435\u0440\u0430 rsb.\n\n    @params -- \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 JSON (dfparser.def_values.DEF_RSH_PARAMS)\n    @return -- \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u0445\u0435\u0434\u0435\u0440 (2048 bytes)\n\n    \"\"\"\n    header = bytearray(np.zeros(2048, np.byte).tostring())\n\n    if \"text_header_size\" in params:\n        header[0:4] = struct.pack('I', params[\"text_header_size\"])\n\n    if \"events_num\" in params:\n        header[8:12] = struct.pack('i', params[\"events_num\"])\n\n    if \"start_time\" in params:\n        start_time = dateutil.parser.parse(params[\"start_time\"]).timestamp()\n        header[16:24] = struct.pack('Q', int(start_time))\n\n    if \"end_time\" in params:\n        end_time = dateutil.parser.parse(params[\"end_time\"]).timestamp()\n        header[24:32] = struct.pack('Q', int(end_time))\n\n    header[32:32 + len(params[\"filepath\"])\n           ] = params['filepath'].encode('cp1251')\n\n    header[288:292] = struct.pack('i', params[\"num_blocks\"])\n\n    header[292:296] = struct.pack('i', int(params[\"aquisition_time\"]))\n\n    header[296:300] = struct.pack('i', params[\"blocks_in_file\"])\n\n    header[300:304] = struct.pack('i', int(params[\"waitTime\"]))\n\n    header[312:320] = struct.pack('d', params[\"threshold\"])\n\n    sync_params = params[\"synchro_control\"]\n    sync_params_num = len(sync_params)\n\n    header[336:340] = struct.pack('I', sync_params_num)\n\n    for i in range(sync_params_num):\n        if sync_params[i] == 'Default':\n            code = 0\n        else:\n            code = synchro_control[sync_params[i]]\n\n        header[320 + i * 4:320 + (i + 1) * 4] = struct.pack('I', code)\n\n    header[344:352] = struct.pack('d', params[\"sample_freq\"])\n    header[352:356] = struct.pack('I', params[\"pre_history\"])\n\n    header[356:360] = struct.pack('i', params[\"packet_number\"])\n\n    header[360:364] = struct.pack('I', params[\"b_size\"])\n\n    header[364:368] = struct.pack('I', params[\"hysteresis\"])\n\n    header[368:372] = struct.pack('I', params[\"channel_number\"])\n\n    for i in range(params[\"channel_number\"]):\n        off = 372 + 56 * i\n\n        ch_param = params['channel'][i]\n\n        header[off + 44: off + 52] = struct.pack('d', ch_param[\"adjustment\"])\n        header[off + 52: off + 56] = struct.pack('I', ch_param[\"gain\"])\n        header[off + 36: off + 40] = struct.pack('I', len(ch_param['params']))\n\n        for j, param in enumerate(ch_param['params']):\n            if param == 'Default':\n                code = 0\n            else:\n                code = channel_control[param]\n                header[off + 4 + j * 4:\n                       off + 4 + (j + 1) * 4] = struct.pack('I', code)\n\n    synchro_channel = params['synchro_channel']\n    header[632:636] = struct.pack('I', len(synchro_channel['params']))\n\n    for i, param in enumerate(synchro_channel['params']):\n        if param == 'Default':\n            code = 0\n        else:\n            code = synchro_channel_control[param]\n            header[600 + i * 4: 600 + (i + 1) * 4] = struct.pack('I', code)\n\n    sync_type = synchro_channel_types[synchro_channel['type']]\n    header[304:308] = struct.pack('I', sync_type)\n\n    header[636:640] = struct.pack('I', synchro_channel[\"gain\"])\n\n    if \"err_lang\" in params:\n        header[640:644] = struct.pack('I', params[\"err_lang\"])\n\n    if \"board_name\" in params:\n        header[644:644 + len(params[\"board_name\"])] = \\\n            params['board_name'].encode('cp1251')\n\n    if \"board_id\" in params:\n        header[900: 904] = struct.pack('I', params[\"board_id\"])\n\n    return bytes(header)","return_type":"bytes","function_name":"serialize_to_rsb","stripped_code":"def serialize_to_rsb(params: dict):\n    \"\"\"\u0421\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f JSON \u0445\u0435\u0434\u0435\u0440\u0430 rsb.\n\n    @params -- \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 JSON (dfparser.def_values.DEF_RSH_PARAMS)\n    @return -- \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u0445\u0435\u0434\u0435\u0440 (2048 bytes)\n\n    \"\"\"\n    header = bytearray(np.zeros(2048, np.byte).tostring())\n\n    if \"text_header_size\" in params:\n        header[0:4] = struct.pack('I', params[\"text_header_size\"])\n\n    if \"events_num\" in params:\n        header[8:12] = struct.pack('i', params[\"events_num\"])\n\n    if \"start_time\" in params:\n        start_time = dateutil.parser.parse(params[\"start_time\"]).timestamp()\n        header[16:24] = struct.pack('Q', int(start_time))\n\n    if \"end_time\" in params:\n        end_time = dateutil.parser.parse(params[\"end_time\"]).timestamp()\n        header[24:32] = struct.pack('Q', int(end_time))\n\n    header[32:32 + len(params[\"filepath\"])\n           ] = params['filepath'].encode('cp1251')\n\n    header[288:292] = struct.pack('i', params[\"num_blocks\"])\n\n    header[292:296] = struct.pack('i', int(params[\"aquisition_time\"]))\n\n    header[296:300] = struct.pack('i', params[\"blocks_in_file\"])\n\n    header[300:304] = struct.pack('i', int(params[\"waitTime\"]))\n\n    header[312:320] = struct.pack('d', params[\"threshold\"])\n\n    sync_params = params[\"synchro_control\"]\n    sync_params_num = len(sync_params)\n\n    header[336:340] = struct.pack('I', sync_params_num)\n\n    for i in range(sync_params_num):\n        if sync_params[i] == 'Default':\n            code = 0\n        else:\n            code = synchro_control[sync_params[i]]\n\n        header[320 + i * 4:320 + (i + 1) * 4] = struct.pack('I', code)\n\n    header[344:352] = struct.pack('d', params[\"sample_freq\"])\n    header[352:356] = struct.pack('I', params[\"pre_history\"])\n\n    header[356:360] = struct.pack('i', params[\"packet_number\"])\n\n    header[360:364] = struct.pack('I', params[\"b_size\"])\n\n    header[364:368] = struct.pack('I', params[\"hysteresis\"])\n\n    header[368:372] = struct.pack('I', params[\"channel_number\"])\n\n    for i in range(params[\"channel_number\"]):\n        off = 372 + 56 * i\n\n        ch_param = params['channel'][i]\n\n        header[off + 44: off + 52] = struct.pack('d', ch_param[\"adjustment\"])\n        header[off + 52: off + 56] = struct.pack('I', ch_param[\"gain\"])\n        header[off + 36: off + 40] = struct.pack('I', len(ch_param['params']))\n\n        for j, param in enumerate(ch_param['params']):\n            if param == 'Default':\n                code = 0\n            else:\n                code = channel_control[param]\n                header[off + 4 + j * 4:\n                       off + 4 + (j + 1) * 4] = struct.pack('I', code)\n\n    synchro_channel = params['synchro_channel']\n    header[632:636] = struct.pack('I', len(synchro_channel['params']))\n\n    for i, param in enumerate(synchro_channel['params']):\n        if param == 'Default':\n            code = 0\n        else:\n            code = synchro_channel_control[param]\n            header[600 + i * 4: 600 + (i + 1) * 4] = struct.pack('I', code)\n\n    sync_type = synchro_channel_types[synchro_channel['type']]\n    header[304:308] = struct.pack('I', sync_type)\n\n    header[636:640] = struct.pack('I', synchro_channel[\"gain\"])\n\n    if \"err_lang\" in params:\n        header[640:644] = struct.pack('I', params[\"err_lang\"])\n\n    if \"board_name\" in params:\n        header[644:644 + len(params[\"board_name\"])] = \\\n            params['board_name'].encode('cp1251')\n\n    if \"board_id\" in params:\n        header[900: 904] = struct.pack('I', params[\"board_id\"])\n\n    return bytes(header)"}
{"code":"def segment_into_chars(utterance: str) -> str:\n    \"\"\" Segments an utterance into space delimited characters. \"\"\"\n\n    if not isinstance(utterance, str):\n        raise TypeError(\"Input type must be a string. Got {}.\".format(type(utterance)))\n\n    utterance.strip()\n    utterance = utterance.replace(\" \", \"\")\n    return \" \".join(utterance)","return_type":"str","function_name":"segment_into_chars","stripped_code":"def segment_into_chars(utterance: str):\n    \"\"\" Segments an utterance into space delimited characters. \"\"\"\n\n    if not isinstance(utterance, str):\n        raise TypeError(\"Input type must be a string. Got {}.\".format(type(utterance)))\n\n    utterance.strip()\n    utterance = utterance.replace(\" \", \"\")\n    return \" \".join(utterance)"}
{"code":"def class_name(self) -> str:\n        \"\"\"\n        Makes the fist letter big, keep the rest of the camelCaseApiName.\n        \"\"\"\n        if not self.api_name:  # empty string\n            return self.api_name\n        # end if\n        return self.api_name[0].upper() + self.api_name[1:]","return_type":"str","function_name":"Function.class_name","stripped_code":"def class_name(self):\n        \"\"\"\n        Makes the fist letter big, keep the rest of the camelCaseApiName.\n        \"\"\"\n        if not self.api_name:  # empty string\n            return self.api_name\n        # end if\n        return self.api_name[0].upper() + self.api_name[1:]"}
{"code":"def write_xsd(cls) -> None:\n        \"\"\"Write the complete base schema file `HydPyConfigBase.xsd` based\n        on the template file `HydPyConfigBase.xsdt`.\n\n        Method |XSDWriter.write_xsd| adds model specific information to the\n        general information of template file `HydPyConfigBase.xsdt` regarding\n        reading and writing of time series data and exchanging parameter\n        and sequence values e.g. during calibration.\n\n        The following example shows that after writing a new schema file,\n        method |XMLInterface.validate_xml| does not raise an error when\n        either applied on the XML configuration files `single_run.xml` or\n        `multiple_runs.xml` of the `LahnH` example project:\n\n        >>> import os\n        >>> from hydpy.auxs.xmltools import XSDWriter, XMLInterface\n        >>> if os.path.exists(XSDWriter.filepath_target):\n        ...     os.remove(XSDWriter.filepath_target)\n        >>> os.path.exists(XSDWriter.filepath_target)\n        False\n        >>> XSDWriter.write_xsd()\n        >>> os.path.exists(XSDWriter.filepath_target)\n        True\n\n        >>> from hydpy import data\n        >>> for configfile in ('single_run.xml', 'multiple_runs.xml'):\n        ...     XMLInterface(configfile, data.get_path('LahnH')).validate_xml()\n        \"\"\"\n        with open(cls.filepath_source) as file_:\n            template = file_.read()\n        template = template.replace(\n            '<!--include model sequence groups-->', cls.get_insertion())\n        template = template.replace(\n            '<!--include exchange items-->', cls.get_exchangeinsertion())\n        with open(cls.filepath_target, 'w') as file_:\n            file_.write(template)","return_type":"None","function_name":"XSDWriter.write_xsd","stripped_code":"def write_xsd(cls):\n        \"\"\"Write the complete base schema file `HydPyConfigBase.xsd` based\n        on the template file `HydPyConfigBase.xsdt`.\n\n        Method |XSDWriter.write_xsd| adds model specific information to the\n        general information of template file `HydPyConfigBase.xsdt` regarding\n        reading and writing of time series data and exchanging parameter\n        and sequence values e.g. during calibration.\n\n        The following example shows that after writing a new schema file,\n        method |XMLInterface.validate_xml| does not raise an error when\n        either applied on the XML configuration files `single_run.xml` or\n        `multiple_runs.xml` of the `LahnH` example project:\n\n        >>> import os\n        >>> from hydpy.auxs.xmltools import XSDWriter, XMLInterface\n        >>> if os.path.exists(XSDWriter.filepath_target):\n        ...     os.remove(XSDWriter.filepath_target)\n        >>> os.path.exists(XSDWriter.filepath_target)\n        False\n        >>> XSDWriter.write_xsd()\n        >>> os.path.exists(XSDWriter.filepath_target)\n        True\n\n        >>> from hydpy import data\n        >>> for configfile in ('single_run.xml', 'multiple_runs.xml'):\n        ...     XMLInterface(configfile, data.get_path('LahnH')).validate_xml()\n        \"\"\"\n        with open(cls.filepath_source) as file_:\n            template = file_.read()\n        template = template.replace(\n            '<!--include model sequence groups-->', cls.get_insertion())\n        template = template.replace(\n            '<!--include exchange items-->', cls.get_exchangeinsertion())\n        with open(cls.filepath_target, 'w') as file_:\n            file_.write(template)"}
{"code":"def get_cgi_parameter_datetime(form: cgi.FieldStorage,\n                               key: str) -> Optional[datetime.datetime]:\n    \"\"\"\n    Extracts a date/time parameter from a CGI form. Applies the LOCAL\n    timezone if none specified.\n    \"\"\"\n    try:\n        s = get_cgi_parameter_str(form, key)\n        if not s:\n            # if you dateutil.parser.parse() an empty string,\n            # you get today's date\n            return None\n        d = dateutil.parser.parse(s)\n        if d.tzinfo is None:  # as it will be\n            d = d.replace(tzinfo=dateutil.tz.tzlocal())\n        return d\n    except ValueError:\n        return None","return_type":"Optional[datetime.datetime]","function_name":"get_cgi_parameter_datetime","stripped_code":"def get_cgi_parameter_datetime(form: cgi.FieldStorage,\n                               key: str):\n    \"\"\"\n    Extracts a date/time parameter from a CGI form. Applies the LOCAL\n    timezone if none specified.\n    \"\"\"\n    try:\n        s = get_cgi_parameter_str(form, key)\n        if not s:\n            # if you dateutil.parser.parse() an empty string,\n            # you get today's date\n            return None\n        d = dateutil.parser.parse(s)\n        if d.tzinfo is None:  # as it will be\n            d = d.replace(tzinfo=dateutil.tz.tzlocal())\n        return d\n    except ValueError:\n        return None"}
{"code":"def addFailure(self, test: unittest.case.TestCase, exc_info: tuple) -> None:\n        \"\"\"\n        Transforms the test in a serializable version of it and sends it to a queue for further analysis\n\n        :param test: the test to save\n        :param exc_info: tuple of the form (Exception class, Exception instance, traceback)\n        \"\"\"\n        # noinspection PyTypeChecker\n        self.add_result(TestState.failure, test, exc_info)","return_type":"None","function_name":"InterProcessResult.addFailure","stripped_code":"def addFailure(self, test: unittest.case.TestCase, exc_info: tuple):\n        \"\"\"\n        Transforms the test in a serializable version of it and sends it to a queue for further analysis\n\n        :param test: the test to save\n        :param exc_info: tuple of the form (Exception class, Exception instance, traceback)\n        \"\"\"\n        # noinspection PyTypeChecker\n        self.add_result(TestState.failure, test, exc_info)"}
{"code":"def build_downstream_edge_predicate(nodes: Iterable[BaseEntity]) -> EdgePredicate:\n    \"\"\"Build an edge predicate that passes for edges for which one of the given nodes is the subject.\"\"\"\n    nodes = set(nodes)\n\n    def downstream_filter(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n        \"\"\"Pass for relations for which one of the given nodes is the subject.\"\"\"\n        return u in nodes and graph[u][v][k][RELATION] in CAUSAL_RELATIONS\n\n    return downstream_filter","return_type":"EdgePredicate","function_name":"build_downstream_edge_predicate","stripped_code":"def build_downstream_edge_predicate(nodes: Iterable[BaseEntity]):\n    \"\"\"Build an edge predicate that passes for edges for which one of the given nodes is the subject.\"\"\"\n    nodes = set(nodes)\n\n    def downstream_filter(graph: BELGraph, u: BaseEntity, v: BaseEntity, k: str) -> bool:\n        \"\"\"Pass for relations for which one of the given nodes is the subject.\"\"\"\n        return u in nodes and graph[u][v][k][RELATION] in CAUSAL_RELATIONS\n\n    return downstream_filter"}
{"code":"def delete_upload_id(cls, tables: I2B2Tables, upload_id: int) -> int:\n        \"\"\"\n        Delete all observation_fact records with the supplied upload_id\n        :param tables: i2b2 sql connection\n        :param upload_id: upload identifier to remove\n        :return: number or records that were deleted\n        \"\"\"\n        return cls._delete_upload_id(tables.crc_connection, tables.observation_fact, upload_id)","return_type":"int","function_name":"ObservationFact.delete_upload_id","stripped_code":"def delete_upload_id(cls, tables: I2B2Tables, upload_id: int):\n        \"\"\"\n        Delete all observation_fact records with the supplied upload_id\n        :param tables: i2b2 sql connection\n        :param upload_id: upload identifier to remove\n        :return: number or records that were deleted\n        \"\"\"\n        return cls._delete_upload_id(tables.crc_connection, tables.observation_fact, upload_id)"}
{"code":"def image_placeholder(width: Union[int, str] = 1920,\n                          height: Union[int, str] = 1080) -> str:\n        \"\"\"Generate a link to the image placeholder.\n\n        :param width: Width of image.\n        :param height: Height of image.\n        :return: URL to image placeholder.\n        \"\"\"\n        url = 'http://placehold.it/{width}x{height}'\n        return url.format(width=width, height=height)","return_type":"str","function_name":"Internet.image_placeholder","stripped_code":"def image_placeholder(width: Union[int, str] = 1920,\n                          height: Union[int, str] = 1080):\n        \"\"\"Generate a link to the image placeholder.\n\n        :param width: Width of image.\n        :param height: Height of image.\n        :return: URL to image placeholder.\n        \"\"\"\n        url = 'http://placehold.it/{width}x{height}'\n        return url.format(width=width, height=height)"}
{"code":"def format(self, record: logging.LogRecord) -> str:\n        \"\"\"\n        Internal function to format the :class:`LogRecord` as HTML.\n\n        See https://docs.python.org/3.4/library/logging.html#logging.LogRecord\n        \"\"\"\n\n        # message = super().format(record)\n        super().format(record)\n        # Since fmt does not contain asctime, the Formatter.format()\n        # will not write asctime (since its usesTime()) function will be\n        # false. Therefore:\n        record.asctime = self.formatTime(record, self.datefmt)\n        bg_col = self.log_background_colors[record.levelno]\n        msg = escape(record.getMessage())\n        # escape() won't replace \\n but will replace & etc.\n        if self.replace_nl_with_br:\n            msg = msg.replace(\"\\n\", \"<br>\")\n        html = (\n            '<span style=\"color:#008B8B\">{time}.{ms:03d} {name}:{lvname}: '\n            '</span><span style=\"color:{color}{bg}\">{msg}</font>{br}'.format(\n                time=record.asctime,\n                ms=int(record.msecs),\n                name=record.name,\n                lvname=record.levelname,\n                color=self.log_colors[record.levelno],\n                msg=msg,\n                bg=\";background-color:{}\".format(bg_col) if bg_col else \"\",\n                br=\"<br>\" if self.append_br else \"\",\n            )\n        )\n        # print(\"record.__dict__: {}\".format(record.__dict__))\n        # print(\"html: {}\".format(html))\n        return html","return_type":"str","function_name":"HtmlColorFormatter.format","stripped_code":"def format(self, record: logging.LogRecord):\n        \"\"\"\n        Internal function to format the :class:`LogRecord` as HTML.\n\n        See https://docs.python.org/3.4/library/logging.html#logging.LogRecord\n        \"\"\"\n\n        # message = super().format(record)\n        super().format(record)\n        # Since fmt does not contain asctime, the Formatter.format()\n        # will not write asctime (since its usesTime()) function will be\n        # false. Therefore:\n        record.asctime = self.formatTime(record, self.datefmt)\n        bg_col = self.log_background_colors[record.levelno]\n        msg = escape(record.getMessage())\n        # escape() won't replace \\n but will replace & etc.\n        if self.replace_nl_with_br:\n            msg = msg.replace(\"\\n\", \"<br>\")\n        html = (\n            '<span style=\"color:#008B8B\">{time}.{ms:03d} {name}:{lvname}: '\n            '</span><span style=\"color:{color}{bg}\">{msg}</font>{br}'.format(\n                time=record.asctime,\n                ms=int(record.msecs),\n                name=record.name,\n                lvname=record.levelname,\n                color=self.log_colors[record.levelno],\n                msg=msg,\n                bg=\";background-color:{}\".format(bg_col) if bg_col else \"\",\n                br=\"<br>\" if self.append_br else \"\",\n            )\n        )\n        # print(\"record.__dict__: {}\".format(record.__dict__))\n        # print(\"html: {}\".format(html))\n        return html"}
{"code":"def reaction_cartesian_expansion(graph: BELGraph, accept_unqualified_edges: bool = True) -> None:\n    \"\"\"Expand all reactions to simple subject-predicate-object networks.\"\"\"\n    for u, v, d in list(graph.edges(data=True)):\n        # Deal with unqualified edges\n        if CITATION not in d and accept_unqualified_edges:\n            _reaction_cartesion_expansion_unqualified_helper(graph, u, v, d)\n            continue\n\n        if isinstance(u, Reaction) and isinstance(v, Reaction):\n            catalysts = _get_catalysts_in_reaction(u) | _get_catalysts_in_reaction(v)\n\n            for reactant, product in chain(itt.product(u.reactants, u.products), itt.product(v.reactants, v.products)):\n                if reactant in catalysts or product in catalysts:\n                    continue\n                graph.add_increases(\n                    reactant, product,\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n            for product, reactant in itt.product(u.products, u.reactants):\n                if reactant in catalysts or product in catalysts:\n                    continue\n\n                graph.add_qualified_edge(\n                    product, reactant,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n        elif isinstance(u, Reaction):\n            catalysts = _get_catalysts_in_reaction(u)\n\n            for product in u.products:\n                # Skip create increases edges between enzymes\n                if product in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if v not in u.products and v not in u.reactants:\n                    graph.add_increases(\n                        product, v,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n                for reactant in u.reactants:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n        elif isinstance(v, Reaction):\n            for reactant in v.reactants:\n                catalysts = _get_catalysts_in_reaction(v)\n\n                # Skip create increases edges between enzymes\n                if reactant in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if u not in v.products and u not in v.reactants:\n                    graph.add_increases(\n                        u, reactant,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n                for product in v.products:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n    _remove_reaction_nodes(graph)","return_type":"None","function_name":"reaction_cartesian_expansion","stripped_code":"def reaction_cartesian_expansion(graph: BELGraph, accept_unqualified_edges: bool = True):\n    \"\"\"Expand all reactions to simple subject-predicate-object networks.\"\"\"\n    for u, v, d in list(graph.edges(data=True)):\n        # Deal with unqualified edges\n        if CITATION not in d and accept_unqualified_edges:\n            _reaction_cartesion_expansion_unqualified_helper(graph, u, v, d)\n            continue\n\n        if isinstance(u, Reaction) and isinstance(v, Reaction):\n            catalysts = _get_catalysts_in_reaction(u) | _get_catalysts_in_reaction(v)\n\n            for reactant, product in chain(itt.product(u.reactants, u.products), itt.product(v.reactants, v.products)):\n                if reactant in catalysts or product in catalysts:\n                    continue\n                graph.add_increases(\n                    reactant, product,\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n            for product, reactant in itt.product(u.products, u.reactants):\n                if reactant in catalysts or product in catalysts:\n                    continue\n\n                graph.add_qualified_edge(\n                    product, reactant,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n        elif isinstance(u, Reaction):\n            catalysts = _get_catalysts_in_reaction(u)\n\n            for product in u.products:\n                # Skip create increases edges between enzymes\n                if product in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if v not in u.products and v not in u.reactants:\n                    graph.add_increases(\n                        product, v,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n                for reactant in u.reactants:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n        elif isinstance(v, Reaction):\n            for reactant in v.reactants:\n                catalysts = _get_catalysts_in_reaction(v)\n\n                # Skip create increases edges between enzymes\n                if reactant in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if u not in v.products and u not in v.reactants:\n                    graph.add_increases(\n                        u, reactant,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n                for product in v.products:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n    _remove_reaction_nodes(graph)"}
{"code":"def is_terminal(self, symbol: str) -> bool:\n        \"\"\"\n        This function will be called on nodes of a logical form tree, which are either non-terminal\n        symbols that can be expanded or terminal symbols that must be leaf nodes.  Returns ``True``\n        if the given symbol is a terminal symbol.\n        \"\"\"\n        # We special-case 'lambda' here because it behaves weirdly in action sequences.\n        return (symbol in self.global_name_mapping or\n                symbol in self.local_name_mapping or\n                'lambda' in symbol)","return_type":"bool","function_name":"World.is_terminal","stripped_code":"def is_terminal(self, symbol: str):\n        \"\"\"\n        This function will be called on nodes of a logical form tree, which are either non-terminal\n        symbols that can be expanded or terminal symbols that must be leaf nodes.  Returns ``True``\n        if the given symbol is a terminal symbol.\n        \"\"\"\n        # We special-case 'lambda' here because it behaves weirdly in action sequences.\n        return (symbol in self.global_name_mapping or\n                symbol in self.local_name_mapping or\n                'lambda' in symbol)"}
{"code":"def ascii_tree(self, no_types: bool = False, val_count: bool = False) -> str:\n        \"\"\"Generate ASCII art representation of the schema tree.\n\n        Args:\n            no_types: Suppress output of data type info.\n            val_count: Show accumulated validation counts.\n\n        Returns:\n            String with the ASCII tree.\n        \"\"\"\n        return self.schema._ascii_tree(\"\", no_types, val_count)","return_type":"str","function_name":"DataModel.ascii_tree","stripped_code":"def ascii_tree(self, no_types: bool = False, val_count: bool = False):\n        \"\"\"Generate ASCII art representation of the schema tree.\n\n        Args:\n            no_types: Suppress output of data type info.\n            val_count: Show accumulated validation counts.\n\n        Returns:\n            String with the ASCII tree.\n        \"\"\"\n        return self.schema._ascii_tree(\"\", no_types, val_count)"}
{"code":"def marketPrice(self) -> float:\n        \"\"\"\n        Return the first available one of\n\n        * last price if within current bid/ask;\n        * average of bid and ask (midpoint);\n        * close price.\n        \"\"\"\n        price = self.last if (\n            self.hasBidAsk() and self.bid <= self.last <= self.ask) else \\\n            self.midpoint()\n        if isNan(price):\n            price = self.close\n        return price","return_type":"float","function_name":"Ticker.marketPrice","stripped_code":"def marketPrice(self):\n        \"\"\"\n        Return the first available one of\n\n        * last price if within current bid/ask;\n        * average of bid and ask (midpoint);\n        * close price.\n        \"\"\"\n        price = self.last if (\n            self.hasBidAsk() and self.bid <= self.last <= self.ask) else \\\n            self.midpoint()\n        if isNan(price):\n            price = self.close\n        return price"}
{"code":"def compile_create_temporary_table(table_name: str,\n                                   column_statement: str,\n                                   primary_key_statement: str) -> str:\n    \"\"\"Postgresql Create Temporary Table statement formatter.\"\"\"\n\n    statement = \"\"\"\n                CREATE TEMPORARY TABLE {table} ({columns} {primary_keys});\n                \"\"\".format(table=table_name,\n                           columns=column_statement,\n                           primary_keys=primary_key_statement)\n    return statement","return_type":"str","function_name":"compile_create_temporary_table","stripped_code":"def compile_create_temporary_table(table_name: str,\n                                   column_statement: str,\n                                   primary_key_statement: str):\n    \"\"\"Postgresql Create Temporary Table statement formatter.\"\"\"\n\n    statement = \"\"\"\n                CREATE TEMPORARY TABLE {table} ({columns} {primary_keys});\n                \"\"\".format(table=table_name,\n                           columns=column_statement,\n                           primary_keys=primary_key_statement)\n    return statement"}
{"code":"def store_extra_keys(self, d: Dict[str, Any]) -> None:\n        \"\"\"\n        Store several extra values in the messaging storage.\n\n        :param d: dictionary entry to merge with current self.extra_keys.\n        :returns: None\n        \"\"\"\n        new_dict = dict(self.extra_keys, **d)\n        self.extra_keys = new_dict.copy()","return_type":"None","function_name":"BotSkeleton.store_extra_keys","stripped_code":"def store_extra_keys(self, d: Dict[str, Any]):\n        \"\"\"\n        Store several extra values in the messaging storage.\n\n        :param d: dictionary entry to merge with current self.extra_keys.\n        :returns: None\n        \"\"\"\n        new_dict = dict(self.extra_keys, **d)\n        self.extra_keys = new_dict.copy()"}
{"code":"def inner_product(vec0: QubitVector, vec1: QubitVector) -> bk.BKTensor:\n    \"\"\" Hilbert-Schmidt inner product between qubit vectors\n\n    The tensor rank and qubits must match.\n    \"\"\"\n    if vec0.rank != vec1.rank or vec0.qubit_nb != vec1.qubit_nb:\n        raise ValueError('Incompatibly vectors. Qubits and rank must match')\n\n    vec1 = vec1.permute(vec0.qubits)  # Make sure qubits in same order\n    return bk.inner(vec0.tensor, vec1.tensor)","return_type":"bk.BKTensor","function_name":"inner_product","stripped_code":"def inner_product(vec0: QubitVector, vec1: QubitVector):\n    \"\"\" Hilbert-Schmidt inner product between qubit vectors\n\n    The tensor rank and qubits must match.\n    \"\"\"\n    if vec0.rank != vec1.rank or vec0.qubit_nb != vec1.qubit_nb:\n        raise ValueError('Incompatibly vectors. Qubits and rank must match')\n\n    vec1 = vec1.permute(vec0.qubits)  # Make sure qubits in same order\n    return bk.inner(vec0.tensor, vec1.tensor)"}
{"code":"def assume_script(self) -> 'Language':\n        \"\"\"\n        Fill in the script if it's missing, and if it can be assumed from the\n        language subtag. This is the opposite of `simplify_script`.\n\n        >>> Language.make(language='en').assume_script()\n        Language.make(language='en', script='Latn')\n\n        >>> Language.make(language='yi').assume_script()\n        Language.make(language='yi', script='Hebr')\n\n        >>> Language.make(language='yi', script='Latn').assume_script()\n        Language.make(language='yi', script='Latn')\n\n        This fills in nothing when the script cannot be assumed -- such as when\n        the language has multiple scripts, or it has no standard orthography:\n\n        >>> Language.make(language='sr').assume_script()\n        Language.make(language='sr')\n\n        >>> Language.make(language='eee').assume_script()\n        Language.make(language='eee')\n\n        It also dosn't fill anything in when the language is unspecified.\n\n        >>> Language.make(region='US').assume_script()\n        Language.make(region='US')\n        \"\"\"\n        if self._assumed is not None:\n            return self._assumed\n        if self.language and not self.script:\n            try:\n                self._assumed = self.update_dict({'script': DEFAULT_SCRIPTS[self.language]})\n            except KeyError:\n                self._assumed = self\n        else:\n            self._assumed = self\n        return self._assumed","return_type":"'Language'","function_name":"Language.assume_script","stripped_code":"def assume_script(self):\n        \"\"\"\n        Fill in the script if it's missing, and if it can be assumed from the\n        language subtag. This is the opposite of `simplify_script`.\n\n        >>> Language.make(language='en').assume_script()\n        Language.make(language='en', script='Latn')\n\n        >>> Language.make(language='yi').assume_script()\n        Language.make(language='yi', script='Hebr')\n\n        >>> Language.make(language='yi', script='Latn').assume_script()\n        Language.make(language='yi', script='Latn')\n\n        This fills in nothing when the script cannot be assumed -- such as when\n        the language has multiple scripts, or it has no standard orthography:\n\n        >>> Language.make(language='sr').assume_script()\n        Language.make(language='sr')\n\n        >>> Language.make(language='eee').assume_script()\n        Language.make(language='eee')\n\n        It also dosn't fill anything in when the language is unspecified.\n\n        >>> Language.make(region='US').assume_script()\n        Language.make(region='US')\n        \"\"\"\n        if self._assumed is not None:\n            return self._assumed\n        if self.language and not self.script:\n            try:\n                self._assumed = self.update_dict({'script': DEFAULT_SCRIPTS[self.language]})\n            except KeyError:\n                self._assumed = self\n        else:\n            self._assumed = self\n        return self._assumed"}
{"code":"def metainfo_to_protobuf(self) -> bytes:\n        '''encode deck into protobuf'''\n\n        deck = deckspawnproto()\n        deck.version = self.version\n        deck.name = self.name\n        deck.number_of_decimals = self.number_of_decimals\n        deck.issue_mode = self.issue_mode\n        if self.asset_specific_data:\n            if not isinstance(self.asset_specific_data, bytes):\n                deck.asset_specific_data = self.asset_specific_data.encode()\n            else:\n                deck.asset_specific_data = self.asset_specific_data\n\n        if deck.ByteSize() > net_query(self.network).op_return_max_bytes:\n            raise OverSizeOPReturn('''\n                        Metainfo size exceeds maximum of {max} bytes supported by this network.'''\n                                   .format(max=net_query(self.network)\n                                           .op_return_max_bytes))\n\n        return deck.SerializeToString()","return_type":"bytes","function_name":"Deck.metainfo_to_protobuf","stripped_code":"def metainfo_to_protobuf(self):\n        '''encode deck into protobuf'''\n\n        deck = deckspawnproto()\n        deck.version = self.version\n        deck.name = self.name\n        deck.number_of_decimals = self.number_of_decimals\n        deck.issue_mode = self.issue_mode\n        if self.asset_specific_data:\n            if not isinstance(self.asset_specific_data, bytes):\n                deck.asset_specific_data = self.asset_specific_data.encode()\n            else:\n                deck.asset_specific_data = self.asset_specific_data\n\n        if deck.ByteSize() > net_query(self.network).op_return_max_bytes:\n            raise OverSizeOPReturn('''\n                        Metainfo size exceeds maximum of {max} bytes supported by this network.'''\n                                   .format(max=net_query(self.network)\n                                           .op_return_max_bytes))\n\n        return deck.SerializeToString()"}
{"code":"def _run_sync(self, method: Callable, *args, **kwargs) -> Any:\n        \"\"\"\n        Utility method to run commands synchronously for testing.\n        \"\"\"\n        if self.loop.is_running():\n            raise RuntimeError(\"Event loop is already running.\")\n\n        if not self.is_connected:\n            self.loop.run_until_complete(self.connect())\n\n        task = asyncio.Task(method(*args, **kwargs), loop=self.loop)\n        result = self.loop.run_until_complete(task)\n\n        self.loop.run_until_complete(self.quit())\n\n        return result","return_type":"Any","function_name":"SMTP._run_sync","stripped_code":"def _run_sync(self, method: Callable, *args, **kwargs):\n        \"\"\"\n        Utility method to run commands synchronously for testing.\n        \"\"\"\n        if self.loop.is_running():\n            raise RuntimeError(\"Event loop is already running.\")\n\n        if not self.is_connected:\n            self.loop.run_until_complete(self.connect())\n\n        task = asyncio.Task(method(*args, **kwargs), loop=self.loop)\n        result = self.loop.run_until_complete(task)\n\n        self.loop.run_until_complete(self.quit())\n\n        return result"}
{"code":"def dict_to_pendulum(d: Dict[str, Any],\n                     pendulum_class: ClassType) -> DateTime:\n    \"\"\"\n    Converts a ``dict`` object back to a ``Pendulum``.\n    \"\"\"\n    return pendulum.parse(d['iso'])","return_type":"DateTime","function_name":"dict_to_pendulum","stripped_code":"def dict_to_pendulum(d: Dict[str, Any],\n                     pendulum_class: ClassType):\n    \"\"\"\n    Converts a ``dict`` object back to a ``Pendulum``.\n    \"\"\"\n    return pendulum.parse(d['iso'])"}
{"code":"def fragment_search(self, fragement:str) -> List[dict]:\n        ''' Returns the rows in InterLex associated with the fragment\n\n        Note:\n            Pressumed to have duplicate fragements in InterLex\n        Args:\n            fragment: The fragment_id of the curie pertaining to the ontology\n        Returns:\n            None or List[dict]\n        '''\n        fragement = self.extract_fragment(fragement)\n        ilx_rows = self.fragment2rows.get(fragement)\n        if not ilx_rows:\n            return None\n        else:\n            return ilx_rows","return_type":"List[dict]","function_name":"InterLexIngestion.fragment_search","stripped_code":"def fragment_search(self, fragement:str):\n        ''' Returns the rows in InterLex associated with the fragment\n\n        Note:\n            Pressumed to have duplicate fragements in InterLex\n        Args:\n            fragment: The fragment_id of the curie pertaining to the ontology\n        Returns:\n            None or List[dict]\n        '''\n        fragement = self.extract_fragment(fragement)\n        ilx_rows = self.fragment2rows.get(fragement)\n        if not ilx_rows:\n            return None\n        else:\n            return ilx_rows"}
{"code":"def write(self, ncfile) -> None:\n        \"\"\"Write the data to the given NetCDF file.\n\n        See the general documentation on class |NetCDFVariableFlat|\n        for some examples.\n        \"\"\"\n        self.insert_subdevices(ncfile)\n        create_variable(ncfile, self.name, 'f8', self.dimensions)\n        ncfile[self.name][:] = self.array","return_type":"None","function_name":"NetCDFVariableFlat.write","stripped_code":"def write(self, ncfile):\n        \"\"\"Write the data to the given NetCDF file.\n\n        See the general documentation on class |NetCDFVariableFlat|\n        for some examples.\n        \"\"\"\n        self.insert_subdevices(ncfile)\n        create_variable(ncfile, self.name, 'f8', self.dimensions)\n        ncfile[self.name][:] = self.array"}
{"code":"def visit_CallTrue(self, node: parsing.CallTrue) -> ast.expr:\n        \"\"\"Generates python code calling the function and returning True.\n\n        lambda: fn(*args) or True\n        \"\"\"\n        return ast.Lambda(\n            ast.arguments([], None, None, [], None, None, [], []),\n            ast.BoolOp(\n                ast.Or(),\n                [\n                    self.visit_Call(node),\n                    ast.Name('True', ast.Load())]))","return_type":"ast.expr","function_name":"RuleVisitor.visit_CallTrue","stripped_code":"def visit_CallTrue(self, node: parsing.CallTrue):\n        \"\"\"Generates python code calling the function and returning True.\n\n        lambda: fn(*args) or True\n        \"\"\"\n        return ast.Lambda(\n            ast.arguments([], None, None, [], None, None, [], []),\n            ast.BoolOp(\n                ast.Or(),\n                [\n                    self.visit_Call(node),\n                    ast.Name('True', ast.Load())]))"}
{"code":"def _taskkill(self, force: bool = False) -> int:\n        \"\"\"\n        Executes a Windows ``TASKKILL /pid PROCESS_ID /t`` command\n        (``/t`` for \"tree kill\" = \"kill all children\").\n\n        Args:\n            force: also add ``/f`` (forcefully)\n\n        Returns:\n            return code from ``TASKKILL``\n\n        **Test code:**\n\n        Firstly we need a program that won't let itself be killed. Save this as\n        ``nokill.py``:\n\n        .. code-block:: python\n\n            #!/usr/bin/env python\n\n            import logging\n            import time\n            import os\n            from cardinal_pythonlib.logs import main_only_quicksetup_rootlogger\n            from cardinal_pythonlib.signalfunc import trap_ctrl_c_ctrl_break\n\n            main_only_quicksetup_rootlogger(level=logging.DEBUG)\n            trap_ctrl_c_ctrl_break()\n\n            while True:\n                print(\"Process ID is {}; time is {} s\".format(os.getpid(), time.clock()))\n                time.sleep(1)\n\n        Now run that with ``python nokill.py``. It should resist CTRL-C and\n        CTRL-BREAK. Start another command prompt in which to play with\n        ``TASKKILL``.\n\n        .. code-block:: bat\n\n            REM Firstly, avoid this single-ampersand syntax:\n            REM     taskkill /im notepad.exe & echo %errorlevel%\n            REM ... as it prints the WRONG (previous?) errorlevel.\n\n            notepad.exe\n            taskkill /im notepad.exe\n            echo %errorlevel%\n            REM ... 0 for success (Windows 10), e.g.\n            REM 'SUCCESS: Sent termination signal to the process \"notepad.exe\" with PID 6988.'\n\n            taskkill /im notepad.exe\n            echo %errorlevel%\n            REM ... 128 for \"not found\" (Windows 10), e.g.\n            REM 'ERROR: The process \"notepad.exe\" not found.'\n\n            REM Now run notepad.exe as Administrator\n            taskkill /im notepad.exe & echo %errorlevel%\n            REM ... 1 for \"access denied\" (Windows 10)\n\n            REM Now kill the nokill.py process by its PID (e.g. 11892 here):\n            taskkill /pid 11892\n            echo %errorlevel%\n            REM ... 1 for \"not allowed\" (Windows 10), e.g.\n            REM 'ERROR: The process with PID 11892 could not be terminated.'\n            REM 'Reason: This process can only be terminated forcefully (with /F option).'\n\n            REM Now forcefully:\n            taskkill /pid 11892 /f\n            echo %errorlevel%\n            REM ... 0 for success (Windows 10), e.g.\n            REM 'SUCCESS: The process with PID 11892 has been terminated.'\n            \n        \"\"\"  # noqa\n        args = [\n            \"taskkill\",  # built in to Windows XP and higher\n            \"/pid\", str(self.process.pid),\n            \"/t\",  # tree kill: kill all children\n        ]\n        if force:\n            args.append(\"/f\")  # forcefully\n        callname = \" \".join(args)\n        retcode = subprocess.call(args)\n        # http://stackoverflow.com/questions/18682681/what-are-exit-codes-from-the-taskkill-utility  # noqa\n        if retcode == winerror.ERROR_SUCCESS:  # 0\n            self.info(\"Killed with \" + repr(callname))\n        elif retcode == winerror.ERROR_INVALID_FUNCTION:  # 1\n            self.warning(\n                repr(callname) +\n                \" failed (error code 1 = ERROR_INVALID_FUNCTION; \"\n                \"can mean 'Access denied', or 'This process can only be \"\n                \"terminated forcefully (with /F option)').\")\n        elif retcode == winerror.ERROR_WAIT_NO_CHILDREN:  # 128\n            self.warning(\n                repr(callname) +\n                \" failed (error code 128 = ERROR_WAIT_NO_CHILDREN \"\n                \"= 'There are no child processes to wait for', but also \"\n                \"occurs when the process doesn't exist, and when processes \"\n                \"require a forceful [/F] termination)\")\n        elif retcode == winerror.ERROR_EA_LIST_INCONSISTENT:  # 255\n            self.warning(\n                repr(callname) +\n                \" failed (error code 255 = ERROR_EA_LIST_INCONSISTENT \"\n                \"= 'The extended attributes are inconsistent.')\")\n        else:\n            self.warning(callname + \" failed: error code {}\".format(retcode))\n        return retcode","return_type":"int","function_name":"ProcessManager._taskkill","stripped_code":"def _taskkill(self, force: bool = False):\n        \"\"\"\n        Executes a Windows ``TASKKILL /pid PROCESS_ID /t`` command\n        (``/t`` for \"tree kill\" = \"kill all children\").\n\n        Args:\n            force: also add ``/f`` (forcefully)\n\n        Returns:\n            return code from ``TASKKILL``\n\n        **Test code:**\n\n        Firstly we need a program that won't let itself be killed. Save this as\n        ``nokill.py``:\n\n        .. code-block:: python\n\n            #!/usr/bin/env python\n\n            import logging\n            import time\n            import os\n            from cardinal_pythonlib.logs import main_only_quicksetup_rootlogger\n            from cardinal_pythonlib.signalfunc import trap_ctrl_c_ctrl_break\n\n            main_only_quicksetup_rootlogger(level=logging.DEBUG)\n            trap_ctrl_c_ctrl_break()\n\n            while True:\n                print(\"Process ID is {}; time is {} s\".format(os.getpid(), time.clock()))\n                time.sleep(1)\n\n        Now run that with ``python nokill.py``. It should resist CTRL-C and\n        CTRL-BREAK. Start another command prompt in which to play with\n        ``TASKKILL``.\n\n        .. code-block:: bat\n\n            REM Firstly, avoid this single-ampersand syntax:\n            REM     taskkill /im notepad.exe & echo %errorlevel%\n            REM ... as it prints the WRONG (previous?) errorlevel.\n\n            notepad.exe\n            taskkill /im notepad.exe\n            echo %errorlevel%\n            REM ... 0 for success (Windows 10), e.g.\n            REM 'SUCCESS: Sent termination signal to the process \"notepad.exe\" with PID 6988.'\n\n            taskkill /im notepad.exe\n            echo %errorlevel%\n            REM ... 128 for \"not found\" (Windows 10), e.g.\n            REM 'ERROR: The process \"notepad.exe\" not found.'\n\n            REM Now run notepad.exe as Administrator\n            taskkill /im notepad.exe & echo %errorlevel%\n            REM ... 1 for \"access denied\" (Windows 10)\n\n            REM Now kill the nokill.py process by its PID (e.g. 11892 here):\n            taskkill /pid 11892\n            echo %errorlevel%\n            REM ... 1 for \"not allowed\" (Windows 10), e.g.\n            REM 'ERROR: The process with PID 11892 could not be terminated.'\n            REM 'Reason: This process can only be terminated forcefully (with /F option).'\n\n            REM Now forcefully:\n            taskkill /pid 11892 /f\n            echo %errorlevel%\n            REM ... 0 for success (Windows 10), e.g.\n            REM 'SUCCESS: The process with PID 11892 has been terminated.'\n            \n        \"\"\"  # noqa\n        args = [\n            \"taskkill\",  # built in to Windows XP and higher\n            \"/pid\", str(self.process.pid),\n            \"/t\",  # tree kill: kill all children\n        ]\n        if force:\n            args.append(\"/f\")  # forcefully\n        callname = \" \".join(args)\n        retcode = subprocess.call(args)\n        # http://stackoverflow.com/questions/18682681/what-are-exit-codes-from-the-taskkill-utility  # noqa\n        if retcode == winerror.ERROR_SUCCESS:  # 0\n            self.info(\"Killed with \" + repr(callname))\n        elif retcode == winerror.ERROR_INVALID_FUNCTION:  # 1\n            self.warning(\n                repr(callname) +\n                \" failed (error code 1 = ERROR_INVALID_FUNCTION; \"\n                \"can mean 'Access denied', or 'This process can only be \"\n                \"terminated forcefully (with /F option)').\")\n        elif retcode == winerror.ERROR_WAIT_NO_CHILDREN:  # 128\n            self.warning(\n                repr(callname) +\n                \" failed (error code 128 = ERROR_WAIT_NO_CHILDREN \"\n                \"= 'There are no child processes to wait for', but also \"\n                \"occurs when the process doesn't exist, and when processes \"\n                \"require a forceful [/F] termination)\")\n        elif retcode == winerror.ERROR_EA_LIST_INCONSISTENT:  # 255\n            self.warning(\n                repr(callname) +\n                \" failed (error code 255 = ERROR_EA_LIST_INCONSISTENT \"\n                \"= 'The extended attributes are inconsistent.')\")\n        else:\n            self.warning(callname + \" failed: error code {}\".format(retcode))\n        return retcode"}
{"code":"def _discover_sensitivity(self, seen) -> None:\n        \"\"\"\n        Doc on parent class :meth:`HdlStatement._discover_sensitivity`\n        \"\"\"\n        assert self._sensitivity is None, self\n        ctx = self._sensitivity = SensitivityCtx()\n\n        casual_sensitivity = set()\n        self.switchOn._walk_sensitivity(casual_sensitivity, seen, ctx)\n        if ctx.contains_ev_dependency:\n            raise HwtSyntaxError(\n                \"Can not switch on event operator result\", self.switchOn)\n        ctx.extend(casual_sensitivity)\n\n        for stm in self._iter_stms():\n            stm._discover_sensitivity(seen)\n            ctx.extend(stm._sensitivity)","return_type":"None","function_name":"SwitchContainer._discover_sensitivity","stripped_code":"def _discover_sensitivity(self, seen):\n        \"\"\"\n        Doc on parent class :meth:`HdlStatement._discover_sensitivity`\n        \"\"\"\n        assert self._sensitivity is None, self\n        ctx = self._sensitivity = SensitivityCtx()\n\n        casual_sensitivity = set()\n        self.switchOn._walk_sensitivity(casual_sensitivity, seen, ctx)\n        if ctx.contains_ev_dependency:\n            raise HwtSyntaxError(\n                \"Can not switch on event operator result\", self.switchOn)\n        ctx.extend(casual_sensitivity)\n\n        for stm in self._iter_stms():\n            stm._discover_sensitivity(seen)\n            ctx.extend(stm._sensitivity)"}
{"code":"def processLedger(self) -> None:\n        \"\"\"\n        Checks ledger config txns and perfomes recent one\n\n        :return:\n        \"\"\"\n        logger.debug('{} processing config ledger for any POOL_CONFIGs'.format(\n            self), extra={\"tags\": [\"pool-config\"]})\n        for _, txn in self.ledger.getAllTxn():\n            if get_type(txn) == POOL_CONFIG:\n                self.handleConfigTxn(txn)","return_type":"None","function_name":"PoolConfig.processLedger","stripped_code":"def processLedger(self):\n        \"\"\"\n        Checks ledger config txns and perfomes recent one\n\n        :return:\n        \"\"\"\n        logger.debug('{} processing config ledger for any POOL_CONFIGs'.format(\n            self), extra={\"tags\": [\"pool-config\"]})\n        for _, txn in self.ledger.getAllTxn():\n            if get_type(txn) == POOL_CONFIG:\n                self.handleConfigTxn(txn)"}
{"code":"def find_entry_point(site_packages: Path, console_script: str) -> str:\n    \"\"\"Find a console_script in a site-packages directory.\n\n    Console script metadata is stored in entry_points.txt per setuptools\n    convention. This function searches all entry_points.txt files and\n    returns the import string for a given console_script argument.\n\n    :param site_packages: A path to a site-packages directory on disk.\n    :param console_script: A console_script string.\n    \"\"\"\n\n    config_parser = ConfigParser()\n    config_parser.read(site_packages.rglob(\"entry_points.txt\"))\n    return config_parser[\"console_scripts\"][console_script]","return_type":"str","function_name":"find_entry_point","stripped_code":"def find_entry_point(site_packages: Path, console_script: str):\n    \"\"\"Find a console_script in a site-packages directory.\n\n    Console script metadata is stored in entry_points.txt per setuptools\n    convention. This function searches all entry_points.txt files and\n    returns the import string for a given console_script argument.\n\n    :param site_packages: A path to a site-packages directory on disk.\n    :param console_script: A console_script string.\n    \"\"\"\n\n    config_parser = ConfigParser()\n    config_parser.read(site_packages.rglob(\"entry_points.txt\"))\n    return config_parser[\"console_scripts\"][console_script]"}
{"code":"def bump_version(v: version.Version, level: str) -> str:\n    \"\"\"Version bump logic\"\"\"\n    release: List[int] = list(v.release)\n    stage: Optional[str]\n    pre: Optional[int]\n    stage, pre = v.pre if v.pre else (None, None)\n    dev: Optional[int] = v.dev\n    post: Optional[int] = v.post\n\n    if level in (\"major\", \"minor\", \"patch\"):\n        segments = 0\n        if level == \"major\":\n            # if the version code is in format of x.0.0, and pre/dev is not empty\n            # do not increase the version number\n            segments = 1\n        elif level == \"minor\":\n            # if the version code is in format of x.x.0, and pre/dev is not empty\n            # do not increase the version number\n            segments = 2\n        elif level == \"patch\":\n            # if the version code is in format of x.x.x, and pre/dev is not empty\n            # do not increase the version number\n            segments = 3\n        if not any(release[segments:]) and (stage is not None or dev is not None):\n            pass\n        else:\n            release[segments - 1] += 1\n        release[segments:] = [0] * max(len(release) - segments, 0)\n        stage = pre = post = dev = None\n    elif level == \"alpha\":\n        if stage is None:\n            if dev is None:\n                release[-1] += 1\n            stage, pre = \"a\", 1\n        elif stage > \"a\":\n            release[-1] += 1\n            stage, pre = \"a\", 1\n        elif stage == \"a\":\n            pre += 1\n        post = dev = None\n    elif level == \"beta\":\n        if stage is None:\n            if dev is None:\n                release[-1] += 1\n            stage, pre = \"b\", 1\n        elif stage > \"b\":\n            release[-1] += 1\n            stage, pre = \"b\", 1\n        elif stage == \"b\":\n            pre += 1\n        elif stage < \"b\":\n            pre = 1\n            stage = \"b\"\n        post = dev = None\n    elif level == \"post\":\n        if post is not None:\n            post += 1\n        else:\n            post = 1\n        dev = None\n    elif level == \"dev\":\n        if dev is not None:\n            dev += 1\n        else:\n            if stage:\n                pre += 1\n            else:\n                release[-1] += 1\n            dev = 1\n\n    ver = \".\".join(str(i) for i in release)\n    if stage is not None:\n        ver += f\"{stage}{pre}\"\n    if post is not None:\n        ver += f\".post{post}\"\n    if dev is not None:\n        ver += f\".dev{dev}\"\n\n    return ver","return_type":"str","function_name":"bump_version","stripped_code":"def bump_version(v: version.Version, level: str):\n    \"\"\"Version bump logic\"\"\"\n    release: List[int] = list(v.release)\n    stage: Optional[str]\n    pre: Optional[int]\n    stage, pre = v.pre if v.pre else (None, None)\n    dev: Optional[int] = v.dev\n    post: Optional[int] = v.post\n\n    if level in (\"major\", \"minor\", \"patch\"):\n        segments = 0\n        if level == \"major\":\n            # if the version code is in format of x.0.0, and pre/dev is not empty\n            # do not increase the version number\n            segments = 1\n        elif level == \"minor\":\n            # if the version code is in format of x.x.0, and pre/dev is not empty\n            # do not increase the version number\n            segments = 2\n        elif level == \"patch\":\n            # if the version code is in format of x.x.x, and pre/dev is not empty\n            # do not increase the version number\n            segments = 3\n        if not any(release[segments:]) and (stage is not None or dev is not None):\n            pass\n        else:\n            release[segments - 1] += 1\n        release[segments:] = [0] * max(len(release) - segments, 0)\n        stage = pre = post = dev = None\n    elif level == \"alpha\":\n        if stage is None:\n            if dev is None:\n                release[-1] += 1\n            stage, pre = \"a\", 1\n        elif stage > \"a\":\n            release[-1] += 1\n            stage, pre = \"a\", 1\n        elif stage == \"a\":\n            pre += 1\n        post = dev = None\n    elif level == \"beta\":\n        if stage is None:\n            if dev is None:\n                release[-1] += 1\n            stage, pre = \"b\", 1\n        elif stage > \"b\":\n            release[-1] += 1\n            stage, pre = \"b\", 1\n        elif stage == \"b\":\n            pre += 1\n        elif stage < \"b\":\n            pre = 1\n            stage = \"b\"\n        post = dev = None\n    elif level == \"post\":\n        if post is not None:\n            post += 1\n        else:\n            post = 1\n        dev = None\n    elif level == \"dev\":\n        if dev is not None:\n            dev += 1\n        else:\n            if stage:\n                pre += 1\n            else:\n                release[-1] += 1\n            dev = 1\n\n    ver = \".\".join(str(i) for i in release)\n    if stage is not None:\n        ver += f\"{stage}{pre}\"\n    if post is not None:\n        ver += f\".post{post}\"\n    if dev is not None:\n        ver += f\".dev{dev}\"\n\n    return ver"}
{"code":"def query_neighbors(self, nodes: List[Node]) -> List[Edge]:\n        \"\"\"Get all edges incident to any of the given nodes.\"\"\"\n        return self.session.query(Edge).filter(self._edge_one_node(nodes)).all()","return_type":"List[Edge]","function_name":"QueryManager.query_neighbors","stripped_code":"def query_neighbors(self, nodes: List[Node]):\n        \"\"\"Get all edges incident to any of the given nodes.\"\"\"\n        return self.session.query(Edge).filter(self._edge_one_node(nodes)).all()"}
{"code":"def _kill(self) -> None:\n        \"\"\"\n        Hard kill.\n        \n        - PROBLEM: originally, via ``self.process.kill()``, could leave orphans\n          under Windows.\n        - SOLUTION: see\n          https://stackoverflow.com/questions/1230669/subprocess-deleting-child-processes-in-windows,\n          which uses ``psutil``.\n\n        \"\"\"  # noqa\n        self.warning(\"Using a recursive hard kill; will assume it worked\")\n        pid = self.process.pid\n        gone, still_alive = kill_proc_tree(pid, including_parent=True,\n                                           timeout_s=self.kill_timeout_sec)\n        self.debug(\"Killed: {!r}\".format(gone))\n        self.warning(\"Still alive: {!r}\".format(still_alive))","return_type":"None","function_name":"ProcessManager._kill","stripped_code":"def _kill(self):\n        \"\"\"\n        Hard kill.\n        \n        - PROBLEM: originally, via ``self.process.kill()``, could leave orphans\n          under Windows.\n        - SOLUTION: see\n          https://stackoverflow.com/questions/1230669/subprocess-deleting-child-processes-in-windows,\n          which uses ``psutil``.\n\n        \"\"\"  # noqa\n        self.warning(\"Using a recursive hard kill; will assume it worked\")\n        pid = self.process.pid\n        gone, still_alive = kill_proc_tree(pid, including_parent=True,\n                                           timeout_s=self.kill_timeout_sec)\n        self.debug(\"Killed: {!r}\".format(gone))\n        self.warning(\"Still alive: {!r}\".format(still_alive))"}
{"code":"def clear(self) -> None:\n        \"\"\"Resets all headers and content for this response.\"\"\"\n        self._headers = httputil.HTTPHeaders(\n            {\n                \"Server\": \"TornadoServer/%s\" % tornado.version,\n                \"Content-Type\": \"text/html; charset=UTF-8\",\n                \"Date\": httputil.format_timestamp(time.time()),\n            }\n        )\n        self.set_default_headers()\n        self._write_buffer = []  # type: List[bytes]\n        self._status_code = 200\n        self._reason = httputil.responses[200]","return_type":"None","function_name":"RequestHandler.clear","stripped_code":"def clear(self):\n        \"\"\"Resets all headers and content for this response.\"\"\"\n        self._headers = httputil.HTTPHeaders(\n            {\n                \"Server\": \"TornadoServer/%s\" % tornado.version,\n                \"Content-Type\": \"text/html; charset=UTF-8\",\n                \"Date\": httputil.format_timestamp(time.time()),\n            }\n        )\n        self.set_default_headers()\n        self._write_buffer = []  # type: List[bytes]\n        self._status_code = 200\n        self._reason = httputil.responses[200]"}
{"code":"def _ending_consonants_only(self, letters: List[str]) -> List[int]:\n        \"\"\"Return a list of positions for ending consonants.\"\"\"\n        reversed_letters = list(reversed(letters))\n        length = len(letters)\n        for idx, letter in enumerate(reversed_letters):\n            if not self._contains_vowels(letter) and self._contains_consonants(letter):\n                return [(length - idx) - 1]\n            if self._contains_vowels(letter):\n                return []\n            if self._contains_vowels(letter) and self._contains_consonants(letter):\n                return []\n        return []","return_type":"List[int]","function_name":"Syllabifier._ending_consonants_only","stripped_code":"def _ending_consonants_only(self, letters: List[str]):\n        \"\"\"Return a list of positions for ending consonants.\"\"\"\n        reversed_letters = list(reversed(letters))\n        length = len(letters)\n        for idx, letter in enumerate(reversed_letters):\n            if not self._contains_vowels(letter) and self._contains_consonants(letter):\n                return [(length - idx) - 1]\n            if self._contains_vowels(letter):\n                return []\n            if self._contains_vowels(letter) and self._contains_consonants(letter):\n                return []\n        return []"}
{"code":"def previous(self) -> \"ArrayEntry\":\n        \"\"\"Return an instance node corresponding to the previous entry.\n\n        Raises:\n            NonexistentInstance: If the receiver is the first entry of the\n                parent array.\n        \"\"\"\n        try:\n            newval, nbef = self.before.pop()\n        except IndexError:\n            raise NonexistentInstance(self.json_pointer(), \"previous of first\") from None\n        return ArrayEntry(\n            self.index - 1, nbef, self.after.cons(self.value), newval,\n            self.parinst, self.schema_node, self.timestamp)","return_type":"\"ArrayEntry\"","function_name":"ArrayEntry.previous","stripped_code":"def previous(self):\n        \"\"\"Return an instance node corresponding to the previous entry.\n\n        Raises:\n            NonexistentInstance: If the receiver is the first entry of the\n                parent array.\n        \"\"\"\n        try:\n            newval, nbef = self.before.pop()\n        except IndexError:\n            raise NonexistentInstance(self.json_pointer(), \"previous of first\") from None\n        return ArrayEntry(\n            self.index - 1, nbef, self.after.cons(self.value), newval,\n            self.parinst, self.schema_node, self.timestamp)"}
{"code":"def is_causal_source(graph: BELGraph, node: BaseEntity) -> bool:\n    \"\"\"Return true of the node is a causal source.\n\n    - Doesn't have any causal in edge(s)\n    - Does have causal out edge(s)\n    \"\"\"\n    # TODO reimplement to be faster\n    return not has_causal_in_edges(graph, node) and has_causal_out_edges(graph, node)","return_type":"bool","function_name":"is_causal_source","stripped_code":"def is_causal_source(graph: BELGraph, node: BaseEntity):\n    \"\"\"Return true of the node is a causal source.\n\n    - Doesn't have any causal in edge(s)\n    - Does have causal out edge(s)\n    \"\"\"\n    # TODO reimplement to be faster\n    return not has_causal_in_edges(graph, node) and has_causal_out_edges(graph, node)"}
{"code":"def parse_match_info(self, req: Request, name: str, field: Field) -> typing.Any:\n        \"\"\"Pull a value from the request's ``match_info``.\"\"\"\n        return core.get_value(req.match_info, name, field)","return_type":"typing.Any","function_name":"AIOHTTPParser.parse_match_info","stripped_code":"def parse_match_info(self, req: Request, name: str, field: Field):\n        \"\"\"Pull a value from the request's ``match_info``.\"\"\"\n        return core.get_value(req.match_info, name, field)"}
{"code":"def reformat_python_docstrings(top_dirs: List[str],\n                               correct_copyright_lines: List[str],\n                               show_only: bool = True,\n                               rewrite: bool = False,\n                               process_only_filenum: int = None) -> None:\n    \"\"\"\n    Walk a directory, finding Python files and rewriting them.\n\n    Args:\n        top_dirs: list of directories to descend into\n        correct_copyright_lines:\n            list of lines (without newlines) representing the copyright\n            docstring block, including the transition lines of equals\n            symbols\n        show_only: show results (to stdout) only; don't rewrite\n        rewrite: write the changes\n        process_only_filenum: only process this file number (1-based index);\n            for debugging only\n    \"\"\"\n    filenum = 0\n    for top_dir in top_dirs:\n        for dirpath, dirnames, filenames in walk(top_dir):\n            for filename in filenames:\n                fullname = join(dirpath, filename)\n                extension = splitext(filename)[1]\n                if extension != PYTHON_EXTENSION:\n                    # log.debug(\"Skipping non-Python file: {}\", fullname)\n                    continue\n\n                filenum += 1\n\n                if process_only_filenum and filenum != process_only_filenum:\n                    continue\n\n                log.info(\"Processing file {}: {}\", filenum, fullname)\n                proc = PythonProcessor(\n                    full_path=fullname,\n                    top_dir=top_dir,\n                    correct_copyright_lines=correct_copyright_lines)\n                if show_only:\n                    proc.show()\n                elif rewrite:\n                    proc.rewrite_file()","return_type":"None","function_name":"reformat_python_docstrings","stripped_code":"def reformat_python_docstrings(top_dirs: List[str],\n                               correct_copyright_lines: List[str],\n                               show_only: bool = True,\n                               rewrite: bool = False,\n                               process_only_filenum: int = None):\n    \"\"\"\n    Walk a directory, finding Python files and rewriting them.\n\n    Args:\n        top_dirs: list of directories to descend into\n        correct_copyright_lines:\n            list of lines (without newlines) representing the copyright\n            docstring block, including the transition lines of equals\n            symbols\n        show_only: show results (to stdout) only; don't rewrite\n        rewrite: write the changes\n        process_only_filenum: only process this file number (1-based index);\n            for debugging only\n    \"\"\"\n    filenum = 0\n    for top_dir in top_dirs:\n        for dirpath, dirnames, filenames in walk(top_dir):\n            for filename in filenames:\n                fullname = join(dirpath, filename)\n                extension = splitext(filename)[1]\n                if extension != PYTHON_EXTENSION:\n                    # log.debug(\"Skipping non-Python file: {}\", fullname)\n                    continue\n\n                filenum += 1\n\n                if process_only_filenum and filenum != process_only_filenum:\n                    continue\n\n                log.info(\"Processing file {}: {}\", filenum, fullname)\n                proc = PythonProcessor(\n                    full_path=fullname,\n                    top_dir=top_dir,\n                    correct_copyright_lines=correct_copyright_lines)\n                if show_only:\n                    proc.show()\n                elif rewrite:\n                    proc.rewrite_file()"}
{"code":"def console_put_char_ex(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    c: Union[int, str],\n    fore: Tuple[int, int, int],\n    back: Tuple[int, int, int],\n) -> None:\n    \"\"\"Draw the character c at x,y using the colors fore and back.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        c (Union[int, AnyStr]): Character to draw, can be an integer or string.\n        fore (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n        back (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n    \"\"\"\n    lib.TCOD_console_put_char_ex(_console(con), x, y, _int(c), fore, back)","return_type":"None","function_name":"console_put_char_ex","stripped_code":"def console_put_char_ex(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    c: Union[int, str],\n    fore: Tuple[int, int, int],\n    back: Tuple[int, int, int],\n):\n    \"\"\"Draw the character c at x,y using the colors fore and back.\n\n    Args:\n        con (Console): Any Console instance.\n        x (int): Character x position from the left.\n        y (int): Character y position from the top.\n        c (Union[int, AnyStr]): Character to draw, can be an integer or string.\n        fore (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n        back (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n    \"\"\"\n    lib.TCOD_console_put_char_ex(_console(con), x, y, _int(c), fore, back)"}
{"code":"def clear_header(self, name: str) -> None:\n        \"\"\"Clears an outgoing header, undoing a previous `set_header` call.\n\n        Note that this method does not apply to multi-valued headers\n        set by `add_header`.\n        \"\"\"\n        if name in self._headers:\n            del self._headers[name]","return_type":"None","function_name":"RequestHandler.clear_header","stripped_code":"def clear_header(self, name: str):\n        \"\"\"Clears an outgoing header, undoing a previous `set_header` call.\n\n        Note that this method does not apply to multi-valued headers\n        set by `add_header`.\n        \"\"\"\n        if name in self._headers:\n            del self._headers[name]"}
{"code":"def get_name(self, **values) -> str:\n        \"\"\"Get a new name string from this object's name values.\n\n        :param values: Variable keyword arguments where the **key** should refer to a field on this object that will\n                       use the provided **value** to build the new name.\n        \"\"\"\n        if not values and self.name:\n            return self.name\n        if values:\n            # if values are provided, solve compounds that may be affected\n            for ck, cvs in _sorted_items(self.compounds):\n                if ck in cvs and ck in values:  # redefined compound name to outer scope e.g. fifth = (fifth, sixth)\n                    continue\n                comp_values = [values.pop(cv, getattr(self, cv)) for cv in cvs]\n                if None not in comp_values:\n                    values[ck] = ''.join(rf'{v}' for v in comp_values)\n        return self._get_nice_name(**values)","return_type":"str","function_name":"_BaseName.get_name","stripped_code":"def get_name(self, **values):\n        \"\"\"Get a new name string from this object's name values.\n\n        :param values: Variable keyword arguments where the **key** should refer to a field on this object that will\n                       use the provided **value** to build the new name.\n        \"\"\"\n        if not values and self.name:\n            return self.name\n        if values:\n            # if values are provided, solve compounds that may be affected\n            for ck, cvs in _sorted_items(self.compounds):\n                if ck in cvs and ck in values:  # redefined compound name to outer scope e.g. fifth = (fifth, sixth)\n                    continue\n                comp_values = [values.pop(cv, getattr(self, cv)) for cv in cvs]\n                if None not in comp_values:\n                    values[ck] = ''.join(rf'{v}' for v in comp_values)\n        return self._get_nice_name(**values)"}
{"code":"def get_project(self) -> str:\n        \"\"\" Get the ihc project and make sure controller is ready before\"\"\"\n        with IHCController._mutex:\n            if self._project is None:\n                if self.client.get_state() != IHCSTATE_READY:\n                    ready = self.client.wait_for_state_change(IHCSTATE_READY,\n                                                              10)\n                    if ready != IHCSTATE_READY:\n                        return None\n                self._project = self.client.get_project()\n        return self._project","return_type":"str","function_name":"IHCController.get_project","stripped_code":"def get_project(self):\n        \"\"\" Get the ihc project and make sure controller is ready before\"\"\"\n        with IHCController._mutex:\n            if self._project is None:\n                if self.client.get_state() != IHCSTATE_READY:\n                    ready = self.client.wait_for_state_change(IHCSTATE_READY,\n                                                              10)\n                    if ready != IHCSTATE_READY:\n                        return None\n                self._project = self.client.get_project()\n        return self._project"}
{"code":"def init_edge_number(self) -> int:\n        \"\"\"Return the number of edges present in the non-compressed graph\"\"\"\n        return len(frozenset(frozenset(edge) for edge in self.initial_edges()))","return_type":"int","function_name":"BubbleTree.init_edge_number","stripped_code":"def init_edge_number(self):\n        \"\"\"Return the number of edges present in the non-compressed graph\"\"\"\n        return len(frozenset(frozenset(edge) for edge in self.initial_edges()))"}
{"code":"def can_claim_fifty_moves(self) -> bool:\n        \"\"\"\n        Draw by the fifty-move rule can be claimed once the clock of halfmoves\n        since the last capture or pawn move becomes equal or greater to 100\n        and the side to move still has a legal move they can make.\n        \"\"\"\n        # Fifty-move rule.\n        if self.halfmove_clock >= 100:\n            if any(self.generate_legal_moves()):\n                return True\n\n        return False","return_type":"bool","function_name":"Board.can_claim_fifty_moves","stripped_code":"def can_claim_fifty_moves(self):\n        \"\"\"\n        Draw by the fifty-move rule can be claimed once the clock of halfmoves\n        since the last capture or pawn move becomes equal or greater to 100\n        and the side to move still has a legal move they can make.\n        \"\"\"\n        # Fifty-move rule.\n        if self.halfmove_clock >= 100:\n            if any(self.generate_legal_moves()):\n                return True\n\n        return False"}
{"code":"def sample(self, trials: int) -> np.ndarray:\n        \"\"\"Measure the state in the computational basis the the given number\n        of trials, and return the counts of each output configuration.\n        \"\"\"\n        # TODO: Can we do this within backend?\n        probs = np.real(bk.evaluate(self.probabilities()))\n        res = np.random.multinomial(trials, probs.ravel())\n        res = res.reshape(probs.shape)\n        return res","return_type":"np.ndarray","function_name":"State.sample","stripped_code":"def sample(self, trials: int):\n        \"\"\"Measure the state in the computational basis the the given number\n        of trials, and return the counts of each output configuration.\n        \"\"\"\n        # TODO: Can we do this within backend?\n        probs = np.real(bk.evaluate(self.probabilities()))\n        res = np.random.multinomial(trials, probs.ravel())\n        res = res.reshape(probs.shape)\n        return res"}
{"code":"def add_days(self, days: int) -> datetime:\n        \"\"\" Adds days \"\"\"\n        self.value = self.value + relativedelta(days=days)\n        return self.value","return_type":"datetime","function_name":"Datum.add_days","stripped_code":"def add_days(self, days: int):\n        \"\"\" Adds days \"\"\"\n        self.value = self.value + relativedelta(days=days)\n        return self.value"}
{"code":"def oauth2_url(self, redirect_uri: str, scope: Optional[str] = None, state: Optional[str] = None) -> str:\n        \"\"\"Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.\n        \"\"\"\n        return OAuth2.url_(self.http.client_id, redirect_uri, scope=scope, state=state)","return_type":"str","function_name":"Client.oauth2_url","stripped_code":"def oauth2_url(self, redirect_uri: str, scope: Optional[str] = None, state: Optional[str] = None):\n        \"\"\"Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.\n        \"\"\"\n        return OAuth2.url_(self.http.client_id, redirect_uri, scope=scope, state=state)"}
{"code":"def should_set_cookie(self, app: 'Quart', session: SessionMixin) -> bool:\n        \"\"\"Helper method to return if the Set Cookie header should be present.\n\n        This triggers if the session is marked as modified or the app\n        is configured to always refresh the cookie.\n        \"\"\"\n        if session.modified:\n            return True\n        save_each = app.config['SESSION_REFRESH_EACH_REQUEST']\n        return save_each and session.permanent","return_type":"bool","function_name":"SessionInterface.should_set_cookie","stripped_code":"def should_set_cookie(self, app: 'Quart', session: SessionMixin):\n        \"\"\"Helper method to return if the Set Cookie header should be present.\n\n        This triggers if the session is marked as modified or the app\n        is configured to always refresh the cookie.\n        \"\"\"\n        if session.modified:\n            return True\n        save_each = app.config['SESSION_REFRESH_EACH_REQUEST']\n        return save_each and session.permanent"}
{"code":"def file_name(self, file_type: Optional[FileType] = None) -> str:\n        \"\"\"Get a random file name with some extension.\n\n        :param file_type: Enum object FileType\n        :return: File name.\n\n        :Example:\n            legislative.txt\n        \"\"\"\n        name = self.__text.word()\n        ext = self.extension(file_type)\n\n        return '{name}{ext}'.format(\n            name=self.__sub(name),\n            ext=ext,\n        )","return_type":"str","function_name":"File.file_name","stripped_code":"def file_name(self, file_type: Optional[FileType] = None):\n        \"\"\"Get a random file name with some extension.\n\n        :param file_type: Enum object FileType\n        :return: File name.\n\n        :Example:\n            legislative.txt\n        \"\"\"\n        name = self.__text.word()\n        ext = self.extension(file_type)\n\n        return '{name}{ext}'.format(\n            name=self.__sub(name),\n            ext=ext,\n        )"}
{"code":"def _to_bel_lines_body(graph) -> Iterable[str]:\n    \"\"\"Iterate the lines of a BEL graph's corresponding BEL script's body.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    qualified_edges = sort_qualified_edges(graph)\n\n    for citation, citation_edges in group_citation_edges(qualified_edges):\n        yield 'SET Citation = {{{}}}\\n'.format(citation)\n\n        for evidence, evidence_edges in group_evidence_edges(citation_edges):\n            yield 'SET SupportingText = \"{}\"'.format(evidence)\n\n            for u, v, _, data in evidence_edges:\n                annotations_data = data.get(ANNOTATIONS)\n\n                keys = sorted(annotations_data) if annotations_data is not None else tuple()\n                for key in keys:\n                    yield _set_annotation_to_str(annotations_data, key)\n\n                yield graph.edge_to_bel(u, v, data)\n\n                if keys:\n                    yield _unset_annotation_to_str(keys)\n\n            yield 'UNSET SupportingText'\n        yield 'UNSET Citation\\n'\n        yield '#' * 80","return_type":"Iterable[str]","function_name":"_to_bel_lines_body","stripped_code":"def _to_bel_lines_body(graph):\n    \"\"\"Iterate the lines of a BEL graph's corresponding BEL script's body.\n\n    :param pybel.BELGraph graph: A BEL graph\n    \"\"\"\n    qualified_edges = sort_qualified_edges(graph)\n\n    for citation, citation_edges in group_citation_edges(qualified_edges):\n        yield 'SET Citation = {{{}}}\\n'.format(citation)\n\n        for evidence, evidence_edges in group_evidence_edges(citation_edges):\n            yield 'SET SupportingText = \"{}\"'.format(evidence)\n\n            for u, v, _, data in evidence_edges:\n                annotations_data = data.get(ANNOTATIONS)\n\n                keys = sorted(annotations_data) if annotations_data is not None else tuple()\n                for key in keys:\n                    yield _set_annotation_to_str(annotations_data, key)\n\n                yield graph.edge_to_bel(u, v, data)\n\n                if keys:\n                    yield _unset_annotation_to_str(keys)\n\n            yield 'UNSET SupportingText'\n        yield 'UNSET Citation\\n'\n        yield '#' * 80"}
{"code":"def from_path(cls, path: pathlib.Path) -> 'Entity':\n        \"\"\"\n        Create an entity from a local path.\n\n        :param path: The path to the entity, either a file or directory.\n        :return: An entity instance representing the path.\n        \"\"\"\n        if path.is_file():\n            return File.from_path(path)\n        return Directory.from_path(path)","return_type":"'Entity'","function_name":"Entity.from_path","stripped_code":"def from_path(cls, path: pathlib.Path):\n        \"\"\"\n        Create an entity from a local path.\n\n        :param path: The path to the entity, either a file or directory.\n        :return: An entity instance representing the path.\n        \"\"\"\n        if path.is_file():\n            return File.from_path(path)\n        return Directory.from_path(path)"}
{"code":"def inject_url_defaults(self, endpoint: str, values: dict) -> None:\n        \"\"\"Injects default URL values into the passed values dict.\n\n        This is used to assist when building urls, see\n        :func:`~quart.helpers.url_for`.\n        \"\"\"\n        functions = self.url_value_preprocessors[None]\n        if '.' in endpoint:\n            blueprint = endpoint.rsplit('.', 1)[0]\n            functions = chain(functions, self.url_value_preprocessors[blueprint])  # type: ignore\n\n        for function in functions:\n            function(endpoint, values)","return_type":"None","function_name":"Quart.inject_url_defaults","stripped_code":"def inject_url_defaults(self, endpoint: str, values: dict):\n        \"\"\"Injects default URL values into the passed values dict.\n\n        This is used to assist when building urls, see\n        :func:`~quart.helpers.url_for`.\n        \"\"\"\n        functions = self.url_value_preprocessors[None]\n        if '.' in endpoint:\n            blueprint = endpoint.rsplit('.', 1)[0]\n            functions = chain(functions, self.url_value_preprocessors[blueprint])  # type: ignore\n\n        for function in functions:\n            function(endpoint, values)"}
{"code":"def surviors_are_inconsistent(survivor_mapping: Mapping[BaseEntity, Set[BaseEntity]]) -> Set[BaseEntity]:\n    \"\"\"Check that there's no transitive shit going on.\"\"\"\n    victim_mapping = set()\n    for victim in itt.chain.from_iterable(survivor_mapping.values()):\n        if victim in survivor_mapping:\n            victim_mapping.add(victim)\n    return victim_mapping","return_type":"Set[BaseEntity]","function_name":"surviors_are_inconsistent","stripped_code":"def surviors_are_inconsistent(survivor_mapping: Mapping[BaseEntity, Set[BaseEntity]]):\n    \"\"\"Check that there's no transitive shit going on.\"\"\"\n    victim_mapping = set()\n    for victim in itt.chain.from_iterable(survivor_mapping.values()):\n        if victim in survivor_mapping:\n            victim_mapping.add(victim)\n    return victim_mapping"}
{"code":"def links(self, base_link, current_page) -> dict:\r\n        \"\"\" Return JSON paginate links \"\"\"\r\n        max_pages = self.max_pages - 1 if \\\r\n            self.max_pages > 0 else self.max_pages\r\n        base_link = '/%s' % (base_link.strip(\"/\"))\r\n        self_page = current_page\r\n        prev = current_page - 1 if current_page is not 0 else None\r\n        prev_link = '%s/page/%s/%s' % (base_link, prev, self.limit) if \\\r\n            prev is not None else None\r\n        next = current_page + 1 if current_page < max_pages else None\r\n        next_link = '%s/page/%s/%s' % (base_link, next, self.limit) if \\\r\n            next is not None else None\r\n        first = 0\r\n        last = max_pages\r\n        return {\r\n            'self': '%s/page/%s/%s' % (base_link, self_page, self.limit),\r\n            'prev': prev_link,\r\n            'next': next_link,\r\n            'first': '%s/page/%s/%s' % (base_link, first, self.limit),\r\n            'last': '%s/page/%s/%s' % (base_link, last, self.limit),\r\n        }","return_type":"dict","function_name":"Paginate.links","stripped_code":"def links(self, base_link, current_page):\r\n        \"\"\" Return JSON paginate links \"\"\"\r\n        max_pages = self.max_pages - 1 if \\\r\n            self.max_pages > 0 else self.max_pages\r\n        base_link = '/%s' % (base_link.strip(\"/\"))\r\n        self_page = current_page\r\n        prev = current_page - 1 if current_page is not 0 else None\r\n        prev_link = '%s/page/%s/%s' % (base_link, prev, self.limit) if \\\r\n            prev is not None else None\r\n        next = current_page + 1 if current_page < max_pages else None\r\n        next_link = '%s/page/%s/%s' % (base_link, next, self.limit) if \\\r\n            next is not None else None\r\n        first = 0\r\n        last = max_pages\r\n        return {\r\n            'self': '%s/page/%s/%s' % (base_link, self_page, self.limit),\r\n            'prev': prev_link,\r\n            'next': next_link,\r\n            'first': '%s/page/%s/%s' % (base_link, first, self.limit),\r\n            'last': '%s/page/%s/%s' % (base_link, last, self.limit),\r\n        }"}
{"code":"def description(self) -> str:\n        \"\"\"\n        Short description.\n        \"\"\"\n        if self.is_parent and self.is_child:\n            desc = \"parent+child\"\n        elif self.is_parent:\n            desc = \"parent\"\n        elif self.is_child:\n            desc = \"child\"\n        else:\n            desc = \"standalone\"\n        if self.circular:\n            desc += \"+CIRCULAR({})\".format(self.circular_description)\n        return desc","return_type":"str","function_name":"TableDependencyClassification.description","stripped_code":"def description(self):\n        \"\"\"\n        Short description.\n        \"\"\"\n        if self.is_parent and self.is_child:\n            desc = \"parent+child\"\n        elif self.is_parent:\n            desc = \"parent\"\n        elif self.is_child:\n            desc = \"child\"\n        else:\n            desc = \"standalone\"\n        if self.circular:\n            desc += \"+CIRCULAR({})\".format(self.circular_description)\n        return desc"}
{"code":"def _date_val(self, dt: datetime) -> None:\n        \"\"\"\n        Add a date value\n        :param dt: datetime to add\n        \"\"\"\n        self._tval_char = dt.strftime('%Y-%m-%d %H:%M')\n        self._nval_num = (dt.year * 10000) + (dt.month * 100) + dt.day + \\\n                         (((dt.hour / 100.0) + (dt.minute / 10000.0)) if isinstance(dt, datetime) else 0)","return_type":"None","function_name":"ObservationFact._date_val","stripped_code":"def _date_val(self, dt: datetime):\n        \"\"\"\n        Add a date value\n        :param dt: datetime to add\n        \"\"\"\n        self._tval_char = dt.strftime('%Y-%m-%d %H:%M')\n        self._nval_num = (dt.year * 10000) + (dt.month * 100) + dt.day + \\\n                         (((dt.hour / 100.0) + (dt.minute / 10000.0)) if isinstance(dt, datetime) else 0)"}
{"code":"def new_withdraw_ong_transaction(self, b58_claimer_address: str, b58_recv_address: str, amount: int,\n                                     b58_payer_address: str, gas_limit: int, gas_price: int) -> Transaction:\n        \"\"\"\n        This interface is used to generate a Transaction object that\n        allow one account to withdraw an amount of ong and transfer them to receive address.\n\n        :param b58_claimer_address: a base58 encode address which is used to indicate who is the claimer.\n        :param b58_recv_address: a base58 encode address which is used to indicate who receive the claimed ong.\n        :param amount: the amount of asset that will be claimed.\n        :param b58_payer_address: a base58 encode address which indicate who will pay for the transaction.\n        :param gas_limit: an int value that indicate the gas limit.\n        :param gas_price: an int value that indicate the gas price.\n        :return: a Transaction object which can be used for withdraw ong.\n        \"\"\"\n        if not isinstance(b58_claimer_address, str) or not isinstance(b58_recv_address, str) or not isinstance(\n                b58_payer_address, str):\n            raise SDKException(ErrorCode.param_err('the data type of base58 encode address should be the string.'))\n        if len(b58_claimer_address) != 34 or len(b58_recv_address) != 34 or len(b58_payer_address) != 34:\n            raise SDKException(ErrorCode.param_err('the length of base58 encode address should be 34 bytes.'))\n        if amount <= 0:\n            raise SDKException(ErrorCode.other_error('the amount should be greater than than zero.'))\n        if gas_price < 0:\n            raise SDKException(ErrorCode.other_error('the gas price should be equal or greater than zero.'))\n        if gas_limit < 0:\n            raise SDKException(ErrorCode.other_error('the gas limit should be equal or greater than zero.'))\n        ont_contract_address = self.get_asset_address('ont')\n        ong_contract_address = self.get_asset_address(\"ong\")\n        args = {\"sender\": Address.b58decode(b58_claimer_address).to_bytes(), \"from\": ont_contract_address,\n                \"to\": Address.b58decode(b58_recv_address).to_bytes(), \"value\": amount}\n        invoke_code = build_native_invoke_code(ong_contract_address, b'\\x00', \"transferFrom\", args)\n        payer_array = Address.b58decode(b58_payer_address).to_bytes()\n        return Transaction(0, 0xd1, int(time()), gas_price, gas_limit, payer_array, invoke_code, bytearray(), list())","return_type":"Transaction","function_name":"Asset.new_withdraw_ong_transaction","stripped_code":"def new_withdraw_ong_transaction(self, b58_claimer_address: str, b58_recv_address: str, amount: int,\n                                     b58_payer_address: str, gas_limit: int, gas_price: int):\n        \"\"\"\n        This interface is used to generate a Transaction object that\n        allow one account to withdraw an amount of ong and transfer them to receive address.\n\n        :param b58_claimer_address: a base58 encode address which is used to indicate who is the claimer.\n        :param b58_recv_address: a base58 encode address which is used to indicate who receive the claimed ong.\n        :param amount: the amount of asset that will be claimed.\n        :param b58_payer_address: a base58 encode address which indicate who will pay for the transaction.\n        :param gas_limit: an int value that indicate the gas limit.\n        :param gas_price: an int value that indicate the gas price.\n        :return: a Transaction object which can be used for withdraw ong.\n        \"\"\"\n        if not isinstance(b58_claimer_address, str) or not isinstance(b58_recv_address, str) or not isinstance(\n                b58_payer_address, str):\n            raise SDKException(ErrorCode.param_err('the data type of base58 encode address should be the string.'))\n        if len(b58_claimer_address) != 34 or len(b58_recv_address) != 34 or len(b58_payer_address) != 34:\n            raise SDKException(ErrorCode.param_err('the length of base58 encode address should be 34 bytes.'))\n        if amount <= 0:\n            raise SDKException(ErrorCode.other_error('the amount should be greater than than zero.'))\n        if gas_price < 0:\n            raise SDKException(ErrorCode.other_error('the gas price should be equal or greater than zero.'))\n        if gas_limit < 0:\n            raise SDKException(ErrorCode.other_error('the gas limit should be equal or greater than zero.'))\n        ont_contract_address = self.get_asset_address('ont')\n        ong_contract_address = self.get_asset_address(\"ong\")\n        args = {\"sender\": Address.b58decode(b58_claimer_address).to_bytes(), \"from\": ont_contract_address,\n                \"to\": Address.b58decode(b58_recv_address).to_bytes(), \"value\": amount}\n        invoke_code = build_native_invoke_code(ong_contract_address, b'\\x00', \"transferFrom\", args)\n        payer_array = Address.b58decode(b58_payer_address).to_bytes()\n        return Transaction(0, 0xd1, int(time()), gas_price, gas_limit, payer_array, invoke_code, bytearray(), list())"}
{"code":"def _descendants(self, qname: Union[QualName, bool] = None,\n                     with_self: bool = False) -> List[\"InstanceNode\"]:\n        \"\"\"XPath - return the list of receiver's descendants.\"\"\"\n        res = ([] if not with_self or (qname and self.qual_name != qname)\n               else [self])\n        for c in self._children():\n            if not qname or c.qual_name == qname:\n                res.append(c)\n            res += c._descendants(qname)\n        return res","return_type":"List[\"InstanceNode\"]","function_name":"InstanceNode._descendants","stripped_code":"def _descendants(self, qname: Union[QualName, bool] = None,\n                     with_self: bool = False):\n        \"\"\"XPath - return the list of receiver's descendants.\"\"\"\n        res = ([] if not with_self or (qname and self.qual_name != qname)\n               else [self])\n        for c in self._children():\n            if not qname or c.qual_name == qname:\n                res.append(c)\n            res += c._descendants(qname)\n        return res"}
{"code":"def tx_serialization_order(provider: Provider, blockhash: str, txid: str) -> int:\n    '''find index of this tx in the blockid'''\n\n    return provider.getblock(blockhash)[\"tx\"].index(txid)","return_type":"int","function_name":"tx_serialization_order","stripped_code":"def tx_serialization_order(provider: Provider, blockhash: str, txid: str):\n    '''find index of this tx in the blockid'''\n\n    return provider.getblock(blockhash)[\"tx\"].index(txid)"}
{"code":"def linked(base_dir: str, rr_id: str) -> str:\n        \"\"\"\n        Get, from the specified directory, the path to the tails file associated with\n        the input revocation registry identifier, or None for no such file.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param rr_id: rev reg id\n        :return: (stringified) path to tails file of interest, or None for no such file.\n        \"\"\"\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        link = join(base_dir, cd_id, rr_id)\n        return join(base_dir, cd_id, readlink(link)) if islink(link) else None","return_type":"str","function_name":"Tails.linked","stripped_code":"def linked(base_dir: str, rr_id: str):\n        \"\"\"\n        Get, from the specified directory, the path to the tails file associated with\n        the input revocation registry identifier, or None for no such file.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param rr_id: rev reg id\n        :return: (stringified) path to tails file of interest, or None for no such file.\n        \"\"\"\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        link = join(base_dir, cd_id, rr_id)\n        return join(base_dir, cd_id, readlink(link)) if islink(link) else None"}
{"code":"def _const_node_to_py_ast(ctx: GeneratorContext, lisp_ast: Const) -> GeneratedPyAST:\n    \"\"\"Generate Python AST nodes for a :const Lisp AST node.\n\n    Nested values in collections for :const nodes are not parsed. Consequently,\n    this function cannot be called recursively for those nested values. Instead,\n    call `_const_val_to_py_ast` on nested values.\"\"\"\n    assert lisp_ast.op == NodeOp.CONST\n    node_type = lisp_ast.type\n    handle_const_node = _CONSTANT_HANDLER.get(node_type)\n    assert handle_const_node is not None, f\"No :const AST type handler for {node_type}\"\n    node_val = lisp_ast.val\n    return handle_const_node(ctx, node_val)","return_type":"GeneratedPyAST","function_name":"_const_node_to_py_ast","stripped_code":"def _const_node_to_py_ast(ctx: GeneratorContext, lisp_ast: Const):\n    \"\"\"Generate Python AST nodes for a :const Lisp AST node.\n\n    Nested values in collections for :const nodes are not parsed. Consequently,\n    this function cannot be called recursively for those nested values. Instead,\n    call `_const_val_to_py_ast` on nested values.\"\"\"\n    assert lisp_ast.op == NodeOp.CONST\n    node_type = lisp_ast.type\n    handle_const_node = _CONSTANT_HANDLER.get(node_type)\n    assert handle_const_node is not None, f\"No :const AST type handler for {node_type}\"\n    node_val = lisp_ast.val\n    return handle_const_node(ctx, node_val)"}
{"code":"def rename(python_data: LdapObject, new_base_dn: str = None,\n           database: Optional[Database] = None, **kwargs) -> LdapObject:\n    \"\"\" Move/rename a LdapObject in the database. \"\"\"\n    table = type(python_data)\n    dn = python_data.get_as_single('dn')\n    assert dn is not None\n\n    database = get_database(database)\n    connection = database.connection\n\n    # extract key and value from kwargs\n    if len(kwargs) == 1:\n        name, value = list(kwargs.items())[0]\n\n        # work out the new rdn of the object\n        split_new_rdn = [[(name, value, 1)]]\n\n        field = _get_field_by_name(table, name)\n        assert field.db_field\n\n        python_data = python_data.merge({\n            name: value,\n        })\n\n    elif len(kwargs) == 0:\n        split_new_rdn = [str2dn(dn)[0]]\n    else:\n        assert False\n\n    new_rdn = dn2str(split_new_rdn)\n\n    connection.rename(\n        dn,\n        new_rdn,\n        new_base_dn,\n    )\n\n    if new_base_dn is not None:\n        split_base_dn = str2dn(new_base_dn)\n    else:\n        split_base_dn = str2dn(dn)[1:]\n\n    tmp_list = [split_new_rdn[0]]\n    tmp_list.extend(split_base_dn)\n\n    new_dn = dn2str(tmp_list)\n\n    python_data = python_data.merge({\n        'dn': new_dn,\n    })\n    return python_data","return_type":"LdapObject","function_name":"rename","stripped_code":"def rename(python_data: LdapObject, new_base_dn: str = None,\n           database: Optional[Database] = None, **kwargs):\n    \"\"\" Move/rename a LdapObject in the database. \"\"\"\n    table = type(python_data)\n    dn = python_data.get_as_single('dn')\n    assert dn is not None\n\n    database = get_database(database)\n    connection = database.connection\n\n    # extract key and value from kwargs\n    if len(kwargs) == 1:\n        name, value = list(kwargs.items())[0]\n\n        # work out the new rdn of the object\n        split_new_rdn = [[(name, value, 1)]]\n\n        field = _get_field_by_name(table, name)\n        assert field.db_field\n\n        python_data = python_data.merge({\n            name: value,\n        })\n\n    elif len(kwargs) == 0:\n        split_new_rdn = [str2dn(dn)[0]]\n    else:\n        assert False\n\n    new_rdn = dn2str(split_new_rdn)\n\n    connection.rename(\n        dn,\n        new_rdn,\n        new_base_dn,\n    )\n\n    if new_base_dn is not None:\n        split_base_dn = str2dn(new_base_dn)\n    else:\n        split_base_dn = str2dn(dn)[1:]\n\n    tmp_list = [split_new_rdn[0]]\n    tmp_list.extend(split_base_dn)\n\n    new_dn = dn2str(tmp_list)\n\n    python_data = python_data.merge({\n        'dn': new_dn,\n    })\n    return python_data"}
{"code":"def rmglob(pattern: str) -> None:\n    \"\"\"\n    Deletes all files whose filename matches the glob ``pattern`` (via\n    :func:`glob.glob`).\n    \"\"\"\n    for f in glob.glob(pattern):\n        os.remove(f)","return_type":"None","function_name":"rmglob","stripped_code":"def rmglob(pattern: str):\n    \"\"\"\n    Deletes all files whose filename matches the glob ``pattern`` (via\n    :func:`glob.glob`).\n    \"\"\"\n    for f in glob.glob(pattern):\n        os.remove(f)"}
{"code":"def add_field(self, field_name: str, field: Field, vocab: Vocabulary = None) -> None:\n        \"\"\"\n        Add the field to the existing fields mapping.\n        If we have already indexed the Instance, then we also index `field`, so\n        it is necessary to supply the vocab.\n        \"\"\"\n        self.fields[field_name] = field\n        if self.indexed:\n            field.index(vocab)","return_type":"None","function_name":"Instance.add_field","stripped_code":"def add_field(self, field_name: str, field: Field, vocab: Vocabulary = None):\n        \"\"\"\n        Add the field to the existing fields mapping.\n        If we have already indexed the Instance, then we also index `field`, so\n        it is necessary to supply the vocab.\n        \"\"\"\n        self.fields[field_name] = field\n        if self.indexed:\n            field.index(vocab)"}
{"code":"def update_chat_username(\n        self,\n        chat_id: Union[int, str],\n        username: Union[str, None]\n    ) -> bool:\n        \"\"\"Use this method to update a channel or a supergroup username.\n        \n        To update your own username (for users only, not bots) you can use :meth:`update_username`.\n\n        Args:\n            chat_id (``int`` | ``str``)\n                Unique identifier (int) or username (str) of the target chat.\n            username (``str`` | ``None``):\n                Username to set. Pass \"\" (empty string) or None to remove the username.\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n            ``ValueError`` if a chat_id belongs to a user or chat.\n        \"\"\"\n\n        peer = self.resolve_peer(chat_id)\n\n        if isinstance(peer, types.InputPeerChannel):\n            return bool(\n                self.send(\n                    functions.channels.UpdateUsername(\n                        channel=peer,\n                        username=username or \"\"\n                    )\n                )\n            )\n        else:\n            raise ValueError(\"The chat_id \\\"{}\\\" belongs to a user or chat\".format(chat_id))","return_type":"bool","function_name":"UpdateChatUsername.update_chat_username","stripped_code":"def update_chat_username(\n        self,\n        chat_id: Union[int, str],\n        username: Union[str, None]\n    ):\n        \"\"\"Use this method to update a channel or a supergroup username.\n        \n        To update your own username (for users only, not bots) you can use :meth:`update_username`.\n\n        Args:\n            chat_id (``int`` | ``str``)\n                Unique identifier (int) or username (str) of the target chat.\n            username (``str`` | ``None``):\n                Username to set. Pass \"\" (empty string) or None to remove the username.\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n            ``ValueError`` if a chat_id belongs to a user or chat.\n        \"\"\"\n\n        peer = self.resolve_peer(chat_id)\n\n        if isinstance(peer, types.InputPeerChannel):\n            return bool(\n                self.send(\n                    functions.channels.UpdateUsername(\n                        channel=peer,\n                        username=username or \"\"\n                    )\n                )\n            )\n        else:\n            raise ValueError(\"The chat_id \\\"{}\\\" belongs to a user or chat\".format(chat_id))"}
{"code":"def deployment(\n    *,\n    block_uri: URI,\n    contract_instance: str,\n    contract_type: str,\n    address: HexStr,\n    transaction: HexStr = None,\n    block: HexStr = None,\n    deployment_bytecode: Dict[str, Any] = None,\n    runtime_bytecode: Dict[str, Any] = None,\n    compiler: Dict[str, Any] = None,\n) -> Manifest:\n    \"\"\"\n    Returns a manifest, with the newly included deployment. Requires a valid blockchain URI,\n    however no validation is provided that this URI is unique amongst the other deployment\n    URIs, so the user must take care that each blockchain URI represents a unique blockchain.\n    \"\"\"\n    return _deployment(\n        contract_instance,\n        contract_type,\n        deployment_bytecode,\n        runtime_bytecode,\n        compiler,\n        block_uri,\n        address,\n        transaction,\n        block,\n    )","return_type":"Manifest","function_name":"deployment","stripped_code":"def deployment(\n    *,\n    block_uri: URI,\n    contract_instance: str,\n    contract_type: str,\n    address: HexStr,\n    transaction: HexStr = None,\n    block: HexStr = None,\n    deployment_bytecode: Dict[str, Any] = None,\n    runtime_bytecode: Dict[str, Any] = None,\n    compiler: Dict[str, Any] = None,\n):\n    \"\"\"\n    Returns a manifest, with the newly included deployment. Requires a valid blockchain URI,\n    however no validation is provided that this URI is unique amongst the other deployment\n    URIs, so the user must take care that each blockchain URI represents a unique blockchain.\n    \"\"\"\n    return _deployment(\n        contract_instance,\n        contract_type,\n        deployment_bytecode,\n        runtime_bytecode,\n        compiler,\n        block_uri,\n        address,\n        transaction,\n        block,\n    )"}
{"code":"def in_placement_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if you can place something at a position. Remember, buildings usually use 2x2, 3x3 or 5x5 of these grid points.\n        Caution: some x and y offset might be required, see ramp code:\n        https://github.com/Dentosal/python-sc2/blob/master/sc2/game_info.py#L17-L18 \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.placement_grid[pos] != 0","return_type":"bool","function_name":"BotAI.in_placement_grid","stripped_code":"def in_placement_grid(self, pos: Union[Point2, Point3, Unit]):\n        \"\"\" Returns True if you can place something at a position. Remember, buildings usually use 2x2, 3x3 or 5x5 of these grid points.\n        Caution: some x and y offset might be required, see ramp code:\n        https://github.com/Dentosal/python-sc2/blob/master/sc2/game_info.py#L17-L18 \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.placement_grid[pos] != 0"}
{"code":"def get_priority(self) -> int:\n        \"\"\"\n        Rerturns the priority of the file from 1 (pre) to 3 (post)\n        :return: the priority\n        \"\"\"\n        dtype = self.get_type()\n        if dtype & DeltaType.PRE:\n            return 1\n        elif dtype & DeltaType.POST:\n            return 3\n        else:\n            return 2","return_type":"int","function_name":"Delta.get_priority","stripped_code":"def get_priority(self):\n        \"\"\"\n        Rerturns the priority of the file from 1 (pre) to 3 (post)\n        :return: the priority\n        \"\"\"\n        dtype = self.get_type()\n        if dtype & DeltaType.PRE:\n            return 1\n        elif dtype & DeltaType.POST:\n            return 3\n        else:\n            return 2"}
{"code":"def export(self, out_file: str, layer: str = None, format: str = \"tab\") -> None:\n\t\t\"\"\"\n\t\tExport the specified layer and row/col attributes as tab-delimited file.\n\n\t\tArgs:\n\t\t\tout_file:\tPath to the output file\n\t\t\tlayer:\tName of the layer to export, or None to export the main matrix\n\t\t\tformat: Desired file format (only 'tab' is supported)\n\t\t\"\"\"\n\t\tif format != \"tab\":\n\t\t\traise NotImplementedError(\"Only 'tab' is supported\")\n\n\t\twith open(out_file, \"w\") as f:\n\t\t\t# Emit column attributes\n\t\t\tfor ca in self.col_attrs.keys():\n\t\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\t\tf.write(\"\\t\")\n\t\t\t\tf.write(ca + \"\\t\")\n\t\t\t\tfor v in self.col_attrs[ca]:\n\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\tf.write(\"\\n\")\n\n\t\t\t# Emit row attribute names\n\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\tf.write(ra + \"\\t\")\n\t\t\tf.write(\"\\t\")\n\t\t\tfor v in range(self.shape[1]):\n\t\t\t\tf.write(\"\\t\")\n\t\t\tf.write(\"\\n\")\n\n\t\t\t# Emit row attr values and matrix values\n\t\t\tfor row in range(self.shape[0]):\n\t\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\t\tf.write(str(self.row_attrs[ra][row]) + \"\\t\")\n\t\t\t\tf.write(\"\\t\")\n\n\t\t\t\tif layer is None:\n\t\t\t\t\tfor v in self[row, :]:\n\t\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\telse:\n\t\t\t\t\tfor v in self.layers[layer][row, :]:\n\t\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\tf.write(\"\\n\")","return_type":"None","function_name":"LoomConnection.export","stripped_code":"def export(self, out_file: str, layer: str = None, format: str = \"tab\"):\n\t\t\"\"\"\n\t\tExport the specified layer and row/col attributes as tab-delimited file.\n\n\t\tArgs:\n\t\t\tout_file:\tPath to the output file\n\t\t\tlayer:\tName of the layer to export, or None to export the main matrix\n\t\t\tformat: Desired file format (only 'tab' is supported)\n\t\t\"\"\"\n\t\tif format != \"tab\":\n\t\t\traise NotImplementedError(\"Only 'tab' is supported\")\n\n\t\twith open(out_file, \"w\") as f:\n\t\t\t# Emit column attributes\n\t\t\tfor ca in self.col_attrs.keys():\n\t\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\t\tf.write(\"\\t\")\n\t\t\t\tf.write(ca + \"\\t\")\n\t\t\t\tfor v in self.col_attrs[ca]:\n\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\tf.write(\"\\n\")\n\n\t\t\t# Emit row attribute names\n\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\tf.write(ra + \"\\t\")\n\t\t\tf.write(\"\\t\")\n\t\t\tfor v in range(self.shape[1]):\n\t\t\t\tf.write(\"\\t\")\n\t\t\tf.write(\"\\n\")\n\n\t\t\t# Emit row attr values and matrix values\n\t\t\tfor row in range(self.shape[0]):\n\t\t\t\tfor ra in self.row_attrs.keys():\n\t\t\t\t\tf.write(str(self.row_attrs[ra][row]) + \"\\t\")\n\t\t\t\tf.write(\"\\t\")\n\n\t\t\t\tif layer is None:\n\t\t\t\t\tfor v in self[row, :]:\n\t\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\telse:\n\t\t\t\t\tfor v in self.layers[layer][row, :]:\n\t\t\t\t\t\tf.write(str(v) + \"\\t\")\n\t\t\t\tf.write(\"\\n\")"}
{"code":"def validate_address(address: Any) -> None:\n    \"\"\"\n    Raise a ValidationError if an address is not canonicalized.\n    \"\"\"\n    if not is_address(address):\n        raise ValidationError(f\"Expected an address, got: {address}\")\n    if not is_canonical_address(address):\n        raise ValidationError(\n            \"Py-EthPM library only accepts canonicalized addresses. \"\n            f\"{address} is not in the accepted format.\"\n        )","return_type":"None","function_name":"validate_address","stripped_code":"def validate_address(address: Any):\n    \"\"\"\n    Raise a ValidationError if an address is not canonicalized.\n    \"\"\"\n    if not is_address(address):\n        raise ValidationError(f\"Expected an address, got: {address}\")\n    if not is_canonical_address(address):\n        raise ValidationError(\n            \"Py-EthPM library only accepts canonicalized addresses. \"\n            f\"{address} is not in the accepted format.\"\n        )"}
{"code":"def not_played(self) -> int:\n        \"\"\"The number of games in the player's promos that they haven't played yet.\"\"\"\n        return len(self._data[MiniSeriesData].progress) - len(self.progress)","return_type":"int","function_name":"MiniSeries.not_played","stripped_code":"def not_played(self):\n        \"\"\"The number of games in the player's promos that they haven't played yet.\"\"\"\n        return len(self._data[MiniSeriesData].progress) - len(self.progress)"}
{"code":"def ZoomByUnit(self, zoomUnit: int, waitTime: float = OPERATION_WAIT_TIME) -> bool:\n        \"\"\"\n        Call IUIAutomationTransformPattern2::ZoomByUnit.\n        Zoom the viewport of the control by the specified unit.\n        zoomUnit: int, a value in class `ZoomUnit`.\n        waitTime: float.\n        Return bool, True if succeed otherwise False.\n        Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtransformpattern2-zoombyunit\n        \"\"\"\n        ret = self.pattern.ZoomByUnit(zoomUnit) == S_OK\n        time.sleep(waitTime)\n        return ret","return_type":"bool","function_name":"TransformPattern2.ZoomByUnit","stripped_code":"def ZoomByUnit(self, zoomUnit: int, waitTime: float = OPERATION_WAIT_TIME):\n        \"\"\"\n        Call IUIAutomationTransformPattern2::ZoomByUnit.\n        Zoom the viewport of the control by the specified unit.\n        zoomUnit: int, a value in class `ZoomUnit`.\n        waitTime: float.\n        Return bool, True if succeed otherwise False.\n        Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtransformpattern2-zoombyunit\n        \"\"\"\n        ret = self.pattern.ZoomByUnit(zoomUnit) == S_OK\n        time.sleep(waitTime)\n        return ret"}
{"code":"def _calculate_dispersion(X: Union[pd.DataFrame, np.ndarray], labels: np.ndarray, centroids: np.ndarray) -> float:\n        \"\"\"\n        Calculate the dispersion between actual points and their assigned centroids\n        \"\"\"\n        disp = np.sum(np.sum([np.abs(inst - centroids[label]) ** 2 for inst, label in zip(X, labels)]))  # type: float\n        return disp","return_type":"float","function_name":"OptimalK._calculate_dispersion","stripped_code":"def _calculate_dispersion(X: Union[pd.DataFrame, np.ndarray], labels: np.ndarray, centroids: np.ndarray):\n        \"\"\"\n        Calculate the dispersion between actual points and their assigned centroids\n        \"\"\"\n        disp = np.sum(np.sum([np.abs(inst - centroids[label]) ** 2 for inst, label in zip(X, labels)]))  # type: float\n        return disp"}
{"code":"def set_target(self, target: EventDispatcherBase) -> None:\r\n        \"\"\"\r\n        This method should be called by the event dispatcher that dispatches this event\r\n        to set its target property.\r\n\r\n        Args:\r\n            target (EventDispatcherBase): The event dispatcher that will dispatch this event.\r\n\r\n        Raises:\r\n            PermissionError: If the target property of the event has already been set.\r\n            TypeError: If `target` is not an `EventDispatcherBase` instance.\r\n        \"\"\"\r\n        if self._target is not None:\r\n            raise PermissionError(\"The target property already has a valid value.\")\r\n\r\n        if not isinstance(target, EventDispatcherBase):\r\n            raise TypeError(\"Invalid target type: {}\".format(target))\r\n\r\n        self._target = target","return_type":"None","function_name":"Event.set_target","stripped_code":"def set_target(self, target: EventDispatcherBase):\r\n        \"\"\"\r\n        This method should be called by the event dispatcher that dispatches this event\r\n        to set its target property.\r\n\r\n        Args:\r\n            target (EventDispatcherBase): The event dispatcher that will dispatch this event.\r\n\r\n        Raises:\r\n            PermissionError: If the target property of the event has already been set.\r\n            TypeError: If `target` is not an `EventDispatcherBase` instance.\r\n        \"\"\"\r\n        if self._target is not None:\r\n            raise PermissionError(\"The target property already has a valid value.\")\r\n\r\n        if not isinstance(target, EventDispatcherBase):\r\n            raise TypeError(\"Invalid target type: {}\".format(target))\r\n\r\n        self._target = target"}
{"code":"def is_running(self) -> bool:\n        \"\"\"Specifies whether or not the thread is running\"\"\"\n        return (\n            self._has_started and\n            self.is_alive() or\n            self.completed_at is None or\n            (datetime.utcnow() - self.completed_at).total_seconds() < 0.5\n        )","return_type":"bool","function_name":"CauldronThread.is_running","stripped_code":"def is_running(self):\n        \"\"\"Specifies whether or not the thread is running\"\"\"\n        return (\n            self._has_started and\n            self.is_alive() or\n            self.completed_at is None or\n            (datetime.utcnow() - self.completed_at).total_seconds() < 0.5\n        )"}
{"code":"def get_causal_sink_nodes(graph: BELGraph, func) -> Set[BaseEntity]:\n    \"\"\"Returns a set of all ABUNDANCE nodes that have an causal out-degree of 0.\n\n    This likely means that the knowledge assembly is incomplete, or there is a curation error.\n    \"\"\"\n    return {\n        node\n        for node in graph\n        if node.function == func and is_causal_sink(graph, node)\n    }","return_type":"Set[BaseEntity]","function_name":"get_causal_sink_nodes","stripped_code":"def get_causal_sink_nodes(graph: BELGraph, func):\n    \"\"\"Returns a set of all ABUNDANCE nodes that have an causal out-degree of 0.\n\n    This likely means that the knowledge assembly is incomplete, or there is a curation error.\n    \"\"\"\n    return {\n        node\n        for node in graph\n        if node.function == func and is_causal_sink(graph, node)\n    }"}
{"code":"def _receiveFromListener(self, quota: Quota) -> int:\n        \"\"\"\n        Receives messages from listener\n        :param quota: number of messages to receive\n        :return: number of received messages\n        \"\"\"\n        i = 0\n        incoming_size = 0\n        while i < quota.count and incoming_size < quota.size:\n            try:\n                ident, msg = self.listener.recv_multipart(flags=zmq.NOBLOCK)\n                if not msg:\n                    # Router probing sends empty message on connection\n                    continue\n                incoming_size += len(msg)\n                i += 1\n                self._verifyAndAppend(msg, ident)\n            except zmq.Again:\n                break\n        if i > 0:\n            logger.trace('{} got {} messages through listener'.\n                         format(self, i))\n        return i","return_type":"int","function_name":"ZStack._receiveFromListener","stripped_code":"def _receiveFromListener(self, quota: Quota):\n        \"\"\"\n        Receives messages from listener\n        :param quota: number of messages to receive\n        :return: number of received messages\n        \"\"\"\n        i = 0\n        incoming_size = 0\n        while i < quota.count and incoming_size < quota.size:\n            try:\n                ident, msg = self.listener.recv_multipart(flags=zmq.NOBLOCK)\n                if not msg:\n                    # Router probing sends empty message on connection\n                    continue\n                incoming_size += len(msg)\n                i += 1\n                self._verifyAndAppend(msg, ident)\n            except zmq.Again:\n                break\n        if i > 0:\n            logger.trace('{} got {} messages through listener'.\n                         format(self, i))\n        return i"}
{"code":"def sequence_cross_entropy_with_logits(logits: torch.FloatTensor,\n                                       targets: torch.LongTensor,\n                                       weights: torch.FloatTensor,\n                                       average: str = \"batch\",\n                                       label_smoothing: float = None) -> torch.FloatTensor:\n    \"\"\"\n    Computes the cross entropy loss of a sequence, weighted with respect to\n    some user provided weights. Note that the weighting here is not the same as\n    in the :func:`torch.nn.CrossEntropyLoss()` criterion, which is weighting\n    classes; here we are weighting the loss contribution from particular elements\n    in the sequence. This allows loss computations for models which use padding.\n\n    Parameters\n    ----------\n    logits : ``torch.FloatTensor``, required.\n        A ``torch.FloatTensor`` of size (batch_size, sequence_length, num_classes)\n        which contains the unnormalized probability for each class.\n    targets : ``torch.LongTensor``, required.\n        A ``torch.LongTensor`` of size (batch, sequence_length) which contains the\n        index of the true class for each corresponding step.\n    weights : ``torch.FloatTensor``, required.\n        A ``torch.FloatTensor`` of size (batch, sequence_length)\n    average: str, optional (default = \"batch\")\n        If \"batch\", average the loss across the batches. If \"token\", average\n        the loss across each item in the input. If ``None``, return a vector\n        of losses per batch element.\n    label_smoothing : ``float``, optional (default = None)\n        Whether or not to apply label smoothing to the cross-entropy loss.\n        For example, with a label smoothing value of 0.2, a 4 class classification\n        target would look like ``[0.05, 0.05, 0.85, 0.05]`` if the 3rd class was\n        the correct label.\n\n    Returns\n    -------\n    A torch.FloatTensor representing the cross entropy loss.\n    If ``average==\"batch\"`` or ``average==\"token\"``, the returned loss is a scalar.\n    If ``average is None``, the returned loss is a vector of shape (batch_size,).\n\n    \"\"\"\n    if average not in {None, \"token\", \"batch\"}:\n        raise ValueError(\"Got average f{average}, expected one of \"\n                         \"None, 'token', or 'batch'\")\n\n    # shape : (batch * sequence_length, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # shape : (batch * sequence_length, num_classes)\n    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n    # shape : (batch * max_len, 1)\n    targets_flat = targets.view(-1, 1).long()\n\n    if label_smoothing is not None and label_smoothing > 0.0:\n        num_classes = logits.size(-1)\n        smoothing_value = label_smoothing / num_classes\n        # Fill all the correct indices with 1 - smoothing value.\n        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(-1, targets_flat, 1.0 - label_smoothing)\n        smoothed_targets = one_hot_targets + smoothing_value\n        negative_log_likelihood_flat = - log_probs_flat * smoothed_targets\n        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n    else:\n        # Contribution to the negative log likelihood only comes from the exact indices\n        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n        # to extract the indices of the num_classes dimension which contribute to the loss.\n        # shape : (batch * sequence_length, 1)\n        negative_log_likelihood_flat = - torch.gather(log_probs_flat, dim=1, index=targets_flat)\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood * weights.float()\n\n    if average == \"batch\":\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        num_non_empty_sequences = ((weights.sum(1) > 0).float().sum() + 1e-13)\n        return per_batch_loss.sum() / num_non_empty_sequences\n    elif average == \"token\":\n        return negative_log_likelihood.sum() / (weights.sum().float() + 1e-13)\n    else:\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        return per_batch_loss","return_type":"torch.FloatTensor","function_name":"sequence_cross_entropy_with_logits","stripped_code":"def sequence_cross_entropy_with_logits(logits: torch.FloatTensor,\n                                       targets: torch.LongTensor,\n                                       weights: torch.FloatTensor,\n                                       average: str = \"batch\",\n                                       label_smoothing: float = None):\n    \"\"\"\n    Computes the cross entropy loss of a sequence, weighted with respect to\n    some user provided weights. Note that the weighting here is not the same as\n    in the :func:`torch.nn.CrossEntropyLoss()` criterion, which is weighting\n    classes; here we are weighting the loss contribution from particular elements\n    in the sequence. This allows loss computations for models which use padding.\n\n    Parameters\n    ----------\n    logits : ``torch.FloatTensor``, required.\n        A ``torch.FloatTensor`` of size (batch_size, sequence_length, num_classes)\n        which contains the unnormalized probability for each class.\n    targets : ``torch.LongTensor``, required.\n        A ``torch.LongTensor`` of size (batch, sequence_length) which contains the\n        index of the true class for each corresponding step.\n    weights : ``torch.FloatTensor``, required.\n        A ``torch.FloatTensor`` of size (batch, sequence_length)\n    average: str, optional (default = \"batch\")\n        If \"batch\", average the loss across the batches. If \"token\", average\n        the loss across each item in the input. If ``None``, return a vector\n        of losses per batch element.\n    label_smoothing : ``float``, optional (default = None)\n        Whether or not to apply label smoothing to the cross-entropy loss.\n        For example, with a label smoothing value of 0.2, a 4 class classification\n        target would look like ``[0.05, 0.05, 0.85, 0.05]`` if the 3rd class was\n        the correct label.\n\n    Returns\n    -------\n    A torch.FloatTensor representing the cross entropy loss.\n    If ``average==\"batch\"`` or ``average==\"token\"``, the returned loss is a scalar.\n    If ``average is None``, the returned loss is a vector of shape (batch_size,).\n\n    \"\"\"\n    if average not in {None, \"token\", \"batch\"}:\n        raise ValueError(\"Got average f{average}, expected one of \"\n                         \"None, 'token', or 'batch'\")\n\n    # shape : (batch * sequence_length, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # shape : (batch * sequence_length, num_classes)\n    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)\n    # shape : (batch * max_len, 1)\n    targets_flat = targets.view(-1, 1).long()\n\n    if label_smoothing is not None and label_smoothing > 0.0:\n        num_classes = logits.size(-1)\n        smoothing_value = label_smoothing / num_classes\n        # Fill all the correct indices with 1 - smoothing value.\n        one_hot_targets = torch.zeros_like(log_probs_flat).scatter_(-1, targets_flat, 1.0 - label_smoothing)\n        smoothed_targets = one_hot_targets + smoothing_value\n        negative_log_likelihood_flat = - log_probs_flat * smoothed_targets\n        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)\n    else:\n        # Contribution to the negative log likelihood only comes from the exact indices\n        # of the targets, as the target distributions are one-hot. Here we use torch.gather\n        # to extract the indices of the num_classes dimension which contribute to the loss.\n        # shape : (batch * sequence_length, 1)\n        negative_log_likelihood_flat = - torch.gather(log_probs_flat, dim=1, index=targets_flat)\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())\n    # shape : (batch, sequence_length)\n    negative_log_likelihood = negative_log_likelihood * weights.float()\n\n    if average == \"batch\":\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        num_non_empty_sequences = ((weights.sum(1) > 0).float().sum() + 1e-13)\n        return per_batch_loss.sum() / num_non_empty_sequences\n    elif average == \"token\":\n        return negative_log_likelihood.sum() / (weights.sum().float() + 1e-13)\n    else:\n        # shape : (batch_size,)\n        per_batch_loss = negative_log_likelihood.sum(1) / (weights.sum(1).float() + 1e-13)\n        return per_batch_loss"}
{"code":"def if_modified_since(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of If-Modified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return self._http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))","return_type":"Optional[datetime.datetime]","function_name":"BaseRequest.if_modified_since","stripped_code":"def if_modified_since(self):\n        \"\"\"The value of If-Modified-Since HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        return self._http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))"}
{"code":"def add_cli_summarize(main: click.Group) -> click.Group:  # noqa: D202\n    \"\"\"Add a ``summarize`` command to main :mod:`click` function.\"\"\"\n\n    @main.command()\n    @click.pass_obj\n    def summarize(manager: AbstractManager):\n        \"\"\"Summarize the contents of the database.\"\"\"\n        if not manager.is_populated():\n            click.secho(f'{manager.module_name} has not been populated', fg='red')\n            sys.exit(1)\n\n        for name, count in sorted(manager.summarize().items()):\n            click.echo(f'{name.capitalize()}: {count}')\n\n    return main","return_type":"click.Group","function_name":"add_cli_summarize","stripped_code":"def add_cli_summarize(main: click.Group):  # noqa: D202\n    \"\"\"Add a ``summarize`` command to main :mod:`click` function.\"\"\"\n\n    @main.command()\n    @click.pass_obj\n    def summarize(manager: AbstractManager):\n        \"\"\"Summarize the contents of the database.\"\"\"\n        if not manager.is_populated():\n            click.secho(f'{manager.module_name} has not been populated', fg='red')\n            sys.exit(1)\n\n        for name, count in sorted(manager.summarize().items()):\n            click.echo(f'{name.capitalize()}: {count}')\n\n    return main"}
{"code":"def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope","return_type":"Scope","function_name":"Scope.get_by_symbol_name","stripped_code":"def get_by_symbol_name(self, name: str):\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope"}
{"code":"def can_attack_air(self) -> bool:\n        \"\"\" Does not include upgrades \"\"\"\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            return weapon is not None\n        return False","return_type":"bool","function_name":"PassengerUnit.can_attack_air","stripped_code":"def can_attack_air(self):\n        \"\"\" Does not include upgrades \"\"\"\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            return weapon is not None\n        return False"}
{"code":"def get_dates(feed: \"Feed\", *, as_date_obj: bool = False) -> List[str]:\n    \"\"\"\n    Return a list of dates for which the given \"Feed\" is valid, which\n    could be the empty list if the \"Feed\" has no calendar information.\n\n    Parameters\n    ----------\n    feed : \"Feed\"\n    as_date_obj : boolean\n        If ``True``, then return the dates as ``datetime.date`` objects;\n        otherwise return them as strings\n\n    Returns\n    -------\n    list\n        Dates\n\n    \"\"\"\n    dates = []\n    if feed.calendar is not None and not feed.calendar.empty:\n        if \"start_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"start_date\"].min())\n        if \"end_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"end_date\"].max())\n    if feed.calendar_dates is not None and not feed.calendar_dates.empty:\n        if \"date\" in feed.calendar_dates.columns:\n            start = feed.calendar_dates[\"date\"].min()\n            end = feed.calendar_dates[\"date\"].max()\n            dates.extend([start, end])\n    if not dates:\n        return []\n\n    start_date, end_date = min(dates), max(dates)\n    start_date, end_date = map(hp.datestr_to_date, [start_date, end_date])\n    num_days = (end_date - start_date).days\n    result = [\n        start_date + rd.relativedelta(days=+d) for d in range(num_days + 1)\n    ]\n\n    # Convert dates back to strings if required\n    if not as_date_obj:\n        result = [hp.datestr_to_date(x, inverse=True) for x in result]\n\n    return result","return_type":"List[str]","function_name":"get_dates","stripped_code":"def get_dates(feed: \"Feed\", *, as_date_obj: bool = False):\n    \"\"\"\n    Return a list of dates for which the given \"Feed\" is valid, which\n    could be the empty list if the \"Feed\" has no calendar information.\n\n    Parameters\n    ----------\n    feed : \"Feed\"\n    as_date_obj : boolean\n        If ``True``, then return the dates as ``datetime.date`` objects;\n        otherwise return them as strings\n\n    Returns\n    -------\n    list\n        Dates\n\n    \"\"\"\n    dates = []\n    if feed.calendar is not None and not feed.calendar.empty:\n        if \"start_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"start_date\"].min())\n        if \"end_date\" in feed.calendar.columns:\n            dates.append(feed.calendar[\"end_date\"].max())\n    if feed.calendar_dates is not None and not feed.calendar_dates.empty:\n        if \"date\" in feed.calendar_dates.columns:\n            start = feed.calendar_dates[\"date\"].min()\n            end = feed.calendar_dates[\"date\"].max()\n            dates.extend([start, end])\n    if not dates:\n        return []\n\n    start_date, end_date = min(dates), max(dates)\n    start_date, end_date = map(hp.datestr_to_date, [start_date, end_date])\n    num_days = (end_date - start_date).days\n    result = [\n        start_date + rd.relativedelta(days=+d) for d in range(num_days + 1)\n    ]\n\n    # Convert dates back to strings if required\n    if not as_date_obj:\n        result = [hp.datestr_to_date(x, inverse=True) for x in result]\n\n    return result"}
{"code":"def is_openxml_good(filename: str) -> bool:\n    \"\"\"\n    Determines whether an OpenXML file appears to be good (not corrupted).\n    \"\"\"\n    try:\n        log.debug(\"Trying: {}\", filename)\n        with ZipFile(filename, 'r') as zip_ref:\n            namelist = zip_ref.namelist()  # type: List[str]\n            # log.critical(\"\\n{}\", pformat(namelist))\n            # -----------------------------------------------------------------\n            # Contains key files?\n            # -----------------------------------------------------------------\n            for mandatory_filename in MANDATORY_FILENAMES:\n                if mandatory_filename not in namelist:\n                    log.debug(\"Bad [missing {!r}]: {}\",\n                              mandatory_filename, filename)\n                    return False\n\n            infolist = zip_ref.infolist()  # type: List[ZipInfo]\n            contains_docx = False\n            contains_pptx = False\n            contains_xlsx = False\n            for info in infolist:\n                # -------------------------------------------------------------\n                # Sensible date check?\n                # ... NO: lots of perfectly good files have this date/time.\n                # -------------------------------------------------------------\n                # if info.date_time == NULL_DATE_TIME:\n                #     log.debug(\"{!r}: {!r}\", info.filename, info.date_time)\n\n                # -------------------------------------------------------------\n                # Only one kind of contents?\n                # ... YES, I think so. This has 100% reliability on my\n                # stash of 34 PPTX, 223 DOCX, 85 XLSX, and labelled none as bad\n                # from an HFC collection of 1866 such files. There are lots of\n                # files emerging from Scalpel (plus my find_recovered_openxml\n                # zip-fixing tool) that fail this test, though.\n                # -------------------------------------------------------------\n                if (not contains_docx and\n                        DOCX_CONTENTS_REGEX.search(info.filename)):\n                    contains_docx = True\n                if (not contains_pptx and\n                        PPTX_CONTENTS_REGEX.search(info.filename)):\n                    contains_pptx = True\n                if (not contains_xlsx and\n                        XLSX_CONTENTS_REGEX.search(info.filename)):\n                    contains_xlsx = True\n                if sum([contains_docx, contains_pptx, contains_xlsx]) > 1:\n                    log.debug(\"Bad [>1 of DOCX, PPTX, XLSX content]: {}\",\n                              filename)\n                    return False\n\n            return True\n    except (BadZipFile, OSError) as e:\n        # ---------------------------------------------------------------------\n        # Duff file. Easy!\n        # ---------------------------------------------------------------------\n        log.debug(\"Bad [BadZipFile or OSError]: {!r}; error was {!r}\",\n                  filename, e)\n        return False","return_type":"bool","function_name":"is_openxml_good","stripped_code":"def is_openxml_good(filename: str):\n    \"\"\"\n    Determines whether an OpenXML file appears to be good (not corrupted).\n    \"\"\"\n    try:\n        log.debug(\"Trying: {}\", filename)\n        with ZipFile(filename, 'r') as zip_ref:\n            namelist = zip_ref.namelist()  # type: List[str]\n            # log.critical(\"\\n{}\", pformat(namelist))\n            # -----------------------------------------------------------------\n            # Contains key files?\n            # -----------------------------------------------------------------\n            for mandatory_filename in MANDATORY_FILENAMES:\n                if mandatory_filename not in namelist:\n                    log.debug(\"Bad [missing {!r}]: {}\",\n                              mandatory_filename, filename)\n                    return False\n\n            infolist = zip_ref.infolist()  # type: List[ZipInfo]\n            contains_docx = False\n            contains_pptx = False\n            contains_xlsx = False\n            for info in infolist:\n                # -------------------------------------------------------------\n                # Sensible date check?\n                # ... NO: lots of perfectly good files have this date/time.\n                # -------------------------------------------------------------\n                # if info.date_time == NULL_DATE_TIME:\n                #     log.debug(\"{!r}: {!r}\", info.filename, info.date_time)\n\n                # -------------------------------------------------------------\n                # Only one kind of contents?\n                # ... YES, I think so. This has 100% reliability on my\n                # stash of 34 PPTX, 223 DOCX, 85 XLSX, and labelled none as bad\n                # from an HFC collection of 1866 such files. There are lots of\n                # files emerging from Scalpel (plus my find_recovered_openxml\n                # zip-fixing tool) that fail this test, though.\n                # -------------------------------------------------------------\n                if (not contains_docx and\n                        DOCX_CONTENTS_REGEX.search(info.filename)):\n                    contains_docx = True\n                if (not contains_pptx and\n                        PPTX_CONTENTS_REGEX.search(info.filename)):\n                    contains_pptx = True\n                if (not contains_xlsx and\n                        XLSX_CONTENTS_REGEX.search(info.filename)):\n                    contains_xlsx = True\n                if sum([contains_docx, contains_pptx, contains_xlsx]) > 1:\n                    log.debug(\"Bad [>1 of DOCX, PPTX, XLSX content]: {}\",\n                              filename)\n                    return False\n\n            return True\n    except (BadZipFile, OSError) as e:\n        # ---------------------------------------------------------------------\n        # Duff file. Easy!\n        # ---------------------------------------------------------------------\n        log.debug(\"Bad [BadZipFile or OSError]: {!r}; error was {!r}\",\n                  filename, e)\n        return False"}
{"code":"def filter_float(n: Node, query: str) -> float:\n    \"\"\"\n    Filter and ensure that the returned value is of type int.\n    \"\"\"\n    return _scalariter2item(n, query, float)","return_type":"float","function_name":"filter_float","stripped_code":"def filter_float(n: Node, query: str):\n    \"\"\"\n    Filter and ensure that the returned value is of type int.\n    \"\"\"\n    return _scalariter2item(n, query, float)"}
{"code":"def dump(self) -> dict:\n        \"\"\"\n        Dumps data from the ConfigKey into a dict.\n        :return: The keys and values from the ConfigKey encapsulated in a dict.\n        \"\"\"\n        d = {}\n        for item in self.__dict__:\n            if item in ['parsed', 'dump', 'parse_data', 'iter_list', 'safe_load']:\n                continue\n            if isinstance(self.__dict__[item], ConfigKey):\n                d[item] = self.__dict__[item].dump()\n            elif isinstance(self.__dict__[item], list):\n                d[item] = self.iter_list_dump(self.__dict__[item])\n            else:\n                d[item] = self.__dict__[item]\n        return d","return_type":"dict","function_name":"ConfigKey.dump","stripped_code":"def dump(self):\n        \"\"\"\n        Dumps data from the ConfigKey into a dict.\n        :return: The keys and values from the ConfigKey encapsulated in a dict.\n        \"\"\"\n        d = {}\n        for item in self.__dict__:\n            if item in ['parsed', 'dump', 'parse_data', 'iter_list', 'safe_load']:\n                continue\n            if isinstance(self.__dict__[item], ConfigKey):\n                d[item] = self.__dict__[item].dump()\n            elif isinstance(self.__dict__[item], list):\n                d[item] = self.iter_list_dump(self.__dict__[item])\n            else:\n                d[item] = self.__dict__[item]\n        return d"}
{"code":"def reverse_taskname(name: str) -> str:\n  \"\"\"\n  Reverses components in the name of task. Reversed convention is used for filenames since\n  it groups log/scratch files of related tasks together\n\n  0.somejob.somerun -> somerun.somejob.0\n  0.somejob -> somejob.0\n  somename -> somename\n\n  Args:\n    name: name of task\n\n  \"\"\"\n  components = name.split('.')\n  assert len(components) <= 3\n  return '.'.join(components[::-1])","return_type":"str","function_name":"reverse_taskname","stripped_code":"def reverse_taskname(name: str):\n  \"\"\"\n  Reverses components in the name of task. Reversed convention is used for filenames since\n  it groups log/scratch files of related tasks together\n\n  0.somejob.somerun -> somerun.somejob.0\n  0.somejob -> somejob.0\n  somename -> somename\n\n  Args:\n    name: name of task\n\n  \"\"\"\n  components = name.split('.')\n  assert len(components) <= 3\n  return '.'.join(components[::-1])"}
{"code":"def _delete_upload_id(conn: Connection, table: Table, upload_id: int) -> int:\n        \"\"\"Remove all table records with the supplied upload_id\n\n        :param conn: sql connection\n        :param table: table to modify\n        :param upload_id: target upload_id\n        :return: number of records removed\n        \"\"\"\n        return conn.execute(delete(table).where(table.c.upload_id == upload_id)).rowcount if upload_id else 0","return_type":"int","function_name":"I2B2CoreWithUploadId._delete_upload_id","stripped_code":"def _delete_upload_id(conn: Connection, table: Table, upload_id: int):\n        \"\"\"Remove all table records with the supplied upload_id\n\n        :param conn: sql connection\n        :param table: table to modify\n        :param upload_id: target upload_id\n        :return: number of records removed\n        \"\"\"\n        return conn.execute(delete(table).where(table.c.upload_id == upload_id)).rowcount if upload_id else 0"}
{"code":"def epd(self, *, shredder: bool = False, en_passant: str = \"legal\", promoted: Optional[bool] = None, **operations: Union[None, str, int, float, Move, Iterable[Move]]) -> str:\n        \"\"\"\n        Gets an EPD representation of the current position.\n\n        See :func:`~chess.Board.fen()` for FEN formatting options (*shredder*,\n        *ep_square* and *promoted*).\n\n        EPD operations can be given as keyword arguments. Supported operands\n        are strings, integers, floats, moves, lists of moves and ``None``.\n        All other operands are converted to strings.\n\n        A list of moves for *pv* will be interpreted as a variation. All other\n        move lists are interpreted as a set of moves in the current position.\n\n        *hmvc* and *fmvc* are not included by default. You can use:\n\n        >>> import chess\n        >>>\n        >>> board = chess.Board()\n        >>> board.epd(hmvc=board.halfmove_clock, fmvc=board.fullmove_number)\n        'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - hmvc 0; fmvc 1;'\n        \"\"\"\n        if en_passant == \"fen\":\n            ep_square = self.ep_square\n        elif en_passant == \"xfen\":\n            ep_square = self.ep_square if self.has_pseudo_legal_en_passant() else None\n        else:\n            ep_square = self.ep_square if self.has_legal_en_passant() else None\n\n        epd = [self.board_fen(promoted=promoted),\n               \"w\" if self.turn == WHITE else \"b\",\n               self.castling_shredder_fen() if shredder else self.castling_xfen(),\n               SQUARE_NAMES[ep_square] if ep_square is not None else \"-\"]\n\n        if operations:\n            epd.append(self._epd_operations(operations))\n\n        return \" \".join(epd)","return_type":"str","function_name":"Board.epd","stripped_code":"def epd(self, *, shredder: bool = False, en_passant: str = \"legal\", promoted: Optional[bool] = None, **operations: Union[None, str, int, float, Move, Iterable[Move]]):\n        \"\"\"\n        Gets an EPD representation of the current position.\n\n        See :func:`~chess.Board.fen()` for FEN formatting options (*shredder*,\n        *ep_square* and *promoted*).\n\n        EPD operations can be given as keyword arguments. Supported operands\n        are strings, integers, floats, moves, lists of moves and ``None``.\n        All other operands are converted to strings.\n\n        A list of moves for *pv* will be interpreted as a variation. All other\n        move lists are interpreted as a set of moves in the current position.\n\n        *hmvc* and *fmvc* are not included by default. You can use:\n\n        >>> import chess\n        >>>\n        >>> board = chess.Board()\n        >>> board.epd(hmvc=board.halfmove_clock, fmvc=board.fullmove_number)\n        'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - hmvc 0; fmvc 1;'\n        \"\"\"\n        if en_passant == \"fen\":\n            ep_square = self.ep_square\n        elif en_passant == \"xfen\":\n            ep_square = self.ep_square if self.has_pseudo_legal_en_passant() else None\n        else:\n            ep_square = self.ep_square if self.has_legal_en_passant() else None\n\n        epd = [self.board_fen(promoted=promoted),\n               \"w\" if self.turn == WHITE else \"b\",\n               self.castling_shredder_fen() if shredder else self.castling_xfen(),\n               SQUARE_NAMES[ep_square] if ep_square is not None else \"-\"]\n\n        if operations:\n            epd.append(self._epd_operations(operations))\n\n        return \" \".join(epd)"}
{"code":"def get_dirs(self) -> List[str]:\n        \"\"\"\n        Get all effect directories for registered effects.\n        \"\"\"\n        for package in self.packages:\n            yield os.path.join(package.path, 'resources')","return_type":"List[str]","function_name":"EffectRegistry.get_dirs","stripped_code":"def get_dirs(self):\n        \"\"\"\n        Get all effect directories for registered effects.\n        \"\"\"\n        for package in self.packages:\n            yield os.path.join(package.path, 'resources')"}
{"code":"def set_end_point_uri(self) -> bool:\n        \"\"\"\n        Extracts the route from the accessed URL and sets it to __end_point_uri\n        :rtype: bool\n        \"\"\"\n        expected_parts = self.__route.split(\"/\")\n        actual_parts = self.__uri.split(\"/\")\n\n        i = 0\n        for part in expected_parts:\n            if part != actual_parts[i]:\n                return False\n            i = i + 1\n\n        uri_prefix = len(self.__route)\n        self.__end_point_uri = self.__uri[uri_prefix:]\n        return True","return_type":"bool","function_name":"Router.set_end_point_uri","stripped_code":"def set_end_point_uri(self):\n        \"\"\"\n        Extracts the route from the accessed URL and sets it to __end_point_uri\n        :rtype: bool\n        \"\"\"\n        expected_parts = self.__route.split(\"/\")\n        actual_parts = self.__uri.split(\"/\")\n\n        i = 0\n        for part in expected_parts:\n            if part != actual_parts[i]:\n                return False\n            i = i + 1\n\n        uri_prefix = len(self.__route)\n        self.__end_point_uri = self.__uri[uri_prefix:]\n        return True"}
{"code":"def recover_codes(tokens: Iterator[Tokenizer]) -> str:\n    \"\"\"\n    from a series of tokenizers to code string. (preserve the indentation)\n    \"\"\"\n\n    tokens = iter(tokens)\n\n    s = []\n\n    try:\n        head = next(tokens)\n\n    except StopIteration:\n        return s\n\n    append = s.append\n    lineno = head.lineno\n\n    start_indent = colno = head.colno\n\n    append(head.value)\n\n    colno += len(s[-1])\n\n    for each in tokens:\n        n = each.lineno - lineno\n\n        if n:\n            append('\\n' * n)\n            lineno = each.lineno\n            colno = each.colno\n\n            if colno - start_indent > 0:\n                append(' ' * colno)\n\n        else:\n            c = each.colno - colno\n            if c:\n                colno = each.colno\n                if c > 0:\n                    append(' ' * c)\n\n        append(each.value)\n\n        colno += len(s[-1])\n\n    return s","return_type":"str","function_name":"recover_codes","stripped_code":"def recover_codes(tokens: Iterator[Tokenizer]):\n    \"\"\"\n    from a series of tokenizers to code string. (preserve the indentation)\n    \"\"\"\n\n    tokens = iter(tokens)\n\n    s = []\n\n    try:\n        head = next(tokens)\n\n    except StopIteration:\n        return s\n\n    append = s.append\n    lineno = head.lineno\n\n    start_indent = colno = head.colno\n\n    append(head.value)\n\n    colno += len(s[-1])\n\n    for each in tokens:\n        n = each.lineno - lineno\n\n        if n:\n            append('\\n' * n)\n            lineno = each.lineno\n            colno = each.colno\n\n            if colno - start_indent > 0:\n                append(' ' * colno)\n\n        else:\n            c = each.colno - colno\n            if c:\n                colno = each.colno\n                if c > 0:\n                    append(' ' * c)\n\n        append(each.value)\n\n        colno += len(s[-1])\n\n    return s"}
{"code":"def do_alias(self, args: argparse.Namespace) -> None:\n        \"\"\"Manage aliases\"\"\"\n        func = getattr(args, 'func', None)\n        if func is not None:\n            # Call whatever sub-command function was selected\n            func(self, args)\n        else:\n            # No sub-command was provided, so call help\n            self.do_help('alias')","return_type":"None","function_name":"Cmd.do_alias","stripped_code":"def do_alias(self, args: argparse.Namespace):\n        \"\"\"Manage aliases\"\"\"\n        func = getattr(args, 'func', None)\n        if func is not None:\n            # Call whatever sub-command function was selected\n            func(self, args)\n        else:\n            # No sub-command was provided, so call help\n            self.do_help('alias')"}
{"code":"def image2np(image:Tensor)->np.ndarray:\n    \"Convert from torch style `image` to numpy/matplotlib style.\"\n    res = image.cpu().permute(1,2,0).numpy()\n    return res[...,0] if res.shape[2]==1 else res","return_type":"np.ndarray","function_name":"image2np","stripped_code":"def image2np(image:Tensor):\n    \"Convert from torch style `image` to numpy/matplotlib style.\"\n    res = image.cpu().permute(1,2,0).numpy()\n    return res[...,0] if res.shape[2]==1 else res"}
{"code":"def draw_polygon(\n            self,\n            *pts,\n            close_path:bool=True,\n            stroke:Color=None,\n            stroke_width:float=1,\n            stroke_dash:typing.Sequence=None,\n            fill:Color=None\n            ) -> None:\n        \"\"\"Draws the given linear path.\"\"\"\n        pass","return_type":"None","function_name":"OutputTarget.draw_polygon","stripped_code":"def draw_polygon(\n            self,\n            *pts,\n            close_path:bool=True,\n            stroke:Color=None,\n            stroke_width:float=1,\n            stroke_dash:typing.Sequence=None,\n            fill:Color=None\n            ):\n        \"\"\"Draws the given linear path.\"\"\"\n        pass"}
{"code":"def parse_select(cls, text: str) -> Set:\n        \"\"\"\n        get columns from select text\n        :param text: col1, col2\n        :return: ALL_COLUMNS or ['col1', 'col2']\n        \"\"\"\n        if text == '*':\n            return ALL_COLUMNS  # None means ALL\n        selected_columns = set(filter(lambda x: x, map(str.strip, text.split(','))))\n        if not selected_columns:\n            raise InvalidParams(\"No column(s) selected\")\n        return selected_columns","return_type":"Set","function_name":"SQLQueryInfo.parse_select","stripped_code":"def parse_select(cls, text: str):\n        \"\"\"\n        get columns from select text\n        :param text: col1, col2\n        :return: ALL_COLUMNS or ['col1', 'col2']\n        \"\"\"\n        if text == '*':\n            return ALL_COLUMNS  # None means ALL\n        selected_columns = set(filter(lambda x: x, map(str.strip, text.split(','))))\n        if not selected_columns:\n            raise InvalidParams(\"No column(s) selected\")\n        return selected_columns"}
{"code":"def _create_figure(kwargs: Mapping[str, Any]) -> dict:\n    \"\"\"Create basic dictionary object with figure properties.\"\"\"\n    return {\n        \"$schema\": \"https://vega.github.io/schema/vega/v3.json\",\n        \"width\": kwargs.pop(\"width\", DEFAULT_WIDTH),\n        \"height\": kwargs.pop(\"height\", DEFAULT_HEIGHT),\n        \"padding\": kwargs.pop(\"padding\", DEFAULT_PADDING)\n    }","return_type":"dict","function_name":"_create_figure","stripped_code":"def _create_figure(kwargs: Mapping[str, Any]):\n    \"\"\"Create basic dictionary object with figure properties.\"\"\"\n    return {\n        \"$schema\": \"https://vega.github.io/schema/vega/v3.json\",\n        \"width\": kwargs.pop(\"width\", DEFAULT_WIDTH),\n        \"height\": kwargs.pop(\"height\", DEFAULT_HEIGHT),\n        \"padding\": kwargs.pop(\"padding\", DEFAULT_PADDING)\n    }"}
{"code":"def remove_nodes_by_function_namespace(graph: BELGraph, func: str, namespace: Strings) -> None:\n    \"\"\"Remove nodes with the given function and namespace.\n\n    This might be useful to exclude information learned about distant species, such as excluding all information\n    from MGI and RGD in diseases where mice and rats don't give much insight to the human disease mechanism.\n    \"\"\"\n    remove_filtered_nodes(graph, function_namespace_inclusion_builder(func, namespace))","return_type":"None","function_name":"remove_nodes_by_function_namespace","stripped_code":"def remove_nodes_by_function_namespace(graph: BELGraph, func: str, namespace: Strings):\n    \"\"\"Remove nodes with the given function and namespace.\n\n    This might be useful to exclude information learned about distant species, such as excluding all information\n    from MGI and RGD in diseases where mice and rats don't give much insight to the human disease mechanism.\n    \"\"\"\n    remove_filtered_nodes(graph, function_namespace_inclusion_builder(func, namespace))"}
{"code":"def enrich_pubmed_citations(manager,\n                            graph,\n                            group_size: Optional[int] = None,\n                            sleep_time: Optional[int] = None,\n                            ) -> Set[str]:\n    \"\"\"Overwrite all PubMed citations with values from NCBI's eUtils lookup service.\n\n    Sets authors as list, so probably a good idea to run :func:`pybel_tools.mutation.serialize_authors` before\n    exporting.\n\n    :type manager: pybel.manager.Manager\n    :type graph: pybel.BELGraph\n    :param group_size: The number of PubMed identifiers to query at a time. Defaults to 200 identifiers.\n    :param sleep_time: Number of seconds to sleep between queries. Defaults to 1 second.\n    :return: A set of PMIDs for which the eUtils service crashed\n    \"\"\"\n    pmids = get_pubmed_identifiers(graph)\n    pmid_data, errors = get_citations_by_pmids(manager, pmids=pmids, group_size=group_size, sleep_time=sleep_time)\n\n    for u, v, k in filter_edges(graph, has_pubmed):\n        pmid = graph[u][v][k][CITATION][CITATION_REFERENCE].strip()\n\n        if pmid not in pmid_data:\n            log.warning('Missing data for PubMed identifier: %s', pmid)\n            errors.add(pmid)\n            continue\n\n        graph[u][v][k][CITATION].update(pmid_data[pmid])\n\n    return errors","return_type":"Set[str]","function_name":"enrich_pubmed_citations","stripped_code":"def enrich_pubmed_citations(manager,\n                            graph,\n                            group_size: Optional[int] = None,\n                            sleep_time: Optional[int] = None,\n                            ):\n    \"\"\"Overwrite all PubMed citations with values from NCBI's eUtils lookup service.\n\n    Sets authors as list, so probably a good idea to run :func:`pybel_tools.mutation.serialize_authors` before\n    exporting.\n\n    :type manager: pybel.manager.Manager\n    :type graph: pybel.BELGraph\n    :param group_size: The number of PubMed identifiers to query at a time. Defaults to 200 identifiers.\n    :param sleep_time: Number of seconds to sleep between queries. Defaults to 1 second.\n    :return: A set of PMIDs for which the eUtils service crashed\n    \"\"\"\n    pmids = get_pubmed_identifiers(graph)\n    pmid_data, errors = get_citations_by_pmids(manager, pmids=pmids, group_size=group_size, sleep_time=sleep_time)\n\n    for u, v, k in filter_edges(graph, has_pubmed):\n        pmid = graph[u][v][k][CITATION][CITATION_REFERENCE].strip()\n\n        if pmid not in pmid_data:\n            log.warning('Missing data for PubMed identifier: %s', pmid)\n            errors.add(pmid)\n            continue\n\n        graph[u][v][k][CITATION].update(pmid_data[pmid])\n\n    return errors"}
{"code":"def randhex(ndigits: int) -> str:\n    \"\"\"Return a random text string of hexadecimal characters.\n\n    The string has *ndigits* random digits.\n    Raises ValueError if ndigits <= 0, and TypeError if it's not an integer.\n\n    >>> randhex(16)  #doctest:+SKIP\n    '56054d728fc56f63'\n\n    \"\"\"\n    if not isinstance(ndigits, int):\n        raise TypeError('number of digits must be an integer')\n    if ndigits <= 0:\n        raise ValueError('number of digits must be greater than zero')\n\n    nbytes = ceil(ndigits / 2)\n    rbytes = random_randbytes(nbytes)\n    hexstr = rbytes.hex()[:ndigits]\n\n    return hexstr","return_type":"str","function_name":"randhex","stripped_code":"def randhex(ndigits: int):\n    \"\"\"Return a random text string of hexadecimal characters.\n\n    The string has *ndigits* random digits.\n    Raises ValueError if ndigits <= 0, and TypeError if it's not an integer.\n\n    >>> randhex(16)  #doctest:+SKIP\n    '56054d728fc56f63'\n\n    \"\"\"\n    if not isinstance(ndigits, int):\n        raise TypeError('number of digits must be an integer')\n    if ndigits <= 0:\n        raise ValueError('number of digits must be greater than zero')\n\n    nbytes = ceil(ndigits / 2)\n    rbytes = random_randbytes(nbytes)\n    hexstr = rbytes.hex()[:ndigits]\n\n    return hexstr"}
{"code":"def GET_save_parameteritemvalues(self) -> None:\n        \"\"\"Save the values of those |ChangeItem| objects which are\n        handling |Parameter| objects.\"\"\"\n        for item in state.parameteritems:\n            state.parameteritemvalues[self._id][item.name] = item.value.copy()","return_type":"None","function_name":"HydPyServer.GET_save_parameteritemvalues","stripped_code":"def GET_save_parameteritemvalues(self):\n        \"\"\"Save the values of those |ChangeItem| objects which are\n        handling |Parameter| objects.\"\"\"\n        for item in state.parameteritems:\n            state.parameteritemvalues[self._id][item.name] = item.value.copy()"}
{"code":"def nfa_intersection(nfa_1: dict, nfa_2: dict) -> dict:\n    \"\"\" Returns a NFA that reads the intersection of the NFAs in\n    input.\n\n    Let :math:`A_1 = (\u03a3,S_1,S_1^0,\u03c1_1,F_1)` and :math:`A_2 =(\u03a3,\n    S_2,S_2^0,\u03c1_2,F_2)` be two NFAs.\n    There is a NFA :math:`A_\u2227` that runs simultaneously both\n    :math:`A_1` and :math:`A_2` on the input word,\n    so :math:`L(A_\u2227) = L(A_1)\u2229L(A_2)`.\n    It is defined as:\n\n    :math:`A_\u2227 = ( \u03a3 , S , S_0 , \u03c1 , F )`\n\n    where\n\n    \u2022 :math:`S = S_1 \u00d7 S_2`\n    \u2022 :math:`S_0 = S_1^0 \u00d7 S_2^0`\n    \u2022 :math:`F = F_1 \u00d7 F_2`\n    \u2022 :math:`((s,t), a, (s_X , t_X)) \u2208 \u03c1` iff :math:`(s, a,s_X )\n      \u2208 \u03c1_1` and :math:`(t, a, t_X ) \u2208 \u03c1_2`\n\n    :param dict nfa_1: first input NFA;\n    :param dict nfa_2: second input NFA;\n    :return: *(dict)* representing the intersected NFA.\n    \"\"\"\n    intersection = {\n        'alphabet': nfa_1['alphabet'].intersection(nfa_2['alphabet']),\n        'states': set(),\n        'initial_states': set(),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n    for init_1 in nfa_1['initial_states']:\n        for init_2 in nfa_2['initial_states']:\n            intersection['initial_states'].add((init_1, init_2))\n\n    intersection['states'].update(intersection['initial_states'])\n\n    boundary = set()\n    boundary.update(intersection['initial_states'])\n    while boundary:\n        (state_nfa_1, state_nfa_2) = boundary.pop()\n        if state_nfa_1 in nfa_1['accepting_states'] \\\n                and state_nfa_2 in nfa_2['accepting_states']:\n            intersection['accepting_states'].add((state_nfa_1, state_nfa_2))\n        for a in intersection['alphabet']:\n            if (state_nfa_1, a) not in nfa_1['transitions'] \\\n                    or (state_nfa_2, a) not in nfa_2['transitions']:\n                continue\n            s1 = nfa_1['transitions'][state_nfa_1, a]\n            s2 = nfa_2['transitions'][state_nfa_2, a]\n\n            for destination_1 in s1:\n                for destination_2 in s2:\n                    next_state = (destination_1, destination_2)\n                    if next_state not in intersection['states']:\n                        intersection['states'].add(next_state)\n                        boundary.add(next_state)\n                    intersection['transitions'].setdefault(\n                        ((state_nfa_1, state_nfa_2), a), set()).add(next_state)\n                    if destination_1 in nfa_1['accepting_states'] \\\n                            and destination_2 in nfa_2['accepting_states']:\n                        intersection['accepting_states'].add(next_state)\n\n    return intersection","return_type":"dict","function_name":"nfa_intersection","stripped_code":"def nfa_intersection(nfa_1: dict, nfa_2: dict):\n    \"\"\" Returns a NFA that reads the intersection of the NFAs in\n    input.\n\n    Let :math:`A_1 = (\u03a3,S_1,S_1^0,\u03c1_1,F_1)` and :math:`A_2 =(\u03a3,\n    S_2,S_2^0,\u03c1_2,F_2)` be two NFAs.\n    There is a NFA :math:`A_\u2227` that runs simultaneously both\n    :math:`A_1` and :math:`A_2` on the input word,\n    so :math:`L(A_\u2227) = L(A_1)\u2229L(A_2)`.\n    It is defined as:\n\n    :math:`A_\u2227 = ( \u03a3 , S , S_0 , \u03c1 , F )`\n\n    where\n\n    \u2022 :math:`S = S_1 \u00d7 S_2`\n    \u2022 :math:`S_0 = S_1^0 \u00d7 S_2^0`\n    \u2022 :math:`F = F_1 \u00d7 F_2`\n    \u2022 :math:`((s,t), a, (s_X , t_X)) \u2208 \u03c1` iff :math:`(s, a,s_X )\n      \u2208 \u03c1_1` and :math:`(t, a, t_X ) \u2208 \u03c1_2`\n\n    :param dict nfa_1: first input NFA;\n    :param dict nfa_2: second input NFA;\n    :return: *(dict)* representing the intersected NFA.\n    \"\"\"\n    intersection = {\n        'alphabet': nfa_1['alphabet'].intersection(nfa_2['alphabet']),\n        'states': set(),\n        'initial_states': set(),\n        'accepting_states': set(),\n        'transitions': dict()\n    }\n    for init_1 in nfa_1['initial_states']:\n        for init_2 in nfa_2['initial_states']:\n            intersection['initial_states'].add((init_1, init_2))\n\n    intersection['states'].update(intersection['initial_states'])\n\n    boundary = set()\n    boundary.update(intersection['initial_states'])\n    while boundary:\n        (state_nfa_1, state_nfa_2) = boundary.pop()\n        if state_nfa_1 in nfa_1['accepting_states'] \\\n                and state_nfa_2 in nfa_2['accepting_states']:\n            intersection['accepting_states'].add((state_nfa_1, state_nfa_2))\n        for a in intersection['alphabet']:\n            if (state_nfa_1, a) not in nfa_1['transitions'] \\\n                    or (state_nfa_2, a) not in nfa_2['transitions']:\n                continue\n            s1 = nfa_1['transitions'][state_nfa_1, a]\n            s2 = nfa_2['transitions'][state_nfa_2, a]\n\n            for destination_1 in s1:\n                for destination_2 in s2:\n                    next_state = (destination_1, destination_2)\n                    if next_state not in intersection['states']:\n                        intersection['states'].add(next_state)\n                        boundary.add(next_state)\n                    intersection['transitions'].setdefault(\n                        ((state_nfa_1, state_nfa_2), a), set()).add(next_state)\n                    if destination_1 in nfa_1['accepting_states'] \\\n                            and destination_2 in nfa_2['accepting_states']:\n                        intersection['accepting_states'].add(next_state)\n\n    return intersection"}
{"code":"def fit(epochs:int, learn:BasicLearner, callbacks:Optional[CallbackList]=None, metrics:OptMetrics=None)->None:\n    \"Fit the `model` on `data` and learn using `loss_func` and `opt`.\"\n    assert len(learn.data.train_dl) != 0, f\"\"\"Your training dataloader is empty, can't train a model.\n        Use a smaller batch size (batch size={learn.data.train_dl.batch_size} for {len(learn.data.train_dl.dataset)} elements).\"\"\"\n    cb_handler = CallbackHandler(callbacks, metrics)\n    pbar = master_bar(range(epochs))\n    cb_handler.on_train_begin(epochs, pbar=pbar, metrics=metrics)\n\n    exception=False\n    try:\n        for epoch in pbar:\n            learn.model.train()\n            cb_handler.set_dl(learn.data.train_dl)\n            cb_handler.on_epoch_begin()\n            for xb,yb in progress_bar(learn.data.train_dl, parent=pbar):\n                xb, yb = cb_handler.on_batch_begin(xb, yb)\n                loss = loss_batch(learn.model, xb, yb, learn.loss_func, learn.opt, cb_handler)\n                if cb_handler.on_batch_end(loss): break\n\n            if not cb_handler.skip_validate and not learn.data.empty_val:\n                val_loss = validate(learn.model, learn.data.valid_dl, loss_func=learn.loss_func,\n                                       cb_handler=cb_handler, pbar=pbar)\n            else: val_loss=None\n            if cb_handler.on_epoch_end(val_loss): break\n    except Exception as e:\n        exception = e\n        raise\n    finally: cb_handler.on_train_end(exception)","return_type":"None","function_name":"fit","stripped_code":"def fit(epochs:int, learn:BasicLearner, callbacks:Optional[CallbackList]=None, metrics:OptMetrics=None):\n    \"Fit the `model` on `data` and learn using `loss_func` and `opt`.\"\n    assert len(learn.data.train_dl) != 0, f\"\"\"Your training dataloader is empty, can't train a model.\n        Use a smaller batch size (batch size={learn.data.train_dl.batch_size} for {len(learn.data.train_dl.dataset)} elements).\"\"\"\n    cb_handler = CallbackHandler(callbacks, metrics)\n    pbar = master_bar(range(epochs))\n    cb_handler.on_train_begin(epochs, pbar=pbar, metrics=metrics)\n\n    exception=False\n    try:\n        for epoch in pbar:\n            learn.model.train()\n            cb_handler.set_dl(learn.data.train_dl)\n            cb_handler.on_epoch_begin()\n            for xb,yb in progress_bar(learn.data.train_dl, parent=pbar):\n                xb, yb = cb_handler.on_batch_begin(xb, yb)\n                loss = loss_batch(learn.model, xb, yb, learn.loss_func, learn.opt, cb_handler)\n                if cb_handler.on_batch_end(loss): break\n\n            if not cb_handler.skip_validate and not learn.data.empty_val:\n                val_loss = validate(learn.model, learn.data.valid_dl, loss_func=learn.loss_func,\n                                       cb_handler=cb_handler, pbar=pbar)\n            else: val_loss=None\n            if cb_handler.on_epoch_end(val_loss): break\n    except Exception as e:\n        exception = e\n        raise\n    finally: cb_handler.on_train_end(exception)"}
{"code":"def extract(self, extractor: Extractor, extractable: Extractable = None, tokenizer: Tokenizer = None,\n                joiner: str = \"  \", **options) -> List[Extraction]:\n\n        \"\"\"\n        Invoke the extractor on the given extractable, accumulating all the extractions in a list.\n\n        Args:\n            extractor (Extractor):\n            extractable (extractable):\n            tokenizer: user can pass custom tokenizer if extractor wants token\n            joiner: user can pass joiner if extractor wants text\n            options: user can pass arguments as a dict to the extract() function of different extractors\n\n        Returns: List of Extraction, containing all the extractions.\n\n        \"\"\"\n        if not extractable:\n            extractable = self\n\n        if not tokenizer:\n            tokenizer = self.etk.default_tokenizer\n\n        extracted_results = list()\n\n        if extractor.input_type == InputType.TOKENS:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\")\n                tokens = extractable.get_tokens(tokenizer)\n                if tokens:\n                    extracted_results = extractor.extract(tokens, **options)\n            else:\n                raise ExtractorValueError(\n                    \"Extractor needs string, tokenizer needs string to tokenize, got \" + str(type(extractable.value)))\n\n        elif extractor.input_type == InputType.TEXT:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\"Extractor needs string, got extractable value as list, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\"Extractor needs string, got extractable value as dict, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as dict, converting to string\")\n                text = extractable.get_string(joiner)\n                if text:\n                    extracted_results = extractor.extract(text, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs string, got \" + str(type(extractable.value)))\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        elif extractor.input_type == InputType.OBJECT:\n            extracted_results = extractor.extract(extractable.value, **options)\n\n        elif extractor.input_type == InputType.HTML:\n            if bool(BeautifulSoup(extractable.value, \"html.parser\").find()):\n                extracted_results = extractor.extract(extractable.value, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs HTML, got non HTML string\")\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        try:\n            jsonPath = extractable.full_path\n        except AttributeError:\n            jsonPath = None\n\n        for e in extracted_results:\n            # for the purpose of provenance hierarrchy tracking, a parent's id for next generation.\n            e.prov_id = self.provenance_id_index\n            extraction_provenance_record: ExtractionProvenanceRecord = ExtractionProvenanceRecord(\n                e.prov_id, jsonPath, e.provenance[\"extractor_name\"],\n                e.provenance[\"start_char\"], e.provenance[\"end_char\"], e.provenance[\"confidence\"], self,\n                extractable.prov_id)\n            self._provenances[e.prov_id] = extraction_provenance_record\n\n            # for the purpose of provenance hierarchy tracking\n            self.provenance_id_index_incrementer()\n            self.create_provenance(extraction_provenance_record)\n\n        return extracted_results","return_type":"List[Extraction]","function_name":"Document.extract","stripped_code":"def extract(self, extractor: Extractor, extractable: Extractable = None, tokenizer: Tokenizer = None,\n                joiner: str = \"  \", **options):\n\n        \"\"\"\n        Invoke the extractor on the given extractable, accumulating all the extractions in a list.\n\n        Args:\n            extractor (Extractor):\n            extractable (extractable):\n            tokenizer: user can pass custom tokenizer if extractor wants token\n            joiner: user can pass joiner if extractor wants text\n            options: user can pass arguments as a dict to the extract() function of different extractors\n\n        Returns: List of Extraction, containing all the extractions.\n\n        \"\"\"\n        if not extractable:\n            extractable = self\n\n        if not tokenizer:\n            tokenizer = self.etk.default_tokenizer\n\n        extracted_results = list()\n\n        if extractor.input_type == InputType.TOKENS:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\")\n                tokens = extractable.get_tokens(tokenizer)\n                if tokens:\n                    extracted_results = extractor.extract(tokens, **options)\n            else:\n                raise ExtractorValueError(\n                    \"Extractor needs string, tokenizer needs string to tokenize, got \" + str(type(extractable.value)))\n\n        elif extractor.input_type == InputType.TEXT:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\"Extractor needs string, got extractable value as list, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\"Extractor needs string, got extractable value as dict, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as dict, converting to string\")\n                text = extractable.get_string(joiner)\n                if text:\n                    extracted_results = extractor.extract(text, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs string, got \" + str(type(extractable.value)))\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        elif extractor.input_type == InputType.OBJECT:\n            extracted_results = extractor.extract(extractable.value, **options)\n\n        elif extractor.input_type == InputType.HTML:\n            if bool(BeautifulSoup(extractable.value, \"html.parser\").find()):\n                extracted_results = extractor.extract(extractable.value, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs HTML, got non HTML string\")\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        try:\n            jsonPath = extractable.full_path\n        except AttributeError:\n            jsonPath = None\n\n        for e in extracted_results:\n            # for the purpose of provenance hierarrchy tracking, a parent's id for next generation.\n            e.prov_id = self.provenance_id_index\n            extraction_provenance_record: ExtractionProvenanceRecord = ExtractionProvenanceRecord(\n                e.prov_id, jsonPath, e.provenance[\"extractor_name\"],\n                e.provenance[\"start_char\"], e.provenance[\"end_char\"], e.provenance[\"confidence\"], self,\n                extractable.prov_id)\n            self._provenances[e.prov_id] = extraction_provenance_record\n\n            # for the purpose of provenance hierarchy tracking\n            self.provenance_id_index_incrementer()\n            self.create_provenance(extraction_provenance_record)\n\n        return extracted_results"}
{"code":"def validate_uncles(self, block: BaseBlock) -> None:\n        \"\"\"\n        Validate the uncles for the given block.\n        \"\"\"\n        has_uncles = len(block.uncles) > 0\n        should_have_uncles = block.header.uncles_hash != EMPTY_UNCLE_HASH\n\n        if not has_uncles and not should_have_uncles:\n            # optimization to avoid loading ancestors from DB, since the block has no uncles\n            return\n        elif has_uncles and not should_have_uncles:\n            raise ValidationError(\"Block has uncles but header suggests uncles should be empty\")\n        elif should_have_uncles and not has_uncles:\n            raise ValidationError(\"Header suggests block should have uncles but block has none\")\n\n        # Check for duplicates\n        uncle_groups = groupby(operator.attrgetter('hash'), block.uncles)\n        duplicate_uncles = tuple(sorted(\n            hash for hash, twins in uncle_groups.items() if len(twins) > 1\n        ))\n        if duplicate_uncles:\n            raise ValidationError(\n                \"Block contains duplicate uncles:\\n\"\n                \" - {0}\".format(' - '.join(duplicate_uncles))\n            )\n\n        recent_ancestors = tuple(\n            ancestor\n            for ancestor\n            in self.get_ancestors(MAX_UNCLE_DEPTH + 1, header=block.header)\n        )\n        recent_ancestor_hashes = {ancestor.hash for ancestor in recent_ancestors}\n        recent_uncle_hashes = _extract_uncle_hashes(recent_ancestors)\n\n        for uncle in block.uncles:\n            if uncle.hash == block.hash:\n                raise ValidationError(\"Uncle has same hash as block\")\n\n            # ensure the uncle has not already been included.\n            if uncle.hash in recent_uncle_hashes:\n                raise ValidationError(\n                    \"Duplicate uncle: {0}\".format(encode_hex(uncle.hash))\n                )\n\n            # ensure that the uncle is not one of the canonical chain blocks.\n            if uncle.hash in recent_ancestor_hashes:\n                raise ValidationError(\n                    \"Uncle {0} cannot be an ancestor of {1}\".format(\n                        encode_hex(uncle.hash), encode_hex(block.hash)))\n\n            # ensure that the uncle was built off of one of the canonical chain\n            # blocks.\n            if uncle.parent_hash not in recent_ancestor_hashes or (\n               uncle.parent_hash == block.header.parent_hash):\n                raise ValidationError(\n                    \"Uncle's parent {0} is not an ancestor of {1}\".format(\n                        encode_hex(uncle.parent_hash), encode_hex(block.hash)))\n\n            # Now perform VM level validation of the uncle\n            self.validate_seal(uncle)\n\n            try:\n                uncle_parent = self.get_block_header_by_hash(uncle.parent_hash)\n            except HeaderNotFound:\n                raise ValidationError(\n                    \"Uncle ancestor not found: {0}\".format(uncle.parent_hash)\n                )\n\n            uncle_vm_class = self.get_vm_class_for_block_number(uncle.block_number)\n            uncle_vm_class.validate_uncle(block, uncle, uncle_parent)","return_type":"None","function_name":"Chain.validate_uncles","stripped_code":"def validate_uncles(self, block: BaseBlock):\n        \"\"\"\n        Validate the uncles for the given block.\n        \"\"\"\n        has_uncles = len(block.uncles) > 0\n        should_have_uncles = block.header.uncles_hash != EMPTY_UNCLE_HASH\n\n        if not has_uncles and not should_have_uncles:\n            # optimization to avoid loading ancestors from DB, since the block has no uncles\n            return\n        elif has_uncles and not should_have_uncles:\n            raise ValidationError(\"Block has uncles but header suggests uncles should be empty\")\n        elif should_have_uncles and not has_uncles:\n            raise ValidationError(\"Header suggests block should have uncles but block has none\")\n\n        # Check for duplicates\n        uncle_groups = groupby(operator.attrgetter('hash'), block.uncles)\n        duplicate_uncles = tuple(sorted(\n            hash for hash, twins in uncle_groups.items() if len(twins) > 1\n        ))\n        if duplicate_uncles:\n            raise ValidationError(\n                \"Block contains duplicate uncles:\\n\"\n                \" - {0}\".format(' - '.join(duplicate_uncles))\n            )\n\n        recent_ancestors = tuple(\n            ancestor\n            for ancestor\n            in self.get_ancestors(MAX_UNCLE_DEPTH + 1, header=block.header)\n        )\n        recent_ancestor_hashes = {ancestor.hash for ancestor in recent_ancestors}\n        recent_uncle_hashes = _extract_uncle_hashes(recent_ancestors)\n\n        for uncle in block.uncles:\n            if uncle.hash == block.hash:\n                raise ValidationError(\"Uncle has same hash as block\")\n\n            # ensure the uncle has not already been included.\n            if uncle.hash in recent_uncle_hashes:\n                raise ValidationError(\n                    \"Duplicate uncle: {0}\".format(encode_hex(uncle.hash))\n                )\n\n            # ensure that the uncle is not one of the canonical chain blocks.\n            if uncle.hash in recent_ancestor_hashes:\n                raise ValidationError(\n                    \"Uncle {0} cannot be an ancestor of {1}\".format(\n                        encode_hex(uncle.hash), encode_hex(block.hash)))\n\n            # ensure that the uncle was built off of one of the canonical chain\n            # blocks.\n            if uncle.parent_hash not in recent_ancestor_hashes or (\n               uncle.parent_hash == block.header.parent_hash):\n                raise ValidationError(\n                    \"Uncle's parent {0} is not an ancestor of {1}\".format(\n                        encode_hex(uncle.parent_hash), encode_hex(block.hash)))\n\n            # Now perform VM level validation of the uncle\n            self.validate_seal(uncle)\n\n            try:\n                uncle_parent = self.get_block_header_by_hash(uncle.parent_hash)\n            except HeaderNotFound:\n                raise ValidationError(\n                    \"Uncle ancestor not found: {0}\".format(uncle.parent_hash)\n                )\n\n            uncle_vm_class = self.get_vm_class_for_block_number(uncle.block_number)\n            uncle_vm_class.validate_uncle(block, uncle, uncle_parent)"}
{"code":"def _fix_gaussian_width(gaussian_samples, amp: float, center: float, sigma: float,\n                        zeroed_width: Union[None, float] = None, rescale_amp: bool = False,\n                        ret_scale_factor: bool = False) -> np.ndarray:\n    r\"\"\"Enforce that the supplied gaussian pulse is zeroed at a specific width.\n\n    This is acheived by subtracting $\\Omega_g(center \\pm zeroed_width/2)$ from all samples.\n\n    amp: Pulse amplitude at `2\\times center+1`.\n    center: Center (mean) of pulse.\n    sigma: Width (standard deviation) of pulse.\n    zeroed_width: Subtract baseline to gaussian pulses to make sure\n             $\\Omega_g(center \\pm zeroed_width/2)=0$ is satisfied. This is used to avoid\n             large discontinuities at the start of a gaussian pulse. If unsupplied,\n             defaults to $2*(center+1)$ such that the samples are zero at $\\Omega_g(-1)$.\n    rescale_amp: If `zeroed_width` is not `None` and `rescale_amp=True` the pulse will\n                 be rescaled so that $\\Omega_g(center)-\\Omega_g(center\\pm zeroed_width/2)=amp$.\n    ret_scale_factor: Return amplitude scale factor.\n    \"\"\"\n    if zeroed_width is None:\n        zeroed_width = 2*(center+1)\n\n    zero_offset = gaussian(np.array([-zeroed_width/2]), amp, center, sigma)\n    gaussian_samples -= zero_offset\n    amp_scale_factor = 1.\n    if rescale_amp:\n        amp_scale_factor = amp/(amp-zero_offset)\n        gaussian_samples *= amp_scale_factor\n\n    if ret_scale_factor:\n        return gaussian_samples, amp_scale_factor\n    return gaussian_samples","return_type":"np.ndarray","function_name":"_fix_gaussian_width","stripped_code":"def _fix_gaussian_width(gaussian_samples, amp: float, center: float, sigma: float,\n                        zeroed_width: Union[None, float] = None, rescale_amp: bool = False,\n                        ret_scale_factor: bool = False):\n    r\"\"\"Enforce that the supplied gaussian pulse is zeroed at a specific width.\n\n    This is acheived by subtracting $\\Omega_g(center \\pm zeroed_width/2)$ from all samples.\n\n    amp: Pulse amplitude at `2\\times center+1`.\n    center: Center (mean) of pulse.\n    sigma: Width (standard deviation) of pulse.\n    zeroed_width: Subtract baseline to gaussian pulses to make sure\n             $\\Omega_g(center \\pm zeroed_width/2)=0$ is satisfied. This is used to avoid\n             large discontinuities at the start of a gaussian pulse. If unsupplied,\n             defaults to $2*(center+1)$ such that the samples are zero at $\\Omega_g(-1)$.\n    rescale_amp: If `zeroed_width` is not `None` and `rescale_amp=True` the pulse will\n                 be rescaled so that $\\Omega_g(center)-\\Omega_g(center\\pm zeroed_width/2)=amp$.\n    ret_scale_factor: Return amplitude scale factor.\n    \"\"\"\n    if zeroed_width is None:\n        zeroed_width = 2*(center+1)\n\n    zero_offset = gaussian(np.array([-zeroed_width/2]), amp, center, sigma)\n    gaussian_samples -= zero_offset\n    amp_scale_factor = 1.\n    if rescale_amp:\n        amp_scale_factor = amp/(amp-zero_offset)\n        gaussian_samples *= amp_scale_factor\n\n    if ret_scale_factor:\n        return gaussian_samples, amp_scale_factor\n    return gaussian_samples"}
{"code":"def from_http(cls, headers: Mapping[str, str], body: bytes,\n                  *, secret: Optional[str] = None) -> \"Event\":\n        \"\"\"Construct an event from HTTP headers and JSON body data.\n\n        The mapping providing the headers is expected to support lowercase keys.\n\n        Since this method assumes the body of the HTTP request is JSON, a check\n        is performed for a content-type of \"application/json\" (GitHub does\n        support other content-types). If the content-type does not match,\n        BadRequest is raised.\n\n        If the appropriate headers are provided for event validation, then it\n        will be performed unconditionally. Any failure in validation\n        (including not providing a secret) will lead to ValidationFailure being\n        raised.\n        \"\"\"\n        if \"x-hub-signature\" in headers:\n                if secret is None:\n                    raise ValidationFailure(\"secret not provided\")\n                validate_event(body, signature=headers[\"x-hub-signature\"],\n                               secret=secret)\n        elif secret is not None:\n            raise ValidationFailure(\"signature is missing\")\n\n        try:\n            data = _decode_body(headers[\"content-type\"], body, strict=True)\n        except (KeyError, ValueError) as exc:\n            raise BadRequest(http.HTTPStatus(415),\n                             \"expected a content-type of \"\n                             \"'application/json' or \"\n                             \"'application/x-www-form-urlencoded'\") from exc\n        return cls(data, event=headers[\"x-github-event\"],\n                   delivery_id=headers[\"x-github-delivery\"])","return_type":"\"Event\"","function_name":"Event.from_http","stripped_code":"def from_http(cls, headers: Mapping[str, str], body: bytes,\n                  *, secret: Optional[str] = None):\n        \"\"\"Construct an event from HTTP headers and JSON body data.\n\n        The mapping providing the headers is expected to support lowercase keys.\n\n        Since this method assumes the body of the HTTP request is JSON, a check\n        is performed for a content-type of \"application/json\" (GitHub does\n        support other content-types). If the content-type does not match,\n        BadRequest is raised.\n\n        If the appropriate headers are provided for event validation, then it\n        will be performed unconditionally. Any failure in validation\n        (including not providing a secret) will lead to ValidationFailure being\n        raised.\n        \"\"\"\n        if \"x-hub-signature\" in headers:\n                if secret is None:\n                    raise ValidationFailure(\"secret not provided\")\n                validate_event(body, signature=headers[\"x-hub-signature\"],\n                               secret=secret)\n        elif secret is not None:\n            raise ValidationFailure(\"signature is missing\")\n\n        try:\n            data = _decode_body(headers[\"content-type\"], body, strict=True)\n        except (KeyError, ValueError) as exc:\n            raise BadRequest(http.HTTPStatus(415),\n                             \"expected a content-type of \"\n                             \"'application/json' or \"\n                             \"'application/x-www-form-urlencoded'\") from exc\n        return cls(data, event=headers[\"x-github-event\"],\n                   delivery_id=headers[\"x-github-delivery\"])"}
{"code":"def install_trigger_function(connection: connection, overwrite: bool=False) -> None:\n    \"\"\"Install the psycopg2-pgevents trigger function against the database.\n\n    Parameters\n    ----------\n    connection: psycopg2.extensions.connection\n        Active connection to a PostGreSQL database.\n    overwrite: bool\n        Whether or not to overwrite existing installation of psycopg2-pgevents\n        trigger function, if existing installation is found.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    prior_install = False\n\n    if not overwrite:\n        prior_install = trigger_function_installed(connection)\n\n    if not prior_install:\n        log('Installing trigger function...', logger_name=_LOGGER_NAME)\n\n        execute(connection, INSTALL_TRIGGER_FUNCTION_STATEMENT)\n    else:\n        log('Trigger function already installed; skipping...', logger_name=_LOGGER_NAME)","return_type":"None","function_name":"install_trigger_function","stripped_code":"def install_trigger_function(connection: connection, overwrite: bool=False):\n    \"\"\"Install the psycopg2-pgevents trigger function against the database.\n\n    Parameters\n    ----------\n    connection: psycopg2.extensions.connection\n        Active connection to a PostGreSQL database.\n    overwrite: bool\n        Whether or not to overwrite existing installation of psycopg2-pgevents\n        trigger function, if existing installation is found.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    prior_install = False\n\n    if not overwrite:\n        prior_install = trigger_function_installed(connection)\n\n    if not prior_install:\n        log('Installing trigger function...', logger_name=_LOGGER_NAME)\n\n        execute(connection, INSTALL_TRIGGER_FUNCTION_STATEMENT)\n    else:\n        log('Trigger function already installed; skipping...', logger_name=_LOGGER_NAME)"}
{"code":"def run(self, host: str = \"0.0.0.0\", port: int = 5000) -> None:\n        \"\"\"\n        debug run\n        :param host: the hostname to listen on, default is ``'0.0.0.0'``\n        :param port: the port of the server, default id ``5000``\n        \"\"\"\n        loop = cast(asyncio.AbstractEventLoop, self._loop)\n        listen = self.listen(host=host, port=port)\n        server = loop.run_until_complete(listen)\n\n        def close() -> None:\n            \"\"\"\n            \u5173\u95ed\u56de\u8c03\n            \"\"\"\n            server.close()\n            loop.stop()\n        # print(type(server))\n        loop.add_signal_handler(SIGTERM, close)\n        loop.add_signal_handler(SIGINT, close)\n        loop.run_forever()","return_type":"None","function_name":"Application.run","stripped_code":"def run(self, host: str = \"0.0.0.0\", port: int = 5000):\n        \"\"\"\n        debug run\n        :param host: the hostname to listen on, default is ``'0.0.0.0'``\n        :param port: the port of the server, default id ``5000``\n        \"\"\"\n        loop = cast(asyncio.AbstractEventLoop, self._loop)\n        listen = self.listen(host=host, port=port)\n        server = loop.run_until_complete(listen)\n\n        def close() -> None:\n            \"\"\"\n            \u5173\u95ed\u56de\u8c03\n            \"\"\"\n            server.close()\n            loop.stop()\n        # print(type(server))\n        loop.add_signal_handler(SIGTERM, close)\n        loop.add_signal_handler(SIGINT, close)\n        loop.run_forever()"}
{"code":"def get_postgres_encoding(python_encoding: str) -> str:\n    \"\"\"Python to postgres encoding map.\"\"\"\n\n    encoding = normalize_encoding(python_encoding.lower())\n    encoding_ = aliases.aliases[encoding.replace('_', '', 1)].upper()\n    pg_encoding = PG_ENCODING_MAP[encoding_.replace('_', '')]\n\n    return pg_encoding","return_type":"str","function_name":"get_postgres_encoding","stripped_code":"def get_postgres_encoding(python_encoding: str):\n    \"\"\"Python to postgres encoding map.\"\"\"\n\n    encoding = normalize_encoding(python_encoding.lower())\n    encoding_ = aliases.aliases[encoding.replace('_', '', 1)].upper()\n    pg_encoding = PG_ENCODING_MAP[encoding_.replace('_', '')]\n\n    return pg_encoding"}
{"code":"def _resize(self, size: Tuple[int, int], axis: int = None) -> None:\n\t\t\"\"\"Resize the dataset, or the specified axis.\n\n\t\tThe dataset must be stored in chunked format; it can be resized up to the \"maximum shape\" (keyword maxshape) specified at creation time.\n\t\tThe rank of the dataset cannot be changed.\n\t\t\"Size\" should be a shape tuple, or if an axis is specified, an integer.\n\n\t\tBEWARE: This functions differently than the NumPy resize() method!\n\t\tThe data is not \"reshuffled\" to fit in the new shape; each axis is grown or shrunk independently.\n\t\tThe coordinates of existing data are fixed.\n\t\t\"\"\"\n\t\tif self.name == \"\":\n\t\t\tself.ds._file['/matrix'].resize(size, axis)\n\t\telse:\n\t\t\tself.ds._file['/layers/' + self.name].resize(size, axis)","return_type":"None","function_name":"LoomLayer._resize","stripped_code":"def _resize(self, size: Tuple[int, int], axis: int = None):\n\t\t\"\"\"Resize the dataset, or the specified axis.\n\n\t\tThe dataset must be stored in chunked format; it can be resized up to the \"maximum shape\" (keyword maxshape) specified at creation time.\n\t\tThe rank of the dataset cannot be changed.\n\t\t\"Size\" should be a shape tuple, or if an axis is specified, an integer.\n\n\t\tBEWARE: This functions differently than the NumPy resize() method!\n\t\tThe data is not \"reshuffled\" to fit in the new shape; each axis is grown or shrunk independently.\n\t\tThe coordinates of existing data are fixed.\n\t\t\"\"\"\n\t\tif self.name == \"\":\n\t\t\tself.ds._file['/matrix'].resize(size, axis)\n\t\telse:\n\t\t\tself.ds._file['/layers/' + self.name].resize(size, axis)"}
{"code":"def appt_exists(self, complex: str, house: str, appt: str) -> bool:\n        \"\"\"\n        Shortcut to check if appt exists in our database.\n        \"\"\"\n        try:\n            self.check_appt(complex, house, appt)\n        except exceptions.RumetrApptNotFound:\n            return False\n\n        return True","return_type":"bool","function_name":"Rumetr.appt_exists","stripped_code":"def appt_exists(self, complex: str, house: str, appt: str):\n        \"\"\"\n        Shortcut to check if appt exists in our database.\n        \"\"\"\n        try:\n            self.check_appt(complex, house, appt)\n        except exceptions.RumetrApptNotFound:\n            return False\n\n        return True"}
{"code":"def mediate_transfer(\n        state: MediatorTransferState,\n        possible_routes: List['RouteState'],\n        payer_channel: NettingChannelState,\n        channelidentifiers_to_channels: ChannelMap,\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n        pseudo_random_generator: random.Random,\n        payer_transfer: LockedTransferSignedState,\n        block_number: BlockNumber,\n) -> TransitionResult[MediatorTransferState]:\n    \"\"\" Try a new route or fail back to a refund.\n\n    The mediator can safely try a new route knowing that the tokens from\n    payer_transfer will cover the expenses of the mediation. If there is no\n    route available that may be used at the moment of the call the mediator may\n    send a refund back to the payer, allowing the payer to try a different\n    route.\n    \"\"\"\n    reachable_routes = filter_reachable_routes(\n        possible_routes,\n        nodeaddresses_to_networkstates,\n    )\n    available_routes = filter_used_routes(\n        state.transfers_pair,\n        reachable_routes,\n    )\n\n    assert payer_channel.partner_state.address == payer_transfer.balance_proof.sender\n\n    transfer_pair, mediated_events = forward_transfer_pair(\n        payer_transfer,\n        available_routes,\n        channelidentifiers_to_channels,\n        pseudo_random_generator,\n        block_number,\n    )\n\n    if transfer_pair is None:\n        assert not mediated_events\n\n        if state.transfers_pair:\n            original_pair = state.transfers_pair[0]\n            original_channel = get_payer_channel(\n                channelidentifiers_to_channels,\n                original_pair,\n            )\n        else:\n            original_channel = payer_channel\n\n        if original_channel:\n            transfer_pair, mediated_events = backward_transfer_pair(\n                original_channel,\n                payer_transfer,\n                pseudo_random_generator,\n                block_number,\n            )\n        else:\n            transfer_pair = None\n            mediated_events = list()\n\n    if transfer_pair is None:\n        assert not mediated_events\n        mediated_events = list()\n        state.waiting_transfer = WaitingTransferState(payer_transfer)\n\n    else:\n        # the list must be ordered from high to low expiration, expiration\n        # handling depends on it\n        state.transfers_pair.append(transfer_pair)\n\n    return TransitionResult(state, mediated_events)","return_type":"TransitionResult[MediatorTransferState]","function_name":"mediate_transfer","stripped_code":"def mediate_transfer(\n        state: MediatorTransferState,\n        possible_routes: List['RouteState'],\n        payer_channel: NettingChannelState,\n        channelidentifiers_to_channels: ChannelMap,\n        nodeaddresses_to_networkstates: NodeNetworkStateMap,\n        pseudo_random_generator: random.Random,\n        payer_transfer: LockedTransferSignedState,\n        block_number: BlockNumber,\n):\n    \"\"\" Try a new route or fail back to a refund.\n\n    The mediator can safely try a new route knowing that the tokens from\n    payer_transfer will cover the expenses of the mediation. If there is no\n    route available that may be used at the moment of the call the mediator may\n    send a refund back to the payer, allowing the payer to try a different\n    route.\n    \"\"\"\n    reachable_routes = filter_reachable_routes(\n        possible_routes,\n        nodeaddresses_to_networkstates,\n    )\n    available_routes = filter_used_routes(\n        state.transfers_pair,\n        reachable_routes,\n    )\n\n    assert payer_channel.partner_state.address == payer_transfer.balance_proof.sender\n\n    transfer_pair, mediated_events = forward_transfer_pair(\n        payer_transfer,\n        available_routes,\n        channelidentifiers_to_channels,\n        pseudo_random_generator,\n        block_number,\n    )\n\n    if transfer_pair is None:\n        assert not mediated_events\n\n        if state.transfers_pair:\n            original_pair = state.transfers_pair[0]\n            original_channel = get_payer_channel(\n                channelidentifiers_to_channels,\n                original_pair,\n            )\n        else:\n            original_channel = payer_channel\n\n        if original_channel:\n            transfer_pair, mediated_events = backward_transfer_pair(\n                original_channel,\n                payer_transfer,\n                pseudo_random_generator,\n                block_number,\n            )\n        else:\n            transfer_pair = None\n            mediated_events = list()\n\n    if transfer_pair is None:\n        assert not mediated_events\n        mediated_events = list()\n        state.waiting_transfer = WaitingTransferState(payer_transfer)\n\n    else:\n        # the list must be ordered from high to low expiration, expiration\n        # handling depends on it\n        state.transfers_pair.append(transfer_pair)\n\n    return TransitionResult(state, mediated_events)"}
{"code":"def sqrt(\n    data: AnnData,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n) -> Optional[AnnData]:\n    \"\"\"Square root the data matrix.\n\n    Computes :math:`X = \\\\sqrt(X)`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape ``n_obs`` \u00d7 ``n_vars``.\n        Rows correspond to cells and columns to genes.\n    copy\n        If an :class:`~scanpy.api.AnnData` is passed,\n        determines whether a copy is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        ``n_obs`` of the chunks to process the data in.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        if chunked:\n            for chunk, start, end in adata.chunked_X(chunk_size):\n                adata.X[start:end] = sqrt(chunk)\n        else:\n            adata.X = sqrt(data.X)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        return np.sqrt(X)\n    else:\n        return X.sqrt()","return_type":"Optional[AnnData]","function_name":"sqrt","stripped_code":"def sqrt(\n    data: AnnData,\n    copy: bool = False,\n    chunked: bool = False,\n    chunk_size: Optional[int] = None,\n):\n    \"\"\"Square root the data matrix.\n\n    Computes :math:`X = \\\\sqrt(X)`.\n\n    Parameters\n    ----------\n    data\n        The (annotated) data matrix of shape ``n_obs`` \u00d7 ``n_vars``.\n        Rows correspond to cells and columns to genes.\n    copy\n        If an :class:`~scanpy.api.AnnData` is passed,\n        determines whether a copy is returned.\n    chunked\n        Process the data matrix in chunks, which will save memory.\n        Applies only to :class:`~anndata.AnnData`.\n    chunk_size\n        ``n_obs`` of the chunks to process the data in.\n\n    Returns\n    -------\n    Returns or updates `data`, depending on `copy`.\n    \"\"\"\n    if isinstance(data, AnnData):\n        adata = data.copy() if copy else data\n        if chunked:\n            for chunk, start, end in adata.chunked_X(chunk_size):\n                adata.X[start:end] = sqrt(chunk)\n        else:\n            adata.X = sqrt(data.X)\n        return adata if copy else None\n    X = data  # proceed with data matrix\n    if not issparse(X):\n        return np.sqrt(X)\n    else:\n        return X.sqrt()"}
{"code":"def GET_save_modifiedconditionitemvalues(self) -> None:\n        \"\"\"ToDo: extend functionality and add tests\"\"\"\n        for item in state.conditionitems:\n            state.modifiedconditionitemvalues[self._id][item.name] = \\\n                list(item.device2target.values())[0].value","return_type":"None","function_name":"HydPyServer.GET_save_modifiedconditionitemvalues","stripped_code":"def GET_save_modifiedconditionitemvalues(self):\n        \"\"\"ToDo: extend functionality and add tests\"\"\"\n        for item in state.conditionitems:\n            state.modifiedconditionitemvalues[self._id][item.name] = \\\n                list(item.device2target.values())[0].value"}
{"code":"def _validate(self, inst: \"InstanceNode\", scope: ValidationScope,\n                  ctype: ContentType) -> None:\n        \"\"\"Extend the superclass method.\"\"\"\n        if scope.value & ValidationScope.semantics.value:\n            self._check_must(inst)        # must expressions\n        super()._validate(inst, scope, ctype)","return_type":"None","function_name":"DataNode._validate","stripped_code":"def _validate(self, inst: \"InstanceNode\", scope: ValidationScope,\n                  ctype: ContentType):\n        \"\"\"Extend the superclass method.\"\"\"\n        if scope.value & ValidationScope.semantics.value:\n            self._check_must(inst)        # must expressions\n        super()._validate(inst, scope, ctype)"}
{"code":"def execute(self, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Overrides the parent method to add log messages.\n\n        :param logger: the logger to use during parsing (optional: None is supported)\n        :param options:\n        :return:\n        \"\"\"\n        in_root_call = False\n        if logger is not None:\n            # log only for the root object, not for the children that will be created by the code below\n            if not hasattr(_BaseParsingPlan.thrd_locals, 'flag_exec') \\\n                    or _BaseParsingPlan.thrd_locals.flag_exec == 0:\n                # print('Executing Parsing Plan for ' + str(self))\n                logger.debug('Executing Parsing Plan for [{location}]'\n                             ''.format(location=self.obj_on_fs_to_parse.get_pretty_location(append_file_ext=False)))\n                _BaseParsingPlan.thrd_locals.flag_exec = 1\n                in_root_call = True\n\n        # Common log message\n        logger.debug('(P) ' + get_parsing_plan_log_str(self.obj_on_fs_to_parse, self.obj_type,\n                                                       log_only_last=not in_root_call, parser=self.parser))\n\n        try:\n            res = super(_BaseParsingPlan, self).execute(logger, options)\n            if logger.isEnabledFor(DEBUG):\n                logger.info('(P) {loc} -> {type} SUCCESS !'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(\n                    blank_parent_part=not GLOBAL_CONFIG.full_paths_in_logs,\n                    compact_file_ext=True),\n                    type=get_pretty_type_str(self.obj_type)))\n            else:\n                logger.info('SUCCESS parsed [{loc}] as a [{type}] successfully. Parser used was [{parser}]'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(compact_file_ext=True),\n                                      type=get_pretty_type_str(self.obj_type),\n                                      parser=str(self.parser)))\n            if in_root_call:\n                # print('Completed parsing successfully')\n                logger.debug('Completed parsing successfully')\n            return res\n\n        finally:\n            # remove threadlocal flag if needed\n            if in_root_call:\n                _BaseParsingPlan.thrd_locals.flag_exec = 0","return_type":"T","function_name":"_BaseParsingPlan.execute","stripped_code":"def execute(self, logger: Logger, options: Dict[str, Dict[str, Any]]):\n        \"\"\"\n        Overrides the parent method to add log messages.\n\n        :param logger: the logger to use during parsing (optional: None is supported)\n        :param options:\n        :return:\n        \"\"\"\n        in_root_call = False\n        if logger is not None:\n            # log only for the root object, not for the children that will be created by the code below\n            if not hasattr(_BaseParsingPlan.thrd_locals, 'flag_exec') \\\n                    or _BaseParsingPlan.thrd_locals.flag_exec == 0:\n                # print('Executing Parsing Plan for ' + str(self))\n                logger.debug('Executing Parsing Plan for [{location}]'\n                             ''.format(location=self.obj_on_fs_to_parse.get_pretty_location(append_file_ext=False)))\n                _BaseParsingPlan.thrd_locals.flag_exec = 1\n                in_root_call = True\n\n        # Common log message\n        logger.debug('(P) ' + get_parsing_plan_log_str(self.obj_on_fs_to_parse, self.obj_type,\n                                                       log_only_last=not in_root_call, parser=self.parser))\n\n        try:\n            res = super(_BaseParsingPlan, self).execute(logger, options)\n            if logger.isEnabledFor(DEBUG):\n                logger.info('(P) {loc} -> {type} SUCCESS !'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(\n                    blank_parent_part=not GLOBAL_CONFIG.full_paths_in_logs,\n                    compact_file_ext=True),\n                    type=get_pretty_type_str(self.obj_type)))\n            else:\n                logger.info('SUCCESS parsed [{loc}] as a [{type}] successfully. Parser used was [{parser}]'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(compact_file_ext=True),\n                                      type=get_pretty_type_str(self.obj_type),\n                                      parser=str(self.parser)))\n            if in_root_call:\n                # print('Completed parsing successfully')\n                logger.debug('Completed parsing successfully')\n            return res\n\n        finally:\n            # remove threadlocal flag if needed\n            if in_root_call:\n                _BaseParsingPlan.thrd_locals.flag_exec = 0"}
{"code":"def find_line_beginning(strings: Sequence[str],\n                        linestart: Optional[str]) -> int:\n    \"\"\"\n    Finds the index of the line in ``strings`` that begins with ``linestart``,\n    or ``-1`` if none is found.\n\n    If ``linestart is None``, match an empty line.\n    \"\"\"\n    if linestart is None:  # match an empty line\n        for i in range(len(strings)):\n            if is_empty_string(strings[i]):\n                return i\n        return -1\n    for i in range(len(strings)):\n        if strings[i].find(linestart) == 0:\n            return i\n    return -1","return_type":"int","function_name":"find_line_beginning","stripped_code":"def find_line_beginning(strings: Sequence[str],\n                        linestart: Optional[str]):\n    \"\"\"\n    Finds the index of the line in ``strings`` that begins with ``linestart``,\n    or ``-1`` if none is found.\n\n    If ``linestart is None``, match an empty line.\n    \"\"\"\n    if linestart is None:  # match an empty line\n        for i in range(len(strings)):\n            if is_empty_string(strings[i]):\n                return i\n        return -1\n    for i in range(len(strings)):\n        if strings[i].find(linestart) == 0:\n            return i\n    return -1"}
{"code":"def mod(computation: BaseComputation) -> None:\n    \"\"\"\n    Modulo\n    \"\"\"\n    value, mod = computation.stack_pop(num_items=2, type_hint=constants.UINT256)\n\n    if mod == 0:\n        result = 0\n    else:\n        result = value % mod\n\n    computation.stack_push(result)","return_type":"None","function_name":"mod","stripped_code":"def mod(computation: BaseComputation):\n    \"\"\"\n    Modulo\n    \"\"\"\n    value, mod = computation.stack_pop(num_items=2, type_hint=constants.UINT256)\n\n    if mod == 0:\n        result = 0\n    else:\n        result = value % mod\n\n    computation.stack_push(result)"}
{"code":"def pages_counter(self, total: int, page_size: int = 100) -> int:\n        \"\"\"Simple helper to handle pagination. Returns the number of pages for a\n        given number of results.\n\n        :param int total: count of metadata in a search request\n        :param int page_size: count of metadata to display in each page\n        \"\"\"\n        if total <= page_size:\n            count_pages = 1\n        else:\n            if (total % page_size) == 0:\n                count_pages = total / page_size\n            else:\n                count_pages = (total / page_size) + 1\n        # method ending\n        return int(count_pages)","return_type":"int","function_name":"IsogeoUtils.pages_counter","stripped_code":"def pages_counter(self, total: int, page_size: int = 100):\n        \"\"\"Simple helper to handle pagination. Returns the number of pages for a\n        given number of results.\n\n        :param int total: count of metadata in a search request\n        :param int page_size: count of metadata to display in each page\n        \"\"\"\n        if total <= page_size:\n            count_pages = 1\n        else:\n            if (total % page_size) == 0:\n                count_pages = total / page_size\n            else:\n                count_pages = (total / page_size) + 1\n        # method ending\n        return int(count_pages)"}
{"code":"def signed_to_twos_comp(val: int, n_bits: int) -> int:\n    \"\"\"\n    Convert a signed integer to its \"two's complement\" representation.\n\n    Args:\n        val: signed integer\n        n_bits: number of bits (which must reflect a whole number of bytes)\n\n    Returns:\n        unsigned integer: two's complement version\n\n    \"\"\"\n    assert n_bits % 8 == 0, \"Must specify a whole number of bytes\"\n    n_bytes = n_bits // 8\n    b = val.to_bytes(n_bytes, byteorder=sys.byteorder, signed=True)\n    return int.from_bytes(b, byteorder=sys.byteorder, signed=False)","return_type":"int","function_name":"signed_to_twos_comp","stripped_code":"def signed_to_twos_comp(val: int, n_bits: int):\n    \"\"\"\n    Convert a signed integer to its \"two's complement\" representation.\n\n    Args:\n        val: signed integer\n        n_bits: number of bits (which must reflect a whole number of bytes)\n\n    Returns:\n        unsigned integer: two's complement version\n\n    \"\"\"\n    assert n_bits % 8 == 0, \"Must specify a whole number of bytes\"\n    n_bytes = n_bits // 8\n    b = val.to_bytes(n_bytes, byteorder=sys.byteorder, signed=True)\n    return int.from_bytes(b, byteorder=sys.byteorder, signed=False)"}
{"code":"def set_level_for_logger_and_its_handlers(log: logging.Logger,\n                                          level: int) -> None:\n    \"\"\"\n    Set a log level for a log and all its handlers.\n\n    Args:\n        log: log to modify\n        level: log level to set\n    \"\"\"\n    log.setLevel(level)\n    for h in log.handlers:  # type: logging.Handler\n        h.setLevel(level)","return_type":"None","function_name":"set_level_for_logger_and_its_handlers","stripped_code":"def set_level_for_logger_and_its_handlers(log: logging.Logger,\n                                          level: int):\n    \"\"\"\n    Set a log level for a log and all its handlers.\n\n    Args:\n        log: log to modify\n        level: log level to set\n    \"\"\"\n    log.setLevel(level)\n    for h in log.handlers:  # type: logging.Handler\n        h.setLevel(level)"}
{"code":"def rst_underline(heading: str, underline_char: str) -> str:\n    \"\"\"\n    Underlines a heading for RST files.\n\n    Args:\n        heading: text to underline\n        underline_char: character to use\n\n    Returns:\n        underlined heading, over two lines (without a final terminating\n        newline)\n    \"\"\"\n    assert \"\\n\" not in heading\n    assert len(underline_char) == 1\n    return heading + \"\\n\" + (underline_char * len(heading))","return_type":"str","function_name":"rst_underline","stripped_code":"def rst_underline(heading: str, underline_char: str):\n    \"\"\"\n    Underlines a heading for RST files.\n\n    Args:\n        heading: text to underline\n        underline_char: character to use\n\n    Returns:\n        underlined heading, over two lines (without a final terminating\n        newline)\n    \"\"\"\n    assert \"\\n\" not in heading\n    assert len(underline_char) == 1\n    return heading + \"\\n\" + (underline_char * len(heading))"}
{"code":"def verify_certs_chain(certs_chain: List[crypto.X509], amazon_cert: crypto.X509) -> bool:\n    \"\"\"Verifies if Amazon and additional certificates creates chain of trust to a root CA.\n\n    Args:\n        certs_chain: List of pycrypto X509 intermediate certificates from signature chain URL.\n        amazon_cert: Pycrypto X509 Amazon certificate.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    \"\"\"\n    store = crypto.X509Store()\n\n    # add certificates from Amazon provided certs chain\n    for cert in certs_chain:\n        store.add_cert(cert)\n\n    # add CA certificates\n    default_verify_paths = ssl.get_default_verify_paths()\n\n    default_verify_file = default_verify_paths.cafile\n    default_verify_file = Path(default_verify_file).resolve() if default_verify_file else None\n\n    default_verify_path = default_verify_paths.capath\n    default_verify_path = Path(default_verify_path).resolve() if default_verify_path else None\n\n    ca_files = [ca_file for ca_file in default_verify_path.iterdir()] if default_verify_path else []\n    if default_verify_file:\n        ca_files.append(default_verify_file)\n\n    for ca_file in ca_files:\n        ca_file: Path\n        if ca_file.is_file():\n            with ca_file.open('r', encoding='ascii') as crt_f:\n                ca_certs_txt = crt_f.read()\n                ca_certs = extract_certs(ca_certs_txt)\n                for cert in ca_certs:\n                    store.add_cert(cert)\n\n    # add CA certificates (Windows)\n    ssl_context = ssl.create_default_context()\n    der_certs = ssl_context.get_ca_certs(binary_form=True)\n    pem_certs = '\\n'.join([ssl.DER_cert_to_PEM_cert(der_cert) for der_cert in der_certs])\n    ca_certs = extract_certs(pem_certs)\n    for ca_cert in ca_certs:\n        store.add_cert(ca_cert)\n\n    store_context = crypto.X509StoreContext(store, amazon_cert)\n\n    try:\n        store_context.verify_certificate()\n        result = True\n    except crypto.X509StoreContextError:\n        result = False\n\n    return result","return_type":"bool","function_name":"verify_certs_chain","stripped_code":"def verify_certs_chain(certs_chain: List[crypto.X509], amazon_cert: crypto.X509):\n    \"\"\"Verifies if Amazon and additional certificates creates chain of trust to a root CA.\n\n    Args:\n        certs_chain: List of pycrypto X509 intermediate certificates from signature chain URL.\n        amazon_cert: Pycrypto X509 Amazon certificate.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    \"\"\"\n    store = crypto.X509Store()\n\n    # add certificates from Amazon provided certs chain\n    for cert in certs_chain:\n        store.add_cert(cert)\n\n    # add CA certificates\n    default_verify_paths = ssl.get_default_verify_paths()\n\n    default_verify_file = default_verify_paths.cafile\n    default_verify_file = Path(default_verify_file).resolve() if default_verify_file else None\n\n    default_verify_path = default_verify_paths.capath\n    default_verify_path = Path(default_verify_path).resolve() if default_verify_path else None\n\n    ca_files = [ca_file for ca_file in default_verify_path.iterdir()] if default_verify_path else []\n    if default_verify_file:\n        ca_files.append(default_verify_file)\n\n    for ca_file in ca_files:\n        ca_file: Path\n        if ca_file.is_file():\n            with ca_file.open('r', encoding='ascii') as crt_f:\n                ca_certs_txt = crt_f.read()\n                ca_certs = extract_certs(ca_certs_txt)\n                for cert in ca_certs:\n                    store.add_cert(cert)\n\n    # add CA certificates (Windows)\n    ssl_context = ssl.create_default_context()\n    der_certs = ssl_context.get_ca_certs(binary_form=True)\n    pem_certs = '\\n'.join([ssl.DER_cert_to_PEM_cert(der_cert) for der_cert in der_certs])\n    ca_certs = extract_certs(pem_certs)\n    for ca_cert in ca_certs:\n        store.add_cert(ca_cert)\n\n    store_context = crypto.X509StoreContext(store, amazon_cert)\n\n    try:\n        store_context.verify_certificate()\n        result = True\n    except crypto.X509StoreContextError:\n        result = False\n\n    return result"}
{"code":"def emit(self, record: logging.LogRecord) -> None:\n        \"\"\"\n        Internal function to process a :class:`LogRecord`.\n        \"\"\"\n        # noinspection PyBroadException\n        try:\n            html = self.format(record)\n            self.logfunction(html)\n        except:  # nopep8\n            self.handleError(record)","return_type":"None","function_name":"HtmlColorHandler.emit","stripped_code":"def emit(self, record: logging.LogRecord):\n        \"\"\"\n        Internal function to process a :class:`LogRecord`.\n        \"\"\"\n        # noinspection PyBroadException\n        try:\n            html = self.format(record)\n            self.logfunction(html)\n        except:  # nopep8\n            self.handleError(record)"}
{"code":"def func_args(func)->bool:\n    \"Return the arguments of `func`.\"\n    code = func.__code__\n    return code.co_varnames[:code.co_argcount]","return_type":"bool","function_name":"func_args","stripped_code":"def func_args(func):\n    \"Return the arguments of `func`.\"\n    code = func.__code__\n    return code.co_varnames[:code.co_argcount]"}
{"code":"def localize_sql(self, sql: str) -> str:\n        \"\"\"Translates ?-placeholder SQL to appropriate dialect.\n\n        For example, MySQLdb uses %s rather than ?.\n        \"\"\"\n        # pyodbc seems happy with ? now (pyodbc.paramstyle is 'qmark');\n        # using ? is much simpler, because we may want to use % with LIKE\n        # fields or (in my case) with date formatting strings for\n        # STR_TO_DATE().\n        # If you get this wrong, you may see \"not all arguments converted\n        # during string formatting\";\n        # http://stackoverflow.com/questions/9337134\n        if self.db_pythonlib in [PYTHONLIB_PYMYSQL, PYTHONLIB_MYSQLDB]:\n            # These engines use %, so we need to convert ? to %, without\n            # breaking literal % values.\n            sql = _PERCENT_REGEX.sub(\"%%\", sql)\n            # ... replace all % with %% first\n            sql = _QUERY_VALUE_REGEX.sub(\"%s\", sql)\n            # ... replace all ? with %s in the SQL\n        # Otherwise: engine uses ?, so we don't have to fiddle.\n        return sql","return_type":"str","function_name":"DatabaseSupporter.localize_sql","stripped_code":"def localize_sql(self, sql: str):\n        \"\"\"Translates ?-placeholder SQL to appropriate dialect.\n\n        For example, MySQLdb uses %s rather than ?.\n        \"\"\"\n        # pyodbc seems happy with ? now (pyodbc.paramstyle is 'qmark');\n        # using ? is much simpler, because we may want to use % with LIKE\n        # fields or (in my case) with date formatting strings for\n        # STR_TO_DATE().\n        # If you get this wrong, you may see \"not all arguments converted\n        # during string formatting\";\n        # http://stackoverflow.com/questions/9337134\n        if self.db_pythonlib in [PYTHONLIB_PYMYSQL, PYTHONLIB_MYSQLDB]:\n            # These engines use %, so we need to convert ? to %, without\n            # breaking literal % values.\n            sql = _PERCENT_REGEX.sub(\"%%\", sql)\n            # ... replace all % with %% first\n            sql = _QUERY_VALUE_REGEX.sub(\"%s\", sql)\n            # ... replace all ? with %s in the SQL\n        # Otherwise: engine uses ?, so we don't have to fiddle.\n        return sql"}
{"code":"def get_components(self, component_config: Union[ConfigTree, List]) -> List:\n        \"\"\"Extracts component specifications from configuration information and returns initialized components.\n\n        Parameters\n        ----------\n        component_config :\n            A hierarchical component specification blob. This configuration information needs to be parsable\n            into a full import path and a set of initialization arguments by the ``parse_component_config``\n            method.\n\n        Returns\n        -------\n        List\n            A list of initialized components.\n        \"\"\"\n        if isinstance(component_config, ConfigTree):\n            component_list = self.parse_component_config(component_config.to_dict())\n        else:  # Components were specified in a list rather than a tree.\n            component_list = component_config\n        component_list = _prep_components(component_list)\n        return _import_and_instantiate_components(component_list)","return_type":"List","function_name":"ComponentConfigurationParser.get_components","stripped_code":"def get_components(self, component_config: Union[ConfigTree, List]):\n        \"\"\"Extracts component specifications from configuration information and returns initialized components.\n\n        Parameters\n        ----------\n        component_config :\n            A hierarchical component specification blob. This configuration information needs to be parsable\n            into a full import path and a set of initialization arguments by the ``parse_component_config``\n            method.\n\n        Returns\n        -------\n        List\n            A list of initialized components.\n        \"\"\"\n        if isinstance(component_config, ConfigTree):\n            component_list = self.parse_component_config(component_config.to_dict())\n        else:  # Components were specified in a list rather than a tree.\n            component_list = component_config\n        component_list = _prep_components(component_list)\n        return _import_and_instantiate_components(component_list)"}
{"code":"def validate_uncle(cls, block: BaseBlock, uncle: BaseBlock, uncle_parent: BaseBlock) -> None:\n        \"\"\"\n        Validate the given uncle in the context of the given block.\n        \"\"\"\n        if uncle.block_number >= block.number:\n            raise ValidationError(\n                \"Uncle number ({0}) is higher than block number ({1})\".format(\n                    uncle.block_number, block.number))\n\n        if uncle.block_number != uncle_parent.block_number + 1:\n            raise ValidationError(\n                \"Uncle number ({0}) is not one above ancestor's number ({1})\".format(\n                    uncle.block_number, uncle_parent.block_number))\n        if uncle.timestamp < uncle_parent.timestamp:\n            raise ValidationError(\n                \"Uncle timestamp ({0}) is before ancestor's timestamp ({1})\".format(\n                    uncle.timestamp, uncle_parent.timestamp))\n        if uncle.gas_used > uncle.gas_limit:\n            raise ValidationError(\n                \"Uncle's gas usage ({0}) is above the limit ({1})\".format(\n                    uncle.gas_used, uncle.gas_limit))","return_type":"None","function_name":"VM.validate_uncle","stripped_code":"def validate_uncle(cls, block: BaseBlock, uncle: BaseBlock, uncle_parent: BaseBlock):\n        \"\"\"\n        Validate the given uncle in the context of the given block.\n        \"\"\"\n        if uncle.block_number >= block.number:\n            raise ValidationError(\n                \"Uncle number ({0}) is higher than block number ({1})\".format(\n                    uncle.block_number, block.number))\n\n        if uncle.block_number != uncle_parent.block_number + 1:\n            raise ValidationError(\n                \"Uncle number ({0}) is not one above ancestor's number ({1})\".format(\n                    uncle.block_number, uncle_parent.block_number))\n        if uncle.timestamp < uncle_parent.timestamp:\n            raise ValidationError(\n                \"Uncle timestamp ({0}) is before ancestor's timestamp ({1})\".format(\n                    uncle.timestamp, uncle_parent.timestamp))\n        if uncle.gas_used > uncle.gas_limit:\n            raise ValidationError(\n                \"Uncle's gas usage ({0}) is above the limit ({1})\".format(\n                    uncle.gas_used, uncle.gas_limit))"}
{"code":"def dictlist_convert_to_string(dict_list: Iterable[Dict], key: str) -> None:\n    \"\"\"\n    Process an iterable of dictionaries. For each dictionary ``d``, convert\n    (in place) ``d[key]`` to a string form, ``str(d[key])``. If the result is a\n    blank string, convert it to ``None``.\n    \"\"\"\n    for d in dict_list:\n        d[key] = str(d[key])\n        if d[key] == \"\":\n            d[key] = None","return_type":"None","function_name":"dictlist_convert_to_string","stripped_code":"def dictlist_convert_to_string(dict_list: Iterable[Dict], key: str):\n    \"\"\"\n    Process an iterable of dictionaries. For each dictionary ``d``, convert\n    (in place) ``d[key]`` to a string form, ``str(d[key])``. If the result is a\n    blank string, convert it to ``None``.\n    \"\"\"\n    for d in dict_list:\n        d[key] = str(d[key])\n        if d[key] == \"\":\n            d[key] = None"}
{"code":"def update_state(self, txn, prev_result, is_committed=True) -> None:\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        self._validate_txn_type(txn)\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.database_manager.attribute_store.set(hashed_value, value)","return_type":"None","function_name":"AttributeHandler.update_state","stripped_code":"def update_state(self, txn, prev_result, is_committed=True):\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        self._validate_txn_type(txn)\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.database_manager.attribute_store.set(hashed_value, value)"}
{"code":"def _merge_data(self, data: AnyMapping, *additional: AnyMapping) -> dict:\n        r\"\"\"Merge base data and additional dicts.\n\n        :param data: Base data.\n        :param \\*additional: Additional data dicts to be merged into base dict.\n        \"\"\"\n        return defaults(\n            dict(data) if not isinstance(data, dict) else data,\n            *(dict(item) for item in additional))","return_type":"dict","function_name":"Schema._merge_data","stripped_code":"def _merge_data(self, data: AnyMapping, *additional: AnyMapping):\n        r\"\"\"Merge base data and additional dicts.\n\n        :param data: Base data.\n        :param \\*additional: Additional data dicts to be merged into base dict.\n        \"\"\"\n        return defaults(\n            dict(data) if not isinstance(data, dict) else data,\n            *(dict(item) for item in additional))"}
{"code":"def astensor(array: TensorLike) -> BKTensor:\n    \"\"\"Convert to product tensor\"\"\"\n    tensor = tf.convert_to_tensor(array, dtype=CTYPE)\n    if DEVICE == 'gpu':\n        tensor = tensor.gpu()  # pragma: no cover\n\n    # size = np.prod(np.array(tensor.get_shape().as_list()))\n    N = int(math.log2(size(tensor)))\n    tensor = tf.reshape(tensor, ([2]*N))\n\n    return tensor","return_type":"BKTensor","function_name":"astensor","stripped_code":"def astensor(array: TensorLike):\n    \"\"\"Convert to product tensor\"\"\"\n    tensor = tf.convert_to_tensor(array, dtype=CTYPE)\n    if DEVICE == 'gpu':\n        tensor = tensor.gpu()  # pragma: no cover\n\n    # size = np.prod(np.array(tensor.get_shape().as_list()))\n    N = int(math.log2(size(tensor)))\n    tensor = tf.reshape(tensor, ([2]*N))\n\n    return tensor"}
{"code":"def _add_default_options(self) -> None:\n        \"\"\"Add default command line options to the parser.\n        \"\"\"\n        # Updating the trust stores\n        update_stores_group = OptionGroup(self._parser, 'Trust stores options', '')\n        update_stores_group.add_option(\n            '--update_trust_stores',\n            help='Update the default trust stores used by SSLyze. The latest stores will be downloaded from '\n                 'https://github.com/nabla-c0d3/trust_stores_observatory. This option is meant to be used separately, '\n                 'and will silence any other command line option supplied to SSLyze.',\n            dest='update_trust_stores',\n            action='store_true',\n        )\n        self._parser.add_option_group(update_stores_group)\n\n        # Client certificate options\n        clientcert_group = OptionGroup(self._parser, 'Client certificate options', '')\n        clientcert_group.add_option(\n            '--cert',\n            help='Client certificate chain filename. The certificates must be in PEM format and must be sorted '\n                 'starting with the subject\\'s client certificate, followed by intermediate CA certificates if '\n                 'applicable.',\n            dest='cert'\n        )\n        clientcert_group.add_option(\n            '--key',\n            help='Client private key filename.',\n            dest='key'\n        )\n        clientcert_group.add_option(\n            '--keyform',\n            help='Client private key format. DER or PEM (default).',\n            dest='keyform',\n            default='PEM'\n        )\n        clientcert_group.add_option(\n            '--pass',\n            help='Client private key passphrase.',\n            dest='keypass',\n            default=''\n        )\n        self._parser.add_option_group(clientcert_group)\n\n        # Input / output\n        output_group = OptionGroup(self._parser, 'Input and output options', '')\n        # XML output\n        output_group.add_option(\n            '--xml_out',\n            help='Write the scan results as an XML document to the file XML_FILE. If XML_FILE is set to \"-\", the XML '\n                 'output will instead be printed to stdout. The corresponding XML Schema Definition is available at '\n                 './docs/xml_out.xsd',\n            dest='xml_file',\n            default=None\n        )\n        # JSON output\n        output_group.add_option(\n            '--json_out',\n            help='Write the scan results as a JSON document to the file JSON_FILE. If JSON_FILE is set to \"-\", the '\n                 'JSON output will instead be printed to stdout. The resulting JSON file is a serialized version of '\n                 'the ScanResult objects described in SSLyze\\'s Python API: the nodes and attributes will be the same. '\n                 'See https://nabla-c0d3.github.io/sslyze/documentation/available-scan-commands.html for more details.',\n            dest='json_file',\n            default=None\n        )\n        # Read targets from input file\n        output_group.add_option(\n            '--targets_in',\n            help='Read the list of targets to scan from the file TARGETS_IN. It should contain one host:port per '\n                 'line.',\n            dest='targets_in',\n            default=None\n        )\n        # No text output\n        output_group.add_option(\n            '--quiet',\n            action='store_true',\n            dest='quiet',\n            help='Do not output anything to stdout; useful when using --xml_out or --json_out.'\n        )\n        self._parser.add_option_group(output_group)\n\n        # Connectivity option group\n        connect_group = OptionGroup(self._parser, 'Connectivity options', '')\n        # Connection speed\n        connect_group.add_option(\n            '--slow_connection',\n            help='Greatly reduce the number of concurrent connections initiated by SSLyze. This will make the scans '\n                 'slower but more reliable if the connection between your host and the server is slow, or if the '\n                 'server cannot handle many concurrent connections. Enable this option if you are getting a lot of '\n                 'timeouts or errors.',\n            action='store_true',\n            dest='slow_connection',\n        )\n        # HTTP CONNECT Proxy\n        connect_group.add_option(\n            '--https_tunnel',\n            help='Tunnel all traffic to the target server(s) through an HTTP CONNECT proxy. HTTP_TUNNEL should be the '\n                 'proxy\\'s URL: \\'http://USER:PW@HOST:PORT/\\'. For proxies requiring authentication, only Basic '\n                 'Authentication is supported.',\n            dest='https_tunnel',\n            default=None\n        )\n        # STARTTLS\n        connect_group.add_option(\n            '--starttls',\n            help='Perform a StartTLS handshake when connecting to the target server(s). '\n                 '{}'.format(self.START_TLS_USAGE),\n            dest='starttls',\n            default=None\n        )\n        connect_group.add_option(\n            '--xmpp_to',\n            help='Optional setting for STARTTLS XMPP. XMPP_TO should be the hostname to be put in the \\'to\\' '\n                 'attribute of the XMPP stream. Default is the server\\'s hostname.',\n            dest='xmpp_to',\n            default=None\n        )\n        # Server Name Indication\n        connect_group.add_option(\n            '--sni',\n            help='Use Server Name Indication to specify the hostname to connect to.  Will only affect TLS 1.0+ '\n                 'connections.',\n            dest='sni',\n            default=None\n        )\n        self._parser.add_option_group(connect_group)","return_type":"None","function_name":"CommandLineParser._add_default_options","stripped_code":"def _add_default_options(self):\n        \"\"\"Add default command line options to the parser.\n        \"\"\"\n        # Updating the trust stores\n        update_stores_group = OptionGroup(self._parser, 'Trust stores options', '')\n        update_stores_group.add_option(\n            '--update_trust_stores',\n            help='Update the default trust stores used by SSLyze. The latest stores will be downloaded from '\n                 'https://github.com/nabla-c0d3/trust_stores_observatory. This option is meant to be used separately, '\n                 'and will silence any other command line option supplied to SSLyze.',\n            dest='update_trust_stores',\n            action='store_true',\n        )\n        self._parser.add_option_group(update_stores_group)\n\n        # Client certificate options\n        clientcert_group = OptionGroup(self._parser, 'Client certificate options', '')\n        clientcert_group.add_option(\n            '--cert',\n            help='Client certificate chain filename. The certificates must be in PEM format and must be sorted '\n                 'starting with the subject\\'s client certificate, followed by intermediate CA certificates if '\n                 'applicable.',\n            dest='cert'\n        )\n        clientcert_group.add_option(\n            '--key',\n            help='Client private key filename.',\n            dest='key'\n        )\n        clientcert_group.add_option(\n            '--keyform',\n            help='Client private key format. DER or PEM (default).',\n            dest='keyform',\n            default='PEM'\n        )\n        clientcert_group.add_option(\n            '--pass',\n            help='Client private key passphrase.',\n            dest='keypass',\n            default=''\n        )\n        self._parser.add_option_group(clientcert_group)\n\n        # Input / output\n        output_group = OptionGroup(self._parser, 'Input and output options', '')\n        # XML output\n        output_group.add_option(\n            '--xml_out',\n            help='Write the scan results as an XML document to the file XML_FILE. If XML_FILE is set to \"-\", the XML '\n                 'output will instead be printed to stdout. The corresponding XML Schema Definition is available at '\n                 './docs/xml_out.xsd',\n            dest='xml_file',\n            default=None\n        )\n        # JSON output\n        output_group.add_option(\n            '--json_out',\n            help='Write the scan results as a JSON document to the file JSON_FILE. If JSON_FILE is set to \"-\", the '\n                 'JSON output will instead be printed to stdout. The resulting JSON file is a serialized version of '\n                 'the ScanResult objects described in SSLyze\\'s Python API: the nodes and attributes will be the same. '\n                 'See https://nabla-c0d3.github.io/sslyze/documentation/available-scan-commands.html for more details.',\n            dest='json_file',\n            default=None\n        )\n        # Read targets from input file\n        output_group.add_option(\n            '--targets_in',\n            help='Read the list of targets to scan from the file TARGETS_IN. It should contain one host:port per '\n                 'line.',\n            dest='targets_in',\n            default=None\n        )\n        # No text output\n        output_group.add_option(\n            '--quiet',\n            action='store_true',\n            dest='quiet',\n            help='Do not output anything to stdout; useful when using --xml_out or --json_out.'\n        )\n        self._parser.add_option_group(output_group)\n\n        # Connectivity option group\n        connect_group = OptionGroup(self._parser, 'Connectivity options', '')\n        # Connection speed\n        connect_group.add_option(\n            '--slow_connection',\n            help='Greatly reduce the number of concurrent connections initiated by SSLyze. This will make the scans '\n                 'slower but more reliable if the connection between your host and the server is slow, or if the '\n                 'server cannot handle many concurrent connections. Enable this option if you are getting a lot of '\n                 'timeouts or errors.',\n            action='store_true',\n            dest='slow_connection',\n        )\n        # HTTP CONNECT Proxy\n        connect_group.add_option(\n            '--https_tunnel',\n            help='Tunnel all traffic to the target server(s) through an HTTP CONNECT proxy. HTTP_TUNNEL should be the '\n                 'proxy\\'s URL: \\'http://USER:PW@HOST:PORT/\\'. For proxies requiring authentication, only Basic '\n                 'Authentication is supported.',\n            dest='https_tunnel',\n            default=None\n        )\n        # STARTTLS\n        connect_group.add_option(\n            '--starttls',\n            help='Perform a StartTLS handshake when connecting to the target server(s). '\n                 '{}'.format(self.START_TLS_USAGE),\n            dest='starttls',\n            default=None\n        )\n        connect_group.add_option(\n            '--xmpp_to',\n            help='Optional setting for STARTTLS XMPP. XMPP_TO should be the hostname to be put in the \\'to\\' '\n                 'attribute of the XMPP stream. Default is the server\\'s hostname.',\n            dest='xmpp_to',\n            default=None\n        )\n        # Server Name Indication\n        connect_group.add_option(\n            '--sni',\n            help='Use Server Name Indication to specify the hostname to connect to.  Will only affect TLS 1.0+ '\n                 'connections.',\n            dest='sni',\n            default=None\n        )\n        self._parser.add_option_group(connect_group)"}
{"code":"def channel_names(self) -> tuple:\n        \"\"\"Channel names.\"\"\"\n        if \"channel_names\" not in self.attrs.keys():\n            self.attrs[\"channel_names\"] = np.array([], dtype=\"S\")\n        return tuple(s.decode() for s in self.attrs[\"channel_names\"])","return_type":"tuple","function_name":"Data.channel_names","stripped_code":"def channel_names(self):\n        \"\"\"Channel names.\"\"\"\n        if \"channel_names\" not in self.attrs.keys():\n            self.attrs[\"channel_names\"] = np.array([], dtype=\"S\")\n        return tuple(s.decode() for s in self.attrs[\"channel_names\"])"}
{"code":"def touch(self, job: Job) -> None:\n        \"\"\"Refreshes the TTR of a reserved job.\n\n        :param job: The job to touch.\n        \"\"\"\n        self._send_cmd(b'touch %d' % job.id, b'TOUCHED')","return_type":"None","function_name":"Client.touch","stripped_code":"def touch(self, job: Job):\n        \"\"\"Refreshes the TTR of a reserved job.\n\n        :param job: The job to touch.\n        \"\"\"\n        self._send_cmd(b'touch %d' % job.id, b'TOUCHED')"}
{"code":"def verify_request_monitoring(\n            self,\n            partner_address: Address,\n            requesting_address: Address,\n    ) -> bool:\n        \"\"\" One should only use this method to verify integrity and signatures of a\n        RequestMonitoring message. \"\"\"\n        if not self.non_closing_signature:\n            return False\n\n        balance_proof_data = pack_balance_proof(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n        )\n        blinded_data = pack_balance_proof_update(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            partner_signature=self.balance_proof.signature,\n        )\n        reward_proof_data = pack_reward_proof(\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            reward_amount=self.reward_amount,\n            nonce=self.balance_proof.nonce,\n        )\n        return (\n            recover(balance_proof_data, self.balance_proof.signature) == partner_address and\n            recover(blinded_data, self.non_closing_signature) == requesting_address and\n            recover(reward_proof_data, self.reward_proof_signature) == requesting_address\n        )","return_type":"bool","function_name":"RequestMonitoring.verify_request_monitoring","stripped_code":"def verify_request_monitoring(\n            self,\n            partner_address: Address,\n            requesting_address: Address,\n    ):\n        \"\"\" One should only use this method to verify integrity and signatures of a\n        RequestMonitoring message. \"\"\"\n        if not self.non_closing_signature:\n            return False\n\n        balance_proof_data = pack_balance_proof(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n        )\n        blinded_data = pack_balance_proof_update(\n            nonce=self.balance_proof.nonce,\n            balance_hash=self.balance_proof.balance_hash,\n            additional_hash=self.balance_proof.additional_hash,\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            partner_signature=self.balance_proof.signature,\n        )\n        reward_proof_data = pack_reward_proof(\n            canonical_identifier=CanonicalIdentifier(\n                chain_identifier=self.balance_proof.chain_id,\n                token_network_address=self.balance_proof.token_network_address,\n                channel_identifier=self.balance_proof.channel_identifier,\n            ),\n            reward_amount=self.reward_amount,\n            nonce=self.balance_proof.nonce,\n        )\n        return (\n            recover(balance_proof_data, self.balance_proof.signature) == partner_address and\n            recover(blinded_data, self.non_closing_signature) == requesting_address and\n            recover(reward_proof_data, self.reward_proof_signature) == requesting_address\n        )"}
{"code":"def next_future_periodic_delta(self) -> Optional[float]:\n        \"\"\"Give the amount of seconds before the next periodic task is due.\"\"\"\n        rv = self._r.zrangebyscore(\n            self._to_namespaced(PERIODIC_TASKS_QUEUE_KEY),\n            '-inf', '+inf', start=0, num=1, withscores=True,\n            score_cast_func=int\n        )\n        if not rv:\n            return None\n\n        now = datetime.now(timezone.utc).timestamp()\n        next_event_time = rv[0][1]\n        if next_event_time < now:\n            return 0\n\n        return next_event_time - now","return_type":"Optional[float]","function_name":"RedisBroker.next_future_periodic_delta","stripped_code":"def next_future_periodic_delta(self):\n        \"\"\"Give the amount of seconds before the next periodic task is due.\"\"\"\n        rv = self._r.zrangebyscore(\n            self._to_namespaced(PERIODIC_TASKS_QUEUE_KEY),\n            '-inf', '+inf', start=0, num=1, withscores=True,\n            score_cast_func=int\n        )\n        if not rv:\n            return None\n\n        now = datetime.now(timezone.utc).timestamp()\n        next_event_time = rv[0][1]\n        if next_event_time < now:\n            return 0\n\n        return next_event_time - now"}
{"code":"def require_scalar(self, *args: Type) -> None:\n        \"\"\"Require the node to be a scalar.\n\n        If additional arguments are passed, these are taken as a list \\\n        of valid types; if the node matches one of these, then it is \\\n        accepted.\n\n        Example:\n            # Match either an int or a string\n            node.require_scalar(int, str)\n\n        Arguments:\n            args: One or more types to match one of.\n        \"\"\"\n        node = Node(self.yaml_node)\n        if len(args) == 0:\n            if not node.is_scalar():\n                raise RecognitionError(('{}{}A scalar is required').format(\n                    self.yaml_node.start_mark, os.linesep))\n        else:\n            for typ in args:\n                if node.is_scalar(typ):\n                    return\n            raise RecognitionError(\n                ('{}{}A scalar of type {} is required').format(\n                    self.yaml_node.start_mark, os.linesep, args))","return_type":"None","function_name":"UnknownNode.require_scalar","stripped_code":"def require_scalar(self, *args: Type):\n        \"\"\"Require the node to be a scalar.\n\n        If additional arguments are passed, these are taken as a list \\\n        of valid types; if the node matches one of these, then it is \\\n        accepted.\n\n        Example:\n            # Match either an int or a string\n            node.require_scalar(int, str)\n\n        Arguments:\n            args: One or more types to match one of.\n        \"\"\"\n        node = Node(self.yaml_node)\n        if len(args) == 0:\n            if not node.is_scalar():\n                raise RecognitionError(('{}{}A scalar is required').format(\n                    self.yaml_node.start_mark, os.linesep))\n        else:\n            for typ in args:\n                if node.is_scalar(typ):\n                    return\n            raise RecognitionError(\n                ('{}{}A scalar of type {} is required').format(\n                    self.yaml_node.start_mark, os.linesep, args))"}
{"code":"def from_df(cls, path:PathOrStr, df:pd.DataFrame, folder:PathOrStr=None, label_delim:str=None, valid_pct:float=0.2,\n                fn_col:IntsOrStrs=0, label_col:IntsOrStrs=1, suffix:str='', **kwargs:Any)->'ImageDataBunch':\n        \"Create from a `DataFrame` `df`.\"\n        src = (ImageList.from_df(df, path=path, folder=folder, suffix=suffix, cols=fn_col)\n                .split_by_rand_pct(valid_pct)\n                .label_from_df(label_delim=label_delim, cols=label_col))\n        return cls.create_from_ll(src, **kwargs)","return_type":"'ImageDataBunch'","function_name":"ImageDataBunch.from_df","stripped_code":"def from_df(cls, path:PathOrStr, df:pd.DataFrame, folder:PathOrStr=None, label_delim:str=None, valid_pct:float=0.2,\n                fn_col:IntsOrStrs=0, label_col:IntsOrStrs=1, suffix:str='', **kwargs:Any):\n        \"Create from a `DataFrame` `df`.\"\n        src = (ImageList.from_df(df, path=path, folder=folder, suffix=suffix, cols=fn_col)\n                .split_by_rand_pct(valid_pct)\n                .label_from_df(label_delim=label_delim, cols=label_col))\n        return cls.create_from_ll(src, **kwargs)"}
{"code":"def name(self) -> str:\n        \"\"\"Return name relevant to direction.\"\"\"\n        if self.direction == DIRECTION_IN:\n            return self.raw.get('Input.Name', '')\n        return self.raw.get('Output.Name', '')","return_type":"str","function_name":"Port.name","stripped_code":"def name(self):\n        \"\"\"Return name relevant to direction.\"\"\"\n        if self.direction == DIRECTION_IN:\n            return self.raw.get('Input.Name', '')\n        return self.raw.get('Output.Name', '')"}
{"code":"def copy(\n        self: 'Model',\n        *,\n        include: 'SetStr' = None,\n        exclude: 'SetStr' = None,\n        update: 'DictStrAny' = None,\n        deep: bool = False,\n    ) -> 'Model':\n        \"\"\"\n        Duplicate a model, optionally choose which fields to include, exclude and change.\n\n        :param include: fields to include in new model\n        :param exclude: fields to exclude from new model, as with values this takes precedence over include\n        :param update: values to change/add in the new model. Note: the data is not validated before creating\n            the new model: you should trust this data\n        :param deep: set to `True` to make a deep copy of the model\n        :return: new model instance\n        \"\"\"\n        if include is None and exclude is None and update is None:\n            # skip constructing values if no arguments are passed\n            v = self.__values__\n        else:\n            return_keys = self._calculate_keys(include=include, exclude=exclude, skip_defaults=False)\n            if return_keys:\n                v = {**{k: v for k, v in self.__values__.items() if k in return_keys}, **(update or {})}\n            else:\n                v = {**self.__values__, **(update or {})}\n\n        if deep:\n            v = deepcopy(v)\n        m = self.__class__.construct(v, self.__fields_set__.copy())\n        return m","return_type":"'Model'","function_name":"BaseModel.copy","stripped_code":"def copy(\n        self: 'Model',\n        *,\n        include: 'SetStr' = None,\n        exclude: 'SetStr' = None,\n        update: 'DictStrAny' = None,\n        deep: bool = False,\n    ):\n        \"\"\"\n        Duplicate a model, optionally choose which fields to include, exclude and change.\n\n        :param include: fields to include in new model\n        :param exclude: fields to exclude from new model, as with values this takes precedence over include\n        :param update: values to change/add in the new model. Note: the data is not validated before creating\n            the new model: you should trust this data\n        :param deep: set to `True` to make a deep copy of the model\n        :return: new model instance\n        \"\"\"\n        if include is None and exclude is None and update is None:\n            # skip constructing values if no arguments are passed\n            v = self.__values__\n        else:\n            return_keys = self._calculate_keys(include=include, exclude=exclude, skip_defaults=False)\n            if return_keys:\n                v = {**{k: v for k, v in self.__values__.items() if k in return_keys}, **(update or {})}\n            else:\n                v = {**self.__values__, **(update or {})}\n\n        if deep:\n            v = deepcopy(v)\n        m = self.__class__.construct(v, self.__fields_set__.copy())\n        return m"}
{"code":"def create_and_register_access_db(filename: str,\n                                  dsn: str,\n                                  description: str) -> bool:\n    \"\"\"\n    (Windows only.)\n    Creates a Microsoft Access database and registers it with ODBC.\n\n    Args:\n        filename: filename of the database to create\n        dsn: ODBC data source name to create\n        description: description of the database\n\n    Returns:\n        bool: was the DSN created?\n    \"\"\"\n    fullfilename = os.path.abspath(filename)\n    create_string = fullfilename + \" General\"\n    # ... filename, space, sort order (\"General\" for English)\n    return (create_user_dsn(access_driver, CREATE_DB=create_string) and\n            register_access_db(filename, dsn, description))","return_type":"bool","function_name":"create_and_register_access_db","stripped_code":"def create_and_register_access_db(filename: str,\n                                  dsn: str,\n                                  description: str):\n    \"\"\"\n    (Windows only.)\n    Creates a Microsoft Access database and registers it with ODBC.\n\n    Args:\n        filename: filename of the database to create\n        dsn: ODBC data source name to create\n        description: description of the database\n\n    Returns:\n        bool: was the DSN created?\n    \"\"\"\n    fullfilename = os.path.abspath(filename)\n    create_string = fullfilename + \" General\"\n    # ... filename, space, sort order (\"General\" for English)\n    return (create_user_dsn(access_driver, CREATE_DB=create_string) and\n            register_access_db(filename, dsn, description))"}
{"code":"def exchange_currency(\n        base: T, to_currency: str, *, conversion_rate: Decimal=None) -> T:\n    \"\"\"\n    Exchanges Money, TaxedMoney and their ranges to the specified currency.\n    get_rate parameter is a callable taking single argument (target currency)\n    that returns proper conversion rate\n    \"\"\"\n    if base.currency == to_currency:\n        return base\n    if base.currency != BASE_CURRENCY and to_currency != BASE_CURRENCY:\n        # Exchange to base currency first\n        base = exchange_currency(base, BASE_CURRENCY)\n\n    if conversion_rate is None:\n        conversion_rate = get_conversion_rate(base.currency, to_currency)\n\n    if isinstance(base, Money):\n        return Money(base.amount * conversion_rate, currency=to_currency)\n    if isinstance(base, MoneyRange):\n        return MoneyRange(\n            exchange_currency(\n                base.start, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.stop, to_currency, conversion_rate=conversion_rate))\n    if isinstance(base, TaxedMoney):\n        return TaxedMoney(\n            exchange_currency(\n                base.net, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.gross, to_currency, conversion_rate=conversion_rate))\n    if isinstance(base, TaxedMoneyRange):\n        return TaxedMoneyRange(\n            exchange_currency(\n                base.start, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.stop, to_currency, conversion_rate=conversion_rate))\n\n    # base.currency was set but we don't know how to exchange given type\n    raise TypeError('Unknown base for exchange_currency: %r' % (base,))","return_type":"T","function_name":"exchange_currency","stripped_code":"def exchange_currency(\n        base: T, to_currency: str, *, conversion_rate: Decimal=None):\n    \"\"\"\n    Exchanges Money, TaxedMoney and their ranges to the specified currency.\n    get_rate parameter is a callable taking single argument (target currency)\n    that returns proper conversion rate\n    \"\"\"\n    if base.currency == to_currency:\n        return base\n    if base.currency != BASE_CURRENCY and to_currency != BASE_CURRENCY:\n        # Exchange to base currency first\n        base = exchange_currency(base, BASE_CURRENCY)\n\n    if conversion_rate is None:\n        conversion_rate = get_conversion_rate(base.currency, to_currency)\n\n    if isinstance(base, Money):\n        return Money(base.amount * conversion_rate, currency=to_currency)\n    if isinstance(base, MoneyRange):\n        return MoneyRange(\n            exchange_currency(\n                base.start, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.stop, to_currency, conversion_rate=conversion_rate))\n    if isinstance(base, TaxedMoney):\n        return TaxedMoney(\n            exchange_currency(\n                base.net, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.gross, to_currency, conversion_rate=conversion_rate))\n    if isinstance(base, TaxedMoneyRange):\n        return TaxedMoneyRange(\n            exchange_currency(\n                base.start, to_currency, conversion_rate=conversion_rate),\n            exchange_currency(\n                base.stop, to_currency, conversion_rate=conversion_rate))\n\n    # base.currency was set but we don't know how to exchange given type\n    raise TypeError('Unknown base for exchange_currency: %r' % (base,))"}
{"code":"def list_templates(self) -> List[str]:\n        \"\"\"Returns a list of all avilable templates in environment.\n\n        This considers the loaders on the :attr:`app` and blueprints.\n        \"\"\"\n        result = set()\n        for loader in self._loaders():\n            for template in loader.list_templates():\n                result.add(str(template))\n        return list(result)","return_type":"List[str]","function_name":"DispatchingJinjaLoader.list_templates","stripped_code":"def list_templates(self):\n        \"\"\"Returns a list of all avilable templates in environment.\n\n        This considers the loaders on the :attr:`app` and blueprints.\n        \"\"\"\n        result = set()\n        for loader in self._loaders():\n            for template in loader.list_templates():\n                result.add(str(template))\n        return list(result)"}
{"code":"def dir_tails(self, rr_id: str) -> str:\n        \"\"\"\n        Return path to the correct directory for the tails file on input revocation registry identifier.\n\n        :param rr_id: revocation registry identifier of interest\n        :return: path to tails dir for input revocation registry identifier\n        \"\"\"\n\n        return Tails.dir(self._dir_tails, rr_id)","return_type":"str","function_name":"HolderProver.dir_tails","stripped_code":"def dir_tails(self, rr_id: str):\n        \"\"\"\n        Return path to the correct directory for the tails file on input revocation registry identifier.\n\n        :param rr_id: revocation registry identifier of interest\n        :return: path to tails dir for input revocation registry identifier\n        \"\"\"\n\n        return Tails.dir(self._dir_tails, rr_id)"}
{"code":"def _handle_request(self, request: dict) -> dict:\n        \"\"\"Processes Alexa requests from skill server and returns responses to Alexa.\n\n        Args:\n            request: Dict with Alexa request payload and metadata.\n        Returns:\n            result: Alexa formatted or error response.\n        \"\"\"\n        request_body: bytes = request['request_body']\n        signature_chain_url: str = request['signature_chain_url']\n        signature: str = request['signature']\n        alexa_request: dict = request['alexa_request']\n\n        if not self._verify_request(signature_chain_url, signature, request_body):\n            return {'error': 'failed certificate/signature check'}\n\n        timestamp_str = alexa_request['request']['timestamp']\n        timestamp_datetime = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%SZ')\n        now = datetime.utcnow()\n\n        delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now\n\n        if abs(delta.seconds) > REQUEST_TIMESTAMP_TOLERANCE_SECS:\n            log.error(f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}')\n            return {'error': 'failed request timestamp check'}\n\n        conversation_key = alexa_request['session']['user']['userId']\n\n        if conversation_key not in self.conversations.keys():\n            if self.config['multi_instance']:\n                conv_agent = self._init_agent()\n                log.info('New conversation instance level agent initiated')\n            else:\n                conv_agent = self.agent\n\n            self.conversations[conversation_key] = \\\n                Conversation(config=self.config,\n                             agent=conv_agent,\n                             conversation_key=conversation_key,\n                             self_destruct_callback=lambda: self._del_conversation(conversation_key))\n\n            log.info(f'Created new conversation, key: {conversation_key}')\n\n        conversation = self.conversations[conversation_key]\n        response = conversation.handle_request(alexa_request)\n\n        return response","return_type":"dict","function_name":"Bot._handle_request","stripped_code":"def _handle_request(self, request: dict):\n        \"\"\"Processes Alexa requests from skill server and returns responses to Alexa.\n\n        Args:\n            request: Dict with Alexa request payload and metadata.\n        Returns:\n            result: Alexa formatted or error response.\n        \"\"\"\n        request_body: bytes = request['request_body']\n        signature_chain_url: str = request['signature_chain_url']\n        signature: str = request['signature']\n        alexa_request: dict = request['alexa_request']\n\n        if not self._verify_request(signature_chain_url, signature, request_body):\n            return {'error': 'failed certificate/signature check'}\n\n        timestamp_str = alexa_request['request']['timestamp']\n        timestamp_datetime = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%SZ')\n        now = datetime.utcnow()\n\n        delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now\n\n        if abs(delta.seconds) > REQUEST_TIMESTAMP_TOLERANCE_SECS:\n            log.error(f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}')\n            return {'error': 'failed request timestamp check'}\n\n        conversation_key = alexa_request['session']['user']['userId']\n\n        if conversation_key not in self.conversations.keys():\n            if self.config['multi_instance']:\n                conv_agent = self._init_agent()\n                log.info('New conversation instance level agent initiated')\n            else:\n                conv_agent = self.agent\n\n            self.conversations[conversation_key] = \\\n                Conversation(config=self.config,\n                             agent=conv_agent,\n                             conversation_key=conversation_key,\n                             self_destruct_callback=lambda: self._del_conversation(conversation_key))\n\n            log.info(f'Created new conversation, key: {conversation_key}')\n\n        conversation = self.conversations[conversation_key]\n        response = conversation.handle_request(alexa_request)\n\n        return response"}
{"code":"def _init_code(self, code: int) -> None:\n        \"\"\" Initialize from an int terminal code. \"\"\"\n        if -1 < code < 256:\n            self.code = '{:02}'.format(code)\n            self.hexval = term2hex(code)\n            self.rgb = hex2rgb(self.hexval)\n        else:\n            raise ValueError(' '.join((\n                'Code must be in the range 0-255, inclusive.',\n                'Got: {} ({})'\n            )).format(code, getattr(code, '__name__', type(code).__name__)))","return_type":"None","function_name":"ColorCode._init_code","stripped_code":"def _init_code(self, code: int):\n        \"\"\" Initialize from an int terminal code. \"\"\"\n        if -1 < code < 256:\n            self.code = '{:02}'.format(code)\n            self.hexval = term2hex(code)\n            self.rgb = hex2rgb(self.hexval)\n        else:\n            raise ValueError(' '.join((\n                'Code must be in the range 0-255, inclusive.',\n                'Got: {} ({})'\n            )).format(code, getattr(code, '__name__', type(code).__name__)))"}
{"code":"def _receiveFromRemotes(self, quotaPerRemote) -> int:\n        \"\"\"\n        Receives messages from remotes\n        :param quotaPerRemote: number of messages to receive from one remote\n        :return: number of received messages\n        \"\"\"\n\n        assert quotaPerRemote\n        totalReceived = 0\n        for ident, remote in self.remotesByKeys.items():\n            if not remote.socket:\n                continue\n            i = 0\n            sock = remote.socket\n            while i < quotaPerRemote:\n                try:\n                    msg, = sock.recv_multipart(flags=zmq.NOBLOCK)\n                    if not msg:\n                        # Router probing sends empty message on connection\n                        continue\n                    i += 1\n                    self._verifyAndAppend(msg, ident)\n                except zmq.Again:\n                    break\n            if i > 0:\n                logger.trace('{} got {} messages through remote {}'.\n                             format(self, i, remote))\n            totalReceived += i\n        return totalReceived","return_type":"int","function_name":"ZStack._receiveFromRemotes","stripped_code":"def _receiveFromRemotes(self, quotaPerRemote):\n        \"\"\"\n        Receives messages from remotes\n        :param quotaPerRemote: number of messages to receive from one remote\n        :return: number of received messages\n        \"\"\"\n\n        assert quotaPerRemote\n        totalReceived = 0\n        for ident, remote in self.remotesByKeys.items():\n            if not remote.socket:\n                continue\n            i = 0\n            sock = remote.socket\n            while i < quotaPerRemote:\n                try:\n                    msg, = sock.recv_multipart(flags=zmq.NOBLOCK)\n                    if not msg:\n                        # Router probing sends empty message on connection\n                        continue\n                    i += 1\n                    self._verifyAndAppend(msg, ident)\n                except zmq.Again:\n                    break\n            if i > 0:\n                logger.trace('{} got {} messages through remote {}'.\n                             format(self, i, remote))\n            totalReceived += i\n        return totalReceived"}
{"code":"def acquaint_insides(swap_gate: ops.Gate,\n                     acquaintance_gate: ops.Operation,\n                     qubits: Sequence[ops.Qid],\n                     before: bool,\n                     layers: Layers,\n                     mapping: Dict[ops.Qid, int]\n                     ) -> None:\n    \"\"\"Acquaints each of the qubits with another set specified by an\n    acquaintance gate.\n\n    Args:\n        qubits: The list of qubits of which half are individually acquainted\n            with another list of qubits.\n        layers: The layers to put gates into.\n        acquaintance_gate: The acquaintance gate that acquaints the end qubit\n            with another list of qubits.\n        before: Whether the acquainting is done before the shift.\n        swap_gate: The gate used to swap logical indices.\n        mapping: The mapping from qubits to logical indices. Used to keep track\n            of the effect of inside-acquainting swaps.\n    \"\"\"\n\n    max_reach = _get_max_reach(len(qubits), round_up=before)\n    reaches = itertools.chain(range(1, max_reach + 1),\n                    range(max_reach, -1, -1))\n    offsets = (0, 1) * max_reach\n    swap_gate = SwapPermutationGate(swap_gate)\n    ops = []\n    for offset, reach in zip(offsets, reaches):\n        if offset == before:\n            ops.append(acquaintance_gate)\n        for dr in range(offset, reach, 2):\n            ops.append(swap_gate(*qubits[dr:dr + 2]))\n    intrastitial_layer = getattr(layers, 'pre' if before else 'post')\n    intrastitial_layer += ops\n\n    # add interstitial gate\n    interstitial_layer = getattr(layers,\n            ('prior' if before else 'posterior') + '_interstitial')\n    interstitial_layer.append(acquaintance_gate)\n\n    # update mapping\n    reached_qubits = qubits[:max_reach + 1]\n    positions = list(mapping[q] for q in reached_qubits)\n    mapping.update(zip(reached_qubits, reversed(positions)))","return_type":"None","function_name":"acquaint_insides","stripped_code":"def acquaint_insides(swap_gate: ops.Gate,\n                     acquaintance_gate: ops.Operation,\n                     qubits: Sequence[ops.Qid],\n                     before: bool,\n                     layers: Layers,\n                     mapping: Dict[ops.Qid, int]\n                     ):\n    \"\"\"Acquaints each of the qubits with another set specified by an\n    acquaintance gate.\n\n    Args:\n        qubits: The list of qubits of which half are individually acquainted\n            with another list of qubits.\n        layers: The layers to put gates into.\n        acquaintance_gate: The acquaintance gate that acquaints the end qubit\n            with another list of qubits.\n        before: Whether the acquainting is done before the shift.\n        swap_gate: The gate used to swap logical indices.\n        mapping: The mapping from qubits to logical indices. Used to keep track\n            of the effect of inside-acquainting swaps.\n    \"\"\"\n\n    max_reach = _get_max_reach(len(qubits), round_up=before)\n    reaches = itertools.chain(range(1, max_reach + 1),\n                    range(max_reach, -1, -1))\n    offsets = (0, 1) * max_reach\n    swap_gate = SwapPermutationGate(swap_gate)\n    ops = []\n    for offset, reach in zip(offsets, reaches):\n        if offset == before:\n            ops.append(acquaintance_gate)\n        for dr in range(offset, reach, 2):\n            ops.append(swap_gate(*qubits[dr:dr + 2]))\n    intrastitial_layer = getattr(layers, 'pre' if before else 'post')\n    intrastitial_layer += ops\n\n    # add interstitial gate\n    interstitial_layer = getattr(layers,\n            ('prior' if before else 'posterior') + '_interstitial')\n    interstitial_layer.append(acquaintance_gate)\n\n    # update mapping\n    reached_qubits = qubits[:max_reach + 1]\n    positions = list(mapping[q] for q in reached_qubits)\n    mapping.update(zip(reached_qubits, reversed(positions)))"}
{"code":"def quirks_from_any_parent(\n        loc: Union[Labware, Well, str, ModuleGeometry, None]) -> List[str]:\n    \"\"\" Walk the tree of wells and labwares and extract quirks \"\"\"\n    def recursive_get_quirks(obj, found):\n        if isinstance(obj, Labware):\n            return found + obj.quirks\n        elif isinstance(obj, Well):\n            return recursive_get_quirks(obj.parent, found)\n        else:\n            return found\n    return recursive_get_quirks(loc, [])","return_type":"List[str]","function_name":"quirks_from_any_parent","stripped_code":"def quirks_from_any_parent(\n        loc: Union[Labware, Well, str, ModuleGeometry, None]):\n    \"\"\" Walk the tree of wells and labwares and extract quirks \"\"\"\n    def recursive_get_quirks(obj, found):\n        if isinstance(obj, Labware):\n            return found + obj.quirks\n        elif isinstance(obj, Well):\n            return recursive_get_quirks(obj.parent, found)\n        else:\n            return found\n    return recursive_get_quirks(loc, [])"}
{"code":"def count_vars(self) -> int:\n        \"\"\" Count var define by this scope \"\"\"\n        n = 0\n        for s in self._hsig.values():\n            if hasattr(s, 'is_var') and s.is_var:\n                n += 1\n        return n","return_type":"int","function_name":"Scope.count_vars","stripped_code":"def count_vars(self):\n        \"\"\" Count var define by this scope \"\"\"\n        n = 0\n        for s in self._hsig.values():\n            if hasattr(s, 'is_var') and s.is_var:\n                n += 1\n        return n"}
{"code":"def update_display_properties(self, display_calibration_info, display_properties: typing.Mapping, display_layers: typing.Sequence[typing.Mapping]) -> None:\n        \"\"\"Update the display values. Called from display panel.\n\n        This method saves the display values and data and triggers an update. It should be as fast as possible.\n\n        As a layer, this canvas item will respond to the update by calling prepare_render on the layer's rendering\n        thread. Prepare render will call prepare_display which will construct new axes and update all of the constituent\n        canvas items such as the axes labels and the graph layers. Each will trigger its own update if its inputs have\n        changed.\n\n        The inefficiencies in this process are that the layer must re-render on each call to this function. There is\n        also a cost within the constituent canvas items to check whether the axes or their data has changed.\n\n        When the display is associated with a single data item, the data will be\n        \"\"\"\n\n        # may be called from thread; prevent a race condition with closing.\n        with self.__closing_lock:\n            if self.__closed:\n                return\n\n            displayed_dimensional_scales = display_calibration_info.displayed_dimensional_scales\n            displayed_dimensional_calibrations = display_calibration_info.displayed_dimensional_calibrations\n            self.__data_scale = displayed_dimensional_scales[-1] if len(displayed_dimensional_scales) > 0 else 1\n            self.__displayed_dimensional_calibration = displayed_dimensional_calibrations[-1] if len(displayed_dimensional_calibrations) > 0 else Calibration.Calibration(scale=displayed_dimensional_scales[-1])\n            self.__intensity_calibration = display_calibration_info.displayed_intensity_calibration\n            self.__calibration_style = display_calibration_info.calibration_style\n            self.__y_min = display_properties.get(\"y_min\")\n            self.__y_max = display_properties.get(\"y_max\")\n            self.__y_style = display_properties.get(\"y_style\", \"linear\")\n            self.__left_channel = display_properties.get(\"left_channel\")\n            self.__right_channel = display_properties.get(\"right_channel\")\n            self.__legend_position = display_properties.get(\"legend_position\")\n            self.__display_layers = display_layers\n\n            if self.__display_values_list and len(self.__display_values_list) > 0:\n                self.__xdata_list = [display_values.display_data_and_metadata if display_values else None for display_values in self.__display_values_list]\n                xdata0 = self.__xdata_list[0]\n                if xdata0:\n                    self.__update_frame(xdata0.metadata)\n            else:\n                self.__xdata_list = list()\n\n            # update the cursor info\n            self.__update_cursor_info()\n\n            # mark for update. prepare display will mark children for update if necesssary.\n            self.update()","return_type":"None","function_name":"LinePlotCanvasItem.update_display_properties","stripped_code":"def update_display_properties(self, display_calibration_info, display_properties: typing.Mapping, display_layers: typing.Sequence[typing.Mapping]):\n        \"\"\"Update the display values. Called from display panel.\n\n        This method saves the display values and data and triggers an update. It should be as fast as possible.\n\n        As a layer, this canvas item will respond to the update by calling prepare_render on the layer's rendering\n        thread. Prepare render will call prepare_display which will construct new axes and update all of the constituent\n        canvas items such as the axes labels and the graph layers. Each will trigger its own update if its inputs have\n        changed.\n\n        The inefficiencies in this process are that the layer must re-render on each call to this function. There is\n        also a cost within the constituent canvas items to check whether the axes or their data has changed.\n\n        When the display is associated with a single data item, the data will be\n        \"\"\"\n\n        # may be called from thread; prevent a race condition with closing.\n        with self.__closing_lock:\n            if self.__closed:\n                return\n\n            displayed_dimensional_scales = display_calibration_info.displayed_dimensional_scales\n            displayed_dimensional_calibrations = display_calibration_info.displayed_dimensional_calibrations\n            self.__data_scale = displayed_dimensional_scales[-1] if len(displayed_dimensional_scales) > 0 else 1\n            self.__displayed_dimensional_calibration = displayed_dimensional_calibrations[-1] if len(displayed_dimensional_calibrations) > 0 else Calibration.Calibration(scale=displayed_dimensional_scales[-1])\n            self.__intensity_calibration = display_calibration_info.displayed_intensity_calibration\n            self.__calibration_style = display_calibration_info.calibration_style\n            self.__y_min = display_properties.get(\"y_min\")\n            self.__y_max = display_properties.get(\"y_max\")\n            self.__y_style = display_properties.get(\"y_style\", \"linear\")\n            self.__left_channel = display_properties.get(\"left_channel\")\n            self.__right_channel = display_properties.get(\"right_channel\")\n            self.__legend_position = display_properties.get(\"legend_position\")\n            self.__display_layers = display_layers\n\n            if self.__display_values_list and len(self.__display_values_list) > 0:\n                self.__xdata_list = [display_values.display_data_and_metadata if display_values else None for display_values in self.__display_values_list]\n                xdata0 = self.__xdata_list[0]\n                if xdata0:\n                    self.__update_frame(xdata0.metadata)\n            else:\n                self.__xdata_list = list()\n\n            # update the cursor info\n            self.__update_cursor_info()\n\n            # mark for update. prepare display will mark children for update if necesssary.\n            self.update()"}
{"code":"def login_user(self,\n                   user: User,\n                   remember: Optional[bool] = None,\n                   duration: Optional[timedelta] = None,\n                   force: bool = False,\n                   fresh: bool = True,\n                   ) -> bool:\n        \"\"\"\n        Logs a user in. You should pass the actual user object to this. If the\n        user's `active` property is ``False``, they will not be logged in\n        unless `force` is ``True``.\n\n        This will return ``True`` if the log in attempt succeeds, and ``False`` if\n        it fails (i.e. because the user is inactive).\n\n        :param user: The user object to log in.\n        :type user: object\n        :param remember: Whether to remember the user after their session expires.\n            Defaults to ``False``.\n        :type remember: bool\n        :param duration: The amount of time before the remember cookie expires. If\n            ``None`` the value set in the settings is used. Defaults to ``None``.\n        :type duration: :class:`datetime.timedelta`\n        :param force: If the user is inactive, setting this to ``True`` will log\n            them in regardless. Defaults to ``False``.\n        :type force: bool\n        :param fresh: setting this to ``False`` will log in the user with a session\n            marked as not \"fresh\". Defaults to ``True``.\n        :type fresh: bool\n        \"\"\"\n        if not force:\n            if not user.active:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.disabled_account'))\n\n            if (not user.confirmed_at\n                    and self.security.confirmable\n                    and not self.security.login_without_confirmation):\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.confirmation_required'))\n\n            if not user.password:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.password_not_set'))\n\n        session['user_id'] = getattr(user, user.Meta.pk)\n        session['_fresh'] = fresh\n        session['_id'] = app.login_manager._session_identifier_generator()\n\n        if remember is None:\n            remember = app.config.SECURITY_DEFAULT_REMEMBER_ME\n        if remember:\n            session['remember'] = 'set'\n            if duration is not None:\n                try:\n                    session['remember_seconds'] = duration.total_seconds()\n                except AttributeError:\n                    raise Exception('duration must be a datetime.timedelta, '\n                                    'instead got: {0}'.format(duration))\n\n        _request_ctx_stack.top.user = user\n        user_logged_in.send(app._get_current_object(), user=user)\n        identity_changed.send(app._get_current_object(),\n                              identity=Identity(user.id))\n        return True","return_type":"bool","function_name":"SecurityService.login_user","stripped_code":"def login_user(self,\n                   user: User,\n                   remember: Optional[bool] = None,\n                   duration: Optional[timedelta] = None,\n                   force: bool = False,\n                   fresh: bool = True,\n                   ):\n        \"\"\"\n        Logs a user in. You should pass the actual user object to this. If the\n        user's `active` property is ``False``, they will not be logged in\n        unless `force` is ``True``.\n\n        This will return ``True`` if the log in attempt succeeds, and ``False`` if\n        it fails (i.e. because the user is inactive).\n\n        :param user: The user object to log in.\n        :type user: object\n        :param remember: Whether to remember the user after their session expires.\n            Defaults to ``False``.\n        :type remember: bool\n        :param duration: The amount of time before the remember cookie expires. If\n            ``None`` the value set in the settings is used. Defaults to ``None``.\n        :type duration: :class:`datetime.timedelta`\n        :param force: If the user is inactive, setting this to ``True`` will log\n            them in regardless. Defaults to ``False``.\n        :type force: bool\n        :param fresh: setting this to ``False`` will log in the user with a session\n            marked as not \"fresh\". Defaults to ``True``.\n        :type fresh: bool\n        \"\"\"\n        if not force:\n            if not user.active:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.disabled_account'))\n\n            if (not user.confirmed_at\n                    and self.security.confirmable\n                    and not self.security.login_without_confirmation):\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.confirmation_required'))\n\n            if not user.password:\n                raise AuthenticationError(\n                    _('flask_unchained.bundles.security:error.password_not_set'))\n\n        session['user_id'] = getattr(user, user.Meta.pk)\n        session['_fresh'] = fresh\n        session['_id'] = app.login_manager._session_identifier_generator()\n\n        if remember is None:\n            remember = app.config.SECURITY_DEFAULT_REMEMBER_ME\n        if remember:\n            session['remember'] = 'set'\n            if duration is not None:\n                try:\n                    session['remember_seconds'] = duration.total_seconds()\n                except AttributeError:\n                    raise Exception('duration must be a datetime.timedelta, '\n                                    'instead got: {0}'.format(duration))\n\n        _request_ctx_stack.top.user = user\n        user_logged_in.send(app._get_current_object(), user=user)\n        identity_changed.send(app._get_current_object(),\n                              identity=Identity(user.id))\n        return True"}
{"code":"def is_sqlatype_date(coltype: TypeEngine) -> bool:\n    \"\"\"\n    Is the SQLAlchemy column type a date type?\n    \"\"\"\n    coltype = _coltype_to_typeengine(coltype)\n    # No longer valid in SQLAlchemy 1.2.11:\n    # return isinstance(coltype, sqltypes._DateAffinity)\n    return (\n        isinstance(coltype, sqltypes.DateTime) or\n        isinstance(coltype, sqltypes.Date)\n    )","return_type":"bool","function_name":"is_sqlatype_date","stripped_code":"def is_sqlatype_date(coltype: TypeEngine):\n    \"\"\"\n    Is the SQLAlchemy column type a date type?\n    \"\"\"\n    coltype = _coltype_to_typeengine(coltype)\n    # No longer valid in SQLAlchemy 1.2.11:\n    # return isinstance(coltype, sqltypes._DateAffinity)\n    return (\n        isinstance(coltype, sqltypes.DateTime) or\n        isinstance(coltype, sqltypes.Date)\n    )"}
{"code":"def uint16_gt(a: int, b: int) -> bool:\n    \"\"\"\n    Return a > b.\n    \"\"\"\n    half_mod = 0x8000\n    return (((a < b) and ((b - a) > half_mod)) or\n            ((a > b) and ((a - b) < half_mod)))","return_type":"bool","function_name":"uint16_gt","stripped_code":"def uint16_gt(a: int, b: int):\n    \"\"\"\n    Return a > b.\n    \"\"\"\n    half_mod = 0x8000\n    return (((a < b) and ((b - a) > half_mod)) or\n            ((a > b) and ((a - b) < half_mod)))"}
{"code":"def do_where(self, taskid: int) -> None:\n        \"\"\"Show stack frames for a task\"\"\"\n        task = task_by_id(taskid, self._loop)\n        if task:\n            self._sout.write(_format_stack(task))\n            self._sout.write('\\n')\n        else:\n            self._sout.write('No task %d\\n' % taskid)","return_type":"None","function_name":"Monitor.do_where","stripped_code":"def do_where(self, taskid: int):\n        \"\"\"Show stack frames for a task\"\"\"\n        task = task_by_id(taskid, self._loop)\n        if task:\n            self._sout.write(_format_stack(task))\n            self._sout.write('\\n')\n        else:\n            self._sout.write('No task %d\\n' % taskid)"}
{"code":"def alexa(self) -> dict:\n        \"\"\"Returns Amazon Alexa compatible state of the PlainText instance.\n\n        Creating Amazon Alexa response blank with populated \"outputSpeech\" and\n        \"card sections.\n\n        Returns:\n            response: Amazon Alexa representation of PlainText state.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.content},\n                'card': {\n                    'type': 'Simple',\n                    'content': self.content\n                }\n            }\n        }\n\n        return response","return_type":"dict","function_name":"PlainText.alexa","stripped_code":"def alexa(self):\n        \"\"\"Returns Amazon Alexa compatible state of the PlainText instance.\n\n        Creating Amazon Alexa response blank with populated \"outputSpeech\" and\n        \"card sections.\n\n        Returns:\n            response: Amazon Alexa representation of PlainText state.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.content},\n                'card': {\n                    'type': 'Simple',\n                    'content': self.content\n                }\n            }\n        }\n\n        return response"}
{"code":"def draw_capitan_score_bitmap(self, export_path: ExportPath) -> None:\n        \"\"\"\n        Draws the 30x30 symbol into the given file\n        :param export_path: The path, where the symbols should be created on disk\n        \"\"\"\n        with Image.fromarray(self.image_data, mode='L') as image:\n            image.save(export_path.get_full_path())","return_type":"None","function_name":"CapitanSymbol.draw_capitan_score_bitmap","stripped_code":"def draw_capitan_score_bitmap(self, export_path: ExportPath):\n        \"\"\"\n        Draws the 30x30 symbol into the given file\n        :param export_path: The path, where the symbols should be created on disk\n        \"\"\"\n        with Image.fromarray(self.image_data, mode='L') as image:\n            image.save(export_path.get_full_path())"}
{"code":"def jump(self, selected_number: int) -> None:\n        \"\"\"Jump to a specific trace frame in a trace.\n\n        Parameters:\n            selected_number: int    the trace frame number from trace output\n        \"\"\"\n        self._verify_entrypoint_selected()\n        if selected_number < 1 or selected_number > len(self.trace_tuples):\n            raise UserError(\n                \"Trace frame number out of bounds \"\n                f\"(expected 1-{len(self.trace_tuples)} but got {selected_number}).\"\n            )\n\n        self.current_trace_frame_index = selected_number - 1\n        self.trace()","return_type":"None","function_name":"Interactive.jump","stripped_code":"def jump(self, selected_number: int):\n        \"\"\"Jump to a specific trace frame in a trace.\n\n        Parameters:\n            selected_number: int    the trace frame number from trace output\n        \"\"\"\n        self._verify_entrypoint_selected()\n        if selected_number < 1 or selected_number > len(self.trace_tuples):\n            raise UserError(\n                \"Trace frame number out of bounds \"\n                f\"(expected 1-{len(self.trace_tuples)} but got {selected_number}).\"\n            )\n\n        self.current_trace_frame_index = selected_number - 1\n        self.trace()"}
{"code":"def svdd(self, data: ['SASdata', str] = None,\n             code: str = None,\n             id: str = None,\n             input: [str, list, dict] = None,\n             kernel: str = None,\n             savestate: str = None,\n             solver: str = None,\n             weight: str = None,\n             procopts: str = None,\n             stmtpassthrough: str = None,\n             **kwargs: dict) -> 'SASresults':\n        \"\"\"\n        Python method to call the SVDD procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_svdd_toc.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm code: The code variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm kernel: The kernel variable can only be a string type.\n        :parm savestate: The savestate variable can only be a string type.\n        :parm solver: The solver variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\"","return_type":"'SASresults'","function_name":"SASViyaML.svdd","stripped_code":"def svdd(self, data: ['SASdata', str] = None,\n             code: str = None,\n             id: str = None,\n             input: [str, list, dict] = None,\n             kernel: str = None,\n             savestate: str = None,\n             solver: str = None,\n             weight: str = None,\n             procopts: str = None,\n             stmtpassthrough: str = None,\n             **kwargs: dict):\n        \"\"\"\n        Python method to call the SVDD procedure\n\n        Documentation link:\n        https://go.documentation.sas.com/?docsetId=casml&docsetTarget=casml_svdd_toc.htm&docsetVersion=8.3&locale=en\n\n        :param data: SASdata object or string. This parameter is required.\n        :parm code: The code variable can only be a string type.\n        :parm id: The id variable can only be a string type.\n        :parm input: The input variable can be a string, list or dict type. It refers to the dependent, y, or label variable.\n        :parm kernel: The kernel variable can only be a string type.\n        :parm savestate: The savestate variable can only be a string type.\n        :parm solver: The solver variable can only be a string type.\n        :parm weight: The weight variable can only be a string type.\n        :parm procopts: The procopts variable is a generic option available for advanced use. It can only be a string type.\n        :parm stmtpassthrough: The stmtpassthrough variable is a generic option available for advanced use. It can only be a string type.\n        :return: SAS Result Object\n        \"\"\""}
{"code":"def _check_compatible_with(\n            self,\n            other: Union[Period, Timestamp, Timedelta, NaTType],\n    ) -> None:\n        \"\"\"\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n\n        Raises\n        ------\n        Exception\n        \"\"\"\n        raise AbstractMethodError(self)","return_type":"None","function_name":"AttributesMixin._check_compatible_with","stripped_code":"def _check_compatible_with(\n            self,\n            other: Union[Period, Timestamp, Timedelta, NaTType],\n    ):\n        \"\"\"\n        Verify that `self` and `other` are compatible.\n\n        * DatetimeArray verifies that the timezones (if any) match\n        * PeriodArray verifies that the freq matches\n        * Timedelta has no verification\n\n        In each case, NaT is considered compatible.\n\n        Parameters\n        ----------\n        other\n\n        Raises\n        ------\n        Exception\n        \"\"\"\n        raise AbstractMethodError(self)"}
{"code":"def is_rising(bins) -> bool:\n    \"\"\"Check whether the bins are in raising order.\n\n    Does not check if the bins are consecutive.\n\n    Parameters\n    ----------\n    bins: array_like\n    \"\"\"\n    # TODO: Optimize for numpy bins\n    bins = make_bin_array(bins)\n    if np.any(bins[:, 0] >= bins[:, 1]):\n        return False\n    if np.any(bins[1:, 0] < bins[:-1, 1]):\n        return False\n    return True","return_type":"bool","function_name":"is_rising","stripped_code":"def is_rising(bins):\n    \"\"\"Check whether the bins are in raising order.\n\n    Does not check if the bins are consecutive.\n\n    Parameters\n    ----------\n    bins: array_like\n    \"\"\"\n    # TODO: Optimize for numpy bins\n    bins = make_bin_array(bins)\n    if np.any(bins[:, 0] >= bins[:, 1]):\n        return False\n    if np.any(bins[1:, 0] < bins[:-1, 1]):\n        return False\n    return True"}
{"code":"def processPrepare(self, prepare: Prepare, sender: str) -> None:\n        \"\"\"\n        Validate and process the PREPARE specified.\n        If validation is successful, create a COMMIT and broadcast it.\n\n        :param prepare: a PREPARE msg\n        :param sender: name of the node that sent the PREPARE\n        \"\"\"\n        key = (prepare.viewNo, prepare.ppSeqNo)\n        self.logger.debug(\"{} received PREPARE{} from {}\".format(self, key, sender))\n\n        # TODO move this try/except up higher\n        try:\n            if self.validatePrepare(prepare, sender):\n                self.addToPrepares(prepare, sender)\n                self.stats.inc(TPCStat.PrepareRcvd)\n                self.logger.debug(\"{} processed incoming PREPARE {}\".format(\n                    self, (prepare.viewNo, prepare.ppSeqNo)))\n            else:\n                # TODO let's have isValidPrepare throw an exception that gets\n                # handled and possibly logged higher\n                self.logger.trace(\"{} cannot process incoming PREPARE\".format(self))\n        except SuspiciousNode as ex:\n            self.report_suspicious_node(ex)","return_type":"None","function_name":"Replica.processPrepare","stripped_code":"def processPrepare(self, prepare: Prepare, sender: str):\n        \"\"\"\n        Validate and process the PREPARE specified.\n        If validation is successful, create a COMMIT and broadcast it.\n\n        :param prepare: a PREPARE msg\n        :param sender: name of the node that sent the PREPARE\n        \"\"\"\n        key = (prepare.viewNo, prepare.ppSeqNo)\n        self.logger.debug(\"{} received PREPARE{} from {}\".format(self, key, sender))\n\n        # TODO move this try/except up higher\n        try:\n            if self.validatePrepare(prepare, sender):\n                self.addToPrepares(prepare, sender)\n                self.stats.inc(TPCStat.PrepareRcvd)\n                self.logger.debug(\"{} processed incoming PREPARE {}\".format(\n                    self, (prepare.viewNo, prepare.ppSeqNo)))\n            else:\n                # TODO let's have isValidPrepare throw an exception that gets\n                # handled and possibly logged higher\n                self.logger.trace(\"{} cannot process incoming PREPARE\".format(self))\n        except SuspiciousNode as ex:\n            self.report_suspicious_node(ex)"}
{"code":"def get_spacy_entites(self, text: str) -> list:\n        \"\"\"\n        Pretrained (spaCy)\n        Uses averaged preceptron\n        Places, dates, people, organizations\n        \"\"\"\n        # NOTE: spaCy doc\n        doc = self._language_model(text)\n\n        entities = [{\"entity\": ent.label_,\n                     \"value\": ent.text,\n                     \"start\": ent.start_char,\n                     \"end\": ent.end_char} for ent in doc.ents]\n\n        return entities","return_type":"list","function_name":"EntityExtraction.get_spacy_entites","stripped_code":"def get_spacy_entites(self, text: str):\n        \"\"\"\n        Pretrained (spaCy)\n        Uses averaged preceptron\n        Places, dates, people, organizations\n        \"\"\"\n        # NOTE: spaCy doc\n        doc = self._language_model(text)\n\n        entities = [{\"entity\": ent.label_,\n                     \"value\": ent.text,\n                     \"start\": ent.start_char,\n                     \"end\": ent.end_char} for ent in doc.ents]\n\n        return entities"}
{"code":"def strip_leading_underscores_from_keys(d: Dict) -> Dict:\n    \"\"\"\n    Clones a dictionary, removing leading underscores from key names.\n    Raises ``ValueError`` if this causes an attribute conflict.\n    \"\"\"\n    newdict = {}\n    for k, v in d.items():\n        if k.startswith('_'):\n            k = k[1:]\n            if k in newdict:\n                raise ValueError(\"Attribute conflict: _{k}, {k}\".format(k=k))\n        newdict[k] = v\n    return newdict","return_type":"Dict","function_name":"strip_leading_underscores_from_keys","stripped_code":"def strip_leading_underscores_from_keys(d: Dict):\n    \"\"\"\n    Clones a dictionary, removing leading underscores from key names.\n    Raises ``ValueError`` if this causes an attribute conflict.\n    \"\"\"\n    newdict = {}\n    for k, v in d.items():\n        if k.startswith('_'):\n            k = k[1:]\n            if k in newdict:\n                raise ValueError(\"Attribute conflict: _{k}, {k}\".format(k=k))\n        newdict[k] = v\n    return newdict"}
{"code":"def masked_mean(vector: torch.Tensor,\n                mask: torch.Tensor,\n                dim: int,\n                keepdim: bool = False,\n                eps: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    To calculate mean along certain dimensions on masked values\n\n    Parameters\n    ----------\n    vector : ``torch.Tensor``\n        The vector to calculate mean.\n    mask : ``torch.Tensor``\n        The mask of the vector. It must be broadcastable with vector.\n    dim : ``int``\n        The dimension to calculate mean\n    keepdim : ``bool``\n        Whether to keep dimension\n    eps : ``float``\n        A small value to avoid zero division problem.\n\n    Returns\n    -------\n    A ``torch.Tensor`` of including the mean values.\n    \"\"\"\n    one_minus_mask = (1.0 - mask).byte()\n    replaced_vector = vector.masked_fill(one_minus_mask, 0.0)\n\n    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)\n    value_count = torch.sum(mask.float(), dim=dim, keepdim=keepdim)\n    return value_sum / value_count.clamp(min=eps)","return_type":"torch.Tensor","function_name":"masked_mean","stripped_code":"def masked_mean(vector: torch.Tensor,\n                mask: torch.Tensor,\n                dim: int,\n                keepdim: bool = False,\n                eps: float = 1e-8):\n    \"\"\"\n    To calculate mean along certain dimensions on masked values\n\n    Parameters\n    ----------\n    vector : ``torch.Tensor``\n        The vector to calculate mean.\n    mask : ``torch.Tensor``\n        The mask of the vector. It must be broadcastable with vector.\n    dim : ``int``\n        The dimension to calculate mean\n    keepdim : ``bool``\n        Whether to keep dimension\n    eps : ``float``\n        A small value to avoid zero division problem.\n\n    Returns\n    -------\n    A ``torch.Tensor`` of including the mean values.\n    \"\"\"\n    one_minus_mask = (1.0 - mask).byte()\n    replaced_vector = vector.masked_fill(one_minus_mask, 0.0)\n\n    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)\n    value_count = torch.sum(mask.float(), dim=dim, keepdim=keepdim)\n    return value_sum / value_count.clamp(min=eps)"}
{"code":"def get_ancestor_hash(self, block_number: int) -> Hash32:\n        \"\"\"\n        Return the hash for the ancestor block with number ``block_number``.\n        Return the empty bytestring ``b''`` if the block number is outside of the\n        range of available block numbers (typically the last 255 blocks).\n        \"\"\"\n        ancestor_depth = self.block_number - block_number - 1\n        is_ancestor_depth_out_of_range = (\n            ancestor_depth >= MAX_PREV_HEADER_DEPTH or\n            ancestor_depth < 0 or\n            block_number < 0\n        )\n        if is_ancestor_depth_out_of_range:\n            return Hash32(b'')\n\n        try:\n            return nth(ancestor_depth, self.execution_context.prev_hashes)\n        except StopIteration:\n            # Ancestor with specified depth not present\n            return Hash32(b'')","return_type":"Hash32","function_name":"BaseState.get_ancestor_hash","stripped_code":"def get_ancestor_hash(self, block_number: int):\n        \"\"\"\n        Return the hash for the ancestor block with number ``block_number``.\n        Return the empty bytestring ``b''`` if the block number is outside of the\n        range of available block numbers (typically the last 255 blocks).\n        \"\"\"\n        ancestor_depth = self.block_number - block_number - 1\n        is_ancestor_depth_out_of_range = (\n            ancestor_depth >= MAX_PREV_HEADER_DEPTH or\n            ancestor_depth < 0 or\n            block_number < 0\n        )\n        if is_ancestor_depth_out_of_range:\n            return Hash32(b'')\n\n        try:\n            return nth(ancestor_depth, self.execution_context.prev_hashes)\n        except StopIteration:\n            # Ancestor with specified depth not present\n            return Hash32(b'')"}
{"code":"def _handle_websocket_headers(self, handler: WebSocketHandler) -> None:\n        \"\"\"Verifies all invariant- and required headers\n\n        If a header is missing or have an incorrect value ValueError will be\n        raised\n        \"\"\"\n        fields = (\"Host\", \"Sec-Websocket-Key\", \"Sec-Websocket-Version\")\n        if not all(map(lambda f: handler.request.headers.get(f), fields)):\n            raise ValueError(\"Missing/Invalid WebSocket headers\")","return_type":"None","function_name":"WebSocketProtocol13._handle_websocket_headers","stripped_code":"def _handle_websocket_headers(self, handler: WebSocketHandler):\n        \"\"\"Verifies all invariant- and required headers\n\n        If a header is missing or have an incorrect value ValueError will be\n        raised\n        \"\"\"\n        fields = (\"Host\", \"Sec-Websocket-Key\", \"Sec-Websocket-Version\")\n        if not all(map(lambda f: handler.request.headers.get(f), fields)):\n            raise ValueError(\"Missing/Invalid WebSocket headers\")"}
{"code":"def other(wxcodes: typing.List[str]) -> str:\n    \"\"\"\n    Format wx codes into a spoken word string\n    \"\"\"\n    ret = []\n    for code in wxcodes:\n        item = translate.wxcode(code)\n        if item.startswith('Vicinity'):\n            item = item.lstrip('Vicinity ') + ' in the Vicinity'\n        ret.append(item)\n    return '. '.join(ret)","return_type":"str","function_name":"other","stripped_code":"def other(wxcodes: typing.List[str]):\n    \"\"\"\n    Format wx codes into a spoken word string\n    \"\"\"\n    ret = []\n    for code in wxcodes:\n        item = translate.wxcode(code)\n        if item.startswith('Vicinity'):\n            item = item.lstrip('Vicinity ') + ' in the Vicinity'\n        ret.append(item)\n    return '. '.join(ret)"}
{"code":"def qname(self, uri: str) -> str:\n        ''' Returns qname of uri in rdflib graph while also saving it '''\n        try:\n            prefix, namespace, name = self.g.compute_qname(uri)\n            qname = prefix + ':' + name\n            return qname\n        except:\n            try:\n                print('prefix:', prefix)\n                print('namespace:', namespace)\n                print('name:', name)\n            except:\n                print('Could not print from compute_qname')\n            exit('No qname for ' + uri)","return_type":"str","function_name":"OntoPandas.qname","stripped_code":"def qname(self, uri: str):\n        ''' Returns qname of uri in rdflib graph while also saving it '''\n        try:\n            prefix, namespace, name = self.g.compute_qname(uri)\n            qname = prefix + ':' + name\n            return qname\n        except:\n            try:\n                print('prefix:', prefix)\n                print('namespace:', namespace)\n                print('name:', name)\n            except:\n                print('Could not print from compute_qname')\n            exit('No qname for ' + uri)"}
{"code":"def segment_str(text: str, phoneme_inventory: Set[str] = PHONEMES) -> str:\n    \"\"\"\n    Takes as input a string in Kunwinjku and segments it into phoneme-like\n    units based on the standard orthographic rules specified at\n    http://bininjgunwok.org.au/\n    \"\"\"\n\n    text = text.lower()\n    text = segment_into_tokens(text, phoneme_inventory)\n    return text","return_type":"str","function_name":"segment_str","stripped_code":"def segment_str(text: str, phoneme_inventory: Set[str] = PHONEMES):\n    \"\"\"\n    Takes as input a string in Kunwinjku and segments it into phoneme-like\n    units based on the standard orthographic rules specified at\n    http://bininjgunwok.org.au/\n    \"\"\"\n\n    text = text.lower()\n    text = segment_into_tokens(text, phoneme_inventory)\n    return text"}
{"code":"def check_backend() -> bool:\n        \"\"\"Check if the backend is available.\"\"\"\n        try:\n            import bluepy.btle  # noqa: F401 #pylint: disable=unused-import\n            return True\n        except ImportError as importerror:\n            _LOGGER.error('bluepy not found: %s', str(importerror))\n        return False","return_type":"bool","function_name":"BluepyBackend.check_backend","stripped_code":"def check_backend():\n        \"\"\"Check if the backend is available.\"\"\"\n        try:\n            import bluepy.btle  # noqa: F401 #pylint: disable=unused-import\n            return True\n        except ImportError as importerror:\n            _LOGGER.error('bluepy not found: %s', str(importerror))\n        return False"}
{"code":"def dictionize(fields: Sequence, records: Sequence) -> Generator:\n    \"\"\"Create dictionaries mapping fields to record data.\"\"\"\n\n    return (dict(zip(fields, rec)) for rec in records)","return_type":"Generator","function_name":"dictionize","stripped_code":"def dictionize(fields: Sequence, records: Sequence):\n    \"\"\"Create dictionaries mapping fields to record data.\"\"\"\n\n    return (dict(zip(fields, rec)) for rec in records)"}
{"code":"def is_catchup_needed_during_view_change(self) -> bool:\n        \"\"\"\n        Check if received a quorum of view change done messages and if yes\n        check if caught up till the\n        Check if all requests ordered till last prepared certificate\n        Check if last catchup resulted in no txns\n        \"\"\"\n        if self.caught_up_for_current_view():\n            logger.info('{} is caught up for the current view {}'.format(self, self.viewNo))\n            return False\n        logger.info('{} is not caught up for the current view {}'.format(self, self.viewNo))\n\n        if self.num_txns_caught_up_in_last_catchup() == 0:\n            if self.has_ordered_till_last_prepared_certificate():\n                logger.info('{} ordered till last prepared certificate'.format(self))\n                return False\n\n        if self.is_catch_up_limit(self.config.MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE):\n            # No more 3PC messages will be processed since maximum catchup\n            # rounds have been done\n            self.master_replica.last_prepared_before_view_change = None\n            return False\n\n        return True","return_type":"bool","function_name":"Node.is_catchup_needed_during_view_change","stripped_code":"def is_catchup_needed_during_view_change(self):\n        \"\"\"\n        Check if received a quorum of view change done messages and if yes\n        check if caught up till the\n        Check if all requests ordered till last prepared certificate\n        Check if last catchup resulted in no txns\n        \"\"\"\n        if self.caught_up_for_current_view():\n            logger.info('{} is caught up for the current view {}'.format(self, self.viewNo))\n            return False\n        logger.info('{} is not caught up for the current view {}'.format(self, self.viewNo))\n\n        if self.num_txns_caught_up_in_last_catchup() == 0:\n            if self.has_ordered_till_last_prepared_certificate():\n                logger.info('{} ordered till last prepared certificate'.format(self))\n                return False\n\n        if self.is_catch_up_limit(self.config.MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE):\n            # No more 3PC messages will be processed since maximum catchup\n            # rounds have been done\n            self.master_replica.last_prepared_before_view_change = None\n            return False\n\n        return True"}
{"code":"def write_bel_namespace(self, file: TextIO, use_names: bool = False) -> None:\n        \"\"\"Write as a BEL namespace file.\"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        if use_names and not self.has_names:\n            raise ValueError\n\n        values = (\n            self._get_namespace_name_to_encoding(desc='writing names')\n            if use_names else\n            self._get_namespace_identifier_to_encoding(desc='writing identifiers')\n        )\n\n        write_namespace(\n            namespace_name=self._get_namespace_name(),\n            namespace_keyword=self._get_namespace_keyword(),\n            namespace_query_url=self.identifiers_url,\n            values=values,\n            file=file,\n        )","return_type":"None","function_name":"BELNamespaceManagerMixin.write_bel_namespace","stripped_code":"def write_bel_namespace(self, file: TextIO, use_names: bool = False):\n        \"\"\"Write as a BEL namespace file.\"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        if use_names and not self.has_names:\n            raise ValueError\n\n        values = (\n            self._get_namespace_name_to_encoding(desc='writing names')\n            if use_names else\n            self._get_namespace_identifier_to_encoding(desc='writing identifiers')\n        )\n\n        write_namespace(\n            namespace_name=self._get_namespace_name(),\n            namespace_keyword=self._get_namespace_keyword(),\n            namespace_query_url=self.identifiers_url,\n            values=values,\n            file=file,\n        )"}
{"code":"def MoveWindow(handle: int, x: int, y: int, width: int, height: int, repaint: int = 1) -> bool:\n    \"\"\"\n    MoveWindow from Win32.\n    handle: int, the handle of a native window.\n    x: int.\n    y: int.\n    width: int.\n    height: int.\n    repaint: int, use 1 or 0.\n    Return bool, True if succeed otherwise False.\n    \"\"\"\n    return bool(ctypes.windll.user32.MoveWindow(ctypes.c_void_p(handle), x, y, width, height, repaint))","return_type":"bool","function_name":"MoveWindow","stripped_code":"def MoveWindow(handle: int, x: int, y: int, width: int, height: int, repaint: int = 1):\n    \"\"\"\n    MoveWindow from Win32.\n    handle: int, the handle of a native window.\n    x: int.\n    y: int.\n    width: int.\n    height: int.\n    repaint: int, use 1 or 0.\n    Return bool, True if succeed otherwise False.\n    \"\"\"\n    return bool(ctypes.windll.user32.MoveWindow(ctypes.c_void_p(handle), x, y, width, height, repaint))"}
{"code":"def recursive_getitem(d: Mapping[str, Any], keys: Union[str, Sequence[str]]) -> Any:\n    \"\"\" Recursively retrieve an item from a nested dict.\n\n    Credit to: https://stackoverflow.com/a/52260663\n\n    Args:\n        d: Mapping of strings to objects.\n        keys: Names of the keys under which the object is stored. Can also just be a single string.\n    Returns:\n        The object stored under the keys.\n    Raises:\n        KeyError: If one of the keys isnt' found.\n    \"\"\"\n    # If only a string, then just just return the item\n    if isinstance(keys, str):\n        return d[keys]\n    else:\n        return functools.reduce(operator.getitem, keys, d)","return_type":"Any","function_name":"recursive_getitem","stripped_code":"def recursive_getitem(d: Mapping[str, Any], keys: Union[str, Sequence[str]]):\n    \"\"\" Recursively retrieve an item from a nested dict.\n\n    Credit to: https://stackoverflow.com/a/52260663\n\n    Args:\n        d: Mapping of strings to objects.\n        keys: Names of the keys under which the object is stored. Can also just be a single string.\n    Returns:\n        The object stored under the keys.\n    Raises:\n        KeyError: If one of the keys isnt' found.\n    \"\"\"\n    # If only a string, then just just return the item\n    if isinstance(keys, str):\n        return d[keys]\n    else:\n        return functools.reduce(operator.getitem, keys, d)"}
{"code":"def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map) -> lmap.Map:\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s: sym.Symbol = entry.key\n            var: Var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers","return_type":"lmap.Map","function_name":"Namespace.__refer_all","stripped_code":"def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map):\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s: sym.Symbol = entry.key\n            var: Var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers"}
{"code":"def to_df(self) -> pd.DataFrame:\n        \"\"\"Convert to pandas dataframe.\"\"\"\n        df = pd.DataFrame(index=RangeIndex(0, self.shape[0], name=None))\n        for key in self.keys():\n            value = self[key]\n            for icolumn, column in enumerate(value.T):\n                df['{}{}'.format(key, icolumn+1)] = column\n        return df","return_type":"pd.DataFrame","function_name":"BoundRecArr.to_df","stripped_code":"def to_df(self):\n        \"\"\"Convert to pandas dataframe.\"\"\"\n        df = pd.DataFrame(index=RangeIndex(0, self.shape[0], name=None))\n        for key in self.keys():\n            value = self[key]\n            for icolumn, column in enumerate(value.T):\n                df['{}{}'.format(key, icolumn+1)] = column\n        return df"}
{"code":"def _expand(self, line: str) -> str:\n        \"\"\"Expand shortcuts and aliases\"\"\"\n\n        # expand aliases\n        # make a copy of aliases so we can edit it\n        tmp_aliases = list(self.aliases.keys())\n        keep_expanding = bool(tmp_aliases)\n        while keep_expanding:\n            for cur_alias in tmp_aliases:\n                keep_expanding = False\n                # apply our regex to line\n                match = self._command_pattern.search(line)\n                if match:\n                    # we got a match, extract the command\n                    command = match.group(1)\n                    if command and command == cur_alias:\n                        # rebuild line with the expanded alias\n                        line = self.aliases[cur_alias] + match.group(2) + line[match.end(2):]\n                        tmp_aliases.remove(cur_alias)\n                        keep_expanding = bool(tmp_aliases)\n                        break\n\n        # expand shortcuts\n        for (shortcut, expansion) in self.shortcuts:\n            if line.startswith(shortcut):\n                # If the next character after the shortcut isn't a space, then insert one\n                shortcut_len = len(shortcut)\n                if len(line) == shortcut_len or line[shortcut_len] != ' ':\n                    expansion += ' '\n\n                # Expand the shortcut\n                line = line.replace(shortcut, expansion, 1)\n                break\n        return line","return_type":"str","function_name":"StatementParser._expand","stripped_code":"def _expand(self, line: str):\n        \"\"\"Expand shortcuts and aliases\"\"\"\n\n        # expand aliases\n        # make a copy of aliases so we can edit it\n        tmp_aliases = list(self.aliases.keys())\n        keep_expanding = bool(tmp_aliases)\n        while keep_expanding:\n            for cur_alias in tmp_aliases:\n                keep_expanding = False\n                # apply our regex to line\n                match = self._command_pattern.search(line)\n                if match:\n                    # we got a match, extract the command\n                    command = match.group(1)\n                    if command and command == cur_alias:\n                        # rebuild line with the expanded alias\n                        line = self.aliases[cur_alias] + match.group(2) + line[match.end(2):]\n                        tmp_aliases.remove(cur_alias)\n                        keep_expanding = bool(tmp_aliases)\n                        break\n\n        # expand shortcuts\n        for (shortcut, expansion) in self.shortcuts:\n            if line.startswith(shortcut):\n                # If the next character after the shortcut isn't a space, then insert one\n                shortcut_len = len(shortcut)\n                if len(line) == shortcut_len or line[shortcut_len] != ' ':\n                    expansion += ' '\n\n                # Expand the shortcut\n                line = line.replace(shortcut, expansion, 1)\n                break\n        return line"}
{"code":"def _is_mergable(self, other) -> bool:\n        \"\"\"\n        :return: True if other can be merged into this statement else False\n        \"\"\"\n        if not isinstance(other, SwitchContainer):\n            return False\n\n        if not (self.switchOn is other.switchOn and\n                len(self.cases) == len(other.cases) and\n                self._is_mergable_statement_list(self.default, other.default)):\n            return False\n\n        for (vA, caseA), (vB, caseB) in zip(self.cases, other.cases):\n            if vA != vB or not self._is_mergable_statement_list(caseA, caseB):\n                return False\n\n        return True","return_type":"bool","function_name":"SwitchContainer._is_mergable","stripped_code":"def _is_mergable(self, other):\n        \"\"\"\n        :return: True if other can be merged into this statement else False\n        \"\"\"\n        if not isinstance(other, SwitchContainer):\n            return False\n\n        if not (self.switchOn is other.switchOn and\n                len(self.cases) == len(other.cases) and\n                self._is_mergable_statement_list(self.default, other.default)):\n            return False\n\n        for (vA, caseA), (vB, caseB) in zip(self.cases, other.cases):\n            if vA != vB or not self._is_mergable_statement_list(caseA, caseB):\n                return False\n\n        return True"}
{"code":"def read_range(self, begin: str, end: str) -> int:\n        \"\"\"\n        Consume head byte if it is >= begin and <= end else return false\n        Same as 'a'..'z' in BNF\n        \"\"\"\n        if self.read_eof():\n            return False\n        c = self._stream.peek_char\n        if begin <= c <= end:\n            self._stream.incpos()\n            return True\n        return False","return_type":"int","function_name":"BasicParser.read_range","stripped_code":"def read_range(self, begin: str, end: str):\n        \"\"\"\n        Consume head byte if it is >= begin and <= end else return false\n        Same as 'a'..'z' in BNF\n        \"\"\"\n        if self.read_eof():\n            return False\n        c = self._stream.peek_char\n        if begin <= c <= end:\n            self._stream.incpos()\n            return True\n        return False"}
{"code":"def get_cxflow_arg_parser(add_common_arguments: bool=False) -> ArgumentParser:\n    \"\"\"\n    Create the **cxflow** argument parser.\n\n    :return: an instance of the parser\n    \"\"\"\n    # create parser\n    main_parser = ArgumentParser('cxflow',\n                                 description='cxflow: lightweight framework for machine learning with '\n                                             'focus on modularization, re-usability and rapid experimenting.',\n                                 epilog='For more info see <https://cognexa.github.io/cxflow>')\n\n    main_parser.add_argument('--version', action='version', help='Print cxflow version and quit.',\n                             version='cxflow {}'.format(pkg_resources.get_distribution('cxflow').version))\n    subparsers = main_parser.add_subparsers(help='cxflow commands')\n\n    # create train sub-parser\n    train_parser = subparsers.add_parser('train', description='Start cxflow training from the ``config_file``.')\n    train_parser.set_defaults(subcommand='train')\n    train_parser.add_argument('config_file', help='path to the config file')\n\n    # create resume sub-parser\n    resume_parser = subparsers.add_parser('resume', description='Resume cxflow training from the ``config_path``.')\n    resume_parser.set_defaults(subcommand='resume')\n    resume_parser.add_argument('config_path', help='path to the config file or the directory in which it is stored')\n    resume_parser.add_argument('restore_from', nargs='?', default=None,\n                               help='information passed to the model constructor (backend-specific); '\n                                    'usually a directory in which the trained model is stored')\n\n    # create predict sub-parser (deprecated)\n    predict_parser = subparsers.add_parser('predict', description='Run prediction with the given ``config_path``.')\n    predict_parser.set_defaults(subcommand='predict')\n    predict_parser.add_argument('config_path', help='path to the config file or the directory in which it is stored')\n    predict_parser.add_argument('restore_from', nargs='?', default=None,\n                                help='information passed to the model constructor (backend-specific); usually a '\n                                     'directory in which the trained model is stored')\n\n    # create eval sub-parser\n    eval_parser = subparsers.add_parser('eval', description='Evaluate the given model on the specified data stream.')\n    eval_parser.set_defaults(subcommand='eval')\n    eval_parser.add_argument('stream_name', help='stream name to be evaluated')\n    eval_parser.add_argument('model_path', help='model path to be evaluated')\n    eval_parser.add_argument('--config', '-c', nargs='?', default=None, help='optional config path to be used')\n\n    # create dataset sub-parser\n    dataset_parser = subparsers.add_parser('dataset', description='Invoke arbitrary dataset method.')\n    dataset_parser.set_defaults(subcommand='dataset')\n    dataset_parser.add_argument('method', help='name of the method to be invoked')\n    dataset_parser.add_argument('config_file', help='path to the config file')\n\n    # create grid-search sub-parser\n    gridsearch_parser = subparsers.add_parser('gridsearch', description='Do parameter grid search (experimental).')\n    gridsearch_parser.set_defaults(subcommand='gridsearch')\n    gridsearch_parser.add_argument('script', help='Script to be grid-searched')\n    gridsearch_parser.add_argument('params', nargs='*', help='Params to be tested. Format: name:type=[value1,value2]. '\n                                                             'Type is optional')\n    gridsearch_parser.add_argument('--dry-run', action='store_true', help='Only print command output instead '\n                                                                          'of executing it right away')\n\n    # create ls sub-parser\n    ls_parser = subparsers.add_parser('ls', description='List training log dirs in the given path.')\n    ls_parser.set_defaults(subcommand='ls')\n    ls_parser.add_argument('dir', nargs='?', default=CXF_DEFAULT_LOG_DIR,\n                           help='path to the log directory to be listed')\n    ls_parser.add_argument('-l', '--long', action='store_true', help='use long listing format')\n    ls_parser.add_argument('-a', '--all', action='store_true', help='include trainings with no epochs done')\n    ls_parser.add_argument('-r', '--recursive', action='store_true',\n                           help='list all the dirs recursively, stop at training dirs')\n    ls_parser.add_argument('-v', '--verbose', action='store_true',\n                           help='print more verbose output, applicable only when a single train dir is listed')\n\n    # create prune sub-parser\n    prune_parser = subparsers.add_parser('prune', description='Prune training log dirs in the given path without finished epochs.')\n    prune_parser.set_defaults(subcommand='prune')\n    prune_parser.add_argument('dir', nargs='?', default=CXF_DEFAULT_LOG_DIR,\n                           help='path to the log directory to be pruned')\n    prune_parser.add_argument('-e', '--epochs', default=1, type=int,\n                              help='keep only training log dirs having at least this many completed epochs, default 1')\n    prune_parser.add_argument('-s', '--subdirs', action='store_true',\n                              help='delete all subdirectories in training directories')\n\n    # add common arguments\n    if add_common_arguments:\n        for parser in [main_parser, train_parser, resume_parser, predict_parser, dataset_parser, eval_parser]:\n            parser.add_argument('--output_root', '-o', default='./log', help='output directory')\n            parser.add_argument('--verbose', '-v', action='store_true', help='increase verbosity to level DEBUG')\n\n    return main_parser","return_type":"ArgumentParser","function_name":"get_cxflow_arg_parser","stripped_code":"def get_cxflow_arg_parser(add_common_arguments: bool=False):\n    \"\"\"\n    Create the **cxflow** argument parser.\n\n    :return: an instance of the parser\n    \"\"\"\n    # create parser\n    main_parser = ArgumentParser('cxflow',\n                                 description='cxflow: lightweight framework for machine learning with '\n                                             'focus on modularization, re-usability and rapid experimenting.',\n                                 epilog='For more info see <https://cognexa.github.io/cxflow>')\n\n    main_parser.add_argument('--version', action='version', help='Print cxflow version and quit.',\n                             version='cxflow {}'.format(pkg_resources.get_distribution('cxflow').version))\n    subparsers = main_parser.add_subparsers(help='cxflow commands')\n\n    # create train sub-parser\n    train_parser = subparsers.add_parser('train', description='Start cxflow training from the ``config_file``.')\n    train_parser.set_defaults(subcommand='train')\n    train_parser.add_argument('config_file', help='path to the config file')\n\n    # create resume sub-parser\n    resume_parser = subparsers.add_parser('resume', description='Resume cxflow training from the ``config_path``.')\n    resume_parser.set_defaults(subcommand='resume')\n    resume_parser.add_argument('config_path', help='path to the config file or the directory in which it is stored')\n    resume_parser.add_argument('restore_from', nargs='?', default=None,\n                               help='information passed to the model constructor (backend-specific); '\n                                    'usually a directory in which the trained model is stored')\n\n    # create predict sub-parser (deprecated)\n    predict_parser = subparsers.add_parser('predict', description='Run prediction with the given ``config_path``.')\n    predict_parser.set_defaults(subcommand='predict')\n    predict_parser.add_argument('config_path', help='path to the config file or the directory in which it is stored')\n    predict_parser.add_argument('restore_from', nargs='?', default=None,\n                                help='information passed to the model constructor (backend-specific); usually a '\n                                     'directory in which the trained model is stored')\n\n    # create eval sub-parser\n    eval_parser = subparsers.add_parser('eval', description='Evaluate the given model on the specified data stream.')\n    eval_parser.set_defaults(subcommand='eval')\n    eval_parser.add_argument('stream_name', help='stream name to be evaluated')\n    eval_parser.add_argument('model_path', help='model path to be evaluated')\n    eval_parser.add_argument('--config', '-c', nargs='?', default=None, help='optional config path to be used')\n\n    # create dataset sub-parser\n    dataset_parser = subparsers.add_parser('dataset', description='Invoke arbitrary dataset method.')\n    dataset_parser.set_defaults(subcommand='dataset')\n    dataset_parser.add_argument('method', help='name of the method to be invoked')\n    dataset_parser.add_argument('config_file', help='path to the config file')\n\n    # create grid-search sub-parser\n    gridsearch_parser = subparsers.add_parser('gridsearch', description='Do parameter grid search (experimental).')\n    gridsearch_parser.set_defaults(subcommand='gridsearch')\n    gridsearch_parser.add_argument('script', help='Script to be grid-searched')\n    gridsearch_parser.add_argument('params', nargs='*', help='Params to be tested. Format: name:type=[value1,value2]. '\n                                                             'Type is optional')\n    gridsearch_parser.add_argument('--dry-run', action='store_true', help='Only print command output instead '\n                                                                          'of executing it right away')\n\n    # create ls sub-parser\n    ls_parser = subparsers.add_parser('ls', description='List training log dirs in the given path.')\n    ls_parser.set_defaults(subcommand='ls')\n    ls_parser.add_argument('dir', nargs='?', default=CXF_DEFAULT_LOG_DIR,\n                           help='path to the log directory to be listed')\n    ls_parser.add_argument('-l', '--long', action='store_true', help='use long listing format')\n    ls_parser.add_argument('-a', '--all', action='store_true', help='include trainings with no epochs done')\n    ls_parser.add_argument('-r', '--recursive', action='store_true',\n                           help='list all the dirs recursively, stop at training dirs')\n    ls_parser.add_argument('-v', '--verbose', action='store_true',\n                           help='print more verbose output, applicable only when a single train dir is listed')\n\n    # create prune sub-parser\n    prune_parser = subparsers.add_parser('prune', description='Prune training log dirs in the given path without finished epochs.')\n    prune_parser.set_defaults(subcommand='prune')\n    prune_parser.add_argument('dir', nargs='?', default=CXF_DEFAULT_LOG_DIR,\n                           help='path to the log directory to be pruned')\n    prune_parser.add_argument('-e', '--epochs', default=1, type=int,\n                              help='keep only training log dirs having at least this many completed epochs, default 1')\n    prune_parser.add_argument('-s', '--subdirs', action='store_true',\n                              help='delete all subdirectories in training directories')\n\n    # add common arguments\n    if add_common_arguments:\n        for parser in [main_parser, train_parser, resume_parser, predict_parser, dataset_parser, eval_parser]:\n            parser.add_argument('--output_root', '-o', default='./log', help='output directory')\n            parser.add_argument('--verbose', '-v', action='store_true', help='increase verbosity to level DEBUG')\n\n    return main_parser"}
{"code":"def append(x: T, xs: Iterable[T]) -> Iterator[T]:\n    \"\"\" Append a value to an iterable.\n\n    Parameters\n    ----------\n    x\n        An element of type T.\n    xs\n        An iterable of elements of type T.\n\n    Returns\n    -------\n    Iterator\n        An iterator that yields elements of *xs*, then yields *x*.\n\n\n    Examples\n    --------\n    >>> from delphi.utils.fp import append\n    >>> list(append(1, [2, 3]))\n    [2, 3, 1]\n\n    \"\"\"\n\n    return chain(xs, [x])","return_type":"Iterator[T]","function_name":"append","stripped_code":"def append(x: T, xs: Iterable[T]):\n    \"\"\" Append a value to an iterable.\n\n    Parameters\n    ----------\n    x\n        An element of type T.\n    xs\n        An iterable of elements of type T.\n\n    Returns\n    -------\n    Iterator\n        An iterator that yields elements of *xs*, then yields *x*.\n\n\n    Examples\n    --------\n    >>> from delphi.utils.fp import append\n    >>> list(append(1, [2, 3]))\n    [2, 3, 1]\n\n    \"\"\"\n\n    return chain(xs, [x])"}
{"code":"def py_module_preamble(ctx: GeneratorContext,) -> GeneratedPyAST:\n    \"\"\"Bootstrap a new module with imports and other boilerplate.\"\"\"\n    preamble: List[ast.AST] = []\n    preamble.extend(_module_imports(ctx))\n    preamble.append(_from_module_import())\n    preamble.append(_ns_var())\n    return GeneratedPyAST(node=ast.NameConstant(None), dependencies=preamble)","return_type":"GeneratedPyAST","function_name":"py_module_preamble","stripped_code":"def py_module_preamble(ctx: GeneratorContext,):\n    \"\"\"Bootstrap a new module with imports and other boilerplate.\"\"\"\n    preamble: List[ast.AST] = []\n    preamble.extend(_module_imports(ctx))\n    preamble.append(_from_module_import())\n    preamble.append(_ns_var())\n    return GeneratedPyAST(node=ast.NameConstant(None), dependencies=preamble)"}
{"code":"def _init_base_objects(self, ssl_version: OpenSslVersionEnum, underlying_socket: Optional[socket.socket]) -> None:\n        \"\"\"Setup the socket and SSL_CTX objects.\n        \"\"\"\n        self._is_handshake_completed = False\n        self._ssl_version = ssl_version\n        self._ssl_ctx = self._NASSL_MODULE.SSL_CTX(ssl_version.value)\n\n        # A Python socket handles transmission of the data\n        self._sock = underlying_socket","return_type":"None","function_name":"SslClient._init_base_objects","stripped_code":"def _init_base_objects(self, ssl_version: OpenSslVersionEnum, underlying_socket: Optional[socket.socket]):\n        \"\"\"Setup the socket and SSL_CTX objects.\n        \"\"\"\n        self._is_handshake_completed = False\n        self._ssl_version = ssl_version\n        self._ssl_ctx = self._NASSL_MODULE.SSL_CTX(ssl_version.value)\n\n        # A Python socket handles transmission of the data\n        self._sock = underlying_socket"}
{"code":"def console_vline(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    l: int,\n    flag: int = BKGND_DEFAULT,\n) -> None:\n    \"\"\"Draw a vertical line on the console.\n\n    This always uses the character 179, the vertical line character.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.vline` instead.\n    \"\"\"\n    lib.TCOD_console_vline(_console(con), x, y, l, flag)","return_type":"None","function_name":"console_vline","stripped_code":"def console_vline(\n    con: tcod.console.Console,\n    x: int,\n    y: int,\n    l: int,\n    flag: int = BKGND_DEFAULT,\n):\n    \"\"\"Draw a vertical line on the console.\n\n    This always uses the character 179, the vertical line character.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.vline` instead.\n    \"\"\"\n    lib.TCOD_console_vline(_console(con), x, y, l, flag)"}
{"code":"def _do_resolve_index(\n    python_bin: str,\n    solver: PythonSolver,\n    *,\n    all_solvers: typing.List[PythonSolver],\n    requirements: typing.List[str],\n    exclude_packages: set = None,\n    transitive: bool = True,\n    subgraph_check_api: str = None,\n) -> dict:\n    \"\"\"Perform resolution of requirements against the given solver.\"\"\"\n    index_url = solver.release_fetcher.index_url\n    source = solver.release_fetcher.source\n\n    packages_seen = set()\n    packages = []\n    errors = []\n    unresolved = []\n    unparsed = []\n    exclude_packages = exclude_packages or {}\n    queue = deque()\n\n    for requirement in requirements:\n        _LOGGER.debug(\"Parsing requirement %r\", requirement)\n        try:\n            dependency = PythonDependencyParser.parse_python(requirement)\n        except Exception as exc:\n            unparsed.append({\"requirement\": requirement, \"details\": str(exc)})\n            continue\n\n        if dependency.name in exclude_packages:\n            continue\n\n        version_spec = _get_dependency_specification(dependency.spec)\n        resolved_versions = _resolve_versions(solver, source, dependency.name, version_spec)\n        if not resolved_versions:\n            _LOGGER.warning(\"No versions were resolved for dependency %r in version %r\", dependency.name, version_spec)\n            unresolved.append({\"package_name\": dependency.name, \"version_spec\": version_spec, \"index\": index_url})\n        else:\n            for version in resolved_versions:\n                entry = (dependency.name, version)\n                packages_seen.add(entry)\n                queue.append(entry)\n\n    while queue:\n        package_name, package_version = queue.pop()\n        _LOGGER.info(\"Using index %r to discover package %r in version %r\", index_url, package_name, package_version)\n        try:\n            with _install_requirement(python_bin, package_name, package_version, index_url):\n                package_info = _pipdeptree(python_bin, package_name, warn=True)\n        except CommandError as exc:\n            _LOGGER.debug(\n                \"There was an error during package %r in version %r discovery from %r: %s\",\n                package_name,\n                package_version,\n                index_url,\n                exc,\n            )\n            errors.append(\n                {\n                    \"package_name\": package_name,\n                    \"index\": index_url,\n                    \"version\": package_version,\n                    \"type\": \"command_error\",\n                    \"details\": exc.to_dict(),\n                }\n            )\n            continue\n\n        if package_info is None:\n            errors.append(\n                {\n                    \"package_name\": package_name,\n                    \"index\": index_url,\n                    \"version\": package_version,\n                    \"type\": \"not_site_package\",\n                    \"details\": {\n                        \"message\": \"Failed to get information about installed package, probably not site package\"\n                    },\n                }\n            )\n            continue\n\n        if package_info[\"package\"][\"installed_version\"] != package_version:\n            _LOGGER.warning(\n                \"Requested to install version %r of package %r, but installed version is %r, error is not fatal\",\n                package_version,\n                package_name,\n                package_info[\"package\"][\"installed_version\"],\n            )\n\n        if package_info[\"package\"][\"package_name\"] != package_name:\n            _LOGGER.warning(\n                \"Requested to install package %r, but installed package name is %r, error is not fatal\",\n                package_name,\n                package_info[\"package\"][\"package_name\"],\n            )\n\n        entry = _create_entry(package_info, source)\n        packages.append(entry)\n\n        for dependency in entry[\"dependencies\"]:\n            dependency_name, dependency_range = dependency[\"package_name\"], dependency[\"required_version\"]\n            dependency[\"resolved_versions\"] = []\n\n            for dep_solver in all_solvers:\n                _LOGGER.info(\n                    \"Resolving dependency versions for %r with range %r from %r\",\n                    dependency_name,\n                    dependency_range,\n                    dep_solver.release_fetcher.index_url,\n                )\n                resolved_versions = _resolve_versions(\n                    dep_solver, dep_solver.release_fetcher.source, dependency_name, dependency_range\n                )\n                _LOGGER.debug(\n                    \"Resolved versions for package %r with range specifier %r: %s\",\n                    dependency_name,\n                    dependency_range,\n                    resolved_versions,\n                )\n                dependency[\"resolved_versions\"].append(\n                    {\"versions\": resolved_versions, \"index\": dep_solver.release_fetcher.index_url}\n                )\n\n                if not transitive:\n                    continue\n\n                for version in resolved_versions:\n                    # Did we check this package already - do not check indexes, we manually insert them.\n                    seen_entry = (dependency_name, version)\n                    if seen_entry not in packages_seen and (\n                        not subgraph_check_api\n                        or (\n                            subgraph_check_api\n                            and _should_resolve_subgraph(subgraph_check_api, dependency_name, version, index_url)\n                        )\n                    ):\n                        _LOGGER.debug(\n                            \"Adding package %r in version %r for next resolution round\", dependency_name, version\n                        )\n                        packages_seen.add(seen_entry)\n                        queue.append((dependency_name, version))\n\n    return {\"tree\": packages, \"errors\": errors, \"unparsed\": unparsed, \"unresolved\": unresolved}","return_type":"dict","function_name":"_do_resolve_index","stripped_code":"def _do_resolve_index(\n    python_bin: str,\n    solver: PythonSolver,\n    *,\n    all_solvers: typing.List[PythonSolver],\n    requirements: typing.List[str],\n    exclude_packages: set = None,\n    transitive: bool = True,\n    subgraph_check_api: str = None,\n):\n    \"\"\"Perform resolution of requirements against the given solver.\"\"\"\n    index_url = solver.release_fetcher.index_url\n    source = solver.release_fetcher.source\n\n    packages_seen = set()\n    packages = []\n    errors = []\n    unresolved = []\n    unparsed = []\n    exclude_packages = exclude_packages or {}\n    queue = deque()\n\n    for requirement in requirements:\n        _LOGGER.debug(\"Parsing requirement %r\", requirement)\n        try:\n            dependency = PythonDependencyParser.parse_python(requirement)\n        except Exception as exc:\n            unparsed.append({\"requirement\": requirement, \"details\": str(exc)})\n            continue\n\n        if dependency.name in exclude_packages:\n            continue\n\n        version_spec = _get_dependency_specification(dependency.spec)\n        resolved_versions = _resolve_versions(solver, source, dependency.name, version_spec)\n        if not resolved_versions:\n            _LOGGER.warning(\"No versions were resolved for dependency %r in version %r\", dependency.name, version_spec)\n            unresolved.append({\"package_name\": dependency.name, \"version_spec\": version_spec, \"index\": index_url})\n        else:\n            for version in resolved_versions:\n                entry = (dependency.name, version)\n                packages_seen.add(entry)\n                queue.append(entry)\n\n    while queue:\n        package_name, package_version = queue.pop()\n        _LOGGER.info(\"Using index %r to discover package %r in version %r\", index_url, package_name, package_version)\n        try:\n            with _install_requirement(python_bin, package_name, package_version, index_url):\n                package_info = _pipdeptree(python_bin, package_name, warn=True)\n        except CommandError as exc:\n            _LOGGER.debug(\n                \"There was an error during package %r in version %r discovery from %r: %s\",\n                package_name,\n                package_version,\n                index_url,\n                exc,\n            )\n            errors.append(\n                {\n                    \"package_name\": package_name,\n                    \"index\": index_url,\n                    \"version\": package_version,\n                    \"type\": \"command_error\",\n                    \"details\": exc.to_dict(),\n                }\n            )\n            continue\n\n        if package_info is None:\n            errors.append(\n                {\n                    \"package_name\": package_name,\n                    \"index\": index_url,\n                    \"version\": package_version,\n                    \"type\": \"not_site_package\",\n                    \"details\": {\n                        \"message\": \"Failed to get information about installed package, probably not site package\"\n                    },\n                }\n            )\n            continue\n\n        if package_info[\"package\"][\"installed_version\"] != package_version:\n            _LOGGER.warning(\n                \"Requested to install version %r of package %r, but installed version is %r, error is not fatal\",\n                package_version,\n                package_name,\n                package_info[\"package\"][\"installed_version\"],\n            )\n\n        if package_info[\"package\"][\"package_name\"] != package_name:\n            _LOGGER.warning(\n                \"Requested to install package %r, but installed package name is %r, error is not fatal\",\n                package_name,\n                package_info[\"package\"][\"package_name\"],\n            )\n\n        entry = _create_entry(package_info, source)\n        packages.append(entry)\n\n        for dependency in entry[\"dependencies\"]:\n            dependency_name, dependency_range = dependency[\"package_name\"], dependency[\"required_version\"]\n            dependency[\"resolved_versions\"] = []\n\n            for dep_solver in all_solvers:\n                _LOGGER.info(\n                    \"Resolving dependency versions for %r with range %r from %r\",\n                    dependency_name,\n                    dependency_range,\n                    dep_solver.release_fetcher.index_url,\n                )\n                resolved_versions = _resolve_versions(\n                    dep_solver, dep_solver.release_fetcher.source, dependency_name, dependency_range\n                )\n                _LOGGER.debug(\n                    \"Resolved versions for package %r with range specifier %r: %s\",\n                    dependency_name,\n                    dependency_range,\n                    resolved_versions,\n                )\n                dependency[\"resolved_versions\"].append(\n                    {\"versions\": resolved_versions, \"index\": dep_solver.release_fetcher.index_url}\n                )\n\n                if not transitive:\n                    continue\n\n                for version in resolved_versions:\n                    # Did we check this package already - do not check indexes, we manually insert them.\n                    seen_entry = (dependency_name, version)\n                    if seen_entry not in packages_seen and (\n                        not subgraph_check_api\n                        or (\n                            subgraph_check_api\n                            and _should_resolve_subgraph(subgraph_check_api, dependency_name, version, index_url)\n                        )\n                    ):\n                        _LOGGER.debug(\n                            \"Adding package %r in version %r for next resolution round\", dependency_name, version\n                        )\n                        packages_seen.add(seen_entry)\n                        queue.append((dependency_name, version))\n\n    return {\"tree\": packages, \"errors\": errors, \"unparsed\": unparsed, \"unresolved\": unresolved}"}
{"code":"def do_exit(self, arg_list: List[str]) -> bool:\n        \"\"\"Exit the application with an optional exit code.\n\nUsage:  exit [exit_code]\n    Where:\n        * exit_code - integer exit code to return to the shell\n\"\"\"\n        # If an argument was provided\n        if arg_list:\n            try:\n                self.exit_code = int(arg_list[0])\n            except ValueError:\n                self.perror(\"{} isn't a valid integer exit code\".format(arg_list[0]))\n                self.exit_code = -1\n\n        self._should_quit = True\n        return self._STOP_AND_EXIT","return_type":"bool","function_name":"ReplWithExitCode.do_exit","stripped_code":"def do_exit(self, arg_list: List[str]):\n        \"\"\"Exit the application with an optional exit code.\n\nUsage:  exit [exit_code]\n    Where:\n        * exit_code - integer exit code to return to the shell\n\"\"\"\n        # If an argument was provided\n        if arg_list:\n            try:\n                self.exit_code = int(arg_list[0])\n            except ValueError:\n                self.perror(\"{} isn't a valid integer exit code\".format(arg_list[0]))\n                self.exit_code = -1\n\n        self._should_quit = True\n        return self._STOP_AND_EXIT"}
{"code":"def from_ewif_hex(cls: Type[SigningKeyType], ewif_hex: str, password: str) -> SigningKeyType:\n        \"\"\"\n        Return SigningKey instance from Duniter EWIF in hexadecimal format\n\n        :param ewif_hex: EWIF string in hexadecimal format\n        :param password: Password of the encrypted seed\n        \"\"\"\n        ewif_bytes = Base58Encoder.decode(ewif_hex)\n        if len(ewif_bytes) != 39:\n            raise Exception(\"Error: the size of EWIF is invalid\")\n\n        # extract data\n        fi = ewif_bytes[0:1]\n        checksum_from_ewif = ewif_bytes[-2:]\n        ewif_no_checksum = ewif_bytes[0:-2]\n        salt = ewif_bytes[1:5]\n        encryptedhalf1 = ewif_bytes[5:21]\n        encryptedhalf2 = ewif_bytes[21:37]\n\n        # check format flag\n        if fi != b\"\\x02\":\n            raise Exception(\"Error: bad format version, not EWIF\")\n\n        # checksum control\n        checksum = libnacl.crypto_hash_sha256(libnacl.crypto_hash_sha256(ewif_no_checksum))[0:2]\n        if checksum_from_ewif != checksum:\n            raise Exception(\"Error: bad checksum of the EWIF\")\n\n        # SCRYPT\n        password_bytes = password.encode(\"utf-8\")\n        scrypt_seed = scrypt(password_bytes, salt, 16384, 8, 8, 64)\n        derivedhalf1 = scrypt_seed[0:32]\n        derivedhalf2 = scrypt_seed[32:64]\n\n        # AES\n        aes = pyaes.AESModeOfOperationECB(derivedhalf2)\n        decryptedhalf1 = aes.decrypt(encryptedhalf1)\n        decryptedhalf2 = aes.decrypt(encryptedhalf2)\n\n        # XOR\n        seed1 = xor_bytes(decryptedhalf1, derivedhalf1[0:16])\n        seed2 = xor_bytes(decryptedhalf2, derivedhalf1[16:32])\n        seed = bytes(seed1 + seed2)\n\n        # Password Control\n        signer = SigningKey(seed)\n        salt_from_seed = libnacl.crypto_hash_sha256(\n            libnacl.crypto_hash_sha256(\n                Base58Encoder.decode(signer.pubkey)))[0:4]\n        if salt_from_seed != salt:\n            raise Exception(\"Error: bad Password of EWIF address\")\n\n        return cls(seed)","return_type":"SigningKeyType","function_name":"SigningKey.from_ewif_hex","stripped_code":"def from_ewif_hex(cls: Type[SigningKeyType], ewif_hex: str, password: str):\n        \"\"\"\n        Return SigningKey instance from Duniter EWIF in hexadecimal format\n\n        :param ewif_hex: EWIF string in hexadecimal format\n        :param password: Password of the encrypted seed\n        \"\"\"\n        ewif_bytes = Base58Encoder.decode(ewif_hex)\n        if len(ewif_bytes) != 39:\n            raise Exception(\"Error: the size of EWIF is invalid\")\n\n        # extract data\n        fi = ewif_bytes[0:1]\n        checksum_from_ewif = ewif_bytes[-2:]\n        ewif_no_checksum = ewif_bytes[0:-2]\n        salt = ewif_bytes[1:5]\n        encryptedhalf1 = ewif_bytes[5:21]\n        encryptedhalf2 = ewif_bytes[21:37]\n\n        # check format flag\n        if fi != b\"\\x02\":\n            raise Exception(\"Error: bad format version, not EWIF\")\n\n        # checksum control\n        checksum = libnacl.crypto_hash_sha256(libnacl.crypto_hash_sha256(ewif_no_checksum))[0:2]\n        if checksum_from_ewif != checksum:\n            raise Exception(\"Error: bad checksum of the EWIF\")\n\n        # SCRYPT\n        password_bytes = password.encode(\"utf-8\")\n        scrypt_seed = scrypt(password_bytes, salt, 16384, 8, 8, 64)\n        derivedhalf1 = scrypt_seed[0:32]\n        derivedhalf2 = scrypt_seed[32:64]\n\n        # AES\n        aes = pyaes.AESModeOfOperationECB(derivedhalf2)\n        decryptedhalf1 = aes.decrypt(encryptedhalf1)\n        decryptedhalf2 = aes.decrypt(encryptedhalf2)\n\n        # XOR\n        seed1 = xor_bytes(decryptedhalf1, derivedhalf1[0:16])\n        seed2 = xor_bytes(decryptedhalf2, derivedhalf1[16:32])\n        seed = bytes(seed1 + seed2)\n\n        # Password Control\n        signer = SigningKey(seed)\n        salt_from_seed = libnacl.crypto_hash_sha256(\n            libnacl.crypto_hash_sha256(\n                Base58Encoder.decode(signer.pubkey)))[0:4]\n        if salt_from_seed != salt:\n            raise Exception(\"Error: bad Password of EWIF address\")\n\n        return cls(seed)"}
{"code":"def _update_batches_if_needed(self)->None:\n        \"one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.\"\n        if self.learn.data.valid_dl is None: return # Running learning rate finder, so return\n        update_batches = self.data is not self.learn.data\n        if not update_batches: return\n        self.data = self.learn.data\n        self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)\n        self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)","return_type":"None","function_name":"LearnerTensorboardWriter._update_batches_if_needed","stripped_code":"def _update_batches_if_needed(self):\n        \"one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.\"\n        if self.learn.data.valid_dl is None: return # Running learning rate finder, so return\n        update_batches = self.data is not self.learn.data\n        if not update_batches: return\n        self.data = self.learn.data\n        self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)\n        self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)"}
{"code":"def close_poll(\n        self,\n        chat_id: Union[int, str],\n        message_id: id\n    ) -> bool:\n        \"\"\"Use this method to close (stop) a poll.\n\n        Closed polls can't be reopened and nobody will be able to vote in it anymore.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n                For your personal cloud (Saved Messages) you can simply use \"me\" or \"self\".\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n            message_id (``int``):\n                Unique poll message identifier inside this chat.\n\n        Returns:\n            On success, True is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        poll = self.get_messages(chat_id, message_id).poll\n\n        self.send(\n            functions.messages.EditMessage(\n                peer=self.resolve_peer(chat_id),\n                id=message_id,\n                media=types.InputMediaPoll(\n                    poll=types.Poll(\n                        id=poll.id,\n                        closed=True,\n                        question=\"\",\n                        answers=[]\n                    )\n                )\n            )\n        )\n\n        return True","return_type":"bool","function_name":"ClosePoll.close_poll","stripped_code":"def close_poll(\n        self,\n        chat_id: Union[int, str],\n        message_id: id\n    ):\n        \"\"\"Use this method to close (stop) a poll.\n\n        Closed polls can't be reopened and nobody will be able to vote in it anymore.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n                For your personal cloud (Saved Messages) you can simply use \"me\" or \"self\".\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n            message_id (``int``):\n                Unique poll message identifier inside this chat.\n\n        Returns:\n            On success, True is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        poll = self.get_messages(chat_id, message_id).poll\n\n        self.send(\n            functions.messages.EditMessage(\n                peer=self.resolve_peer(chat_id),\n                id=message_id,\n                media=types.InputMediaPoll(\n                    poll=types.Poll(\n                        id=poll.id,\n                        closed=True,\n                        question=\"\",\n                        answers=[]\n                    )\n                )\n            )\n        )\n\n        return True"}
{"code":"def cols_(self) -> pd.DataFrame:\n        \"\"\"\n        Returns a dataframe with columns info\n\n        :return: a pandas dataframe\n        :rtype: pd.DataFrame\n\n        :example: ``ds.cols_()``\n        \"\"\"\n        try:\n            s = self.df.iloc[0]\n            df = pd.DataFrame(s)\n            df = df.rename(columns={0: \"value\"})\n\n            def run(row):\n                t = row[0]\n                return type(t).__name__\n\n            s = df.apply(run, axis=1)\n            df = df.rename(columns={0: \"value\"})\n            df[\"types\"] = s\n            return df\n        except Exception as e:\n            self.err(e)","return_type":"pd.DataFrame","function_name":"View.cols_","stripped_code":"def cols_(self):\n        \"\"\"\n        Returns a dataframe with columns info\n\n        :return: a pandas dataframe\n        :rtype: pd.DataFrame\n\n        :example: ``ds.cols_()``\n        \"\"\"\n        try:\n            s = self.df.iloc[0]\n            df = pd.DataFrame(s)\n            df = df.rename(columns={0: \"value\"})\n\n            def run(row):\n                t = row[0]\n                return type(t).__name__\n\n            s = df.apply(run, axis=1)\n            df = df.rename(columns={0: \"value\"})\n            df[\"types\"] = s\n            return df\n        except Exception as e:\n            self.err(e)"}
{"code":"def reqMktData(\n            self, contract: Contract, genericTickList: str = '',\n            snapshot: bool = False, regulatorySnapshot: bool = False,\n            mktDataOptions: List[TagValue] = None) -> Ticker:\n        \"\"\"\n        Subscribe to tick data or request a snapshot.\n        Returns the Ticker that holds the market data. The ticker will\n        initially be empty and gradually (after a couple of seconds)\n        be filled.\n\n        https://interactivebrokers.github.io/tws-api/md_request.html\n\n        Args:\n            contract: Contract of interest.\n            genericTickList: Comma separated IDs of desired\n                generic ticks that will cause corresponding Ticker fields\n                to be filled:\n\n                =====  ================================================\n                ID     Ticker fields\n                =====  ================================================\n                100    ``putVolume``, ``callVolume`` (for options)\n                101    ``putOpenInterest``, ``callOpenInterest`` (for options)\n                104    ``histVolatility`` (for options)\n                105    ``avOptionVolume`` (for options)\n                106    ``impliedVolatility`` (for options)\n                162    ``indexFuturePremium``\n                165    ``low13week``, ``high13week``, ``low26week``,\n                       ``high26week``, ``low52week``, ``high52week``,\n                       ``avVolume``\n                221    ``markPrice``\n                233    ``last``, ``lastSize``, ``rtVolume``, ``vwap``\n                       (Time & Sales)\n                236    ``shortableShares``\n                258    ``fundamentalRatios`` (of type\n                       :class:`ib_insync.objects.FundamentalRatios`)\n                293    ``tradeCount``\n                294    ``tradeRate``\n                295    ``volumeRate``\n                411    ``rtHistVolatility``\n                456    ``dividends`` (of type\n                       :class:`ib_insync.objects.Dividends`)\n                588    ``futuresOpenInterest``\n                =====  ================================================\n\n            snapshot: If True then request a one-time snapshot, otherwise\n                subscribe to a stream of realtime tick data.\n            regulatorySnapshot: Request NBBO snapshot (may incur a fee).\n            mktDataOptions: Unknown\n        \"\"\"\n        reqId = self.client.getReqId()\n        ticker = self.wrapper.startTicker(reqId, contract, 'mktData')\n        self.client.reqMktData(\n            reqId, contract, genericTickList, snapshot,\n            regulatorySnapshot, mktDataOptions)\n        return ticker","return_type":"Ticker","function_name":"IB.reqMktData","stripped_code":"def reqMktData(\n            self, contract: Contract, genericTickList: str = '',\n            snapshot: bool = False, regulatorySnapshot: bool = False,\n            mktDataOptions: List[TagValue] = None):\n        \"\"\"\n        Subscribe to tick data or request a snapshot.\n        Returns the Ticker that holds the market data. The ticker will\n        initially be empty and gradually (after a couple of seconds)\n        be filled.\n\n        https://interactivebrokers.github.io/tws-api/md_request.html\n\n        Args:\n            contract: Contract of interest.\n            genericTickList: Comma separated IDs of desired\n                generic ticks that will cause corresponding Ticker fields\n                to be filled:\n\n                =====  ================================================\n                ID     Ticker fields\n                =====  ================================================\n                100    ``putVolume``, ``callVolume`` (for options)\n                101    ``putOpenInterest``, ``callOpenInterest`` (for options)\n                104    ``histVolatility`` (for options)\n                105    ``avOptionVolume`` (for options)\n                106    ``impliedVolatility`` (for options)\n                162    ``indexFuturePremium``\n                165    ``low13week``, ``high13week``, ``low26week``,\n                       ``high26week``, ``low52week``, ``high52week``,\n                       ``avVolume``\n                221    ``markPrice``\n                233    ``last``, ``lastSize``, ``rtVolume``, ``vwap``\n                       (Time & Sales)\n                236    ``shortableShares``\n                258    ``fundamentalRatios`` (of type\n                       :class:`ib_insync.objects.FundamentalRatios`)\n                293    ``tradeCount``\n                294    ``tradeRate``\n                295    ``volumeRate``\n                411    ``rtHistVolatility``\n                456    ``dividends`` (of type\n                       :class:`ib_insync.objects.Dividends`)\n                588    ``futuresOpenInterest``\n                =====  ================================================\n\n            snapshot: If True then request a one-time snapshot, otherwise\n                subscribe to a stream of realtime tick data.\n            regulatorySnapshot: Request NBBO snapshot (may incur a fee).\n            mktDataOptions: Unknown\n        \"\"\"\n        reqId = self.client.getReqId()\n        ticker = self.wrapper.startTicker(reqId, contract, 'mktData')\n        self.client.reqMktData(\n            reqId, contract, genericTickList, snapshot,\n            regulatorySnapshot, mktDataOptions)\n        return ticker"}
{"code":"def statics(self) -> typing.Iterator['Static']:\n        \"\"\"\n        Returns: generator over all statics in this coalition\n        \"\"\"\n        for country in self.countries:\n\n            for static in country.statics:\n\n                yield static","return_type":"typing.Iterator['Static']","function_name":"Coalition.statics","stripped_code":"def statics(self):\n        \"\"\"\n        Returns: generator over all statics in this coalition\n        \"\"\"\n        for country in self.countries:\n\n            for static in country.statics:\n\n                yield static"}
{"code":"def complete(\n    text: str, kw_cache: atom.Atom[\"PMap[int, Keyword]\"] = __INTERN\n) -> Iterable[str]:\n    \"\"\"Return an iterable of possible completions for the given text.\"\"\"\n    assert text.startswith(\":\")\n    interns = kw_cache.deref()\n    text = text[1:]\n\n    if \"/\" in text:\n        prefix, suffix = text.split(\"/\", maxsplit=1)\n        results = filter(\n            lambda kw: (kw.ns is not None and kw.ns == prefix)\n            and kw.name.startswith(suffix),\n            interns.itervalues(),\n        )\n    else:\n        results = filter(\n            lambda kw: kw.name.startswith(text)\n            or (kw.ns is not None and kw.ns.startswith(text)),\n            interns.itervalues(),\n        )\n\n    return map(str, results)","return_type":"Iterable[str]","function_name":"complete","stripped_code":"def complete(\n    text: str, kw_cache: atom.Atom[\"PMap[int, Keyword]\"] = __INTERN\n):\n    \"\"\"Return an iterable of possible completions for the given text.\"\"\"\n    assert text.startswith(\":\")\n    interns = kw_cache.deref()\n    text = text[1:]\n\n    if \"/\" in text:\n        prefix, suffix = text.split(\"/\", maxsplit=1)\n        results = filter(\n            lambda kw: (kw.ns is not None and kw.ns == prefix)\n            and kw.name.startswith(suffix),\n            interns.itervalues(),\n        )\n    else:\n        results = filter(\n            lambda kw: kw.name.startswith(text)\n            or (kw.ns is not None and kw.ns.startswith(text)),\n            interns.itervalues(),\n        )\n\n    return map(str, results)"}
{"code":"def first(self, rows: List[Row]) -> List[Row]:\n        \"\"\"\n        Takes an expression that evaluates to a list of rows, and returns the first one in that\n        list.\n        \"\"\"\n        if not rows:\n            logger.warning(\"Trying to get first row from an empty list\")\n            return []\n        return [rows[0]]","return_type":"List[Row]","function_name":"WikiTablesLanguage.first","stripped_code":"def first(self, rows: List[Row]):\n        \"\"\"\n        Takes an expression that evaluates to a list of rows, and returns the first one in that\n        list.\n        \"\"\"\n        if not rows:\n            logger.warning(\"Trying to get first row from an empty list\")\n            return []\n        return [rows[0]]"}
{"code":"def get_logical_form(self,\n                         action_sequence: List[str],\n                         add_var_function: bool = True) -> str:\n        \"\"\"\n        Takes an action sequence and constructs a logical form from it. This is useful if you want\n        to get a logical form from a decoded sequence of actions generated by a transition based\n        semantic parser.\n\n        Parameters\n        ----------\n        action_sequence : ``List[str]``\n            The sequence of actions as strings (eg.: ``['{START_SYMBOL} -> t', 't -> <e,t>', ...]``).\n        add_var_function : ``bool`` (optional)\n             ``var`` is a special function that some languages use within lambda functions to\n             indicate the use of a variable (eg.: ``(lambda x (fb:row.row.year (var x)))``). Due to\n             the way constrained decoding is currently implemented, it is easier for the decoder to\n             not produce these functions. In that case, setting this flag adds the function in the\n             logical form even though it is not present in the action sequence.\n        \"\"\"\n        # Basic outline: we assume that the bracketing that we get in the RHS of each action is the\n        # correct bracketing for reconstructing the logical form.  This is true when there is no\n        # currying in the action sequence.  Given this assumption, we just need to construct a tree\n        # from the action sequence, then output all of the leaves in the tree, with brackets around\n        # the children of all non-terminal nodes.\n\n        remaining_actions = [action.split(\" -> \") for action in action_sequence]\n        tree = Tree(remaining_actions[0][1], [])\n\n        try:\n            remaining_actions = self._construct_node_from_actions(tree,\n                                                                  remaining_actions[1:],\n                                                                  add_var_function)\n        except ParsingError:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            raise\n\n        if remaining_actions:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            logger.error(\"Remaining actions were: %s\", remaining_actions)\n            raise ParsingError(\"Extra actions in action sequence\")\n        return nltk_tree_to_logical_form(tree)","return_type":"str","function_name":"World.get_logical_form","stripped_code":"def get_logical_form(self,\n                         action_sequence: List[str],\n                         add_var_function: bool = True):\n        \"\"\"\n        Takes an action sequence and constructs a logical form from it. This is useful if you want\n        to get a logical form from a decoded sequence of actions generated by a transition based\n        semantic parser.\n\n        Parameters\n        ----------\n        action_sequence : ``List[str]``\n            The sequence of actions as strings (eg.: ``['{START_SYMBOL} -> t', 't -> <e,t>', ...]``).\n        add_var_function : ``bool`` (optional)\n             ``var`` is a special function that some languages use within lambda functions to\n             indicate the use of a variable (eg.: ``(lambda x (fb:row.row.year (var x)))``). Due to\n             the way constrained decoding is currently implemented, it is easier for the decoder to\n             not produce these functions. In that case, setting this flag adds the function in the\n             logical form even though it is not present in the action sequence.\n        \"\"\"\n        # Basic outline: we assume that the bracketing that we get in the RHS of each action is the\n        # correct bracketing for reconstructing the logical form.  This is true when there is no\n        # currying in the action sequence.  Given this assumption, we just need to construct a tree\n        # from the action sequence, then output all of the leaves in the tree, with brackets around\n        # the children of all non-terminal nodes.\n\n        remaining_actions = [action.split(\" -> \") for action in action_sequence]\n        tree = Tree(remaining_actions[0][1], [])\n\n        try:\n            remaining_actions = self._construct_node_from_actions(tree,\n                                                                  remaining_actions[1:],\n                                                                  add_var_function)\n        except ParsingError:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            raise\n\n        if remaining_actions:\n            logger.error(\"Error parsing action sequence: %s\", action_sequence)\n            logger.error(\"Remaining actions were: %s\", remaining_actions)\n            raise ParsingError(\"Extra actions in action sequence\")\n        return nltk_tree_to_logical_form(tree)"}
{"code":"def write_error(self, status_code: int, **kwargs: Any) -> None:\n        \"\"\"Override to implement custom error pages.\n\n        ``write_error`` may call `write`, `render`, `set_header`, etc\n        to produce output as usual.\n\n        If this error was caused by an uncaught exception (including\n        HTTPError), an ``exc_info`` triple will be available as\n        ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n        the \"current\" exception for purposes of methods like\n        ``sys.exc_info()`` or ``traceback.format_exc``.\n        \"\"\"\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            # in debug mode, try to send a traceback\n            self.set_header(\"Content-Type\", \"text/plain\")\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                self.write(line)\n            self.finish()\n        else:\n            self.finish(\n                \"<html><title>%(code)d: %(message)s</title>\"\n                \"<body>%(code)d: %(message)s</body></html>\"\n                % {\"code\": status_code, \"message\": self._reason}\n            )","return_type":"None","function_name":"RequestHandler.write_error","stripped_code":"def write_error(self, status_code: int, **kwargs: Any):\n        \"\"\"Override to implement custom error pages.\n\n        ``write_error`` may call `write`, `render`, `set_header`, etc\n        to produce output as usual.\n\n        If this error was caused by an uncaught exception (including\n        HTTPError), an ``exc_info`` triple will be available as\n        ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n        the \"current\" exception for purposes of methods like\n        ``sys.exc_info()`` or ``traceback.format_exc``.\n        \"\"\"\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            # in debug mode, try to send a traceback\n            self.set_header(\"Content-Type\", \"text/plain\")\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                self.write(line)\n            self.finish()\n        else:\n            self.finish(\n                \"<html><title>%(code)d: %(message)s</title>\"\n                \"<body>%(code)d: %(message)s</body></html>\"\n                % {\"code\": status_code, \"message\": self._reason}\n            )"}
{"code":"def consult_robots_txt(self, request: HTTPRequest) -> bool:\n        '''Consult by fetching robots.txt as needed.\n\n        Args:\n            request: The request to be made\n                to get the file.\n\n        Returns:\n            True if can fetch\n\n        Coroutine\n        '''\n        if not self._robots_txt_checker:\n            return True\n\n        result = yield from self._robots_txt_checker.can_fetch(request)\n        return result","return_type":"bool","function_name":"FetchRule.consult_robots_txt","stripped_code":"def consult_robots_txt(self, request: HTTPRequest):\n        '''Consult by fetching robots.txt as needed.\n\n        Args:\n            request: The request to be made\n                to get the file.\n\n        Returns:\n            True if can fetch\n\n        Coroutine\n        '''\n        if not self._robots_txt_checker:\n            return True\n\n        result = yield from self._robots_txt_checker.can_fetch(request)\n        return result"}
{"code":"def to_html_(self) -> str:\n        \"\"\"Convert the main dataframe to html\n\n        :return: html data\n        :rtype: str\n\n        :example: ``ds.to_html_()``\n        \"\"\"\n        try:\n            renderer = pytablewriter.HtmlTableWriter\n            data = self._build_export(renderer)\n            return data\n        except Exception as e:\n            self.err(e, \"Can not convert data to html\")","return_type":"str","function_name":"Export.to_html_","stripped_code":"def to_html_(self):\n        \"\"\"Convert the main dataframe to html\n\n        :return: html data\n        :rtype: str\n\n        :example: ``ds.to_html_()``\n        \"\"\"\n        try:\n            renderer = pytablewriter.HtmlTableWriter\n            data = self._build_export(renderer)\n            return data\n        except Exception as e:\n            self.err(e, \"Can not convert data to html\")"}
{"code":"def sql_dequote_string(s: str) -> str:\n    \"\"\"Reverses sql_quote_string.\"\"\"\n    if len(s) < 2:\n        # Something wrong.\n        return s\n    s = s[1:-1]  # strip off the surrounding quotes\n    return s.replace(\"''\", \"'\")","return_type":"str","function_name":"sql_dequote_string","stripped_code":"def sql_dequote_string(s: str):\n    \"\"\"Reverses sql_quote_string.\"\"\"\n    if len(s) < 2:\n        # Something wrong.\n        return s\n    s = s[1:-1]  # strip off the surrounding quotes\n    return s.replace(\"''\", \"'\")"}
{"code":"def _isNewTxn(self, identifier, reply, txnId) -> bool:\n        \"\"\"\n        If client is not in `processedRequests` or requestId is not there in\n        processed requests and txnId is present then its a new reply\n        \"\"\"\n        return (identifier not in self.processedRequests or\n                reply.reqId not in self.processedRequests[identifier]) and \\\n            txnId is not None","return_type":"bool","function_name":"TransactionStore._isNewTxn","stripped_code":"def _isNewTxn(self, identifier, reply, txnId):\n        \"\"\"\n        If client is not in `processedRequests` or requestId is not there in\n        processed requests and txnId is present then its a new reply\n        \"\"\"\n        return (identifier not in self.processedRequests or\n                reply.reqId not in self.processedRequests[identifier]) and \\\n            txnId is not None"}
{"code":"def bsp_resize(node: tcod.bsp.BSP, x: int, y: int, w: int, h: int) -> None:\n    \"\"\"\n    .. deprecated:: 2.0\n        Assign directly to :any:`BSP` attributes instead.\n    \"\"\"\n    node.x = x\n    node.y = y\n    node.width = w\n    node.height = h","return_type":"None","function_name":"bsp_resize","stripped_code":"def bsp_resize(node: tcod.bsp.BSP, x: int, y: int, w: int, h: int):\n    \"\"\"\n    .. deprecated:: 2.0\n        Assign directly to :any:`BSP` attributes instead.\n    \"\"\"\n    node.x = x\n    node.y = y\n    node.width = w\n    node.height = h"}
{"code":"def close_allowed(self) -> bool:\n        \"\"\"Door can be closed unattended.\"\"\"\n        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'isunattendedcloseallowed')\\\n            == \"1\"","return_type":"bool","function_name":"MyQDevice.close_allowed","stripped_code":"def close_allowed(self):\n        \"\"\"Door can be closed unattended.\"\"\"\n        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'isunattendedcloseallowed')\\\n            == \"1\""}
{"code":"def build_route_timetable(\n    feed: \"Feed\", route_id: str, dates: List[str]\n) -> DataFrame:\n    \"\"\"\n    Return a timetable for the given route and dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    route_id : string\n        ID of a route in ``feed.routes``\n    dates : string or list\n        A YYYYMMDD date string or list thereof\n\n    Returns\n    -------\n    DataFrame\n        The columns are all those in ``feed.trips`` plus those in\n        ``feed.stop_times`` plus ``'date'``, and the trip IDs\n        are restricted to the given route ID.\n        The result is sorted first by date and then by grouping by\n        trip ID and sorting the groups by their first departure time.\n\n        Skip dates outside of the Feed's dates.\n\n        If there is no route activity on the given dates, then return\n        an empty DataFrame.\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stop_times``\n    - Those used in :func:`.trips.get_trips`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    t = pd.merge(feed.trips, feed.stop_times)\n    t = t[t[\"route_id\"] == route_id].copy()\n    a = feed.compute_trip_activity(dates)\n\n    frames = []\n    for date in dates:\n        # Slice to trips active on date\n        ids = a.loc[a[date] == 1, \"trip_id\"]\n        f = t[t[\"trip_id\"].isin(ids)].copy()\n        f[\"date\"] = date\n        # Groupby trip ID and sort groups by their minimum departure time.\n        # For some reason NaN departure times mess up the transform below.\n        # So temporarily fill NaN departure times as a workaround.\n        f[\"dt\"] = f[\"departure_time\"].fillna(method=\"ffill\")\n        f[\"min_dt\"] = f.groupby(\"trip_id\")[\"dt\"].transform(min)\n        frames.append(f)\n\n    f = pd.concat(frames)\n    return f.sort_values([\"date\", \"min_dt\", \"stop_sequence\"]).drop(\n        [\"min_dt\", \"dt\"], axis=1\n    )","return_type":"DataFrame","function_name":"build_route_timetable","stripped_code":"def build_route_timetable(\n    feed: \"Feed\", route_id: str, dates: List[str]\n):\n    \"\"\"\n    Return a timetable for the given route and dates.\n\n    Parameters\n    ----------\n    feed : Feed\n    route_id : string\n        ID of a route in ``feed.routes``\n    dates : string or list\n        A YYYYMMDD date string or list thereof\n\n    Returns\n    -------\n    DataFrame\n        The columns are all those in ``feed.trips`` plus those in\n        ``feed.stop_times`` plus ``'date'``, and the trip IDs\n        are restricted to the given route ID.\n        The result is sorted first by date and then by grouping by\n        trip ID and sorting the groups by their first departure time.\n\n        Skip dates outside of the Feed's dates.\n\n        If there is no route activity on the given dates, then return\n        an empty DataFrame.\n\n    Notes\n    -----\n    Assume the following feed attributes are not ``None``:\n\n    - ``feed.stop_times``\n    - Those used in :func:`.trips.get_trips`\n\n    \"\"\"\n    dates = feed.restrict_dates(dates)\n    if not dates:\n        return pd.DataFrame()\n\n    t = pd.merge(feed.trips, feed.stop_times)\n    t = t[t[\"route_id\"] == route_id].copy()\n    a = feed.compute_trip_activity(dates)\n\n    frames = []\n    for date in dates:\n        # Slice to trips active on date\n        ids = a.loc[a[date] == 1, \"trip_id\"]\n        f = t[t[\"trip_id\"].isin(ids)].copy()\n        f[\"date\"] = date\n        # Groupby trip ID and sort groups by their minimum departure time.\n        # For some reason NaN departure times mess up the transform below.\n        # So temporarily fill NaN departure times as a workaround.\n        f[\"dt\"] = f[\"departure_time\"].fillna(method=\"ffill\")\n        f[\"min_dt\"] = f.groupby(\"trip_id\")[\"dt\"].transform(min)\n        frames.append(f)\n\n    f = pd.concat(frames)\n    return f.sort_values([\"date\", \"min_dt\", \"stop_sequence\"]).drop(\n        [\"min_dt\", \"dt\"], axis=1\n    )"}
{"code":"def is_ipv4(ip: str) -> bool:\n    \"\"\"\n    Returns True if the IPv4 address ia valid, otherwise returns False.\n    \"\"\"\n    try:\n        socket.inet_aton(ip)\n    except socket.error:\n        return False\n    return True","return_type":"bool","function_name":"is_ipv4","stripped_code":"def is_ipv4(ip: str):\n    \"\"\"\n    Returns True if the IPv4 address ia valid, otherwise returns False.\n    \"\"\"\n    try:\n        socket.inet_aton(ip)\n    except socket.error:\n        return False\n    return True"}
{"code":"def start(self, value: typing.Union[float, NormPointType]) -> None:\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        self.set_property(\"start\", value)","return_type":"None","function_name":"Graphic.start","stripped_code":"def start(self, value: typing.Union[float, NormPointType]):\n        \"\"\"Set the end property in relative coordinates.\n\n        End may be a float when graphic is an Interval or a tuple (y, x) when graphic is a Line.\"\"\"\n        self.set_property(\"start\", value)"}
{"code":"def register_postparsing_hook(self, func: Callable[[plugin.PostparsingData], plugin.PostparsingData]) -> None:\n        \"\"\"Register a function to be called after parsing user input but before running the command\"\"\"\n        self._validate_postparsing_callable(func)\n        self._postparsing_hooks.append(func)","return_type":"None","function_name":"Cmd.register_postparsing_hook","stripped_code":"def register_postparsing_hook(self, func: Callable[[plugin.PostparsingData], plugin.PostparsingData]):\n        \"\"\"Register a function to be called after parsing user input but before running the command\"\"\"\n        self._validate_postparsing_callable(func)\n        self._postparsing_hooks.append(func)"}
{"code":"def format_action(nonterminal: str,\n                  right_hand_side: str,\n                  is_string: bool = False,\n                  is_number: bool = False,\n                  keywords_to_uppercase: List[str] = None) -> str:\n    \"\"\"\n    This function formats an action as it appears in models. It\n    splits productions based on the special `ws` and `wsp` rules,\n    which are used in grammars to denote whitespace, and then\n    rejoins these tokens a formatted, comma separated list.\n    Importantly, note that it `does not` split on spaces in\n    the grammar string, because these might not correspond\n    to spaces in the language the grammar recognises.\n\n    Parameters\n    ----------\n    nonterminal : ``str``, required.\n        The nonterminal in the action.\n    right_hand_side : ``str``, required.\n        The right hand side of the action\n        (i.e the thing which is produced).\n    is_string : ``bool``, optional (default = False).\n        Whether the production produces a string.\n        If it does, it is formatted as ``nonterminal -> ['string']``\n    is_number : ``bool``, optional, (default = False).\n        Whether the production produces a string.\n        If it does, it is formatted as ``nonterminal -> ['number']``\n    keywords_to_uppercase: ``List[str]``, optional, (default = None)\n        Keywords in the grammar to uppercase. In the case of sql,\n        this might be SELECT, MAX etc.\n    \"\"\"\n    keywords_to_uppercase = keywords_to_uppercase or []\n    if right_hand_side.upper() in keywords_to_uppercase:\n        right_hand_side = right_hand_side.upper()\n\n    if is_string:\n        return f'{nonterminal} -> [\"\\'{right_hand_side}\\'\"]'\n\n    elif is_number:\n        return f'{nonterminal} -> [\"{right_hand_side}\"]'\n\n    else:\n        right_hand_side = right_hand_side.lstrip(\"(\").rstrip(\")\")\n        child_strings = [token for token in WHITESPACE_REGEX.split(right_hand_side) if token]\n        child_strings = [tok.upper() if tok.upper() in keywords_to_uppercase else tok for tok in child_strings]\n        return f\"{nonterminal} -> [{', '.join(child_strings)}]\"","return_type":"str","function_name":"format_action","stripped_code":"def format_action(nonterminal: str,\n                  right_hand_side: str,\n                  is_string: bool = False,\n                  is_number: bool = False,\n                  keywords_to_uppercase: List[str] = None):\n    \"\"\"\n    This function formats an action as it appears in models. It\n    splits productions based on the special `ws` and `wsp` rules,\n    which are used in grammars to denote whitespace, and then\n    rejoins these tokens a formatted, comma separated list.\n    Importantly, note that it `does not` split on spaces in\n    the grammar string, because these might not correspond\n    to spaces in the language the grammar recognises.\n\n    Parameters\n    ----------\n    nonterminal : ``str``, required.\n        The nonterminal in the action.\n    right_hand_side : ``str``, required.\n        The right hand side of the action\n        (i.e the thing which is produced).\n    is_string : ``bool``, optional (default = False).\n        Whether the production produces a string.\n        If it does, it is formatted as ``nonterminal -> ['string']``\n    is_number : ``bool``, optional, (default = False).\n        Whether the production produces a string.\n        If it does, it is formatted as ``nonterminal -> ['number']``\n    keywords_to_uppercase: ``List[str]``, optional, (default = None)\n        Keywords in the grammar to uppercase. In the case of sql,\n        this might be SELECT, MAX etc.\n    \"\"\"\n    keywords_to_uppercase = keywords_to_uppercase or []\n    if right_hand_side.upper() in keywords_to_uppercase:\n        right_hand_side = right_hand_side.upper()\n\n    if is_string:\n        return f'{nonterminal} -> [\"\\'{right_hand_side}\\'\"]'\n\n    elif is_number:\n        return f'{nonterminal} -> [\"{right_hand_side}\"]'\n\n    else:\n        right_hand_side = right_hand_side.lstrip(\"(\").rstrip(\")\")\n        child_strings = [token for token in WHITESPACE_REGEX.split(right_hand_side) if token]\n        child_strings = [tok.upper() if tok.upper() in keywords_to_uppercase else tok for tok in child_strings]\n        return f\"{nonterminal} -> [{', '.join(child_strings)}]\""}
{"code":"def num_features_model(m:nn.Module)->int:\n    \"Return the number of output features for `model`.\"\n    sz = 64\n    while True:\n        try: return model_sizes(m, size=(sz,sz))[-1][1]\n        except Exception as e:\n            sz *= 2\n            if sz > 2048: raise","return_type":"int","function_name":"num_features_model","stripped_code":"def num_features_model(m:nn.Module):\n    \"Return the number of output features for `model`.\"\n    sz = 64\n    while True:\n        try: return model_sizes(m, size=(sz,sz))[-1][1]\n        except Exception as e:\n            sz *= 2\n            if sz > 2048: raise"}
{"code":"def _make_sent_vector(self, sent: List, bucket_length: int =None) -> np.ndarray:\n        \"\"\"Transforms a sentence to Numpy array, which will be the network input.\n\n        Args:\n            sent: input sentence\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 3d array, answer[i][j][k] contains the index of k-th letter\n            in j-th word of i-th input sentence.\n        \"\"\"\n        bucket_length = bucket_length or len(sent)\n        answer = np.zeros(shape=(bucket_length, MAX_WORD_LENGTH+2), dtype=np.int32)\n        for i, word in enumerate(sent):\n            answer[i, 0] = self.tags.tok2idx(\"BEGIN\")\n            m = min(len(word), MAX_WORD_LENGTH)\n            for j, x in enumerate(word[-m:]):\n                answer[i, j+1] = self.symbols.tok2idx(x)\n            answer[i, m+1] = self.tags.tok2idx(\"END\")\n            answer[i, m+2:] = self.tags.tok2idx(\"PAD\")\n        return answer","return_type":"np.ndarray","function_name":"CharacterTagger._make_sent_vector","stripped_code":"def _make_sent_vector(self, sent: List, bucket_length: int =None):\n        \"\"\"Transforms a sentence to Numpy array, which will be the network input.\n\n        Args:\n            sent: input sentence\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 3d array, answer[i][j][k] contains the index of k-th letter\n            in j-th word of i-th input sentence.\n        \"\"\"\n        bucket_length = bucket_length or len(sent)\n        answer = np.zeros(shape=(bucket_length, MAX_WORD_LENGTH+2), dtype=np.int32)\n        for i, word in enumerate(sent):\n            answer[i, 0] = self.tags.tok2idx(\"BEGIN\")\n            m = min(len(word), MAX_WORD_LENGTH)\n            for j, x in enumerate(word[-m:]):\n                answer[i, j+1] = self.symbols.tok2idx(x)\n            answer[i, m+1] = self.tags.tok2idx(\"END\")\n            answer[i, m+2:] = self.tags.tok2idx(\"PAD\")\n        return answer"}
{"code":"def delete(self, doc_id: str) -> bool:\n        \"\"\"Delete a document with id.\"\"\"\n\n        try:\n            self.instance.delete(self.index, self.doc_type, doc_id)\n        except RequestError as ex:\n            logging.error(ex)\n            return False\n        else:\n            return True","return_type":"bool","function_name":"ElasticIndex.delete","stripped_code":"def delete(self, doc_id: str):\n        \"\"\"Delete a document with id.\"\"\"\n\n        try:\n            self.instance.delete(self.index, self.doc_type, doc_id)\n        except RequestError as ex:\n            logging.error(ex)\n            return False\n        else:\n            return True"}
{"code":"def gen_classdef(self, clsname: str,  cls: ClassDefinition) -> str:\n        \"\"\" Generate python definition for class clsname \"\"\"\n        parentref = f'({self.python_name_for(cls.is_a) if cls.is_a else \"YAMLRoot\"})'\n        slotdefs = self.gen_slot_variables(cls)\n        postinits = self.gen_postinits(cls)\n        if not slotdefs:\n            slotdefs = 'pass'\n        wrapped_description = f'''\n    \"\"\"\n    {wrapped_annotation(be(cls.description))}\n    \"\"\"''' if be(cls.description) else ''\n        return f'''\n@dataclass\nclass {camelcase(clsname)}{parentref}:{wrapped_description}\n    {slotdefs}\n    {postinits}'''","return_type":"str","function_name":"PythonGenerator.gen_classdef","stripped_code":"def gen_classdef(self, clsname: str,  cls: ClassDefinition):\n        \"\"\" Generate python definition for class clsname \"\"\"\n        parentref = f'({self.python_name_for(cls.is_a) if cls.is_a else \"YAMLRoot\"})'\n        slotdefs = self.gen_slot_variables(cls)\n        postinits = self.gen_postinits(cls)\n        if not slotdefs:\n            slotdefs = 'pass'\n        wrapped_description = f'''\n    \"\"\"\n    {wrapped_annotation(be(cls.description))}\n    \"\"\"''' if be(cls.description) else ''\n        return f'''\n@dataclass\nclass {camelcase(clsname)}{parentref}:{wrapped_description}\n    {slotdefs}\n    {postinits}'''"}
{"code":"def all(self) -> List[Any]:\n        \"\"\"\n        Load all entry points (if not already loaded) in this namespace and return the resulting\n        objects as a list.\n\n        \"\"\"\n        values = []\n        for name, value in self._entrypoints.items():\n            if isinstance(value, EntryPoint):\n                value = self._entrypoints[name] = value.load()\n\n            values.append(value)\n\n        return values","return_type":"List[Any]","function_name":"PluginContainer.all","stripped_code":"def all(self):\n        \"\"\"\n        Load all entry points (if not already loaded) in this namespace and return the resulting\n        objects as a list.\n\n        \"\"\"\n        values = []\n        for name, value in self._entrypoints.items():\n            if isinstance(value, EntryPoint):\n                value = self._entrypoints[name] = value.load()\n\n            values.append(value)\n\n        return values"}
{"code":"def _neighbors_graph(self, **params) -> Dict:\n        \"\"\"\n        Get neighbors of a node\n\n        parameters are directly passed through to SciGraph: e.g. depth, relationshipType\n        \"\"\"\n        response = self._get_response(\"graph/neighbors\", format=\"json\", **params)\n        return response.json()","return_type":"Dict","function_name":"RemoteScigraphOntology._neighbors_graph","stripped_code":"def _neighbors_graph(self, **params):\n        \"\"\"\n        Get neighbors of a node\n\n        parameters are directly passed through to SciGraph: e.g. depth, relationshipType\n        \"\"\"\n        response = self._get_response(\"graph/neighbors\", format=\"json\", **params)\n        return response.json()"}
{"code":"def rewrite_file(self) -> None:\n        \"\"\"\n        Rewrites the source file.\n        \"\"\"\n        if not self.needs_rewriting:\n            return\n        self._info(\"Rewriting file\")\n        with open(self.full_path, \"w\") as outfile:\n            self._write(outfile)","return_type":"None","function_name":"PythonProcessor.rewrite_file","stripped_code":"def rewrite_file(self):\n        \"\"\"\n        Rewrites the source file.\n        \"\"\"\n        if not self.needs_rewriting:\n            return\n        self._info(\"Rewriting file\")\n        with open(self.full_path, \"w\") as outfile:\n            self._write(outfile)"}
{"code":"def visit_Dict(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return dict representation of `node`s elements.\"\"\"\n        items = (': '.join((self.visit(key), self.visit(value)))\n                 for key, value in zip(node.keys, node.values))\n        return f\"{{{', '.join(items)}}}\"","return_type":"str","function_name":"SourceGenerator.visit_Dict","stripped_code":"def visit_Dict(self, node: AST, dfltChaining: bool = True):\n        \"\"\"Return dict representation of `node`s elements.\"\"\"\n        items = (': '.join((self.visit(key), self.visit(value)))\n                 for key, value in zip(node.keys, node.values))\n        return f\"{{{', '.join(items)}}}\""}
{"code":"def __multi_arity_dispatch_fn(  # pylint: disable=too-many-arguments,too-many-locals\n    ctx: GeneratorContext,\n    name: str,\n    arity_map: Mapping[int, str],\n    default_name: Optional[str] = None,\n    max_fixed_arity: Optional[int] = None,\n    meta_node: Optional[MetaNode] = None,\n    is_async: bool = False,\n) -> GeneratedPyAST:\n    \"\"\"Return the Python AST nodes for a argument-length dispatch function\n    for multi-arity functions.\n\n    def fn(*args):\n        nargs = len(args)\n        method = __fn_dispatch_map.get(nargs)\n        if method:\n            return method(*args)\n        # Only if default\n        if nargs > max_fixed_arity:\n            return default(*args)\n        raise RuntimeError\n    \"\"\"\n    dispatch_map_name = f\"{name}_dispatch_map\"\n\n    dispatch_keys, dispatch_vals = [], []\n    for k, v in arity_map.items():\n        dispatch_keys.append(ast.Num(k))\n        dispatch_vals.append(ast.Name(id=v, ctx=ast.Load()))\n\n    # Async functions should return await, otherwise just return\n    handle_return = __handle_async_return if is_async else __handle_return\n\n    nargs_name = genname(\"nargs\")\n    method_name = genname(\"method\")\n    body = [\n        ast.Assign(\n            targets=[ast.Name(id=nargs_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Name(id=\"len\", ctx=ast.Load()),\n                args=[ast.Name(id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.Assign(\n            targets=[ast.Name(id=method_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Attribute(\n                    value=ast.Name(id=dispatch_map_name, ctx=ast.Load()),\n                    attr=\"get\",\n                    ctx=ast.Load(),\n                ),\n                args=[ast.Name(id=nargs_name, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.If(\n            test=ast.Compare(\n                left=ast.NameConstant(None),\n                ops=[ast.IsNot()],\n                comparators=[ast.Name(id=method_name, ctx=ast.Load())],\n            ),\n            body=[\n                handle_return(\n                    ast.Call(\n                        func=ast.Name(id=method_name, ctx=ast.Load()),\n                        args=[\n                            ast.Starred(\n                                value=ast.Name(\n                                    id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                ),\n                                ctx=ast.Load(),\n                            )\n                        ],\n                        keywords=[],\n                    )\n                )\n            ],\n            orelse=[]\n            if default_name is None\n            else [\n                ast.If(\n                    test=ast.Compare(\n                        left=ast.Name(id=nargs_name, ctx=ast.Load()),\n                        ops=[ast.GtE()],\n                        comparators=[ast.Num(max_fixed_arity)],\n                    ),\n                    body=[\n                        handle_return(\n                            ast.Call(\n                                func=ast.Name(id=default_name, ctx=ast.Load()),\n                                args=[\n                                    ast.Starred(\n                                        value=ast.Name(\n                                            id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                        ),\n                                        ctx=ast.Load(),\n                                    )\n                                ],\n                                keywords=[],\n                            )\n                        )\n                    ],\n                    orelse=[],\n                )\n            ],\n        ),\n        ast.Raise(\n            exc=ast.Call(\n                func=_load_attr(\"basilisp.lang.runtime.RuntimeException\"),\n                args=[\n                    ast.Str(f\"Wrong number of args passed to function: {name}\"),\n                    ast.Name(id=nargs_name, ctx=ast.Load()),\n                ],\n                keywords=[],\n            ),\n            cause=None,\n        ),\n    ]\n\n    py_fn_node = ast.AsyncFunctionDef if is_async else ast.FunctionDef\n    meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n    return GeneratedPyAST(\n        node=ast.Name(id=name, ctx=ast.Load()),\n        dependencies=chain(\n            [\n                ast.Assign(\n                    targets=[ast.Name(id=dispatch_map_name, ctx=ast.Store())],\n                    value=ast.Dict(keys=dispatch_keys, values=dispatch_vals),\n                )\n            ],\n            meta_deps,\n            [\n                py_fn_node(\n                    name=name,\n                    args=ast.arguments(\n                        args=[],\n                        kwarg=None,\n                        vararg=ast.arg(arg=_MULTI_ARITY_ARG_NAME, annotation=None),\n                        kwonlyargs=[],\n                        defaults=[],\n                        kw_defaults=[],\n                    ),\n                    body=body,\n                    decorator_list=list(chain(meta_decorators, [_BASILISP_FN_FN_NAME])),\n                    returns=None,\n                )\n            ],\n        ),\n    )","return_type":"GeneratedPyAST","function_name":"__multi_arity_dispatch_fn","stripped_code":"def __multi_arity_dispatch_fn(  # pylint: disable=too-many-arguments,too-many-locals\n    ctx: GeneratorContext,\n    name: str,\n    arity_map: Mapping[int, str],\n    default_name: Optional[str] = None,\n    max_fixed_arity: Optional[int] = None,\n    meta_node: Optional[MetaNode] = None,\n    is_async: bool = False,\n):\n    \"\"\"Return the Python AST nodes for a argument-length dispatch function\n    for multi-arity functions.\n\n    def fn(*args):\n        nargs = len(args)\n        method = __fn_dispatch_map.get(nargs)\n        if method:\n            return method(*args)\n        # Only if default\n        if nargs > max_fixed_arity:\n            return default(*args)\n        raise RuntimeError\n    \"\"\"\n    dispatch_map_name = f\"{name}_dispatch_map\"\n\n    dispatch_keys, dispatch_vals = [], []\n    for k, v in arity_map.items():\n        dispatch_keys.append(ast.Num(k))\n        dispatch_vals.append(ast.Name(id=v, ctx=ast.Load()))\n\n    # Async functions should return await, otherwise just return\n    handle_return = __handle_async_return if is_async else __handle_return\n\n    nargs_name = genname(\"nargs\")\n    method_name = genname(\"method\")\n    body = [\n        ast.Assign(\n            targets=[ast.Name(id=nargs_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Name(id=\"len\", ctx=ast.Load()),\n                args=[ast.Name(id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.Assign(\n            targets=[ast.Name(id=method_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Attribute(\n                    value=ast.Name(id=dispatch_map_name, ctx=ast.Load()),\n                    attr=\"get\",\n                    ctx=ast.Load(),\n                ),\n                args=[ast.Name(id=nargs_name, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.If(\n            test=ast.Compare(\n                left=ast.NameConstant(None),\n                ops=[ast.IsNot()],\n                comparators=[ast.Name(id=method_name, ctx=ast.Load())],\n            ),\n            body=[\n                handle_return(\n                    ast.Call(\n                        func=ast.Name(id=method_name, ctx=ast.Load()),\n                        args=[\n                            ast.Starred(\n                                value=ast.Name(\n                                    id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                ),\n                                ctx=ast.Load(),\n                            )\n                        ],\n                        keywords=[],\n                    )\n                )\n            ],\n            orelse=[]\n            if default_name is None\n            else [\n                ast.If(\n                    test=ast.Compare(\n                        left=ast.Name(id=nargs_name, ctx=ast.Load()),\n                        ops=[ast.GtE()],\n                        comparators=[ast.Num(max_fixed_arity)],\n                    ),\n                    body=[\n                        handle_return(\n                            ast.Call(\n                                func=ast.Name(id=default_name, ctx=ast.Load()),\n                                args=[\n                                    ast.Starred(\n                                        value=ast.Name(\n                                            id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                        ),\n                                        ctx=ast.Load(),\n                                    )\n                                ],\n                                keywords=[],\n                            )\n                        )\n                    ],\n                    orelse=[],\n                )\n            ],\n        ),\n        ast.Raise(\n            exc=ast.Call(\n                func=_load_attr(\"basilisp.lang.runtime.RuntimeException\"),\n                args=[\n                    ast.Str(f\"Wrong number of args passed to function: {name}\"),\n                    ast.Name(id=nargs_name, ctx=ast.Load()),\n                ],\n                keywords=[],\n            ),\n            cause=None,\n        ),\n    ]\n\n    py_fn_node = ast.AsyncFunctionDef if is_async else ast.FunctionDef\n    meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n    return GeneratedPyAST(\n        node=ast.Name(id=name, ctx=ast.Load()),\n        dependencies=chain(\n            [\n                ast.Assign(\n                    targets=[ast.Name(id=dispatch_map_name, ctx=ast.Store())],\n                    value=ast.Dict(keys=dispatch_keys, values=dispatch_vals),\n                )\n            ],\n            meta_deps,\n            [\n                py_fn_node(\n                    name=name,\n                    args=ast.arguments(\n                        args=[],\n                        kwarg=None,\n                        vararg=ast.arg(arg=_MULTI_ARITY_ARG_NAME, annotation=None),\n                        kwonlyargs=[],\n                        defaults=[],\n                        kw_defaults=[],\n                    ),\n                    body=body,\n                    decorator_list=list(chain(meta_decorators, [_BASILISP_FN_FN_NAME])),\n                    returns=None,\n                )\n            ],\n        ),\n    )"}
{"code":"def receivables_account(self) -> Account:\n        \"\"\"\n        Returns receivables account. Receivables account is assumed to be the one were invoice rows were recorded.\n        :return: Account or None\n        \"\"\"\n        row = AccountEntry.objects.filter(source_invoice=self).order_by('id').first()\n        return row.account if row else None","return_type":"Account","function_name":"Invoice.receivables_account","stripped_code":"def receivables_account(self):\n        \"\"\"\n        Returns receivables account. Receivables account is assumed to be the one were invoice rows were recorded.\n        :return: Account or None\n        \"\"\"\n        row = AccountEntry.objects.filter(source_invoice=self).order_by('id').first()\n        return row.account if row else None"}
{"code":"def _read_embeddings_from_hdf5(embeddings_filename: str,\n                               embedding_dim: int,\n                               vocab: Vocabulary,\n                               namespace: str = \"tokens\") -> torch.FloatTensor:\n    \"\"\"\n    Reads from a hdf5 formatted file. The embedding matrix is assumed to\n    be keyed by 'embedding' and of size ``(num_tokens, embedding_dim)``.\n    \"\"\"\n    with h5py.File(embeddings_filename, 'r') as fin:\n        embeddings = fin['embedding'][...]\n\n    if list(embeddings.shape) != [vocab.get_vocab_size(namespace), embedding_dim]:\n        raise ConfigurationError(\n                \"Read shape {0} embeddings from the file, but expected {1}\".format(\n                        list(embeddings.shape), [vocab.get_vocab_size(namespace), embedding_dim]))\n\n    return torch.FloatTensor(embeddings)","return_type":"torch.FloatTensor","function_name":"_read_embeddings_from_hdf5","stripped_code":"def _read_embeddings_from_hdf5(embeddings_filename: str,\n                               embedding_dim: int,\n                               vocab: Vocabulary,\n                               namespace: str = \"tokens\"):\n    \"\"\"\n    Reads from a hdf5 formatted file. The embedding matrix is assumed to\n    be keyed by 'embedding' and of size ``(num_tokens, embedding_dim)``.\n    \"\"\"\n    with h5py.File(embeddings_filename, 'r') as fin:\n        embeddings = fin['embedding'][...]\n\n    if list(embeddings.shape) != [vocab.get_vocab_size(namespace), embedding_dim]:\n        raise ConfigurationError(\n                \"Read shape {0} embeddings from the file, but expected {1}\".format(\n                        list(embeddings.shape), [vocab.get_vocab_size(namespace), embedding_dim]))\n\n    return torch.FloatTensor(embeddings)"}
{"code":"def formatMessage(self, record: logging.LogRecord) -> str:\n        \"\"\"Convert the already filled log record to a string.\"\"\"\n        level_color = \"0\"\n        text_color = \"0\"\n        fmt = \"\"\n        if record.levelno <= logging.DEBUG:\n            fmt = \"\\033[0;37m\" + logging.BASIC_FORMAT + \"\\033[0m\"\n        elif record.levelno <= logging.INFO:\n            level_color = \"1;36\"\n            lmsg = record.message.lower()\n            if self.GREEN_RE.search(lmsg):\n                text_color = \"1;32\"\n        elif record.levelno <= logging.WARNING:\n            level_color = \"1;33\"\n        elif record.levelno <= logging.CRITICAL:\n            level_color = \"1;31\"\n        if not fmt:\n            fmt = \"\\033[\" + level_color + \\\n                  \"m%(levelname)s\\033[0m:%(rthread)s:%(name)s:\\033[\" + text_color + \\\n                  \"m%(message)s\\033[0m\"\n        fmt = _fest + fmt\n        record.rthread = reduce_thread_id(record.thread)\n        return fmt % record.__dict__","return_type":"str","function_name":"AwesomeFormatter.formatMessage","stripped_code":"def formatMessage(self, record: logging.LogRecord):\n        \"\"\"Convert the already filled log record to a string.\"\"\"\n        level_color = \"0\"\n        text_color = \"0\"\n        fmt = \"\"\n        if record.levelno <= logging.DEBUG:\n            fmt = \"\\033[0;37m\" + logging.BASIC_FORMAT + \"\\033[0m\"\n        elif record.levelno <= logging.INFO:\n            level_color = \"1;36\"\n            lmsg = record.message.lower()\n            if self.GREEN_RE.search(lmsg):\n                text_color = \"1;32\"\n        elif record.levelno <= logging.WARNING:\n            level_color = \"1;33\"\n        elif record.levelno <= logging.CRITICAL:\n            level_color = \"1;31\"\n        if not fmt:\n            fmt = \"\\033[\" + level_color + \\\n                  \"m%(levelname)s\\033[0m:%(rthread)s:%(name)s:\\033[\" + text_color + \\\n                  \"m%(message)s\\033[0m\"\n        fmt = _fest + fmt\n        record.rthread = reduce_thread_id(record.thread)\n        return fmt % record.__dict__"}
{"code":"def uninstall(self, package: str) -> None:\n        '''Remove this app package from the device.'''\n        if package not in self.view_packgets_list():\n            raise NoSuchPackageException(\n                f'There is no such package {package!r}.')\n        self._execute('-s', self.device_sn, 'uninstall', package)","return_type":"None","function_name":"BaseAndroidDriver.uninstall","stripped_code":"def uninstall(self, package: str):\n        '''Remove this app package from the device.'''\n        if package not in self.view_packgets_list():\n            raise NoSuchPackageException(\n                f'There is no such package {package!r}.')\n        self._execute('-s', self.device_sn, 'uninstall', package)"}
{"code":"def supports_suggested_actions(channel_id: str, button_cnt: int = 100) -> bool:\n        \"\"\"Determine if a number of Suggested Actions are supported by a Channel.\n\n        Args:\n            channel_id (str): The Channel to check the if Suggested Actions are supported in.\n            button_cnt (int, optional): Defaults to 100. The number of Suggested Actions to check for the Channel.\n\n        Returns:\n            bool: True if the Channel supports the button_cnt total Suggested Actions, False if the Channel does not support that number of Suggested Actions.\n        \"\"\"\n\n        max_actions = {\n            # https://developers.facebook.com/docs/messenger-platform/send-messages/quick-replies\n            Channels.facebook: 10,\n            Channels.skype: 10,\n            # https://developers.line.biz/en/reference/messaging-api/#items-object\n            Channels.line: 13,\n            # https://dev.kik.com/#/docs/messaging#text-response-object\n            Channels.kik: 20,\n            Channels.telegram: 100,\n            Channels.slack: 100,\n            Channels.emulator: 100,\n            Channels.direct_line: 100,\n            Channels.webchat: 100,\n        }\n        return button_cnt <= max_actions[channel_id] if channel_id in max_actions else False","return_type":"bool","function_name":"Channel.supports_suggested_actions","stripped_code":"def supports_suggested_actions(channel_id: str, button_cnt: int = 100):\n        \"\"\"Determine if a number of Suggested Actions are supported by a Channel.\n\n        Args:\n            channel_id (str): The Channel to check the if Suggested Actions are supported in.\n            button_cnt (int, optional): Defaults to 100. The number of Suggested Actions to check for the Channel.\n\n        Returns:\n            bool: True if the Channel supports the button_cnt total Suggested Actions, False if the Channel does not support that number of Suggested Actions.\n        \"\"\"\n\n        max_actions = {\n            # https://developers.facebook.com/docs/messenger-platform/send-messages/quick-replies\n            Channels.facebook: 10,\n            Channels.skype: 10,\n            # https://developers.line.biz/en/reference/messaging-api/#items-object\n            Channels.line: 13,\n            # https://dev.kik.com/#/docs/messaging#text-response-object\n            Channels.kik: 20,\n            Channels.telegram: 100,\n            Channels.slack: 100,\n            Channels.emulator: 100,\n            Channels.direct_line: 100,\n            Channels.webchat: 100,\n        }\n        return button_cnt <= max_actions[channel_id] if channel_id in max_actions else False"}
{"code":"def _check_channel_state_for_update(\n            self,\n            channel_identifier: ChannelID,\n            closer: Address,\n            update_nonce: Nonce,\n            block_identifier: BlockSpecification,\n    ) -> Optional[str]:\n        \"\"\"Check the channel state on chain to see if it has been updated.\n\n        Compare the nonce, we are about to update the contract with, with the\n        updated nonce in the onchain state and, if it's the same, return a\n        message with which the caller should raise a RaidenRecoverableError.\n\n        If all is okay return None.\n        \"\"\"\n        msg = None\n        closer_details = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=closer,\n            partner=self.node_address,\n            block_identifier=block_identifier,\n        )\n        if closer_details.nonce == update_nonce:\n            msg = (\n                'updateNonClosingBalanceProof transaction has already '\n                'been mined and updated the channel succesfully.'\n            )\n\n        return msg","return_type":"Optional[str]","function_name":"TokenNetwork._check_channel_state_for_update","stripped_code":"def _check_channel_state_for_update(\n            self,\n            channel_identifier: ChannelID,\n            closer: Address,\n            update_nonce: Nonce,\n            block_identifier: BlockSpecification,\n    ):\n        \"\"\"Check the channel state on chain to see if it has been updated.\n\n        Compare the nonce, we are about to update the contract with, with the\n        updated nonce in the onchain state and, if it's the same, return a\n        message with which the caller should raise a RaidenRecoverableError.\n\n        If all is okay return None.\n        \"\"\"\n        msg = None\n        closer_details = self._detail_participant(\n            channel_identifier=channel_identifier,\n            participant=closer,\n            partner=self.node_address,\n            block_identifier=block_identifier,\n        )\n        if closer_details.nonce == update_nonce:\n            msg = (\n                'updateNonClosingBalanceProof transaction has already '\n                'been mined and updated the channel succesfully.'\n            )\n\n        return msg"}
{"code":"def get_object(self, **kwargs) ->Model:\n        \"\"\"\n        Returns the object the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.\n        \"\"\"\n        queryset = self.filter_queryset(\n            queryset=self.get_queryset(**kwargs),\n            **kwargs\n        )\n\n        # Perform the lookup filtering.\n        lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field\n\n        assert lookup_url_kwarg in kwargs, (\n                'Expected view %s to be called with a URL keyword argument '\n                'named \"%s\". Fix your URL conf, or set the `.lookup_field` '\n                'attribute on the view correctly.' %\n                (self.__class__.__name__, lookup_url_kwarg)\n        )\n\n        filter_kwargs = {self.lookup_field: kwargs[lookup_url_kwarg]}\n\n        obj = get_object_or_404(queryset, **filter_kwargs)\n        # TODO check_object_permissions\n\n        return obj","return_type":"Model","function_name":"GenericAsyncAPIConsumer.get_object","stripped_code":"def get_object(self, **kwargs):\n        \"\"\"\n        Returns the object the view is displaying.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.\n        \"\"\"\n        queryset = self.filter_queryset(\n            queryset=self.get_queryset(**kwargs),\n            **kwargs\n        )\n\n        # Perform the lookup filtering.\n        lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field\n\n        assert lookup_url_kwarg in kwargs, (\n                'Expected view %s to be called with a URL keyword argument '\n                'named \"%s\". Fix your URL conf, or set the `.lookup_field` '\n                'attribute on the view correctly.' %\n                (self.__class__.__name__, lookup_url_kwarg)\n        )\n\n        filter_kwargs = {self.lookup_field: kwargs[lookup_url_kwarg]}\n\n        obj = get_object_or_404(queryset, **filter_kwargs)\n        # TODO check_object_permissions\n\n        return obj"}
{"code":"def nb_to_html_cells(nb) -> list:\n    \"\"\"\n    Converts notebook to an iterable of BS4 HTML nodes. Images are inline.\n    \"\"\"\n    html_exporter = HTMLExporter()\n    html_exporter.template_file = 'basic'\n\n    (body, resources) = html_exporter.from_notebook_node(nb)\n    return BeautifulSoup(body, 'html.parser').findAll('div', class_='cell')","return_type":"list","function_name":"nb_to_html_cells","stripped_code":"def nb_to_html_cells(nb):\n    \"\"\"\n    Converts notebook to an iterable of BS4 HTML nodes. Images are inline.\n    \"\"\"\n    html_exporter = HTMLExporter()\n    html_exporter.template_file = 'basic'\n\n    (body, resources) = html_exporter.from_notebook_node(nb)\n    return BeautifulSoup(body, 'html.parser').findAll('div', class_='cell')"}
{"code":"def get_upregulated_genes_network(self) -> Graph:\n        \"\"\"Get the graph of up-regulated genes.\n\n        :return Graph: Graph of up-regulated genes.\n        \"\"\"\n        logger.info(\"In get_upregulated_genes_network()\")\n\n        deg_graph = self.graph.copy()  # deep copy graph\n        not_diff_expr = self.graph.vs(up_regulated_eq=False)\n\n        # delete genes which are not differentially expressed or have no connections to others\n        deg_graph.delete_vertices(not_diff_expr.indices)\n        deg_graph.delete_vertices(deg_graph.vs.select(_degree_eq=0))\n\n        return deg_graph","return_type":"Graph","function_name":"FilteredNetwork.get_upregulated_genes_network","stripped_code":"def get_upregulated_genes_network(self):\n        \"\"\"Get the graph of up-regulated genes.\n\n        :return Graph: Graph of up-regulated genes.\n        \"\"\"\n        logger.info(\"In get_upregulated_genes_network()\")\n\n        deg_graph = self.graph.copy()  # deep copy graph\n        not_diff_expr = self.graph.vs(up_regulated_eq=False)\n\n        # delete genes which are not differentially expressed or have no connections to others\n        deg_graph.delete_vertices(not_diff_expr.indices)\n        deg_graph.delete_vertices(deg_graph.vs.select(_degree_eq=0))\n\n        return deg_graph"}
{"code":"def remove_component(self, entity: int, component_type: Any) -> int:\n        \"\"\"Remove a Component instance from an Entity, by type.\n\n        A Component instance can be removed by providing it's type.\n        For example: world.delete_component(enemy_a, Velocity) will remove\n        the Velocity instance from the Entity enemy_a.\n\n        Raises a KeyError if either the given entity or Component type does\n        not exist in the database.\n        :param entity: The Entity to remove the Component from.\n        :param component_type: The type of the Component to remove.\n        \"\"\"\n        self._components[component_type].discard(entity)\n\n        if not self._components[component_type]:\n            del self._components[component_type]\n\n        del self._entities[entity][component_type]\n\n        if not self._entities[entity]:\n            del self._entities[entity]\n\n        self.clear_cache()\n        return entity","return_type":"int","function_name":"World.remove_component","stripped_code":"def remove_component(self, entity: int, component_type: Any):\n        \"\"\"Remove a Component instance from an Entity, by type.\n\n        A Component instance can be removed by providing it's type.\n        For example: world.delete_component(enemy_a, Velocity) will remove\n        the Velocity instance from the Entity enemy_a.\n\n        Raises a KeyError if either the given entity or Component type does\n        not exist in the database.\n        :param entity: The Entity to remove the Component from.\n        :param component_type: The type of the Component to remove.\n        \"\"\"\n        self._components[component_type].discard(entity)\n\n        if not self._components[component_type]:\n            del self._components[component_type]\n\n        del self._entities[entity][component_type]\n\n        if not self._entities[entity]:\n            del self._entities[entity]\n\n        self.clear_cache()\n        return entity"}
{"code":"def almost_unitary(gate: Gate) -> bool:\n    \"\"\"Return true if gate tensor is (almost) unitary\"\"\"\n    res = (gate @ gate.H).asoperator()\n    N = gate.qubit_nb\n    return np.allclose(asarray(res), np.eye(2**N), atol=TOLERANCE)","return_type":"bool","function_name":"almost_unitary","stripped_code":"def almost_unitary(gate: Gate):\n    \"\"\"Return true if gate tensor is (almost) unitary\"\"\"\n    res = (gate @ gate.H).asoperator()\n    N = gate.qubit_nb\n    return np.allclose(asarray(res), np.eye(2**N), atol=TOLERANCE)"}
{"code":"def integer_squareroot(value: int) -> int:\n    \"\"\"\n    Return the integer square root of ``value``.\n\n    Uses Python's decimal module to compute the square root of ``value`` with\n    a precision of 128-bits. The value 128 is chosen since the largest square\n    root of a 256-bit integer is a 128-bit integer.\n    \"\"\"\n    if not isinstance(value, int) or isinstance(value, bool):\n        raise ValueError(\n            \"Value must be an integer: Got: {0}\".format(\n                type(value),\n            )\n        )\n    if value < 0:\n        raise ValueError(\n            \"Value cannot be negative: Got: {0}\".format(\n                value,\n            )\n        )\n\n    with decimal.localcontext() as ctx:\n        ctx.prec = 128\n        return int(decimal.Decimal(value).sqrt())","return_type":"int","function_name":"integer_squareroot","stripped_code":"def integer_squareroot(value: int):\n    \"\"\"\n    Return the integer square root of ``value``.\n\n    Uses Python's decimal module to compute the square root of ``value`` with\n    a precision of 128-bits. The value 128 is chosen since the largest square\n    root of a 256-bit integer is a 128-bit integer.\n    \"\"\"\n    if not isinstance(value, int) or isinstance(value, bool):\n        raise ValueError(\n            \"Value must be an integer: Got: {0}\".format(\n                type(value),\n            )\n        )\n    if value < 0:\n        raise ValueError(\n            \"Value cannot be negative: Got: {0}\".format(\n                value,\n            )\n        )\n\n    with decimal.localcontext() as ctx:\n        ctx.prec = 128\n        return int(decimal.Decimal(value).sqrt())"}
{"code":"def make_data_splits(self, max_samples: int) -> None:\n        \"\"\" Splits the utterances into training, validation and test sets.\"\"\"\n\n        train_f_exists = self.train_prefix_fn.is_file()\n        valid_f_exists = self.valid_prefix_fn.is_file()\n        test_f_exists = self.test_prefix_fn.is_file()\n\n        if train_f_exists and valid_f_exists and test_f_exists:\n            logger.debug(\"Split for training, validation and tests specified by files\")\n            self.train_prefixes = self.read_prefixes(self.train_prefix_fn)\n            self.valid_prefixes = self.read_prefixes(self.valid_prefix_fn)\n            self.test_prefixes = self.read_prefixes(self.test_prefix_fn)\n            return\n\n        # Otherwise we now need to load prefixes for other cases addressed\n        # below\n        prefixes = self.determine_prefixes()\n        prefixes = utils.filter_by_size(\n            self.feat_dir, prefixes, self.feat_type, max_samples)\n\n        if not train_f_exists and not valid_f_exists and not test_f_exists:\n            logger.debug(\"No files supplied to define the split for training, validation\"\n                         \" and tests. Using default.\")\n            train_prefixes, valid_prefixes, test_prefixes = self.divide_prefixes(prefixes)\n            self.train_prefixes = train_prefixes\n            self.valid_prefixes = valid_prefixes\n            self.test_prefixes = test_prefixes\n            self.write_prefixes(train_prefixes, self.train_prefix_fn)\n            self.write_prefixes(valid_prefixes, self.valid_prefix_fn)\n            self.write_prefixes(test_prefixes, self.test_prefix_fn)\n        elif not train_f_exists and valid_f_exists and test_f_exists:\n            # Then we just make all other prefixes training prefixes.\n            self.valid_prefixes = self.read_prefixes(self.valid_prefix_fn)\n            self.test_prefixes = self.read_prefixes(self.test_prefix_fn)\n            train_prefixes = list(\n                set(prefixes) - set(self.valid_prefixes))\n            self.train_prefixes = list(\n                set(train_prefixes) - set(self.test_prefixes))\n            self.write_prefixes(self.train_prefixes, self.train_prefix_fn)\n        else:\n            raise NotImplementedError(\n                \"The following case has not been implemented:\" + \n                \"{} exists - {}\\n\".format(self.train_prefix_fn, train_f_exists) +\n                \"{} exists - {}\\n\".format(self.valid_prefix_fn, valid_f_exists) +\n                \"{} exists - {}\\n\".format(self.test_prefix_fn, test_f_exists))","return_type":"None","function_name":"Corpus.make_data_splits","stripped_code":"def make_data_splits(self, max_samples: int):\n        \"\"\" Splits the utterances into training, validation and test sets.\"\"\"\n\n        train_f_exists = self.train_prefix_fn.is_file()\n        valid_f_exists = self.valid_prefix_fn.is_file()\n        test_f_exists = self.test_prefix_fn.is_file()\n\n        if train_f_exists and valid_f_exists and test_f_exists:\n            logger.debug(\"Split for training, validation and tests specified by files\")\n            self.train_prefixes = self.read_prefixes(self.train_prefix_fn)\n            self.valid_prefixes = self.read_prefixes(self.valid_prefix_fn)\n            self.test_prefixes = self.read_prefixes(self.test_prefix_fn)\n            return\n\n        # Otherwise we now need to load prefixes for other cases addressed\n        # below\n        prefixes = self.determine_prefixes()\n        prefixes = utils.filter_by_size(\n            self.feat_dir, prefixes, self.feat_type, max_samples)\n\n        if not train_f_exists and not valid_f_exists and not test_f_exists:\n            logger.debug(\"No files supplied to define the split for training, validation\"\n                         \" and tests. Using default.\")\n            train_prefixes, valid_prefixes, test_prefixes = self.divide_prefixes(prefixes)\n            self.train_prefixes = train_prefixes\n            self.valid_prefixes = valid_prefixes\n            self.test_prefixes = test_prefixes\n            self.write_prefixes(train_prefixes, self.train_prefix_fn)\n            self.write_prefixes(valid_prefixes, self.valid_prefix_fn)\n            self.write_prefixes(test_prefixes, self.test_prefix_fn)\n        elif not train_f_exists and valid_f_exists and test_f_exists:\n            # Then we just make all other prefixes training prefixes.\n            self.valid_prefixes = self.read_prefixes(self.valid_prefix_fn)\n            self.test_prefixes = self.read_prefixes(self.test_prefix_fn)\n            train_prefixes = list(\n                set(prefixes) - set(self.valid_prefixes))\n            self.train_prefixes = list(\n                set(train_prefixes) - set(self.test_prefixes))\n            self.write_prefixes(self.train_prefixes, self.train_prefix_fn)\n        else:\n            raise NotImplementedError(\n                \"The following case has not been implemented:\" + \n                \"{} exists - {}\\n\".format(self.train_prefix_fn, train_f_exists) +\n                \"{} exists - {}\\n\".format(self.valid_prefix_fn, valid_f_exists) +\n                \"{} exists - {}\\n\".format(self.test_prefix_fn, test_f_exists))"}
{"code":"def pick_up_tip(self, location: Union[types.Location, Well] = None,\n                    presses: int = 3,\n                    increment: float = 1.0) -> 'InstrumentContext':\n        \"\"\"\n        Pick up a tip for the pipette to run liquid-handling commands with\n\n        If no location is passed, the Pipette will pick up the next available\n        tip in its :py:attr:`InstrumentContext.tip_racks` list.\n\n        The tip to pick up can be manually specified with the `location`\n        argument. The `location` argument can be specified in several ways:\n\n        * If the only thing to specify is which well from which to pick\n          up a tip, `location` can be a :py:class:`.Well`. For instance,\n          if you have a tip rack in a variable called `tiprack`, you can\n          pick up a specific tip from it with\n          ``instr.pick_up_tip(tiprack.wells()[0])``. This style of call can\n          be used to make the robot pick up a tip from a tip rack that\n          was not specified when creating the :py:class:`.InstrumentContext`.\n\n        * If the position to move to in the well needs to be specified,\n          for instance to tell the robot to run its pick up tip routine\n          starting closer to or farther from the top of the tip,\n          `location` can be a :py:class:`.types.Location`; for instance,\n          you can call ``instr.pick_up_tip(tiprack.wells()[0].top())``.\n\n        :param location: The location from which to pick up a tip.\n        :type location: :py:class:`.types.Location` or :py:class:`.Well` to\n                        pick up a tip from.\n        :param presses: The number of times to lower and then raise the pipette\n                        when picking up a tip, to ensure a good seal (0 [zero]\n                        will result in the pipette hovering over the tip but\n                        not picking it up--generally not desireable, but could\n                        be used for dry-run).\n        :type presses: int\n        :param increment: The additional distance to travel on each successive\n                          press (e.g.: if `presses=3` and `increment=1.0`, then\n                          the first press will travel down into the tip by\n                          3.5mm, the second by 4.5mm, and the third by 5.5mm).\n        :type increment: float\n\n        :returns: This instance\n        \"\"\"\n        num_channels = self.channels\n\n        def _select_tiprack_from_list(tip_racks) -> Tuple[Labware, Well]:\n            try:\n                tr = tip_racks[0]\n            except IndexError:\n                raise OutOfTipsError\n            next_tip = tr.next_tip(num_channels)\n            if next_tip:\n                return tr, next_tip\n            else:\n                return _select_tiprack_from_list(tip_racks[1:])\n\n        if location and isinstance(location, types.Location):\n            if isinstance(location.labware, Labware):\n                tiprack = location.labware\n                target: Well = tiprack.next_tip(num_channels)  # type: ignore\n                if not target:\n                    raise OutOfTipsError\n            elif isinstance(location.labware, Well):\n                tiprack = location.labware.parent\n                target = location.labware\n        elif location and isinstance(location, Well):\n            tiprack = location.parent\n            target = location\n        elif not location:\n            tiprack, target = _select_tiprack_from_list(self.tip_racks)\n        else:\n            raise TypeError(\n                \"If specified, location should be an instance of \"\n                \"types.Location (e.g. the return value from \"\n                \"tiprack.wells()[0].top()) or a Well (e.g. tiprack.wells()[0].\"\n                \" However, it is a {}\".format(location))\n\n        assert tiprack.is_tiprack, \"{} is not a tiprack\".format(str(tiprack))\n\n        self.move_to(target.top())\n\n        self._hw_manager.hardware.pick_up_tip(\n            self._mount, tiprack.tip_length, presses, increment)\n        # Note that the hardware API pick_up_tip action includes homing z after\n\n        tiprack.use_tips(target, num_channels)\n        self._last_tip_picked_up_from = target\n        return self","return_type":"'InstrumentContext'","function_name":"InstrumentContext.pick_up_tip","stripped_code":"def pick_up_tip(self, location: Union[types.Location, Well] = None,\n                    presses: int = 3,\n                    increment: float = 1.0):\n        \"\"\"\n        Pick up a tip for the pipette to run liquid-handling commands with\n\n        If no location is passed, the Pipette will pick up the next available\n        tip in its :py:attr:`InstrumentContext.tip_racks` list.\n\n        The tip to pick up can be manually specified with the `location`\n        argument. The `location` argument can be specified in several ways:\n\n        * If the only thing to specify is which well from which to pick\n          up a tip, `location` can be a :py:class:`.Well`. For instance,\n          if you have a tip rack in a variable called `tiprack`, you can\n          pick up a specific tip from it with\n          ``instr.pick_up_tip(tiprack.wells()[0])``. This style of call can\n          be used to make the robot pick up a tip from a tip rack that\n          was not specified when creating the :py:class:`.InstrumentContext`.\n\n        * If the position to move to in the well needs to be specified,\n          for instance to tell the robot to run its pick up tip routine\n          starting closer to or farther from the top of the tip,\n          `location` can be a :py:class:`.types.Location`; for instance,\n          you can call ``instr.pick_up_tip(tiprack.wells()[0].top())``.\n\n        :param location: The location from which to pick up a tip.\n        :type location: :py:class:`.types.Location` or :py:class:`.Well` to\n                        pick up a tip from.\n        :param presses: The number of times to lower and then raise the pipette\n                        when picking up a tip, to ensure a good seal (0 [zero]\n                        will result in the pipette hovering over the tip but\n                        not picking it up--generally not desireable, but could\n                        be used for dry-run).\n        :type presses: int\n        :param increment: The additional distance to travel on each successive\n                          press (e.g.: if `presses=3` and `increment=1.0`, then\n                          the first press will travel down into the tip by\n                          3.5mm, the second by 4.5mm, and the third by 5.5mm).\n        :type increment: float\n\n        :returns: This instance\n        \"\"\"\n        num_channels = self.channels\n\n        def _select_tiprack_from_list(tip_racks) -> Tuple[Labware, Well]:\n            try:\n                tr = tip_racks[0]\n            except IndexError:\n                raise OutOfTipsError\n            next_tip = tr.next_tip(num_channels)\n            if next_tip:\n                return tr, next_tip\n            else:\n                return _select_tiprack_from_list(tip_racks[1:])\n\n        if location and isinstance(location, types.Location):\n            if isinstance(location.labware, Labware):\n                tiprack = location.labware\n                target: Well = tiprack.next_tip(num_channels)  # type: ignore\n                if not target:\n                    raise OutOfTipsError\n            elif isinstance(location.labware, Well):\n                tiprack = location.labware.parent\n                target = location.labware\n        elif location and isinstance(location, Well):\n            tiprack = location.parent\n            target = location\n        elif not location:\n            tiprack, target = _select_tiprack_from_list(self.tip_racks)\n        else:\n            raise TypeError(\n                \"If specified, location should be an instance of \"\n                \"types.Location (e.g. the return value from \"\n                \"tiprack.wells()[0].top()) or a Well (e.g. tiprack.wells()[0].\"\n                \" However, it is a {}\".format(location))\n\n        assert tiprack.is_tiprack, \"{} is not a tiprack\".format(str(tiprack))\n\n        self.move_to(target.top())\n\n        self._hw_manager.hardware.pick_up_tip(\n            self._mount, tiprack.tip_length, presses, increment)\n        # Note that the hardware API pick_up_tip action includes homing z after\n\n        tiprack.use_tips(target, num_channels)\n        self._last_tip_picked_up_from = target\n        return self"}
{"code":"def entropy_bits(\n        lst: Union[\n            List[Union[int, str, float, complex]],\n            Tuple[Union[int, str, float, complex]]\n        ]\n) -> float:\n    \"\"\"Calculate the number of entropy bits in a list or tuple of elements.\"\"\"\n    # Based on https://stackoverflow.com/a/45091961\n    if not isinstance(lst, (list, tuple)):\n        raise TypeError('lst must be a list or a tuple')\n\n    for num in lst:\n        if not isinstance(num, (int, str, float, complex)):\n            raise TypeError('lst can only be comprised of int, str, float, '\n                            'complex')\n\n    n_lst = len(lst)\n\n    if n_lst <= 1:\n        return 0.0\n\n    # Some NumPy replacements\n    counts = [lst.count(x) for x in lst]\n    probs = [c / n_lst for c in counts]\n\n    # Compute entropy\n    entropy = 0.0\n    for prob in probs:\n        entropy -= prob * log2(prob)\n\n    return entropy","return_type":"float","function_name":"entropy_bits","stripped_code":"def entropy_bits(\n        lst: Union[\n            List[Union[int, str, float, complex]],\n            Tuple[Union[int, str, float, complex]]\n        ]\n):\n    \"\"\"Calculate the number of entropy bits in a list or tuple of elements.\"\"\"\n    # Based on https://stackoverflow.com/a/45091961\n    if not isinstance(lst, (list, tuple)):\n        raise TypeError('lst must be a list or a tuple')\n\n    for num in lst:\n        if not isinstance(num, (int, str, float, complex)):\n            raise TypeError('lst can only be comprised of int, str, float, '\n                            'complex')\n\n    n_lst = len(lst)\n\n    if n_lst <= 1:\n        return 0.0\n\n    # Some NumPy replacements\n    counts = [lst.count(x) for x in lst]\n    probs = [c / n_lst for c in counts]\n\n    # Compute entropy\n    entropy = 0.0\n    for prob in probs:\n        entropy -= prob * log2(prob)\n\n    return entropy"}
{"code":"def _get_environs(self, prefix: str = None) -> dict:\n        \"\"\"\n        Fetches set environment variables if such exist, via the :func:`~notifiers.utils.helpers.dict_from_environs`\n        Searches for `[PREFIX_NAME]_[PROVIDER_NAME]_[ARGUMENT]` for each of the arguments defined in the schema\n\n        :param prefix: The environ prefix to use. If not supplied, uses the default\n        :return: A dict of arguments and value retrieved from environs\n        \"\"\"\n        if not prefix:\n            log.debug(\"using default environ prefix\")\n            prefix = DEFAULT_ENVIRON_PREFIX\n        return dict_from_environs(prefix, self.name, list(self.arguments.keys()))","return_type":"dict","function_name":"SchemaResource._get_environs","stripped_code":"def _get_environs(self, prefix: str = None):\n        \"\"\"\n        Fetches set environment variables if such exist, via the :func:`~notifiers.utils.helpers.dict_from_environs`\n        Searches for `[PREFIX_NAME]_[PROVIDER_NAME]_[ARGUMENT]` for each of the arguments defined in the schema\n\n        :param prefix: The environ prefix to use. If not supplied, uses the default\n        :return: A dict of arguments and value retrieved from environs\n        \"\"\"\n        if not prefix:\n            log.debug(\"using default environ prefix\")\n            prefix = DEFAULT_ENVIRON_PREFIX\n        return dict_from_environs(prefix, self.name, list(self.arguments.keys()))"}
{"code":"def state_dict(self) -> dict:\n        \"\"\" Calculate hidden state dictionary \"\"\"\n        hidden_state = {}\n\n        if self.optimizer is not None:\n            hidden_state['optimizer'] = self.optimizer.state_dict()\n\n        for callback in self.callbacks:\n            callback.write_state_dict(self.training_info, hidden_state)\n\n        return hidden_state","return_type":"dict","function_name":"EpochInfo.state_dict","stripped_code":"def state_dict(self):\n        \"\"\" Calculate hidden state dictionary \"\"\"\n        hidden_state = {}\n\n        if self.optimizer is not None:\n            hidden_state['optimizer'] = self.optimizer.state_dict()\n\n        for callback in self.callbacks:\n            callback.write_state_dict(self.training_info, hidden_state)\n\n        return hidden_state"}
{"code":"def is_schema_of_common_names(schema: GraphQLSchema) -> bool:\n    \"\"\"Check whether this schema uses the common naming convention.\n\n    GraphQL schema define root types for each type of operation. These types are the\n    same as any other type and can be named in any manner, however there is a common\n    naming convention:\n\n    schema {\n      query: Query\n      mutation: Mutation\n    }\n\n    When using this naming convention, the schema description can be omitted.\n    \"\"\"\n    query_type = schema.query_type\n    if query_type and query_type.name != \"Query\":\n        return False\n\n    mutation_type = schema.mutation_type\n    if mutation_type and mutation_type.name != \"Mutation\":\n        return False\n\n    subscription_type = schema.subscription_type\n    if subscription_type and subscription_type.name != \"Subscription\":\n        return False\n\n    return True","return_type":"bool","function_name":"is_schema_of_common_names","stripped_code":"def is_schema_of_common_names(schema: GraphQLSchema):\n    \"\"\"Check whether this schema uses the common naming convention.\n\n    GraphQL schema define root types for each type of operation. These types are the\n    same as any other type and can be named in any manner, however there is a common\n    naming convention:\n\n    schema {\n      query: Query\n      mutation: Mutation\n    }\n\n    When using this naming convention, the schema description can be omitted.\n    \"\"\"\n    query_type = schema.query_type\n    if query_type and query_type.name != \"Query\":\n        return False\n\n    mutation_type = schema.mutation_type\n    if mutation_type and mutation_type.name != \"Mutation\":\n        return False\n\n    subscription_type = schema.subscription_type\n    if subscription_type and subscription_type.name != \"Subscription\":\n        return False\n\n    return True"}
{"code":"def has_running_jobs(self) -> bool:\n        \"\"\"\"Return a boolean indicating if the experiment has any running jobs\"\"\"\n        return self.jobs.exclude(status__status__in=ExperimentLifeCycle.DONE_STATUS).exists()","return_type":"bool","function_name":"Experiment.has_running_jobs","stripped_code":"def has_running_jobs(self):\n        \"\"\"\"Return a boolean indicating if the experiment has any running jobs\"\"\"\n        return self.jobs.exclude(status__status__in=ExperimentLifeCycle.DONE_STATUS).exists()"}
{"code":"def async_event_handler(self, event: dict) -> None:\n        \"\"\"Receive event from websocket and identifies where the event belong.\n\n        {\n            \"t\": \"event\",\n            \"e\": \"changed\",\n            \"r\": \"sensors\",\n            \"id\": \"12\",\n            \"state\": { \"buttonevent\": 2002 }\n        }\n        \"\"\"\n        if event['e'] == 'added':\n\n            if event['r'] == 'lights' and event['id'] not in self.lights:\n                device_type = 'light'\n                device = self.lights[event['id']] = DeconzLight(\n                    event['id'], event['light'], self.async_put_state)\n\n            elif event['r'] == 'sensors' and event['id'] not in self.sensors:\n                if supported_sensor(event['sensor']):\n                    device_type = 'sensor'\n                    device = self.sensors[event['id']] = create_sensor(\n                        event['id'], event['sensor'], self.async_put_state)\n                else:\n                    _LOGGER.warning('Unsupported sensor %s', event)\n                    return\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n                return\n\n            if self.async_add_device_callback:\n                self.async_add_device_callback(device_type, device)\n\n        elif event['e'] == 'changed':\n\n            if event['r'] == 'groups' and event['id'] in self.groups:\n                self.groups[event['id']].async_update(event)\n\n            elif event['r'] == 'lights' and event['id'] in self.lights:\n                self.lights[event['id']].async_update(event)\n                self.update_group_color([event['id']])\n\n            elif event['r'] == 'sensors' and event['id'] in self.sensors:\n                self.sensors[event['id']].async_update(event)\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n\n        elif event['e'] == 'deleted':\n            _LOGGER.debug('Removed event %s', event)\n\n        else:\n            _LOGGER.debug('Unsupported event %s', event)","return_type":"None","function_name":"DeconzSession.async_event_handler","stripped_code":"def async_event_handler(self, event: dict):\n        \"\"\"Receive event from websocket and identifies where the event belong.\n\n        {\n            \"t\": \"event\",\n            \"e\": \"changed\",\n            \"r\": \"sensors\",\n            \"id\": \"12\",\n            \"state\": { \"buttonevent\": 2002 }\n        }\n        \"\"\"\n        if event['e'] == 'added':\n\n            if event['r'] == 'lights' and event['id'] not in self.lights:\n                device_type = 'light'\n                device = self.lights[event['id']] = DeconzLight(\n                    event['id'], event['light'], self.async_put_state)\n\n            elif event['r'] == 'sensors' and event['id'] not in self.sensors:\n                if supported_sensor(event['sensor']):\n                    device_type = 'sensor'\n                    device = self.sensors[event['id']] = create_sensor(\n                        event['id'], event['sensor'], self.async_put_state)\n                else:\n                    _LOGGER.warning('Unsupported sensor %s', event)\n                    return\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n                return\n\n            if self.async_add_device_callback:\n                self.async_add_device_callback(device_type, device)\n\n        elif event['e'] == 'changed':\n\n            if event['r'] == 'groups' and event['id'] in self.groups:\n                self.groups[event['id']].async_update(event)\n\n            elif event['r'] == 'lights' and event['id'] in self.lights:\n                self.lights[event['id']].async_update(event)\n                self.update_group_color([event['id']])\n\n            elif event['r'] == 'sensors' and event['id'] in self.sensors:\n                self.sensors[event['id']].async_update(event)\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n\n        elif event['e'] == 'deleted':\n            _LOGGER.debug('Removed event %s', event)\n\n        else:\n            _LOGGER.debug('Unsupported event %s', event)"}
