{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The purpose of this experiment if for an LLM to predict the return type of a function by using next-token prediction (not using the residual stream).",
   "id": "55d748b9c72f929a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "DATA_DIR = \"\"\n",
    "RESULTS_DIR = \"\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "LLM_NAMES = [\n",
    "    \"EleutherAI/pythia-2.8b-deduped-v0\",\n",
    "    \"codellama/CodeLlama-7b-Python-hf\",\n",
    "    \"codellama/CodeLlama-13b-Python-hf\",\n",
    "    \"bigcode/starcoder2-15b\",\n",
    "    \"Qwen/Qwen2.5-Coder-32B\",\n",
    "    \"deepseek-ai/deepseek-coder-33b-instruct\",\n",
    "]\n",
    "\n",
    "print(f\"Using device: {device} ({torch.cuda.get_device_name(device)})\")"
   ],
   "id": "892b92afaccd3d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "cab7d9b40219c70a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "vague_dataset_name = \"python_vague_test\"\n",
    "misleading_dataset_name = \"python_misleading_test\"\n",
    "real_dataset_name = \"python_real_test\"\n",
    "\n",
    "full_misleading_df = pd.read_json(f\"{DATA_DIR}/{misleading_dataset_name}.jsonl\", lines=True)\n",
    "full_real_df = pd.read_json(f\"{DATA_DIR}/{real_dataset_name}.jsonl\", lines=True)\n",
    "full_vague_df = pd.read_json(f\"{DATA_DIR}/{vague_dataset_name}.jsonl\", lines=True)"
   ],
   "id": "a588d14f53e88276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "full_misleading_df",
   "id": "a572860e9c06442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "return_type_counts = full_misleading_df['return_type'].value_counts()\n",
    "# filter out return types with 3 or less samples\n",
    "return_type_options = return_type_counts[return_type_counts > 3].index.tolist()\n",
    "# Strip ' and \" from beginning and end of return types\n",
    "return_type_options = [return_type.strip('\\'\"') for return_type in return_type_options]\n",
    "\n",
    "misleading_filtered_df = full_misleading_df[full_misleading_df['return_type'].isin(return_type_options)]\n",
    "\n",
    "real_return_type_counts = full_real_df['return_type'].value_counts()\n",
    "# filter out return types with 3 or less samples\n",
    "real_return_type_options = real_return_type_counts[real_return_type_counts > 3].index.tolist()\n",
    "real_filtered_df = full_real_df[full_real_df['return_type'].isin(real_return_type_options)]\n",
    "\n",
    "vague_return_type_counts = full_vague_df['return_type'].value_counts()\n",
    "# filter out return types with 3 or less samples\n",
    "vague_return_type_options = vague_return_type_counts[vague_return_type_counts > 3].index.tolist()\n",
    "vague_filtered_df = full_vague_df[full_vague_df['return_type'].isin(vague_return_type_options)]"
   ],
   "id": "b81988ed63af3b99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model",
   "id": "8f7654c222923199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_llm(llm_name: str) -> (AutoModelForCausalLM, AutoTokenizer):\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "    return llm, tokenizer"
   ],
   "id": "9f9ff104b1f428ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict Return Type",
   "id": "2febcb90b33c9574"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def predict_return_type(code: str, llm: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get everything before the first colon\n",
    "    function_signature = re.search(r'(def .*?\\(.*?\\)):', code, re.DOTALL).group(1)\n",
    "    input = f\"The function without hinted return type:```python\\n{code}\\n```\\n\\nSame function with hinted return type: ```python\\n{function_signature} -> \"\n",
    "    token_ids = tokenizer.encode(input, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    # Get allowed tokens from tokenizer (colon is also allowed)\n",
    "    return_type_options_token_ids = [tokenizer.encode(type_token_ids, return_tensors=\"pt\", add_special_tokens=False)[0] for type_token_ids in return_type_options]\n",
    "    colon_tokens_ids = [tokenizer.encode(colon_token, return_tensors=\"pt\", add_special_tokens=False)[0] for colon_token in [\":\", \":\\n\", \": \"]]\n",
    "    colon_tokens_ids = [colon_tokens_id[0] for colon_tokens_id in colon_tokens_ids if colon_tokens_id.shape[0] == 1]\n",
    "    predicted_type_token_ids = []\n",
    "    while True:\n",
    "        torch.cuda.empty_cache()\n",
    "        # Restrict allowed tokens to those that are valid return types (i.e. the first predicted token must be the first token of a valid return type, the second predicted token must be the second token of a valid return type, etc.), and 'colon'\n",
    "        allowed_token_ids_list = []\n",
    "        for return_type_option_token_ids in return_type_options_token_ids:\n",
    "            # If we have completely predicted this return type, we can predict a colon\n",
    "            if return_type_option_token_ids.equal(torch.Tensor(predicted_type_token_ids)):\n",
    "                allowed_token_ids_list.extend(colon_tokens_ids)\n",
    "            # If the currently predicted tokens is the beginning of a return type, we can continue with that return type.\n",
    "            if len(predicted_type_token_ids) < len(return_type_option_token_ids) and return_type_option_token_ids[:len(predicted_type_token_ids)].equal(torch.Tensor(predicted_type_token_ids)):\n",
    "                allowed_token_ids_list.append(return_type_option_token_ids[len(predicted_type_token_ids)])\n",
    "        allowed_token_ids = torch.stack(allowed_token_ids_list)\n",
    "        logits = llm.forward(token_ids).logits\n",
    "        unbatched_next_token_logits = logits[0, -1]\n",
    "        # Zero all tokens that are not in the allowed tokens\n",
    "        mask = torch.ones_like(unbatched_next_token_logits, dtype=torch.bool, device=logits.device)\n",
    "        mask[allowed_token_ids] = False\n",
    "        unbatched_next_token_logits[mask] = float('-inf')\n",
    "        predicted_next_token_id = unbatched_next_token_logits.argmax(-1)\n",
    "        predicted_type_token_ids.append(predicted_next_token_id)\n",
    "        token_ids = torch.cat([token_ids, predicted_next_token_id.reshape([1, 1])], dim=1)\n",
    "        predicted_return_type = tokenizer.decode(predicted_type_token_ids)\n",
    "        if \":\" in predicted_return_type:\n",
    "            break\n",
    "        if len(predicted_type_token_ids) > 10:\n",
    "            raise Exception(\"Limit reached\")\n",
    "    # Remove colon and everything after it\n",
    "    predicted_return_type = predicted_return_type.split(\":\")[0].strip()\n",
    "    torch.cuda.empty_cache()\n",
    "    return predicted_return_type\n"
   ],
   "id": "6b20b1235a109b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Accuracy",
   "id": "e9968afcab8737b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "def get_accuracy(df: pd.DataFrame, llm: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> (int, int):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index, row in tqdm.tqdm(df.iterrows()):\n",
    "        stripped_code = row['stripped_code']\n",
    "        try:\n",
    "            predicted_return_type = predict_return_type(stripped_code, llm, tokenizer)\n",
    "            total += 1\n",
    "            if predicted_return_type not in return_type_options:\n",
    "                raise Exception(f\"Predicted return type: '{predicted_return_type}' is not an option\")\n",
    "            if predicted_return_type == row['return_type']:\n",
    "                correct += 1\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    return correct, total"
   ],
   "id": "dc40f07552d7ffa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_accuracy_for_dataset(llm_name: str, df: pd.DataFrame, dataset_name: str):\n",
    "    results_filepath = os.path.join(RESULTS_DIR, f\"{llm_name.replace('/', '__')}_{dataset_name}_accuracy.txt\")\n",
    "    if os.path.exists(results_filepath):\n",
    "        print(f\"Results already exist for {llm_name} on {dataset_name}\")\n",
    "        return\n",
    "    llm, tokenizer = load_llm(llm_name)\n",
    "    correct, total = get_accuracy(df, llm, tokenizer)\n",
    "    with open(results_filepath, \"w\") as f:\n",
    "        f.write(f\"{correct}/{total}\")\n",
    "    print(f\"Accuracy for {llm_name}: {correct}/{total}; Saved to {results_filepath}_accuracy.txt\")\n",
    "\n",
    "for llm_name in LLM_NAMES:\n",
    "    print(f\"=== Calculating accuracy for {llm_name}===\")\n",
    "    print(\"Real dataset\")\n",
    "    save_accuracy_for_dataset(llm_name, real_filtered_df, real_dataset_name)\n",
    "    print(\"Misleading dataset\")\n",
    "    save_accuracy_for_dataset(llm_name, misleading_filtered_df, misleading_dataset_name)\n",
    "    print(\"Vague dataset\")\n",
    "    save_accuracy_for_dataset(llm_name, vague_filtered_df, vague_dataset_name)"
   ],
   "id": "15744f70c5e3098d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
